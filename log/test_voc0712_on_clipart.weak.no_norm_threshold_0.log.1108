INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/baselines/e2e_faster_rcnn_R-101-FPN_1x-VOC_use_weak_test.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 60000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 20000, 40000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': True},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 20000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 60000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 20000, 40000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': True},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 20000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.951s + 0.037s (eta: 0:02:02)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.415s + 0.065s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.377s + 0.063s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.366s + 0.059s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.383s + 0.055s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.372s + 0.054s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.371s + 0.056s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.373s + 0.056s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.372s + 0.055s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.373s + 0.054s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.371s + 0.053s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.376s + 0.054s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.379s + 0.053s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 60000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 20000, 40000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': True},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 20000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.958s + 0.032s (eta: 0:02:02)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.437s + 0.048s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.408s + 0.050s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.394s + 0.055s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.387s + 0.051s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.403s + 0.052s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.434s + 0.052s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.421s + 0.054s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.412s + 0.052s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.413s + 0.052s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.407s + 0.052s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.406s + 0.052s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.396s + 0.051s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 60000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 20000, 40000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': True},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 20000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.967s + 0.034s (eta: 0:02:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.410s + 0.046s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.399s + 0.050s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.408s + 0.048s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.394s + 0.047s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.388s + 0.048s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.380s + 0.048s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.380s + 0.048s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.378s + 0.047s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.374s + 0.047s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.376s + 0.048s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.376s + 0.049s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.378s + 0.049s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 60000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 20000, 40000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': True},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 20000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 1.055s + 0.036s (eta: 0:02:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.421s + 0.051s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.401s + 0.051s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.385s + 0.048s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.380s + 0.047s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.371s + 0.048s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.370s + 0.050s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.366s + 0.049s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.365s + 0.049s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.366s + 0.049s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.363s + 0.049s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.361s + 0.048s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.363s + 0.048s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 72.199s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1772 2858 1073 ... 1219  898 1558]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0139
INFO voc_eval.py: 171: [ 357  384  594 ...  998 1006 1751]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.2642
INFO voc_eval.py: 171: [2972 2024 3013 ...  756 2477    0]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0510
INFO voc_eval.py: 171: [ 942 2157  187 ... 1636 2219 1317]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0494
INFO voc_eval.py: 171: [2735  597 2767 ... 2051 1621 1446]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0200
INFO voc_eval.py: 171: [2235  868  106 ... 1542  332  637]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.3868
INFO voc_eval.py: 171: [ 737 1711   86 ... 1932 1918  592]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1649
INFO voc_eval.py: 171: [242 339 453 ... 821 881 663]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0293
INFO voc_eval.py: 171: [ 598 2457 3146 ... 3415  357 1096]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1308
INFO voc_eval.py: 171: [486 678 530 ... 960 812 249]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0122
INFO voc_eval.py: 171: [1680 1651  984 ... 4066 2694 4476]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.1702
INFO voc_eval.py: 171: [ 373 1431  693 ... 1385 1436 1875]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0989
INFO voc_eval.py: 171: [445 775 127 ... 147 544 500]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.1850
INFO voc_eval.py: 171: [1973  258  367 ...  873 1566  607]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4805
INFO voc_eval.py: 171: [1968   26 3666 ...  346 1061 2325]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.1352
INFO voc_eval.py: 171: [ 985  805 2192 ... 2024 1949 1579]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.1909
INFO voc_eval.py: 171: [1153  411 1545 ...  328 1061    0]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0242
INFO voc_eval.py: 171: [1147  482  858 ...  333 1058  709]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0084
INFO voc_eval.py: 171: [2170 1628 1018 ...  270  838  714]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.1844
INFO voc_eval.py: 171: [ 767 1861 2374 ...  256 1660 1823]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1277
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1364
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.014
INFO cross_voc_dataset_evaluator.py: 134: 0.264
INFO cross_voc_dataset_evaluator.py: 134: 0.051
INFO cross_voc_dataset_evaluator.py: 134: 0.049
INFO cross_voc_dataset_evaluator.py: 134: 0.020
INFO cross_voc_dataset_evaluator.py: 134: 0.387
INFO cross_voc_dataset_evaluator.py: 134: 0.165
INFO cross_voc_dataset_evaluator.py: 134: 0.029
INFO cross_voc_dataset_evaluator.py: 134: 0.131
INFO cross_voc_dataset_evaluator.py: 134: 0.012
INFO cross_voc_dataset_evaluator.py: 134: 0.170
INFO cross_voc_dataset_evaluator.py: 134: 0.099
INFO cross_voc_dataset_evaluator.py: 134: 0.185
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 134: 0.135
INFO cross_voc_dataset_evaluator.py: 134: 0.191
INFO cross_voc_dataset_evaluator.py: 134: 0.024
INFO cross_voc_dataset_evaluator.py: 134: 0.008
INFO cross_voc_dataset_evaluator.py: 134: 0.184
INFO cross_voc_dataset_evaluator.py: 134: 0.128
INFO cross_voc_dataset_evaluator.py: 135: 0.136
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
