Start testing on iteration 499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.619s + 0.037s (eta: 0:01:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.364s + 0.044s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.364s + 0.041s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.358s + 0.040s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.360s + 0.040s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.356s + 0.041s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.354s + 0.041s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.355s + 0.040s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.353s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.354s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.355s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.358s + 0.039s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.359s + 0.039s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.369s + 0.026s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.345s + 0.040s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.360s + 0.041s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.377s + 0.042s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.371s + 0.042s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.374s + 0.042s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.386s + 0.042s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.383s + 0.041s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.380s + 0.041s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.380s + 0.040s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.376s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.378s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.372s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.476s + 0.032s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.367s + 0.038s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.362s + 0.042s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.368s + 0.041s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.367s + 0.041s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.359s + 0.040s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.357s + 0.041s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.358s + 0.041s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.356s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.358s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.357s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.356s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.357s + 0.039s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.453s + 0.040s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.370s + 0.040s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.359s + 0.037s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.348s + 0.037s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.345s + 0.037s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.348s + 0.039s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.351s + 0.040s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.350s + 0.040s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.352s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.351s + 0.039s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.349s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.349s + 0.039s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.347s + 0.039s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.312s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 322 1366  319 ... 1147 1144 1145]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2591
INFO voc_eval.py: 171: [131  72 586 133 290 137 523  71 432 341 623  74 547  75 638 639 303 553
 320 141 650 436 132 635 644 527 645 641 625  76 590  73 587 433 634 151
 139 135 543  86 605 660 296 528 604 343 438  82 448 357 347 144 616 629
  90  78 592  98 352 636 532 596 286 504 142 138 473 150 643 435 537 526
 622 443 534 617 658  48 292  81 318 365 102 342 329 620 621 322 396 488
 140 615 143 647 447 536 253 470 453 656 345 480 289 538 499 350 330 579
 294 535 287 434  92 321 530 551 176 642 659 348 588 295 291 653 458  28
  85  35 355 392 466 457 455 177 503  51 379 589 474 383 232 619  32 314
 493 558 654  29 648 224 501 479 481  38 533 519 397 134 360 439 482 451
 618  89 148 359 485 461 627 661  79 663 388 154 319 197 539 216 406 211
 323 593 153 288 463 655 233 508 591  54  10 103 362  99  45 228 299   7
 606 257 525 597 456 520  56 594 219 607 300 513 100 595 126   6 364 613
 489 201 327 600 462 395 344 101 222 374 452 652 496 444 136 309 512 354
 166 646 581  44  40 465 328 632 494 215 651 662 657 445 183  93 230  91
  47  24 476 562  88 524 351 582 487 264 297 349 204 393 361 384 125 372
 332 145  30 603 196 460  27 124 308 212  77  67 609 626 556 316 637 306
 666 226 199  61 495 252 601 665 505  65 311  13 649  31 304 640 277  84
 611 208 522  80 624 421 541  26 210  66 472 146 631 602 337 410 174 370
 315   2 385   1 529 464 612 225 298 630 403 468 490 149  58   5 152 220
  11 188 510 509 437 563 442 544 147 193 492 572 213 190 515 573 164 128
 614 165 155 373 550 518 575 540 217 449 446 559 106 111 192 491  60 500
 467 238 338 331 511 206 186  49 628   4 377 506 531 502 486 227  42 307
 390 112 115 478 189 560 209 399 178 521 371 310  68 548 413 412  87 475
 185 251 191 441 507 333  21 246 239 557  55 278 375 358  25 414 542 633
 517 107 242  12 116 214 356 218 484 194  34 514  39 425 207   3 248 516
  33  64 599   9 312 120 121 162 346 156 168 427  15 276 235 576  46 405
 440 236 265 159 336  62  36  19 275 394 339 179 205 408 244 263 241 407
 610 574  41 430 608 598  95 202 180 169 187 182 167 110  59 546 366  22
 313  20 157 129 431  37 459 419 203  23 367 160 417 398 173 411 353 305
  57  69 404 400 221 231 418 240 552 284 118 229 181 237 429 109 498 483
 477 334 335  96 469 409  17 454 280 301 114 369 561 382 130 428  63 402
 108 283 184 422 113  53 580 171 584  83 200  14 545 104 368  52 555 250
 105 363 198  50 564 223   8 471 161 420 245 195 415 259 583 258 172 391
 450 158 549  94 279 378 272 340 261  18 585 234 262 293  70 117 317 254
 416 387 554 302 401  97 571  16 423 386 376 274 389 170 565 247 497 119
 243 267 175   0 269 577 285 578 273 255 256 122 566 123 260 426 380 249
 270 424 664 127  43 271 281 381 570 324 163 568 282 325 326 567 569 266
 268]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.4795
INFO voc_eval.py: 171: [ 616 1625 3050 ... 3042 3040 3043]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1952
INFO voc_eval.py: 171: [ 622  684  736 ... 2236 2246 2239]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2471
INFO voc_eval.py: 171: [407 450 619 354  11  20 713 326 399 449 406 711 455 717 357 681 658 376
 451 596 669 415  22 325 646 528 367 901 718 366 170  21 900 736 798 633
 623 417 729 360 666 356 358 620 597 400  13 716  17 374 720  15 740 727
 371 408 663 730 915  18 801 453 377 355 712 737 789 902 599 517 330 675
 410 454  81  14 741 295 544 332 327 800 797 565 310 329 370 414  47 598
 762 626 715 815  79 372 749 403 636 530 531 253 413 331 200  25 409 752
 233 632 866 739 375 638 339  24 418 140 333 647 654 719 773 650 208 520
 344 158 539 523 543 625 688 665 656 604 629 205 467 290 742 441 248 345
  48 744 228  46 547 721 574  33 673 549 519 642  49 238 867 337 684 606
 192 603  12 607 305 368 237 882 365 682 621 496 643 297 284 651 349  54
  16 568 672 246 177 622 645 452 864 135 689 685 734 754 634 816 359 183
 194 803 759 917  80 735 750 630 911 738 229 252 835  67 369 725 471 343
 624 362 758 920 775 648 600 751 908 865 551 444 746 870 776 631 807 328
 363 176 877 429 723 649 309  92  35 373 753 602 652 311 548 483 198 179
 674 804 141 733 661 401 664 761 635 659 785 495 601 120 171 747 464 247
 709 907 805 868 411 814 861 640 777 641 443 465 724 728 340 308 231  26
 660  91 628  74 916 637 605 215  93 732 757 714 806 743 361 745 863  52
 277 162 532 138 546 772 121 529 115 114 497 542  65 272 195 159 419 271
 813 347 303 174 285 567 137 671 209 334 582 524 823  50 416 152 817 756
 627 896 573  19 442 412 319 655 670 350 921 307 281 302 249 160 579 235
 903 161 767 312 404 644 926 156 139 687 639 259 440 338 862 897  23 748
  90 676 145 499 119 364 572 760 211 786 351 898 133 550 279 653  62  60
 570 657 662 498 808 846 667 234 306 581 288 540 251 232 763 335 230 856
 577  36 509  28 126 825 726 118 731 199 668 227 304 460 691 314 136 873
 421 113 132 134  43 839 280 378 268 466 545 778 239 827 155 432  57 912
 899 101 779 774 226 836 278 755 904 768 722 341 879 541 884 788   4 790
 213 787 533 569 207 289 566 929  44 535 802 236 402 831 117 462 191 336
 352 878  45 346 855 491 838 342 181 291 405 380 100 116 214  61 203 883
 153 829 389 292 610 463 300 318 224 578 430 206 575 216 522 895 167 267
 913 875 296 576 123 500 446 468  73 276 826 193 918 819 218 521 283 810
  99 217 260 269 212 588 143 828 256   2 131 242 690 261 420 445 580 201
 294  51 301 190 910 592 154 792 555 534 225 298 202 692 833 484 204 175
 142 824 830 516 571 461 927 287 906 173 919 479 518 834  32 316  85 243
  53 299 384 793 832 315  66 791 783 515 189 840 781 478 448 510   5 150
 104 107 398 554   1 108 282 794 710 105 812 881 187 858  69  27 923  29
  31 151 469 611 511 157 683 472 928 590 147 124 837 799 885 811  30 184
 122 470 103  58 317 182 196 293 780  38 818 809 275 188 909 795 930  63
 387 869  83 172 254 526 706 698 186 112 286 513  98 880 106 591 594 348
 109 782 433 392 144 678 250 178 589 129 820 244 222 210 848 538 613 221
 860 556 614 871 388 536 617  84  88 180 583 874 527 425 508 512 537 255
 615 677 876 492 679 168   0 127 905 494 764   3 166 164 130 922 525  87
 477 680 393  55 169 505 197 490 485 391 595 618 148 612   7  68  76 379
 859 185  70 924 265 313  56 857 390 847 257 686 562 322 593 616 125 487
 557 397 770 697 240 872 383 584 321  89 435 422 609 851 514 427 223 493
 914  40 708 111 766 262  34 821  41 585 473 765 110  42  64 438 436 424
 608 696  39  37 394 506 128 784 480 925 395 258 102 273 586 488  72 587
  86  59 323 431 796 559 489 264 353 704 507 771 423  82 263 382 320 241
 552 475 324 553 426  78 270 486 699 844 845 842 695 843 163  94   8   9
  75  71 396 385 564 274 889 474   6 434 439 146 769  97 504 447  96 703
  10 437 822  95 266 459 476 563 854 481 849 705 386 558 456 693  77 841
 890 888 149 458 891 852 561 220 853 700 886 165 560 894 428 893 503 245
 707 701 457 381 850 482 892 694 702 887 219 501 502]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3505
INFO voc_eval.py: 171: [170 481  84  86 171  67  64 483 482  85 500  90  89 435 111  65  27 102
 177  68 439  88 397 134  80 451 445  69  95 204 164 279  43 252  79 421
 110 396 429  73  75  30 389 305 206 425  96 281 335 133 507 272 140 212
 414 325 467  66 228 271 480 237 465 413  72 400 109 379 359 195 186 106
 522 282 274  74 139 356  76 236  56 354 141 422  92 158 257 486 209 263
 434 166 340 104  42 296 377 291 361 256 262 115 150 395 196 426 202  71
 457 337  50 411 362  70  78 424  77 455 386 268 464 375 295 390 278 488
 409 320 198  41 497 443 294 132 208 479  44  87   9 149 114 165 459 238
 266 336 391 135 194 226 520 350   5 501  11 253 197 280 259 328 273 207
 388 224  51 308 476 408 360 410   6  93 382 333 475 190 105 122 118 129
 366  28  32 351 137 519 447  45 184 423 387 108 113 112 276 119 289 440
 222 518 306 517 221  97 453 230   1 168  31 260 353 428  40 403 174 107
 385 378 116 155 329 502 341 120 299 358 203 384 154 431 326 433  37 265
  34 470 338 217 261 117 444 407  55 191 438  12 508 254 471 437   4 376
 199 417 441 288 225 446 142 293 452 128 454 521  81 127 100 401 215 297
 201 307 352 160 270  39 234 200 125 285 458 339 330  91 229 101 365  53
 427 147 369 348 506 513 169 381 145 468 267 349 420  47 136 138   7  29
 357 430 450 383  98 287 355 232 205 478 103  52 144 153 143 432 121  94
 380   8 323 456 258   3 406 245 461 156 416 436 269 477 157  20 448 173
 442 331 175 146 255 183 503 301  38 415 487 243  33 462 298 466 214 345
 148 473 474 363 469 192 286 304 402 399 218 511 460 332 463 264 510 124
 211 499 161 412 167 327 284 392 290 123  35  48 394   2   0 152 318 370
 126 248 449 334 324  99 216 300 239 311 283 180  82 223 235 231 240 178
 302 275  54  49 512 472 393 162 364 303 405  36 241 159 187 516 246 292
 418 398 220  13 322 419 251 233 342 312  61 247 163 367  62  83  24 244
 344 176 227 504 505 277 316 321  26 509 319 346 404 242 250 343 213 249
 314  58 347 210  57 492 485 514 151 130 182  16 313 372 179 489  17 193
 373 515 495  60  46 172 188 498  18 309 131 494 310 317 315 219 493  25
 368 496  59 181 371 185  23 490 484  14 189  15  21  63 491  10  19  22
 374]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4311
INFO voc_eval.py: 171: [1552  710 1489 ... 1682 1677 1680]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.3099
INFO voc_eval.py: 171: [ 30  64 260 261  65 106  22 151 314  66  25 285 259 313 283 115 113 108
 152 123 253  68 293 230  28 196 130 256 287 134  83  48  99 139 248  49
 211 235  18 288  47  75 107 119 233 174 306  74  23 150 231  42  91 112
 210 163  27 295  93 290  80  70 167 102 215 238 185 122  63 169 200 224
 186  77 188 168 216  19 217  51 291  71 155 103 162 132  20  53 234 180
 294 249 198 312 116 232  89  21  24 308 187 109 279 254  79  10  52  26
 156 114 135 131 289 154 104 243 307 201 286  29 118 129 140 222  81  82
   6 310  50 153  72 157 124 121  86 126 125 292 142 133  69 237 141 236
 165 202 219 251 269 120 189  92 255 194 220 137 148 149  46 284 190 316
 258 110 264 221   0 246  88  85 159 280 192  13 282  44 301 300 213 191
 223  84 309  39 176 214  67 270 250 278  41 311  11 111   1 228 203 204
  45  14 177 239 277   2   4 218 101 105 128  31 195 302 193   5 100 117
 262 161 281 197 136 227  97 245  94  36 273  43 199  87  40 164 179  98
 205 127  34 244 257  90 229  37  58 274 206  17 268 207 171 242 166 241
 170 272  33 267 209  95 240   9  38   7   8 276  54  12   3 297  96 298
 271 299 225 208 212 158 263  59 183  32 178 173  15  56 184 143 147  55
  61  73  16 275 303  78 175  76 247 265 266 181 226 160  62  60 146 145
  57 305 172 252 144  35 182 138 304 315 296]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0685
INFO voc_eval.py: 171: [6039 2580  713 ... 4180 4514 4511]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3279
INFO voc_eval.py: 171: [288 611 386 235 186 255  38 243 190 392 340 232 130 663  37 639 391 248
 231 552 209 563  59 535  58 241 328 365 674  50 670 236  48 662 125 184
  61 366  54  51 192 289 534 393 251 170 678 398  21 336  52 188 389 618
 323 185 252 654 545 397 187 331  56 494 291 638 300 142 253 551 118 550
 474 564 279 470 537 384 166 246 395 668  47 557 216 388 224 355 238 495
 673 294 159  41 234  81 240 305 613 213 635 665 242 544 339 364 302 496
 239 223 669 247 137 566 620 616 244 210 536 396 373 644 258 283 146 500
 677 549 200 372 648  33 466 650 667 492 317   4  13 382  64 672 265 473
 341 140 385 191 553  73 298 143 643 464 457 337 346 313 387 222 260 193
 296 675 104 539 354 507 171  18 308 475 504  42 161 314  46 163 548 647
 342 122 558 565 250 129 206 454 194 653 501 183 378 664 233 198 203 554
 390 134 652 581 119 116 576 173 463 438 127 165   5 290 168 257 123 162
 367 412 626 292 169 609 261 132 641 631 543 259 245 497 107 471  10  80
 189 676 318 625  19 256 634 655 406 369 221 368  17 117 172 264 614 447
 262  36 399 237 541 592 199 121 627 651  94 133 621 646 580 338 493   7
  75 100 174 642 448 371  77 383  71 547 120 636 689 307 335 623 249 640
 460 164 666 597 679 208 568  34 167 284 486 467 254  57 598 332 212 103
 370 377 588 502 230  78  68 472 348 659 263 584 649 585 622 353   0 343
 506 542 573 578 128   6 671  95 394  55 690 179 476 310 637 345  63 629
 645 427 347  44 281 160 526 131 295 293 612 458 661 505  35 320 561 145
 329 503 150 594 379 681 469 225 278 693 599 297 656 419 657 446 499 351
 309 531 660 136 101 577  53  79 608 105 141 439 349 157 694  99 418 201
  49 102 430 352 605 571 440  84 285  30 632 546 400 182 196 477 319 445
 218 498 478 596 358 197 304 425 591 286  20 468 540 600 587 658 456 465
 692 220  83 344 135 422 153 511 512 211 356  43 115  96 417 350 582 357
 287 431 619 202 416  82 217 126 432 532 282 267 316 315  97 603  72 610
 572 204   1  40 333  66 615 579 514 219 154  24  98 280 205  93 606 214
 682 138 124 215  32 691   8  25 420  22 530 195 227 181  70 444 559 114
 327 325 529  23  85 321  28 108 176 617 528 680 334 513 479 407 538  12
 683 152 111  86 228 428 569 277 229 555 520 487 560 570 147 177 525 436
 421 324 424 523 207 226  62 449 593 322 624 275 574 144  67 527 595 462
 628 151 455 491  16 175  14 330  74 423  76 459  39  69  65 567 158  29
  60 483 306 112  11  45  26 604 461 271  92 426 590 148 376  31 522  15
 149   9 453 178 575 589 110 533 521 480 360  27 481 113 490   3 433 415
 374 556 274 408 155 562 586 401 437 272 583 482 485 429 524 375 109 434
 442 156 405 450   2 359 106 409 435 273 180 630 488 489 508 381 410 361
 380 311 299 414 363 441 484 402 268 270 687 601 607 303 403 451 266 404
 362 326 443 269 686 519 312 301 602 411 276 510 633 684 518 452 517 413
 688 685  88 139 515  89 516 509  90  87  91]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1474
INFO voc_eval.py: 171: [3025 1322 1375 ... 3253 2282  417]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3425
INFO voc_eval.py: 171: [385 487 387 920 458 450 416 542 491 515  57 490  73 503 390 154 484 597
 168 217 854 682  58 593  53 156 160  50 166 384 275 922 163 801 544 872
 916 864  52 873 144 926  72 395 887 414 222 853 852 393 165  49 173 505
 681 835 456 929 475  76  56 857 314 910 562 921 550 309 455 269 412 417
 153 449 268 488 649 501 164 898 521  59 481 849 462 798  61 914 157 396
 541 457 159 253 245 303 555 546 234 918 276 575 274  75 634 237 294 705
 251 673 683 652 172 413 212 162 855 739 551 522 397  71 224 838 650 896
 504 260  90 370 641 307 860 509  51 453 917 549 225 878 155 837 474 161
 478 241 394  78 882 141 169 799 203 927 283 333 765 656 638 671 591 278
 473 448 680 519 619  38 812 879 206 506 594  64 598 221 880  24 677 892
 218 859 270 425 781 713 310 729 452 365 233 291 661 902 836 846 328 569
 327 726 554 232 295 592 143 723 881 150 738 895 461 869 936 325 885 386
 247 924 210 238 470 558  60 768 158 691 122 793 451 809 312 420 923 254
 145 360  22 167 479  77  92 687 485 219 262  26 391 590 477 454 304 839
 263 465 618 146 787 911 688 489 706 476  55 240 704 373 242 795 264 928
 877 595 721 421 808 508 185 223  63 147 856 548 719  40 728 876 893 480
 209 845 764 261 663 770 322 653 939 848 344 732 281 733 235 220 695 300
 208 352 734 516 559 243 271 423  74 472 669 128 527  68 660 321 123 466
 648 600 349 207 483 353 324 170 847 183 109 205 643 211  66 299 382 822
 248 171  94 392 460 512 518 148 259 731 513 802 737 389 937 694 818 761
 875 388  17 703 250 279 236 371 422 890  36  27 589 320 313  93 657 909
 277 760 403 744 894 615 374 191 676 658 889 888  69 730 672 930 267 337
 297 424 616 884 788 784 816 252 372 502 906  67 933 142 239 678 686 317
 285 266 824 316 935 292 821 858 468 883 897  18 523 315 338 785 552 202
 736 258 431 786 689 418 823 347 763 904 121 667 654 621 230 464 831 596
 244 339 903 735 769 743 135  62 486 265 861 399 524 272 129 302 674 617
 136 298 891 709 742 940 430  25 543 707 149 750   8 586 901 306 675 899
 851 817 567 601 662 934  21 640 255 342 670   5 229  39 800 400 646  70
  23 228 642 335 366 356 900 603 120  19 651 874 130 249   0 246 789  31
 326 545 498 467   2  65 886 293 308 772 301 599 757 381 105 311 931 204
 469 609 573 574 692 193 820 833 773 712 305 138 415 753 620 566 938  91
 690 752 850 696 329  34 140 762 932 655 331  44 482 557 791 724 323 131
 103 195 341 319 568  28  97 357 496 863  95 862 679 741 664 377 828  96
   3 318 814 282 925 383 941 134 747 539 411 684 182 125 665 792 330 124
  79   6 699 587 177 101 286 602 398 100 273 334 635 622  99 637  13 805
 740 376 530  47 190   9 610 905 778 402 745 336 507 796 607 226  42 819
 815 644 459 442 794  98 188 126   1 332 623 375 803 538 748 340  54 588
 715 514 666 471 767 409 577 192 348 842 790 565 362  33 174 359  84 127
 701 797 408 110 289 181 826 668 806 343 441 614 625 776 346 720 708 868
 758 517 560 919 132  88 358 363 290 756 827  37 563 296 913 151 751 435
 419 529 915 693 137  86 759 280 175 865 133 198 463 749 912 525  29 711
 698 871 139 107 870 582 355 492 624 777  20 197 186 378 578 361 581 354
 429 531 350 511 775 825 494 771 804  32 102  48   7 493 526 287 576 697
 528  81 106 700 499 636  80 410 401 345 659  41 112 427 725 407  10 647
 579 867   4 832 580 176 152 351 532 368 570 632 645 813 584 807 716 500
 216 116 866 717  83  89  85 184 495  45 533 604 364 447 583 404 718 631
 571 633 534 117 830  87 187 639 196 213  46 746  43  82 585 702 834 446
 440 443 779  11 782 766 284 119 710 811 104 714 612 605 810 627 630 405
 444 426 727 754 227 628 611 755 497 113 572 108 114  30 535  35 111 199
 194 231 406 433 189 445  15 434 288 722 200  14 520  16 608 510 214 556
 564 553 829 606 256 367 432 115 685 780 783 561 613 536 201 843 774  12
 626 547 118 257 844 215 178 537 179 437 369 629 180 439 436 428 379 380
 841 438 840 907 540 908]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1949
INFO voc_eval.py: 171: [935 125 849 ... 681 507 682]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3066
INFO voc_eval.py: 171: [ 54 330 183  29  53 191 200 317  73 333  55  81 184 141  78 140 220  60
 211  87  59  35 321  64  47 188 239  61  39  17 343 303 320 294 323  37
 244  42  33  28 253 346  43 241  69 137  67 199  85 306 340 240 163  15
  92 295 207 155 315 322 307 216 325  22  20 222  56 142  62 337 344 186
 278  58  95   7 193 234 112 139 247  93  68  32 198 185 210 318 260  10
 284  49 115 235  79 190 217  83 329 297 202 327 180 267  90 342  27 348
  96 205 201  75 270 233  13 206  40 104 300  65 249 138  18  36 272 246
  31 116 308 279 174  66 131 347  88 196 214  70 103 345  91   1  74 296
  19 152  26  45 312 135 113   4 265 114 219 195 298 101 316 230 194 177
 169   5 310  25  84 182 256 221 120  80 119 304  86  21 285 275 280  46
 218 215 197  44 118 102   0 176 181 293 341 309 187  11 175  76 328 236
 226 157  30 189  77 130 302 208 128 192 271 223 108 165 127  23 106 274
 324 151 305 301 164 105 259   3 237 100 134 231  71 282  16 159  41   9
 287 254 212 173 145 179 129 158 107 262 268 232 332  48  89 245  82 178
 283  52  38 286 126 121  94  57  24 229 243  72 150 331  63   8   6 146
  99 172 266 133 117 124 123 153  34 281 251 242 269 338   2 148 258  12
 171 209 132 252 289 228 136 122 213 125 255  51  50 311 225 238 276 248
 162 224 154 227 277 147 326 250 261  97 257 263 288 299 203 204 264 273
  14 319 334 160 149 143 339 336 291 335 156 161 144 290  98 110 292 314
 111 168 167 109 313 170 166]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5691
INFO voc_eval.py: 171: [ 8303 13543  1017 ...  9593  9597 10523]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5416
INFO voc_eval.py: 171: [2690  772  283 ...   50 1970 2781]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4309
INFO voc_eval.py: 171: [ 38 326  39 541 343 550  67  47  61 141 544  41 339 346 546 340 542  42
 490 320 356 616 567 169 235 266 555 547 412 376 359 355 324  73 503 645
  50 554  81 297 171 284 133 548 637 319 334 620 358 624 417 545 360 148
 341 238 551 424 265 377 132  74 240 247  69 144 621 176 150 269 623 561
 139 330 349 543 338 335 416 375 607 488 170 431 166  76 131 524 374 281
 152 146 267 333 622 348  44 246 149 174 230 496 639 151 619 378 609 470
  79 145 243 625 244 342 626 618 280 450  63 451 336 646 506 565 215 362
 331 415 532 419 229  57 337 632 568  99 293 575 647 323 179  70 640 634
 325 301 666 365 299 172 491 142 361 217 178 354 615 497 242 256 143 147
 167 418 270  51 231 681 667 350 160 138 236 487 232  75 100 570 552 510
 159 189 357 204 173  68 471 134  48 630  46 351 347 329 631 283 327 136
 608 508 627 177 426 268 469 511 237 411 322 414 241 211 158 629 562 610
 328 644 216 642 321 103 233 638  35 344 413 352 332 345 175 295 353 553
 628 380 279 669 208 386 529 641 517 383 108 239 521 576 495 643 594 556
 535 135 157 611 617 137 277  33 566 245 234 200 436 276 636 205 635 668
 185 260 387 282 564  80 492 683 633 516 318 531  78 598  62   9 671 199
 684 613  55 580   6 257 304 180 366 209 569 181 161 273 486 429  59 549
  40 588  72 536 650 182  19 258 493 498 514 364 207 286 210 206 363 128
  45 168  49 678 573  87 183 515 367 140 259 581 505 313 389 214 522 278
 489 513 101 527  30 191 649 272 485  90 558 129  86 557   2 117 448 224
 677 289  65 670 309 106 287 379 225 572 614 578 675 264 285  36 526  71
 104 275 305 472   0 196 193 102 404 579  89 427 648  18 458 317 398 597
 563  29  93 612 312  98 512 674 111 523 274 680  27 467 443 300 483 310
 685 473 509 682 271 468   3 187  28 220 186  77 311  52 422 605 465 519
 227 288 480  31  58  82 428 584 577 164 596 520 197   1 651 194 105 525
 291  20 425 113  21  32  54 560 587 298 368 434  96 195   4 574 123 501
 444 533 302  83 672 314 484 518  84 602 534 445 447 466 296 440  43 294
 673  53 114 446  85  64 528 463 586 676 253 559 115 261  34 410 475   5
  66  60 116 263 124 449 433 393  22 461 421 464 679 437 198 112 438 500
  56  26  97 599 306 292  91 223 482  25 601 507 591 600 315 595  37  23
  12 165   7 571 435  94 107 381 655 462 119 530 392 504 290 316 590 481
 499 494 218 502 255 262 122  88 432 162 382 303  24 130 188   8 478 420
 219 406 120 430 212 657 604 397 369  17 662 582 213 203 121 118 221 585
 154 401 371 653 202 395 126 222 405 589 457 127  10 583 390 476 391 538
 479 439 125  92 477  14  95 370 656 592  15 226 603  16 190 593 663 456
 399 184  11  13 409 408 394 385 652 248 228 407 537 402 403 109 664 388
 308 654 252 153 192 460 474 384 250 201 372 396 251 156 307 660 155 665
 606 400 540 539 254 453 249 455 661 452 659 373 163 454 658 459 423 442
 110 441]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2946
INFO voc_eval.py: 171: [ 34 109   5  35  38 177 306 173 114 258 174   4 140  41 179 215 113 144
 171 168  11 219 138 214  52 235 305 167 259 307 141 254 227 105 175 218
 116 216  39   6 300 256 251 135  93 106 207 170 132 131 156 176  51 257
  68 304 172 169 160 115  49 178 142 299 145 143 111 236 146  74 217 233
  69  62  10  91 201 147  94 129  42  18 296 161 197 253  84 284  40 117
  85 139 262 231 226 301 267 228 308 104  36  90 234 220 294 243 208 232
 249 229  50   8  92  86 119  19  61  37 230 221  23 137 280  95 245  75
 297 286 238 269  78 211 285  77  87  88 110  70  99 101  55 130 223 120
 246 180   9 242  17 191 184 118  25  44 112  66 100 261 133 263  63 213
  53 252 279 291 255 204 107 303 202 290 309  22  24  80 295 108  65 293
 302 239 292 310  54 224 298 124  46 200 102 241 193 203 186 187   7 158
  72  47  73 250 247 163 268  67 288 276 134 222 260 194 209 205 198  98
 206  48  71 153 287 240  43 103 183 282 188 264  20  13 281 212 270 210
 136 122 237 155  45 196 150  76 283   0 271   1 278  64 244 248 275 189
 162  16 277  27  97 192 165   3  21 185 182  96 152 159 125 181 190  28
 265 127  79 151 225 128 154 166  14  26 149  15  32  12 164 157 126  33
 274 195 148 272   2 289 123  31 273 121  60  59 266  29  81  30  57  82
  58  89  56  83 199]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1596
INFO voc_eval.py: 171: [619 370 622  82 119 378  81 377 309 204 621 281 165 125 371 624 121 308
 872 282 623 208  91 207  80 438 494 126  90 180 524 386 441 286 778 776
 105 492 100 206 214 439 849 873 372 390 637 205 646 310 188 316 620 176
 886 804 210 122 714 284  94 426 134 382 181  65 197 120 292 315 475 531
 311  22 446 419 373 865 702 283 632 626 293 684 841 211 379 879 374 164
 790 443 118 285 422 445 135 104 194 376 653 883 881 168 500 423 270 527
 319 509 685  78  95 380 313 816 312 174  36 113  87 442 444 314 227 781
 143 202 170  45 808 791 300 857 440  63 203 815 427  92 852 129 437  97
 805 290 724 243 640 775 511 754 606 716 169 677 798 183 878 138 801 193
 163  20  70 433 568 516 590 526 905 166 178 525 541 493 818 185   6 853
 162  61 241 689  68 294 111 610 703 385 476 515 777 896  84 859 471 201
  24 757 863 298 709 324 225 255  57 246 797  40 288 259 885 167 109 182
 799 803  39 686 123 154 184 692  98 179 850  23  35 191 496 449 867 821
 661 436 706   8 195  62  46 213 802 297 691 860 578 417 533 722 651 365
 289 712 843 532 341 447 415 173 720 874 512 388 381 819 237 838  59  76
  55 172 325 232 470 479 231 595 394 175 345 796 834 272 763 177 869 889
 658 593 116 375 448 604 247 502 107 212 888 814 209 851 654 257 254 158
 784  86 514 741 416 453 795 813 101 234 903 463 549 556 454 362 321  67
 307 870 811 877 387  74 469 160 538 430 679 238 240 901 248 133 151 721
 682 226  43 513 662 171 343 484 701 715 291 880 727 224 534 830 779 728
 275 607 363 256  17 882 244 655 676 847 273 704 700 112 785  38 304 687
 451  96 344 800 844 659 652 887  77 725 598 810 660 876 504 499 510 566
 812 529 558 906 236 367 342  69 577 198 564 264 855 902 450 323 196 643
 187 656 235 233  79 842 723 866 452 366 278 787 267 762  33 831 477  75
 517 554 871 318 904 508 306 239 258   7 413  93 807 657 726 200 428 771
  10 346 710  48  88 616 750 249 708  42 369 856 186 671 518 602 670 766
 302 299 793   4 287 192 650 565 846 548 481 497  60 228 592 569 106 761
 895 303 594 848 806  21 391 707 576 424 483  41 457  13 501 384  89 736
 393 609 530 295 597 103 747 361   1 730 161 591 574 252  11 875 480 608
 395 305  56 854  85 418 146 537 223  25 114 505 634 717 764 218 301 826
 364 864 485 124 389 861 409 765  18 600 680   9 559 618 599 276 539 434
  58 788 408 190 683 265 199 355 648 718 296 503 262  34 884 326  37 542
   5 758 635 755 459 669 794 348 739 407   2 271 383 546 354 820 159 368
 596 833 688 845  64 738 674 420 743 263 421 681 458 130  26 279 102 117
 605 462 827 614 817 760 780 678 862 729 792   0 266 353 543 432 429 425
 150 571 770  73 868 253 832 567 675 694 858 767 333 506 809 141 189 740
 705 536  47  72 783 110 759 317 229 359 557 490  83 545 487   3 585 332
 242 581  16 575 570  99 719 456  54 108 277 261 774 245 789 601 274 695
  14 328 561 460  44 613 251 488 649 155 711  66 435 562 478 147 751 115
 473 836 773 403  32 356  71 693 404  15 782 338 431 752 840 519 713  49
 572 583 523 327 696 465 829 482 230 555  50 400 673 335 603 260 520 269
 406 772 540 142 149 745  52 587 157 360 698  12 579 890 412 734 611 756
 672 628  19 351 899 329 582 153 560 769 455 347 145 786 580 148 468 472
 535 893 464 466 152 563 699 489 584 405 250 280 486 411 744 357 742 753
 392 398 396 697 617 748 588  53 589 467  51 528 340 410 690 474 507 627
 495 586 824 746 823 350 156 358 668 828 737  30 522 461 547 732 414 521
 498  31 330 749 222 897 544 397 219 839 573 641 268 731 139 822 131 215
 140 735 334 825 837 612 835 220 349 216 128 625 553 663 550 352 551 733
 491 221 401 402 898 217 615 337 644 339 633 900 336 399 666 891 768  29
 331 320 639 647 667 137 892 322 894  28 636 645  27 664 144 665 127 552
 136 132 638 630 631 629 642]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2940
INFO voc_eval.py: 171: [313 506 728 364 174 520   2 557 227  43  72 528  29 315 306 584 392 390
 564 659 474 393 321 568 524 317 690 162  44 508 426 422   8 505 512 562
  70 423 366 705 164 629 531   4  94 169  36 739 402  95 314 433 527 610
 745 284  18 627 365  96 571 182 176 282 761 397 134  50 386 116 136 563
 558 207 729 744 391 574 567 266  62 385 231  22 371 279 184 320 400 746
 260 472 375 444 338 268  45 742 475 325 573 384 277 710 763  34 548 137
 288 516 748 300 147 670 452 649 264 683 522 559 533   7 590 446 447 427
 209 454 436 550 561 368 566 269 175 238 151 383 263 569 292 712 367 437
 191 107  31 441 594 327 243   3 691  47 565 229 352 692  52 487 138 163
  58 208 201 698 425 373 450  11 751  97 235 553 519 730 389  38 265 336
 430 281 323 140 719 387 477 319 334 135 534 560 707 307 370 259 230 310
 224  46 295  66 161 394 732 658 276 604 697 753 572 490 586  80 203 195
 347 294 570 699  48 374 473 346 608  32  67  14 225 662 369 193  35 637
 344 762 382 117 757  20 155 399 428 360 273 118   9 172 654  82  15 283
 144 378 769 245 650 343  64 149 448 663 478 638 456  57 381 503 152 351
 521 388 223 672 486 554 413 731 711 445 747 262 108 674 752 750 511 301
 481  51 234 183 657 165 376 119 171 655 583  19 722 405 582 236 476 424
 275 496 585 592 128 605 631 141 345 125 628 492 639 665 673 640 764 197
 337 647 377 133 509  49 131  77 293 111 435 439 664 588  65 341 271 741
 682 239  78 526 543 680  27 121 380 349 759 681 749 102 685 379 760 228
 451 340 758 504 755 768 434 607 331  28  40 348 587 339  68 267 575 641
 593 274 250 754 403 595 615 333 272 591 514 204 611 270 709 221 442 342
 244  98 535 200 330 621  75 619 166  76  69 280 720 606 286  86 318 671
 159 181  21 124  30 409 401 721 537 261 120 122 127 440 114 443 609 736
 738  71 589 467 643 630 123 406 142 101  12 255 168 372 160 296  63 146
 254 455 105 237 186 350 258 198 740 756 202 513 432 148 714 539 765  79
 214 187 642 103  10 329 355 614 431 461 646 743 625   5 130 453 633 624
 143 766 479  61 429  13 199  73 634 645 449 154 549 716  42 438 678 737
 546 668 523 157 536 515 600 109 689 529 298 362  26 552 190 316 767  93
 530 510 636 179 212 648 356 416 335 484 493 602  17 704 139 661 167 150
 460  90 547 206  87 395 660 706 632 112 173 104 192 353 688 110 421 717
 666 635 312 667 644 518 100 708 252  33 233  59 311 686 723 156  56 178
 287 713 308 153  74 617 196 210 145 291 205 158 170 715 652 462 459 222
 517 656 482 613 507 489 618 285 290 113 297 249 463 466 414 189  37 177
 253 653 278 188 684 185 232 363 354 361 471  41 289 309 407 180 675 322
 126 480 597 194  83 700 626 669 217 326 612 411   6 468 494  91 220 324
 488 211  39 702 603 415 398 251 495  60 525 240 532  89 491 551 332 620
 242 485 299 115 676 616  92 555 226 129 106 241 679  55 701 541 581 303
 464 469 579 677 483 215 132 465 622 687 216   1  99 733 213 693 305 304
 419 417 457   0  25 540 412 598 408 396 601 623  88 596 410 577 404  85
 358 359 651 599 538 420  81 470  84 357 418 695 458 248  24  23 498 703
 578 328 718 218 545  53 542 544 497 219 735 576 694 734 247  54 256 696
 502 257 726 302 580 246 499 500 501 727 724 725  16 556]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3755
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3163
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.259
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 134: 0.195
INFO cross_voc_dataset_evaluator.py: 134: 0.247
INFO cross_voc_dataset_evaluator.py: 134: 0.350
INFO cross_voc_dataset_evaluator.py: 134: 0.431
INFO cross_voc_dataset_evaluator.py: 134: 0.310
INFO cross_voc_dataset_evaluator.py: 134: 0.068
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.147
INFO cross_voc_dataset_evaluator.py: 134: 0.343
INFO cross_voc_dataset_evaluator.py: 134: 0.195
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.569
INFO cross_voc_dataset_evaluator.py: 134: 0.542
INFO cross_voc_dataset_evaluator.py: 134: 0.431
INFO cross_voc_dataset_evaluator.py: 134: 0.295
INFO cross_voc_dataset_evaluator.py: 134: 0.160
INFO cross_voc_dataset_evaluator.py: 134: 0.294
INFO cross_voc_dataset_evaluator.py: 134: 0.375
INFO cross_voc_dataset_evaluator.py: 135: 0.316
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.363s + 0.026s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.345s + 0.042s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.355s + 0.048s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.363s + 0.048s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.369s + 0.045s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.369s + 0.046s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.365s + 0.044s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.364s + 0.044s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.364s + 0.043s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.364s + 0.043s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.365s + 0.042s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.368s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.369s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.511s + 0.051s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.402s + 0.041s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.386s + 0.039s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.373s + 0.039s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.369s + 0.039s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.373s + 0.038s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.382s + 0.039s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.379s + 0.039s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.375s + 0.039s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.373s + 0.039s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.370s + 0.038s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.371s + 0.038s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.368s + 0.038s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.564s + 0.028s (eta: 0:01:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.362s + 0.035s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.357s + 0.038s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.366s + 0.039s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.358s + 0.039s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.358s + 0.038s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.358s + 0.038s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.358s + 0.038s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.360s + 0.038s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.359s + 0.038s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.359s + 0.038s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.360s + 0.038s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.359s + 0.038s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.410s + 0.039s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.382s + 0.039s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.369s + 0.037s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.367s + 0.038s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.364s + 0.038s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.360s + 0.038s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.364s + 0.038s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.361s + 0.038s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.360s + 0.038s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.359s + 0.038s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.356s + 0.038s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.357s + 0.038s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.354s + 0.038s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.196s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [960 237 240 ...  24 807 851]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2250
INFO voc_eval.py: 171: [116 112 401  62 378  75 183  66 291 399 113 464 226 207 376 347  67 117
 466 127 292 397 457 470 434 471 462 443 348 481 350 459  64 290 120 349
 354  88 184 119  65  68 115  63 351 211 445  70 293 122 285 357 322 189
 228 437 482 124 134 408 368 123 217 402 188 359  69  10 364  93 375 430
 440 468 456 237 358 306 311 118 229 305 197 353 405 436 227 448 451 202
 478 230 404 477 132 289 423 128 480  31 186  24 362 407 365 444 196 185
 344 479 256 288  76 195 191 345  97 469  72 340 465 355 331 222 241 231
 442  94   2  13 411 298  36 463  95 473 114 215 425  17 460 324 450 379
 144 412 182 414 391  78 166 467 225 474 424 413 297 447 133  25 338 416
 371 223  20 461 269 135 325 301 244 247  34 369 125  30 475  11  91  38
  86 455 158  99 168  74 265 476 356 246 333 251 201 249 421 193 218 483
  21  16 129 216 330  98 452 446  35 205  77  83 248 380 422 130 472  44
  48 372 389 427 137 310 346 240 313 374 449 393 295 458  33  82 272 363
  23 319  12   1 194 254 339 263 233  89 381 453  80 121 221 243 198 131
 213 403 415 308  84 161 252  32 259 264 360 417 329  71 426 238 439 234
 224  41 318  22 279 255 159 126 315 136 341 160  27 361 304 106 242 366
 162 400  52 377   0 326 187 370 382  85 208   3 332 286 261 171 200  14
 143  26 250 300 303  19 180 275 337 454 210  28  58 327  46 277  18 307
 383 396 433 343 157 328 429 235 410 266 151 192 431  87 199 167  96  50
 418 236 428 398 105 154  15 268 392 110 103 287 438 153 316 214 309 367
 239 342 395 352 232 253 373  53 164 145 271   7 394  90 388 152 321  51
 156 409  37 441 296 419 147  55 165  57 206 190 146 273 299 274 312 294
 432   5 178 435  49 270 212 284 336 245 204 420 323 317 203  54 390 111
 302  40 406 209 138 260   9 104 169 320 172  42 276 314 163  92 283 267
 150 107 181 278   6 334 149 140  59 258 262   8 155 139 257  45  73   4
 102 148 100 101 170  43 385  29  81 335  60 173  79  56  47  39 177 387
 174  61 179 281 280 282 386 176 384 108 141 142 175 219 109 220]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3865
INFO voc_eval.py: 171: [ 726 1945 3687 ... 5413 5417 5408]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1806
INFO voc_eval.py: 171: [ 625  652  629 ... 3252 3254 3241]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2297
INFO voc_eval.py: 171: [346 545 377 291 337 535 347 617  19  11 623 619 593 555 267 379 506 581
 294 316 378 543 509 274 662 572 349 630 553 570 126 302 534 628 804 448
 295 665 805 272 293 507 723  18 521 624 307 271 268 739 315 622 636 594
 806 647 308 351 275 306  21 703 281 819 355 638  17 653 278 482 541 621
  20 618 632  13 340 354 301 380 536 137 508 599  24 552 705 625 807 284
 305 439 554 464  22 462 728 348 537 300 627 512 292 567 825 270 479 540
 338 724 253 546 513 202 160 352 648 584 299 598  64  12 313 679 736 681
 812 562 810 187 657 551 273 314 669 651 658  40 520 442 668 544 816 578
 635 602 814 179 666 146 381 560  14 133 571 105 195 671 676 228 158  15
 549 538 243  74 643 576 561 276 483 586 582  65 309 714 644 450 788 465
 460 280 157 663 776 199 820 312 451 285 344 350 514 664 748 190 692 518
 677 808 678 162 254 277 461 777 597 574 774  23 558 392 239 675 353 577
 197 542 297 128 101 250  95 279 517  29 639 566 606 650 485 557 519  39
 103 466 251 375 365 629 701 655  44 163 539 568 733 438 642 640 579 691
 510 756 339 746 223 296 224 356 107 680 142 737 823 486 286 654  41 833
 369 135 511 118  98 725 304 778 311 667 203 376 684 303 186 234 255 515
 491 824 298 245 815 620 196 152 773 556 659 634 660 580 408 573 559 430
 407 550 751 161 222 779 674 685 469 139 185 102 757  30 310  73 220 342
 775 834 390  27 645 547 166 443 563 153 790 672 282 248 389 453 600 670
 235 516 696 269 440 656  16 151 732 822 226 468 649 575 454 106 396 366
 565 646 134 218 191 587 564 673 419 569 463  53 114  48 420 702 626 332
 839  94 585 548 249 256 682 837 181 603 341 583 661 611  78   6 604  34
 413 121 633 459 433 164 754 492 688 283 182 637 652 155   2  91 129 744
 763 595  72 136 641 488 631 713 343 183 210  42 683 740 467 104 109 393
 374 257 252 749  75 755 449 108  43 481 401 829 159 205 836  93 726  38
 144  96 167 741 323  52 731 431 596  37 434 708 784 230 130 802 198 457
 238  25 758 695 165 697 743 184 317 707  31 240 693 131  36 117 734 704
 689 231  70  84 288 138 809 400 180 838 753 399  85 832 244 761 156 455
 747 168   3 787 830 403 759 456 227 116 169  97 821  60 367 388 113 246
  26 716 232 237 738  89 780 423 154 236 480 490 616 711 590 432 148 258
 437 247 500  99  92  68 452 842 212 391  90 217 225 471 221 789 750 188
 772 143 422 421 446 145 609   1 204 831 394 178 445 229 100 502 333 147
 402 115 415 140 428 607 441 398 710 742 409 727 472 387  76 261 207 817
 242  32 526 730 484 215 127  62 762 525 497 706 262  28 189  87 760 319
 345 206 260 458 473 699  46 416 487 132 729  47 397  77 209 233 717 499
 505 786 120 715 745  66  83  79 201 141 735  59 395 764 686  33 826 241
 498 523 436   5 605 265 418 608 533 840 501 359 531 493 287 476 610  88
 124 698 694 718 712 324 601 828   4 259 532 263 529 503 150 214  10 125
 149 813 334 362 528  56 330 123 709 208 504 489 405 783 200 328 827  67
 530 176 781 811 495 386 785 429 447 358 494 588 361  35  69 412   0 444
 496 414 331 211 122 803 782 589 475 474 318 613 290 766 194 687   8 329
 527  51 835 435 427 363 614 592 325  57 818 524  45 364 799 841 417 326
 410  50 327 404 470 522 721 264 360  81 321  54 173 591 752 336 289 615
  71 368 720 767 370 700 335 690 372 797 722 266 216 213  49  86 174 110
   9 768 426 192  63 177  55 477 175 322 357 719 373 371 219 771 478 193
  61 320 769 406 791 798 770  58  80 800  82 801 382 384 119 425 765 796
 793   7 383 385 612 171 112 170 111 172 795 424 794 411 792]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3199
INFO voc_eval.py: 171: [393 154  89 391 153 392  91  77 167  92  99 419  79  90  93  74 326 100
 395 362 191 114  23  78  95  43  82  73 145  76 355 249 342 350  37  86
 367 175 327  58 243  97 187  80  87  98  38 147 156  88 403  83 357 183
 333 325 223 157 322 361 110  81 409 356 321  94 354 144 248 189 181 352
 158 378 190 266 289 317 377  66 111   1 180 239 185  96  27 424 398 169
 161 128 246  84 435  25 411  85 301  75 182 343 231  53 155 112 241  36
  50 237 250 113 150 389 308 341 240 425 212  16  39  22 164 300 323 383
 299  41  44 215   6 186 358 139 288 351 374 304  45 369 405 135 116 206
 320 296 396 159 336 286 230 106 138 204 256  15  55 184 244 318  52 402
 107 422 359 115 134 373 324 233 162 338 260 285 170 132 353 252 290 366
 337 101 232  40 117 129 400  21 316 195 259 287 339 213 198 238 295 349
 229 131 208 216 283 127  54 376 401 413  24 105 348 262  20 148 133 152
   2 192 251 123 407  26 297 126 330 205 236 118 228 255 188 410 428 209
 302  29 224  65  34 319 347 141 269 372 226 434 430 360 388 265 254 306
 253 298 364 109 394 334 311  42 281 397 121 408 268 421 345 340 429 166
  30 303 120 151 328 418 143 258  35  31 263 234 235 346 379 217 193 160
 282 197 385 386 142 390  68 225 331  61 245 264 310 119 271 387 344 426
 108 227  46 196 130 103 210 177 382 102 381 200 273 384 211 165   9 332
 375  51 137 222 261 218 242  18 363 149 104 291 146 380 329  48 371 431
 370 284 293 423 335 122  33 136 247 201 432   4 178 427 420 257 433 365
   0 399 307 267 368  19 274 124 140 312  63 292  13  32 278 294  67 214
  28 314 176  56 171 277 173 270  60 415 280 414 305 406 179 220  70 416
 207 275 203 219 276 163 309 279 221 272 404 199  10  14 194   8 412   3
  72 174 168 313  47  64 172  69 202 417   7  62  71  59 315   5  57  49
  11 125  12  17]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4167
INFO voc_eval.py: 171: [1646  739 2304 ... 1786 1869 2495]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2762
INFO voc_eval.py: 171: [216  88 252 122  18  55  99  23 165 250 177  52 253  15 218 103 104 232
 175 213 100 167  57 164  90  14 230  20 209  97 180  61 162  93 128 179
 139  80 236 124 126 155 188 189  73  53 217  10  44 154 140 196  46  98
 251 234 168  67  11  72 136  60  64 102  42 203  85 141 202  17  13  68
 163 135 127 117  24 243 191  21 244  51 109  79 108 192 119  95 233 181
 143 123 146 198  12 221  62  92  37  76 110  48 190 210 125 115 172 142
  59 235 195  84  16 208  75  94  70 248 121 185 194 245 114  43 214 201
  47 158 156 207 157 231  96 193  45 176 107 105 247 118  26  66  77 182
  69 215  74  22  65  19 134 151  71  54 131 229 183  25 246 200 197 223
 227 171  87  83 249 111 199  38   9 112   4 211 147 113 184 204  89  40
 226  91 159 205   2  86 212   3 173 186  82 228  39 254 222 220  78 138
 160 106 101  32  81 239  41  36   6  28  30 169 178  63 225  35  50   1
 149  27 187   0 166 130 170 174  49 137  31 133  58 120   8 224 132  56
 148 219 237 116 238  29  34 161 129 150 206  33   7 241 145 144   5 152
 240 242 153]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1303
INFO voc_eval.py: 171: [5293 2309 4654 ... 3352 4072 3948]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.2795
INFO voc_eval.py: 171: [354 183 266 260 482 267 365 414 264 189 445 261 185 265 279  54 476  62
 447 204 295 591 451 173 271 276 797 283 810 679 186 488 478 646 182  83
 483 197 372  65 480 416  80 209 411 748 168 370 103 592 287 801 595 485
  21 187 455 237 284 754  56 413 217 212 223  64 282 270 803 246 278 806
 289 205 277 121 420   5 275 496 611  78 489 338 667 181 596 798 796   6
 751 161 315 381 756 200 236 645 487 755 400 484 486 811 477 228 450 682
 288 263 201  75 458 425 116 294 495  73 195 377 439 184  53 449 652 219
 299 555 520 285 371  55 191 262 446 341  28 286 802 356 444 422 594 508
 410 767 479 772 132 805 766 653 167  35 406 800 752 749 494 171 196 362
 269   7 551 364 456 301 664 292 467 253 432 274 165 304 761 681 392 701
  72 243 809 305 290 757 600 130 325 564  10 357 589 493  79 302 415 376
 391 438 272 683 677 684 491 593 762 172 379 670   8  31 804 760 759 316
 224 142 746 174 222 689 556 647 821 298 133  52  49 666 190 448 599 334
 729 513 651 773 502 708 598 340 650 175 367 590 105 176 799 232 192  76
  59 144 137 784 747 361 164 131 394 319 375 750 252 492 320 680 273 213
 120 698 188 227 668 459 815 743 610 134 257 685 768 368 293 307 732  34
 202 297  27 160 102 323 141 206 233 355 412 661 234 812 378 665  30 198
 654  66 170 349 638 808 443 363 687 744 607  69  26 790 454 384 281 457
 123 359 327 328 303 604 481 548 358 109 570 326 306 138 549 380 453 169
  57 753 258 239 280 426 626  51 115 817 770 140 672  39 787 519 230 146
 778 242  85 395 814 490  84 347 563 813 321 819 373 136 742 291  43 831
 128 177 597 240 424 832 774 452 427 435 566 771 635 561 122  97 702 780
 166 764 193 464 163 154 248 671 834 613 660 221 758 430 300 550 526 203
 119 215 296 268 460 656 648 588 336  23 782 461 706  20 151 417 763 419
 421 830 351  19 554 418 559 150 794 330 339  11  13 692 244 462 765 649
 820 332 776 194 101 807 352 324 658 669 322 117 318 612 769  98 605  50
  40 781 429 745 342 374  18 517 703  92 423 207  12 541 383 317  58 386
 220 535 565 431 366 628 245 791 816 153 818 360 397 713 348 705 504 777
 158  99 333  17 609 718 608 404 369 110 529 558 208 833  63 567 678 690
 694 602 500 697 606 437  24 785 738 783 433  36 789 527  25 255 617 518
  32 148 516 639 346 788 214 603  95 100 795 149 740 560 428 259 218 401
  61 629 127 229 525 436 434 152 717 577 627 569 250 143 691 734 125 471
 398 225 571 719 601 126 511 673 562 531 614 552 162 711 583 108 135 792
  33 733 568  16 147 216 786  37 675 385 714 726 793 657 700 829 775 156
 659 396 247 403 350 337 159 155 737 779 835 104  15 707 331 114 329 524
 557 145 157 553 408 335  70 344 199 111 624   9 399 402  71 534 345 235
 536  38 343  68 709 621 693 721 730  96 822  22 643 254 353  81 642 631
  48 699 210   0 630 662  89 828 473 211 579 231 634 124 256 523 674 723
 637 715 241 249 542  14  82 620 644 390 736  86 618 731 226  87 528 251
  29 393 238 696 741 619 712 688 676 179 625  94 515 540  60 507 827 735
  74 472  67 640 498 686 139  45 475   3 574 663 632 823 129 724 470 474
 710 728 497  41 725 716 722 530 405  44 466 469 633 465 695 503 544 107
 623 468 636 533 113 584 704 463 440 314 543 545 585  46  93 522 641  77
 532 575 118  90 727 106  88 521 538 178 180 739  47 573  42 388 578  91
 506 441 576 537 586 387 505 539 112 582 409 572 407 587 509 581 655 512
 580 501 499 442 510   4 310   2 514   1 389 309 313 546 547 382 312 616
 311 720 308 615 825 622 824 826]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2022
INFO voc_eval.py: 171: [2578 1136 1571 ... 2876 2878 2877]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2958
INFO voc_eval.py: 171: [245 248 253 374 297 328 348 269 330 199 405 118 377  34 626 296 463 326
 388 197  29 587 117 347 112 252  54 192 123 343 352 378 411 345  25 410
 168 131 273 396 259 193 361  27 299 115 633 122  31 127 561  47 294  33
 114  48 642 246  32 600 154 584 119  23 255 588 383 149 634 208 202 349
 132  52 364 321 375 582 128 150 100 466 325 465 438 158  35 298 621  63
 462 332 126 164 480 376 256 270 394 157 295 331 601 552 165 344 632 381
 353 129 194  38 409 496 406 501 322  49 544 640 500 257 613 585  37 151
 301 540 324 497 327  26 583 602 247 272 605 581 586 218 384 354 454 116
  41  53 155 147 201 574 608  46 510 592 258 627  99  36 174 320 526 195
 302 121 509 166 619 316 459 274 101 439 152 449 589 504 623 558 437 536
  50 478 186 644 580 656 124 434 244 145 207 593 230 130 591  24 412 146
 611 249 494 645 479 120 315 125 205 614 560 431 413 113 137 549 594 196
 189  73 312 456 204 482 303 365 167 407 300 203 554 311  43 144 472 415
 579  45  51 616 553 198 133 578 323 515 232 545 317 635 651 447 507 650
 363  42 523 200 563 556 346 380 206 637 102 596 400  18 441 215  30 546
  71 238 460 160  65 184 305 590 453 110  89 231  88 161 636 318 187  39
 452  40 250 429 606 162 615 511 159 105 514 643 457 550 185 173 177 367
 458 609 607  95 617 216 226 622 485 191 529 646 390 217   6 156 604 254
 505 329 629 170 243 433 502  72  67 183 566 427 220 251 416 641 624 408
  66 512  85  28 209 392 153 222 277 103 420 211 488 379 516 522 527  10
 306 104  69 487  11 275 532 533 461 304  90  93 106 610 319  70 440 212
 188 178 271  98 547  86 639 260 430 451 620 182 506  44 470 172 163  74
 595 612 474 618 386 356 481 548 450 239 210 213 233 309 653 603  77 513
 414 455 508 648 530 307 404 227 551 657 521  80 537 498 398  68 240 360
 647  64 139 448 428 350 471 359 242 524 169 443 225 542 351   8 288 495
 493 190 223 464 628 236 308 518 337 389  62 473 476 111  91 142 417 214
 286 652 421 649 293 143  97 486 141 655 393 435 403 445 228 564 265 572
 221 446 276 555 369 358  19 630 654 520 292 503 263 314 517 281 442 535
 219  55 171 148 108 107 268  78 569 631 467 262 181  82 490 475   1  87
 176 598 138 140 224 136   2 562 338 559  92  16 310 534 489 229 492 341
 597 557 313 418 387  13  76 357 333  12 402 362 576  94 109  15  96 444
 340 261  14 477 180 175 419 423   5   0  17  61 491 599 484 568 625 397
   9  20 241 335 499 519 565 638 339 541  81 355 432  60 291 483  83 571
 179  79 528  58  75 287 399 366 436  57 538  59 266 391 573 267 539 371
 531  21 290 370 385 525  56 264 235 334   7  22 282 342 567 336 543 424
 382 570 289 280 395 368   3 283 237 422 401 468 469  84 426 278 425 284
 372 135 134 279 234 575 373 577   4 285]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1862
INFO voc_eval.py: 171: [449 961 126 ... 478 580 777]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.2673
INFO voc_eval.py: 171: [225  41 122 224 128  23  40  65 123 242  42  47 132  21  77 229  58  43
 129 164 106 238  46 124 237  64 246  25 113  54 217 137 174 234 130 183
 146 141  83 233 218 172  15  59  66  72 153  75 170 235 140 207 227 247
  22  49 105 131 243   7  63 211 190 168  71  31  98 175 138  52  26 101
 228  19  44  50  69  76 126 230 182 249 208 206  73  74 215 133  51  29
 125  34 135   2 248 121   5 178 236 102 136  11  14 139 214  89  16 220
  18 226 196  24 115 127 219 104  70  56 205 176 202  90 112   4  57  94
 134 209  62 154 187 213 142 110 117  93 144  20 173  32  27 203  68 120
 231   0  85 193 165  84 191  13  88  55 148  99 201  17 192 212 216 200
  81  87  92  86 147 158   9 150  28   6 244 222 169 198 194 241 184  96
 180 210 151  97 149 152 100 103  80 145  95 163 167  67 161 156   3 179
  30 188   8 157 177  33 119 195 189  91 155 159 232  78 160  39 143  60
 109 240 107 111 239 245 185 118  79 171  53  37 162  45 199 186  10  12
 166 181 197  82  61   1  48 221 108  38 223  35 204  36 114 116]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4742
INFO voc_eval.py: 171: [ 9273    61  7384 ... 17896 17895 17898]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5281
INFO voc_eval.py: 171: [2235  246  247 ... 2401 2398 2397]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4092
INFO voc_eval.py: 171: [257  40  47 274 474 487  29  57  32 107 279 266  60 480 479  52 477 263
 258  54 273 286  46 113 475 128 536  55  63 292  36 131 255 127 417 485
 108 295  61 121 186  33 573 428 476 357 493 134 535 104 195 253 112 110
 492 354 368 482 260 538  48 351 550  70 484 452 114 254 137 236 554 542
 109 491 549 486 287 256 276 405 140 289 478 422 272 499 116  30 115 277
 545 418 192 194 551 567 252 433 285 117 119  43 593 138 364 565 552 294
 442 557 544 188 118 184 120 296 315 356 290 283 141 537 300 126 359 447
 197 547 380 196 560 556 298 278 379 259 106 265 505  83 465 271 503 111
 133 215 130 561 284 185 426 183 358 267 269 217 434 360 139 448 361 291
 239 543 103 352 270 261  42 102 376 250  82 416 198 316 365 129 427 227
 224 541 563 262 191 572 540 293 534 229 470 468 500 353  44 502 574 539
 124 440 430 419 299 443 275 504 553 488 406 105 548 437 144 297 280 497
 555 438 420 450 281 533 132 282 498 125 164 166 268 288 558 546 251 564
 562 570  96 568 264   8  69 189 218 569  11 444 199 424 571 566 136  34
 241 559 575 576 388 490 423 168 200 237 101 243 463 187 143 309 355 238
  92 415 167 367 221 594 421  91 495 170 524  98  31 169 425 302 174 458
 165 464 513 135  84 305 459 301 142 431 209 461 214 466 304 429  95 411
 161 483  39 303 597  86 225 247 223 489 436  20 441 216  65 394 190 100
  14 501 193 508  22 226  12 210  19  64 233  97 160  38  53 222  23  56
 371 462 389 230 240  16 246  87 467 162  85 177 208  21  51 407 244 231
 366  67 362  45  18   9 469  77  62 242 435 339  17  50 453 439 317 245
 331 326 446  26 179   1 404 220   4 386  13 409   2 496 333 527  68 219
 393  10 457 385 445 234 523 211  94 529  15 150 481 577 455   0 363  81
  93 414 412 349 320 212  76 158 308 159 176 408 526 432  58 514 456 311
 319  73 249 344 494  41 383 334 528 449  37 511  28 451  27 598 375 310
 410  35 522 460 390 181 454 387  72 391 178 392 207 525 314 232 510  25
 175  71 384  49 595 321  74  66 413  59 509 507 596 180 373 146 325 506
 381 235 182  24   3 583 530 306 532 163 312 307  88  99 515 322 213 374
 206 323 173 382  75 228 403 201 369 512 152  89 345 318 516 579 248 518
 329 372  78  79 330  90 154  80 151   6 332 370 203 348 171 313 342 378
 377 172 149 148 531   5 202 155 205 521 581 336 153 396 517 350 145 472
 473 337 471   7 204 343 156 520 588 519 327 324 401 122 335 397 157 147
 587 586 123 395 340 328 592 589 341 346 338 347 399 580 400 398 590 402
 578 591 584 582 585]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2288
INFO voc_eval.py: 171: [ 46  54   5 210 321 315  55 212 136   4 205 207 408 313  57 211  48 265
 138  13  65 156 111 209 170 132 179  66 398 320 316 115  85 403  47 214
 182 213 266 113 175 116 314   8 407 312 317  64  84 206 165 120 198 366
 173 176 322 361  10   9 262 319 365 183 323 140 117 400 163  49  51 108
 386  91 318 180  58 367 411 285 110 269 291 308 368  20  62 208 282 148
 143  53 190 162 174 169 260 200  82 385  56 244 241 178 250  14  15 166
  52 177 114  90  96 384  50 107 194 181 351 147  63 261 287 296 292 109
 133 263 229 119  16 253 121 268 306  17 145 264 160  59  93 199 310 105
 295 219  24  95 396 267 387 272 303 134  68 135 112 118  94 248  88 288
 290 139  26 392 294 281  99 185  77 293 300 376 100 405 412 399 217 186
 289 158  28 188  12  98 101 144  78  36 362  67  86 231 184  89 103 283
 130 390 271  97 239 215 304 409 360 278 247 364 381 196 388 216 137 284
 286 378 142   7 164 222 104 389  87  71 232 401 299 161 189 372 309 152
 146 193 307   6 397 141 301 237 391 172 363 195 347 255 374 157 168  11
 302 159 225 171 245  41 352 128 371 402 218  70 404  81 354 348  23  69
 373 167 258 410 311 353 257   2 270  21 127  37  22  79  18 406 228 380
 129 393  83  45 126 246 221 379 395   3  25 249 342  80  27 251 336 346
 236 343 256 375 273 254 233 377 252   0 235 227 357 187 224  61 131 298
 153 149 150 154 201 259 124 345 344  44 204 394  42 220  29  60  92 226
 305  40  43 240 155   1 341 355 359 297 337 151 340 122 192  38  76  30
 230 335 234 277 191 203  39 223 370 358 356 349 383 125 338 332 331 197
 238 334 333 339 202  32  19 280 350 279  72 123 369 382  33 242  75  74
  31 327  34  35 106 243 102  73 326 325 330 274 324 329 328 275 276]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1297
INFO voc_eval.py: 171: [638 643 397 335  79 221 396 300 640 119 902 417 115 174 645 336 192 322
 121  83 406 644 807 299 110 639 803 199 124  76 301 334  89  80 642 224
 223 471 650 130 200 522 556 118 814 415 405 222 912 347 310 849 404 915
 338 220 302 904 127 542 914 337 416 903 855  94 191 186 854 400 845 231
 477 227 398 908 172  84 709 207 916 346 355 205 217  75 472 145 546 208
 561 470 320 226 229 677  41 303 181 143 402 230 187 475 560 307 232 107
 658 738  88 874 485 314 225 198 920 848 215 911 806  43 727 547 474 275
 343 686 813 401 306  78 173 116 409 836 177  91 898 210 907 840 712 717
 194  63 108 176 178 148 508 917  20 525 728 734 878  19 808 345 185 136
 313  42 455 319 237 839  87 812 169 312 750 657 457 726 329 838 710 889
 548 888 476 887 414 575 342 841 357 922 291 235 479  96 918 566 170 622
 913 189 781 564 919 279 238 574 114 236 316  86 558 531 846 905 175 527
  65 458  64 182 242 482 183 254 707 257 506  95 882 818 473 304  61 101
 549 481 530 171 452 817 834 412 259 877  56 418 715 413 860 906 308 252
 209 847 792 141 805 742 113 311  72 260 190 810 410 272 875 271 729 399
 129 711  28 218 369 744  98 832 892 149 125 421  66 659 850 332 856 552
 648 716 518 325 682 106  60 833 228  15 515 886 692 720 331 790 938 885
 763 924 541 731 211 297 179  71  50 713 234 216 762 261 765  73 524 359
 665 105 678 816 461 630 551 689 897 563 454 193 341 204 351 391 339 112
  40   3 166  48 831 287 133 188 685 258 212 939 420 666 273  18 923 201
 835 432 802 590  33 504 596 276  67 492  77  11 626 488 368  17 104 926
 251 735 606 899  45 246 662 184 161 450 633 423 245 267 392 756 819 449
 550  59 295 373 895 282 411 393 123 529 852 842 545 687 603 582 589 247
 921 732 925 486 641 111 180 480 787 453 305 567 503 285  97 318 881 478
 909 280 703 532 910 131 876 669 403 940 544 605 451 250 736  29 213   2
  74 240 233 309 465 761 693 491 376  54 595 647 256 489 438 766 809 573
 681 241 277 487 827 262 281 757 195 615 867  85 614 278  51 695 741 117
 609 691 408  99 578 356  53 459 794 197 864 510  81 739 274 422 559 385
 497 901  52 931 239   7  37 315 702 683 610 843 749 203  31 779  36 168
 283 746 900  46 891 577 653 484 870 748 879 942 100 576 883 863 360 330
 167 521 456 219 249 628   0 723 321 758  39 202 445 782 526 619 862 592
  30 460 784 494 439 375 374 103 747 791 694 540 719 837 759 932 328 263
 323  49 586 568 795 777 333 618 520 298  82 289  27 866 570 214 244 394
 754  57 608 361 269 286 649 290  38 704 829 822 670 591 512  13  32 623
 894 724 624 708  35 419 196 255 594 737 620 611 629 745 788 516 535 783
  90 593  92 865 796 463 789 690 688 440 780 102 824 363 896 725 163 636
 327 483 588 253 264 937 441 160 880 206  34 407 326 684 132 613  16  44
 509 793 511 490 941 604 934 164 884 425 151  55 362 557 581  93 799 607
 893 752 778 539 680 292 844 927 354   8 349 562 499 468 553 699 437 612
 381 534 517  58 705 706 804 600 324  68 294 826 740 380 109 663 284   5
 270 435 700 853 751  12 714 825 718 637 378 890 830 533 505 821 585 733
 523   1 760 248 344 743 513 755 288 858 554 634 265 501  70 469 293 388
 811 820 498 444 786 721 317 543 144 447 514 268 823 828 296 597 156  69
 625 601 602 764   6 565 377 139 580 679 519 656 538 536 815 730 785 466
 587 352 340 467 464 389  62 462 753 507 928 660 537 140 598 722 652 142
 571 158   4 243 599 801 493 428 395 776 701 500 797  10 384 617 159 154
 372 572 627 383 502 671   9 798 857 379 366 800 872 443 673 569 528 266
 616 496 387 382 146 579 851 555 135  14  21 165 773 427 775 429 495 859
 431 668 424 353 371 861 126 365 774 138 426  26 448 446 935 162 651 386
 364 442 157 635 436 631 583 434 868 370 621 930 632 137 390 933  47  25
 120 936 430 152 772 869 134 348 771 350 873 150  24 128 667 358 871 664
 768  22 147 433 767 153 122 367 584 674 770 646 769 155 698 697 672 675
 676  23 929 696 661 655 654]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2819
INFO voc_eval.py: 171: [381 219 542 398 445  20 142 419 112 444 272 434  39 466 318 236 440 220
   1 547 290 293 383 267 418 447 285 154 109 393 315 360 257  34 407 316
 564 435  55 332 511 151 403 350  36 534 218 498 295 233 437 291 196 325
  12  37 261  87 317 298 145 563 110 282 281 314 223 127 384 144  27 113
 335 226 263 446 543 153 186 128 549 126 560   3 327 225 231  85 482 269
 283  50  68 292 169 415  33  56 573 104 385 333 260 439 284 326  89 268
 400 190 510  15 204  40 431 331 545 396 497  69  23 172 191 211  48 452
 386  99  22 264 174   4  25 503 477 181 373  21 572 567 443 171 541 430
 562 259 266  46  84 442 438  13 147  26 561 288  43 556 565 348 465 441
 513 329 270 217 271 537 535 306 242 105 451 347 152 274 334   7 100 495
 117  54 362  28 358 436 133  17 111 193 143 566  60 361 509 120  35 115
 310 279 571 480  16 345 161  11 177 559 462 517 353 409 478  95 141 189
  92 450  10   2  14  41 175 158 122 352 250  38  74  78 372 574 156 258
 363 449  76 399 389 102  88 406 278 197 148  44  24  67 173 349 183 569
 321  93 578  63 410 119 106 179 118 359 273 323  32 227 337  83 492 380
 422   5 140 192 546 483 164 408 184 157 448 121 304 302 265 489 276 160
 130 346 464 468 479 459 275 178 286 159  98 538 485 101 508 481  47  49
 170 568 514 570 544 124 484 262 364 412 506 228 185 330 463 421 455 525
  82 243 460 123 391 351 490 520 241 155 526 245 379  45  66 255 548 244
  79  96 280 232 370 340 203 558 150 248 394 533 505 322 342 420 187 454
  62 246 369 146 277  29 307 108 339 235   6  30 493 338  51 215  86 461
 208 467 365 296  58 116 405  71 176 324 344 453 199 343  31 576 311  77
 319 129 515 496  59 138 229 305  70 524 303 411 557  42 341 180 300 432
 502 195 336 131 487 182 301 512 397 531  73 501 530 504 289 212 125 222
 247 320 309 114 395 328   8 392 166 224  97  81 382 417 423 499 388  52
 256 240  64 390 308   0 387 416  91 404  80 107 149 376  90  94 377 540
 491 414   9  65 200 230 500 522 214 555 528 294 575 577 103 473  57 209
 188 402 523 507 249 375 486 357 168 368 494 539 163 425 532 521 221 165
 401  61 413 297 519 516 254  75 213 374 234 167 201 471 313 527 371 136
 518  72 469  53 529 355 134 137 367 378 488 206 476 252 428 299 312 194
 135 475 354 366 210 287 553  19 216 470 429  18 202 552 139 536 207 205
 198 253 551 251 426 474 472 424 356 554 427 238 550 239 237 433 457 132
 162 456 458]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.2715
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.2860
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.386
INFO cross_voc_dataset_evaluator.py: 134: 0.181
INFO cross_voc_dataset_evaluator.py: 134: 0.230
INFO cross_voc_dataset_evaluator.py: 134: 0.320
INFO cross_voc_dataset_evaluator.py: 134: 0.417
INFO cross_voc_dataset_evaluator.py: 134: 0.276
INFO cross_voc_dataset_evaluator.py: 134: 0.130
INFO cross_voc_dataset_evaluator.py: 134: 0.280
INFO cross_voc_dataset_evaluator.py: 134: 0.202
INFO cross_voc_dataset_evaluator.py: 134: 0.296
INFO cross_voc_dataset_evaluator.py: 134: 0.186
INFO cross_voc_dataset_evaluator.py: 134: 0.267
INFO cross_voc_dataset_evaluator.py: 134: 0.474
INFO cross_voc_dataset_evaluator.py: 134: 0.528
INFO cross_voc_dataset_evaluator.py: 134: 0.409
INFO cross_voc_dataset_evaluator.py: 134: 0.229
INFO cross_voc_dataset_evaluator.py: 134: 0.130
INFO cross_voc_dataset_evaluator.py: 134: 0.282
INFO cross_voc_dataset_evaluator.py: 134: 0.271
INFO cross_voc_dataset_evaluator.py: 135: 0.286
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.494s + 0.025s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.390s + 0.038s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.367s + 0.036s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.367s + 0.035s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.370s + 0.036s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.371s + 0.036s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.366s + 0.037s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.367s + 0.038s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.369s + 0.038s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.370s + 0.039s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.370s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.374s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.374s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.415s + 0.027s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.360s + 0.039s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.368s + 0.040s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.363s + 0.043s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.366s + 0.043s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.369s + 0.042s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.378s + 0.042s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.375s + 0.040s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.373s + 0.040s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.374s + 0.040s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.372s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.371s + 0.039s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.368s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.387s + 0.027s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.357s + 0.038s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.352s + 0.036s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.360s + 0.037s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.362s + 0.038s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.355s + 0.037s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.361s + 0.037s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.363s + 0.037s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.363s + 0.037s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.362s + 0.036s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.362s + 0.036s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.361s + 0.036s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.363s + 0.036s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.498s + 0.039s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.386s + 0.038s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.371s + 0.037s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.366s + 0.037s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.363s + 0.038s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.358s + 0.038s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.359s + 0.038s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.355s + 0.038s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.358s + 0.038s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.356s + 0.038s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.354s + 0.038s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.354s + 0.038s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.353s + 0.038s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.649s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [831 205 154 202 851 231  90 210 345 849 171 875  42 212 348 295  46 407
 255 834 211 625  43 627 869 657 156 184 215 848 756 637 201  65 350 927
 628  66 234 288 240 570 220  53  57  96 630 233 888  39 417 265 636 852
 178 219 538 207 376 873 258 410 117  80 271 423  75  55  84 304 441 757
 520  62 935 378  73  11 291 388 238 285 881 301 641 296 261 413  70 838
 346 912 229 445 839 283 632 843 694  63 393 886  68 801 446 182 965 366
  41 347 533 646 721 846 290 832 926 368 913 297 356 375 725 369 416 264
 924 235 540 492 490 527 855 835  93 919 633 176   6 868 840 289 644 879
 521  35 355 224 440 651 761 259  85 411 481 486 523  54  36 572 914 799
  67 648 198 127  52 414 262 885 204 303 662 292  47  61 853 298 704 592
 100 299  79 828  44 294 519 168 661 237 810 112 130 418 266 293 731  81
 650 491 647 569 216 836 268 420  89 307 732 796 860 883 314 142 649 119
  49 724 222 663 214 351 342 110 531 931 544 841 242 536 546 371  45 929
 183 537 359 217 658 256 408 525 165 186 749 529 975 352 878 728 157 172
 421 269 374 302 946 892  29 170 524 236 305 199 489 867 634 635 563 227
 863  50  86  97 106 587 729 916 223 116 344 444 530 769 267 419 300 206
 659 225 286 559 482 187 571 487 200 485 365  88 120 558 208 705 284 484
 480 893 377 847 488 360 833 370 181 197 934 920 161 324 479 306 645 450
 626 976 478 837 343 653 671 391 315 160 174 922 652 422 476 477 270 977
 164 895 714 665 534 543 580 367   0  51 532 213 723 666 655 103 442 884
 603 876 392 910 829 483  72  64 727   5 770 334 707  48 609 818 607 962
 925 287 159 624  60  28  56  59  92  58 719 257 333 409 349 282 526 874
 561 921 364 260 203 218 412 325 595 583 158 720 751 936 402   1  83 722
 354 726 859 967 759 730 984 638 118 185 310 510 263 175 771 415 964 811
 917 915 230 453 894  69 696 328 335 806 539 109 767 155 128 401 535 629
 147 740 809 718 594 357 332 830 330 528 604 933 695 459 541 177 397 610
 209  77 522 904 232 114 243 974 656 221 241 239 549 710 585 702 800 403
 562 937 250 850 890  98 802  37 932 842 614 858 654 581 323 228 765 643
 560 817   7 443 750 889 596 752 309 113 639 670 400 380 308 430 870 812
 132 353 631 136 406 108 542  76 708 620 669   8 226 706 427 253 668 882
 404 493 593 146 331 551 909 911 887 363 815 820 169 387 458 502 717 983
 143 966 494 321 312 861 866 804 399 939 963 135 923 167 179 122 148 854
 340 675 862 891 322 918 405 797 251 326 733 623 149 640 844 642 969 880
 971 316 864 254 871   3 698 252 982 504 697 940 567 713 700 703 358 979
 787 712 141 898 608 930 814 978 743 613 856 664 938 857 667 318 686 766
 568 660   4 970 754 764 548 123 447 473 968 805 618 575 845 899 798 339
  33 398 612 803 496 945 793 622 547 384 715 372 336 133 617 180 973 755
 313 711 709 748 601 943 134 173 813 872 606 505 760 816 699  91 701 196
 684 822 772 877 162 311 819 981 683  24 249 166 972   2 908 792 395  95
 461 716 824 611 902 865 101 104 621  94 758 327 362 905 823 337 191 508
  78 744 827 808  31 616 741  74 781 516 550 556 512 190 599 195 381 807
 949 980 425  12 455  16 900 319 513 163 129 903 475 897 424 153 390 383
 248 825 944 928 150 901 589 821 681 107 680 906 552 753 105 768 564 952
 557 745 452 763 742 137 431 762 514 672 320 907 144 145 317 124 500 685
  10 329 115 746 747 138 577 947 509 462 941 189 506 111 139 467 553  82
 193 131 140 687  40 619 673 948 382 435 678 426  99 396 826 244  21 555
 495 121 679 615 102 791 566 246 454 682 794 507 503 385 460 456 677 192
 942 497 573 438 470 194 448 449  32 188 379 386 429 591 545  71 517 432
  87 590 373 471 463 389 896 565 961 457  30 394 437 474 247 469 361 674
 126 278 676  18 515 428 501  19 433 574 774   9 783 586 518  15 436 466
 511 597  26 465 464 125 578 451 795 582 468 152 951 773 554 341 788 576
 472 584 602 600 272 151 498 245 499 776 275 434 959 276  38 273 955 960
 957 598  14 956 692 588 439  25  17 579 785 277 605 738  20  22 789 784
 950 778 690  34 786 274 790  13 954 693 688 735  23  27 737 689 691 779
 338 736 782 775 279 953 958 780 777 280 281 734 739]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1922
INFO voc_eval.py: 171: [103 345 104  55  54 113 291 107 408 155 193 293 181 412 255 402 357 294
 382 295 405 290  56 344 391 422 406 342 251  73 114 416 413 156 299 121
  69 296 287  68 310 108 245 298 128  58 381 110 303  87 106 159 127 348
 254 163 250  57 359 164 404   5 195 306 356 297 349 312 288 126 111 311
 160 174 304  25 397 115 305 200 420 242 389 384 117 169 263 425  60 410
 415 392 125 203 366 158 157 206 373 109 418 130 403 261 417  16 207 186
 409 262 194 396 343 112 390 386 374 274  61   0  63 346 421 249 170   9
 400 414 317  21 105 215 322 199 302 189 219  12 355  27  11 269 361 172
 118 124 246  33  88 360 122  91 393 318  76 334 309 358 364 380 399 289
  89  64 300  65 375 385  32  90  37 161  23 407 188 352  26 267 123 276
 286 369 378 119 419 327 208 363 192  70 401 266  72  38 332 177 145 423
 221 256 167  82 301 411 424 271 282  14 292 244 166 379 216 341 319 315
 137 337 213  22   6 116 248  15 321 129 308  79  24  62 120 202 191 320
 275  78  75 383 398 351 353 362 143  13 231 190 268 395 376 204 273 284
 258 270  34   7 225 283  59 171 209 141 198 175 340 278 387 365 184  98
 285 339  74   8 222 314 211 165 247  20  10 212  18 144 173 336 229 264
 277 307 140 252 324 313  80 180 354 138  40  19 265  30 197 162 367 234
  17 182   4 178  41 326 260  42  77 183 388 323 347 377 241  93 239 281
 272  95  51 185 201 205 325 228 176 316 217 154 335 394  28 243  31 333
 136 259 240 210 230 372 196 253 235 257 233 232  71 350 226  45 368  48
  94   3 223 338 371   1  83  86 220 227 214 168 370 179  99 224 134 142
 139  36  35 237 149 218  85  46 279  29  96 280  92 135   2  44 238  81
  50 100 236  84  97  39  66 330 331  47  53  67  43 153 147  52  49 146
 150 151 152 133 148 101 329 132 131 328 102 187]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3449
INFO voc_eval.py: 171: [2002  777 3792 ... 5493 5496 5500]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1665
INFO voc_eval.py: 171: [ 605  609  610 ... 3096 3098 3103]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2069
INFO voc_eval.py: 171: [296 474 315 479 466 242 436 288 550 545 523 280  27 217 289 548 317 578
  11 293 465  15 244 438 515 554 587 316 477 218 228 463 512 254 556 508
 243 724 435 722 226  93 258 259 252 246 290 648 318 551 384 580 225 298
 579  20 571 588 618 229 532 665 481 299 437 493 292 255 564 256 723 565
 235 484 297 498 446 547 566 443 464  59 553  18 749 615 449 533  22 250
  21 470 286 282 557 413 733 294 632 216  97 488 756 128 247 495 440 496
 634 101 731 509 319  26 453 410 251 585 555 468 291 654 594 506 224 537
 287 499 526 767 662 729 525 376 727 595 164 649 375 399 295 126  14 233
  17  12 206 494 444 599 516 398 600 475 561 234 602 245  94 562 501 219
 590 725 574 490 476  24 231 483 539 500 707 109 754  13 511 604 610 473
  28 442 445 513 417 755 601 221 400 742 260 148 154 673 502 617 487 239
  78 546 159 386 227 414 611 696  68 697 467 568  30 675 485 664 542 447
  86 454 619 746 573 439 156 744 730 612 249  16 486 248 385 189 125 563
 397 560 679 659 699 653 472 220 608 285 130 505 567 581 253 236  25 559
 576 761 552 469 584 589 402 583 204  40 577 652 311 633 497 261 200 558
 489 623 387 451 629 257 701 314 232 377 365 616 403 207 586 536 582 184
 161  81 743 107 763  23 530 676 127  46 529 570 147 203 131  19 760 271
 492 186 503 281 695 752 569  31 766 238 708 541 661 482 694 223 592 230
 395 630 613 181 396 698 369 614 373 606 524 480 423 748 650 491 510 750
 452 237 113 596  85 312 519 549 517 222 597 106  83 575 518 100  84 478
 202 208 180 393 598 504 471 367 703 163 137 591 741 368 342 514 162 313
 391 507 609 188  43 670 622 390  32 668 739 593 328 129 607 448   2 146
  41 645 412 605 450 572 348 114 441 341 603 667   5 326 366  45  66 122
 422 104  47 152 179 160 178  99 193   3 401 191 671 678 333  34 138 190
 119 120  29 666 205  48 759 209 108 153 176 540 112 134 636 411 655 196
  91 734 166 132 747 105  73 638 182 135 538 116 123 157 337 327 115 199
  69 383 334 284 642 706 111  57 283 757 198 262 133 378 681  63 685 187
 535 738  61  39 136 121 361 626 740 110 102 457 735 644 201  33 737 167
 620 416 430 139 364 728 660 345 682  80 420 762 684  67 432 330 624  96
 534 736  79 124  42 265  87 672 527 688  72 392 151 421  77 721 194   1
 183 646 332 656 175 700 352 165 277 726 704  98 275 674  89 195 631 354
 658 427 117 389 149 331 150 543 267 185 103 769 168 192 641 353 461 651
 306  90 765 639 669 663 276 343 351 374 425  70 264 732 394 169 155 325
 753 531 415 687 380 428   7 381 418  82  56 266  54 270 372 329 770 643
  65 197 677 336  36 433 705 657 335 170 305  95 388 279   0  35  75 520
 702   6 407 528 272 680 745  76  62 686 426 274 683 355 456 362 627 211
 212 768 210 349 635 346 640 419 347 434 462 241 424 455 363 340   8 764
 751 758 522 628 637 429 625 431 268   4 379 459  37 521 689 263 371 460
  53 458  92 269 406 405 174 213  55  44  64 214 382 310 370 360 324 621
 350 716 118 278  58 404  60 544  52  38 307 240 273  71 302 300 141 308
 647 303 144 304  50 309  74 172 143 142 173 408   9  51 158 171 720 177
 719 718 145 709 215 338 339 358  49 301 692  10 409 715 323 717 322 320
 714 359 690 693 321 711 344 691 713  88 140 357 712 356 710]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.2982
INFO voc_eval.py: 171: [375 138  86 398 137  99  74  87  83  84 140 380 381  85  75 179  91  43
 177 409 163  72 126 314  92 378  80  96 235  79  78  76 132  50  81 165
 139 350 345 329 174 337 226  22 386  93 102 149  46 233  73  82 169 342
 315  48 311  33 144 234  49  26 379 352  32  71 211  42 382 395 107  40
 377 310 146 150  77 148 391 323 390 392 313  36 134  88 232 219 294 258
 412 344 167 175 106 239 143 172 125   2 346 176 178 166 367 108 170 101
 331 399 168 200 141 229 376 155 365 349 103  37 284 415  25 318  94 105
 282 228 338 142  67 288  41 201 247 171  61 411 422 306  24  23 353  10
 335 285 241 328 173 283 221   5 373 224 212 421 238 135 281 336 305 197
 385 218 316 246  58  35 290 402 217 327 309 131 222 339 361 223  90 116
 230 320 185 304 321 360 286 277 280 242   3 366  54 326 362 394  31 413
  68 216 279  39 109 220 343 164 341 214 332  89  27 122 225 154  97 180
 198 330 183 147  34  55 115  53 293 324 289 291 384 114 121 104  11 260
 348 340  29 317 257 231 120 372 319 287 119 325 276 213 127  44 253 308
 333  51 123 194 334 133 406 118  18  45 383 354 414 204 249 182 370   9
 128 298 403 250 254 417 215 368 251 209 100 357 364 248 112 245   6 374
 158 227 363 244 195 237 312  57 278 203  95  98 389 307 396 351 111 416
 199 265 322 162 124 259 356 358 347 255 410 117  28 240 252 272 243 113
 190 136 110 388 404 236  17 369 292 355 419 208 296 359 129 256  30 193
 405 397 189 262 300 267  21 160 371 202 271 270 184 130   7 186  62  47
 153 274 269  64 152 407 295 297 187 268 299 210 420   8 151 264 273 145
 161  14   0 156   1 196 188 159 261 205 275 263  65 418  38  59 266   4
  66 207 401 192 206  63 387 400 408  70 181  52 301 393 157 191  13  16
  56  60 302  69  12 303  19  15  20]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4189
INFO voc_eval.py: 171: [1618 1539  703 ... 1859 1860 2488]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2676
INFO voc_eval.py: 171: [204  85  75 234 207  86 108 156  18 168 167  81 205 202  41 110 158 237
  84  91 152  10 235 233 170  87  82  19 171 141  83 222  22  45  89 142
 123 180 155 220 221 151 113 112 169  74  58  14  35  68 111  93  53  55
 140 236  52 104 206 125 143  88 178   7  43 195  56  47 212  66 191 122
  80  62  40   8  12 127 181  79 165 128  92 223 177  13 196 189   9  20
  15 126 184 109 228  21  34  48  61  96  39 124  72 182  16  30  98 232
  17 225 120 162  38 145 179 105 193 183 160 186 198 106  95  36  37 231
 230 144 185  57 146 121 197 159 175  59 219  44 107  11 187 173  31 224
 201  63  76 214  54 229  94 100 154  64  71 134 200 192 131  70  46 157
  67 164 172  99 199 188  50 190  65  78 174  77 216   5  51  33 213  73
 211 119 101 138 163 133  42 136  90 135 153 147 114 218   2  60  32 116
  69 203  49 210   4  25   3 117   1 161 217   0 132  97 166  26 176 115
  23 209  24 149 148  28 103 208 137 226 129 102   6 215 130 150  27 194
 118 139  29 227]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1156
INFO voc_eval.py: 171: [4590 4594 2017 ... 3516 2920 3517]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.2718
INFO voc_eval.py: 171: [374 273 187 504 266 510 275 267 375 466 513 270 213 188 432 101 309 473
 382 193 104 169 400 627 501 292 301  62 468 288 191 390 429 506 276 308
  20 210 322 481 102 332 397 472 231 287 274 395 505 392 184  63 684 820
 222 186  90 769 436 821 279 433 223 172 631 628 485  36 304 282 291 771
 482  78 185 712 434 502 105   3 312 386 388 687 489 284 487 634 777 776
 517  73   2 203 819 290 174 311 261 639 652 269 208 195   7 815 471 331
  67 313 650  79 354 302 182 108 109 379 814  76 199 782 394 793 630 268
 438 507 524 298 508 333 518 683 285 770 173 181 791  61 230 689 440 289
 516 381 509 294 467 177 515 319 430 240  64 280 832 554 452 714 158 779
 391 161 486 451 483 380 469 772 520 421 142 633 320 175 103 167  25 389
  22 235  55 823 511 129 342 122 816  71  23 350 443 794 305 644 299 316
 236 376 818 672  84 406 512 521 278 170 164 581 338 358 503 166 775 677
 787 822 310 480 217 729 646  39 474 286 138 785  43   5 384 132 780 534
 212 323 206 234 594 428 701 404 824 228  60 526  12 622 760 176 393 315
 127 190  88 118 447 437 165 781 272 719 244   4   9 522 838 774 327 328
  75 640   8 162 788 740 784 828 765 416 817 768 147 632 197 356 435 566
 629 773 802 202 715 531 154 783  85  40 178 491 705 647 297 168 329 458
 519  66 580 413 295  58 303 369 795 265 457 346 565 283  74 140 713 137
  10 171 829 789 690 700 441 831 263 833 661 766  24 494 792 635 484 620
 130 344 224 826 227 385  83 314 490 446 444 478 317  30 442 747 133 623
 335 688 830 401 686 786 641 378 589 804 141 243 685 221 723 717 584 239
  53 675 340  65 417 405 470 462 250 117  11 638 835  54 691 837 214 475
 648 218 183 721 718 567 399  57  29 799 739 128 595 551  68 307 797 249
 107 146 258 704 352  14 281 403 114 778  70 643 427 456 357 226  34 745
 488 798 711 734 850 271  28 387 621 807 293 139 396 790 366 418 724 300
 732 759 330 658 836 737 402 296 211 476 803 767 845 251 801 726 598 599
 306 692 642 834 559 853 669 196 477 479 637 370 339 460 326  56 245  97
 827 810 398 248 355 461 453 229 277 459 359 645 539 593 741 846 558 553
 549 121 448 597 800 377 493 144 215 825  46 205  59 383 811 619 636 450
 189 150  13 449 334 666 325 163  45 364 702  77 590 116 735 192 347 431
 264 660 587 736 848 693 454 708 445 148 247 678 796 198  96  41 343 744
 126 155 151 582 624 321 680 372  37 337  50 455  80 596 351 194 345 153
 336 361  81  47  95 665  19 209 673 200 812 159 557 716 149 809 664 514
 160 324 851 806 157 528 143 840 112 676 233 439 583 625 808 100 813 225
 120 568 341  98 667 659 204 550 123 805 360 259 710 751 348 728 722 649
 419 547 262  91 588 415 363  16 651 706 703  42  17 152 548 424 697  92
 762  44 219  27  38  18 852 145 255 749 422 156 254 626 423  15 556 746
 362 696 125 849 585 600  86 246 720 124  99 607 569 106 654 586 575  21
 725  35 260 256 241 136 252 135 552 496 367  26 368 653 407  32 592  82
 353 216 134 529 679 257 601 242 349   6 563 694 201 237 682 207 695 414
 731 530 698 847 420  89 232 564 748 253 591 573 238 699 371 733  31  33
 663 523 738 365 220 115 131 570 655 373 707 497 492 842 543 498 662 656
 606 743 752 562  69 709 561 761 603 533 525 674 681 839 727 532 730  48
 670 671 742  87 668 750  72 499 560 495 179 111 758 535 572  52 605 113
 571 464 844 180 843 540 541 536 611   1 608 465 426 755  94 555 604 613
 757 753 756 754 602 409   0  49  51 544 574 617 463  93 764 527 615 609
 618 542 546 612 538 545 408 577 119 614 537 110 578 610 500 616 576 425
 411 763 579 657 412 318 410 841]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1743
INFO voc_eval.py: 171: [2324 1012 1415 ... 2582 2578 2586]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2686
INFO voc_eval.py: 171: [205 295 316 207 168 335  93 272  98 223 353 531 274 162 333 271  30 244
 543 269 204 289 324 541 307 107  94 309 354 301  88 268  24 341 100 396
 106 308 496 305  16 273 213 161  96 104 318  90  43 222 320 208 103  23
 311 423 493  91 109 160 328 164  87 140 247 304 264 130 212 351 291 514
  95 402 499 216 227  26  38 245  28 378 403  20 421 170 503 262 299  25
 125 290 356 395 173 536 124 302 323 474 519  89  31 492  34 465 215 393
 532 394  78  40 425 128 163 506 293 508 347 385  42 522 257 516 275 417
 270 167 137 260 127 135 431 111 510 459 136 292  92 317  49 210 505 214
 171 112 336 546  44 141 129 179 524 166 126 329 494 545 504 502 343 422
 540  33  39 172 520 539 325 513 256 101 377 400 348 102 225 363 169 370
 202 544 206 265 276 501 466 110  77 487 228 259 155 194 120 105 267  81
  41  57 498 420 553 165  19 261 525  53 389 392 246 495  37 457 497  27
 523 528 478 435 406 322 401  80 108 263  99 387 405 477  29 156  17  97
  32 472 306 379 266 277 349 255 153 419 131 355 193 360 211 427 181 139
 430 500  75 456  36 384 491 443 132 372 352 297 445 116 159 331  67 469
 157 133 434  56 433 558 490 312 429 374 515 381 527  52 209 365 507 424
  50 418 468 391  85  22 151 224 453 203 509 313 476 489 350 371 511 428
  54 382  11 467 119 226 134 248 408 470  79 449 287 471 142 526 319 432
 175 294 533  35 174 518 243 254 473 521   6 183  84 332 241 556  55 534
 554  58 340 412 517 373 296 398 426  71 559 300 250  74 450 154 251 410
  82 552 122 404 185 447 366 249 444 475   9 158 138 442  70 200 231  18
 182 436 197 143  62 148 446  65 555 397 512  51 190 407 310  68 383 439
 346 551 177 359 455 178  10 229  73  21 186 149 369 414 416 557  59 230
 535 218 357 191  83 479 144 548 146 549 117  48 358 298 184 238 253 217
 463 547 386 399 411 303 327 361 281 152 550   7 413 252 176 286  63 330
  86 258 458 180 460 375 187 482 334  72   0  69 388 189 147  60 118  76
 221 145  45 538 537 196 409 282 438 123 364 480 326  64 192 441 362 485
   5 279 437 345 440 529 219 380 285 342 451 201 452 150 188 240 283 542
  61   8 232 195 121 454 481 242 314 239 464 484 321  13 337 284 339 390
 376 415 233 448 288  47  46 278 280  66 462 338 220 486 483  14  12  15
 367   2 235   3 234 199   1 488 236 198 114 113   4 315 237 115 368 461
 344 530]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1571
INFO voc_eval.py: 171: [ 461  986  122 ... 1054 1049 1050]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.1951
INFO voc_eval.py: 171: [200  38 111 117 198 202  20  36  35  39  41 116 206  46 209  51 145 119
 216  75 123  17  42 185 108 114 207 204  63 154 113 121 205  22 163  54
 189 214 148 112 133  43  61 128 218 215  45  66  62  99 203   7 110  67
  19 219  53 150  70  95  91  24 146 170 162 208 118  80 127  47  50 101
   3 217  92 210 155 102 125  72  49 126  16  26  25 183  30  56 184 186
 115 122  87 180  10 201 124 211  60  48 158 120 129   5  13 143 187 192
  29  79  73  59  15 161  64 140 160  28 149 194  27 156 100  52  90   9
  76  37 167 179   1  93  14 109  68 172  77 147 178 175  97  44 157 176
 181  23 195  89 139 142  69  21 152  57 130 103 132 141 199 164  81  85
 173 188 193 213 190   8   6  84  55  98  83  58 191  18 196 212 168 131
  86 151 153 174   4  82 182 159  12 134 144 220  94  74  32  78 138 169
 165  65 171 137 135  40  33  88 166 136  11  96   2 177  71  34 197  31
 105 104 107   0 106]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4381
INFO voc_eval.py: 171: [10116 16743  8089 ... 19610 19603 19600]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.4870
INFO voc_eval.py: 171: [2009  222  223 ... 1608 2142 2143]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.3428
INFO voc_eval.py: 171: [280  41  51 486 496 516 124  53  67  24 282 521  40 277  35 263 128  27
 275  29 302 284 137 136  34 134 492 508 487 554 446 140 141  50 498 304
 152 501 270 570 499 286 497 145 130 474 269 489 506 307 119 371 132  64
 135 276 433 127 129 490 491  68 261 505 195 266 369 493 582 550 414 278
 303 306 268 553 500 189 519 308 367 552 584 150 125 372  61 237 469 199
 151 518  75 462 289 126 569 366 571 511 590 522  69 131 563  70 154  30
 100 298 144 271 297 561  99  39 549 551 194 461 484 188 299 567 264 558
 458 434 507 291 495 585 374 272 295 565 274  87 368  54 586 373 370 435
 572 265 273 568 390 587 556 509 283 564 101 139 524 143 432 123 579 592
 494 472 437 578 133 279 118 251 217 562 466 305 442 292 117 288 293 593
 219 555 301 416 583 436 517 581 504 290 573 449 520 454 285 201 267 415
 294 262 230 309 197 287 120 296 281 525 142 589  52 566  43  28 121 158
 447 179 365 483 149 577 588 300 439 220 559 459 580 576 457 473 216 444
 448 147 438 177 468 560 383 122  98 574 153 591 451 202 247 557 575  25
 322  91 193  38 180 453 184 445 223 175 225  88 608 389 488 327 181 471
 464 401 156  12 250 463  10 114 109  16 430 460 174   7 379 477  15 198
 475  31 423 224 192 441  76 393 231 208 176 381 320  62  71 102 249 178
  90 113 196 155 104  11 111 246  77 443  44 116 107 530 418  57 228 157
  45 358 103 206 465 138 200  56 106  93 538 467 405 324 146 110  36 424
 148  59 159 232  92  89 480 190  65 313  74 400 512  17 314 187 115 385
 429 171  14 452 404 222 332  13 108 427 470 317 478 191 329 240  32 349
 419 333 245  26 346  42 312 482 431 328 403  37 510 481 211 377 315  84
 218  58 399  72 420 227 226 402  46 417   2 221 514  66 350 248 450 502
  60 326  23 170 594 242 258  21 323 185 407 412  33 254 543 515  82 455
 456 476 255 310 440 354 406 321 318 479   9 244  83 595 325 316 338 535
 387 235 386 376  22 356 542 204 536 422  49 205 421  47 485 428 213 236
 375 209 342  79  73 183 529 233 388  48 348 523 413 503 541 311   1  55
  63  19 319 537 426 241 539 526 112 234 540 425 243 513 382 391 238 239
 186 378 395 210 380  95  86 528 527  18  20  78  80 335  81  94 203 165
 214 344 392 364   0  96 384 212 336 394 173 341 172 229 257 256 207 105
 340  85 396 398 334 357  97 182 347 330 606 215 259 409 397 337 339 359
 355 533 260 545 166 167 331 361   3 534 161 345 352   4   8 351 547 600
 343 532   5   6 548 169 353 408 546 544 363 362 162 360 163 168 160 601
 164 531 602 599 596 605 253 252 411 603 598 410 597 607 604]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2084
INFO voc_eval.py: 171: [ 82  89  11  91 294 425 194 299 297 433  93 428 298 539 357 158  99 293
 164 222 432 530 281 427 162 195 228  21  13 160 100 429 524 248 124 341
 123 355 489 354 481 538  85  97 193 486 424 421 163 166 235 359 541 435
 155 434 422 260  83 487 255 129  86 303 426 201 527 252 296 488  16 269
 335 283 356  20  90 229 490  37  84  15 431 511 423 247 168 430 190 295
 203 300 157 399  92 358 156  25 397 202 198 380 381 279 138 286 392 246
 254  94  30 339 239 119 470 305 385 398 284 212 412 416 197 311 509 172
 395 314 261 205 208  14 282  88 467 223 263 525 532 140  96 353 482 169
 510  31 526 196  23 377 142  87  22 251 333 139 232 418  24  44 352 144
 204 170 318 206 171 351 389 165 264 319 237 253 513 159 417 226 483 387
 216 104 496 407 285 249 391  17 479 267 199 152 250 393 274 161 371 265
 390 108 167  45  63 145 244 370  72  53 396 514 262 268 516 192 329 388
 523  81 121 307 125 185 515 141 128 542  34 304 102 233 471 280 273 143
 394 369 242 402 238 271 224 386 116 200 498 485 122  74  64 324  41 151
 349 103 368 406 480 517 497 272 518 495 342 191 187 535 484 316 245 379
 534 218   4 106 537 360 338 468 420 126 209  65   3 101 512  38 404  18
 315 382 469 415 464 310 207 105  39 337 519  73 504 225 536 383 184  52
  32 494 131  46 540 211 266 236 414 241 334 378 153 348 328  51  12 463
 301  80 220 384  33 227 501 154 215 243  19 132 183  78 182 306  48  10
 521 326 347 107  75  49  76 332 234  98 219 336 520 221 343 493 135 302
 230 309 361 350 240 308 505 403  47 411 340 345 533 257 231  40 500  50
 346 419 529 528 317 320 136 344 292  70 188 405  43 503 543 134  79 117
 186 120   0 461 401  77 499 400 323 502 508 130 133 127  36 179 214 258
 270 287 462 454 313  69 455 325 118 312  42 217 413 475 213 531 291 459
 114 290 149 189 256 457 148 259 147 460 176 409 458 410   2  62  67 491
 452  66 408  68 477 115 174  95 473 478 474 327 492 466 322   1 277  26
 276 451   5 522 453 175 321 275 376  54 450 476 109 137 374 173 447 449
 178 448 465  29 288   7  60 506 375 472 278   9 507   8 373 372  55  58
 210   6 177  35 289  56 444 456  27  28 110 112  61 181 331  71 443  57
 330 180  59 438 113 441 111 439 440 363 436 362 442 445 437 446 150 146
 365 364 367 366]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1384
INFO voc_eval.py: 171: [633 631 637 342 394 417  78 661 222 410 675 632 171  79  75 902 302 805
 117 634 104 187 641 393 195 344 804 303 126 472 115 636 304 924 839 227
 114 226 644 813  88 341 401 200 812  76 913 113 904 313 176 664 343 225
 238 554 408 909 345 305 414 419 845 185 914 651 122 915 518  92 177 317
 120 396  94 402  82 725 223  71 190 841  89 347 739 648 912 101 903 234
  35 418 102 348 173 399 855 471  38 525 319 201 353 473 142 921 196 204
 214 103 838 557 923 232 475 167 705 306 119 858  73  83 908  86 891 194
  87 920 815 240 928 757 421 729  85 842 561 230 422 239 208 546 409  36
 149 731 758 310 321 282 528 685 351 716  77 859 192 530 819 643 312 911
 470 875 836 307  95 545 709 111 424 100 405 121 905 809 743 635 328 752
 170 213  97 700 925 362 843 369 543 349 522 476 474 917 559 919 727  72
 165 723 483 198 501 138 415  61 147 821 512 184 320 576 181  56 150 857
 806  15 577 844 191 713 272 186 822 639 169 878 653 180 112 278 892 730
 168 649 136  93 612 416 654 361 166 135  58 189 708 265 458 922  84 786
 480 247 178 825 851 231 333 309  37 550 810 876 835 175 355 854 407  99
 707 926 235 719 668 885 656 881 837 124 366 849 242 277 295 188 827 457
 276 364 706 236  30 210 324 823 887 372 400 123 767 260 133 798 538  66
 279 874 308 264 551 266 217 245 726 263 370  52 461 733 918 459  63 741
 715 125 206 534  48 182 688 212 896 248 507 888 110 800 900 172  60 219
 183 711  67  32 710 346 412 564 331 193 301 732 642 456 224 267 203 680
 519 834 332 375 555 840 481 106 624 820 286 146 132 108  40 734 311  46
 433 499 916 831 391 269 824 795 174 460 116  96 334 879 289 691 511 689
 684 830 929 540 547 588 666 229 906 567 283 513 544 291 738 179 927 907
 880 669 281 255  70 197 613 241  43 268 910 582 940 848 478 297 237 453
 663 251 939 322 162  34  68 429 199 532 566 207 754  10 938  23 285  42
 537 318  25 411 438 249  74 753  49 244 548 403  81 646 270 130  90 254
 605 164 860 280 287 259 587 894 672 533 589 826  69 258 152 562 325 228
 148  16 243 128 462 392 630 565 614 856 846 793 690 105  17 771 610 742
 693 404 828 720 445 477 284 107 897 549 395  53 703 357 737  39 735 681
   8 413 590 454 215 479 679 808 750 560 942  12 692 335 784 316 427 288
 469 889 406 645 488 762 360 368 211  24 717  80 863 246 570 161 640  98
 882 865  13 398 367 759 506  64 862 487 574  91 397 465 604 420 606 527
 493 486 704 233 503 329 773 749 886 455 294 575  44 901 484 464 899 489
 447   2 756 337 315 425 797 450 446   3 655 598 515 109 620 895 660 694
 552 580 253 602  51 252 740  47 893 423 293 883 890 485 338 290 592 463
 650 382 339 220  59 510 340 787   0 572 500 594 523 256 323 390  33 697
 262 794   1 818 623  57 468 160 365 788 336 221 539  45 521 205 563 509
 816 943 608 616 702 607 941 558  27  55 300  50 770 376  11 807 792 765
 273 443 216 585 898 568 852 350 790 359 873 877  29 482 701 791 629 209
 591 218 434  31 492 502 202 330 356 155 772  26 250 698 514  41 936 687
 847 599 495 298 850 520 441 783 683 581 736 619 678 722 440 829 327 261
 699 490 542 745  28 584 373 156 744 714 508 296 354 801 747 621 814 861
 937 314 377 257 884  14 517 811 586 817 553 556 755 832 141 516 496 933
 686 497 326 596 748 682 869 760 746 781 299 833 274 467 718 151 603 789
   7 796 379 529 571 724  62 751 930 785 163 712 769 536 466 799 609 153
   4 505 593 139 494 157 768 431 384 853 451 144 378 275 597 728 625 764
 271 721 535 385 137 774 766 531 803 387 617 358 449 526 601   9  54 380
 595 867 782 611 498 600   6 615 435 430 802  65 428 763 761 127 579 696
 524 578 504 158   5 626 541 622 870 573 569 872 381 426 627 674 383 673
 628 448 129 352 386 618 638 437 432 652 388 866 444 583 442 145 159 662
 491  19 134 659 647 389 439 452 140 143 118 154  22 871 374  18 868 436
 934 780 935  21 658 371 363 677 131 667 657 670 671  20 864 776 778 777
 775 665 779 676 932 931 292 695]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.1881
INFO voc_eval.py: 171: [430 384 389 220 135 513 520 433 108 391 410  18 427   0 306 144 417 517
 452 234  35 297 250 286 380 282 431 263 222 249 315  50 439 477 105 351
 381 516 434 483 340 299 146 312  37 188 283 318 143  32 225 273 148 529
 145 184 107 254 307  10 429 428 285 219 376 278  36 221 213 531 162 122
 382 223 293 111 305  53 396 123 377 515  85 189   2 210 535 507 317 275
 322 106 432  25 284  51 346 190 437  86 252 436 112   3 137 164 232  54
 482 259  14 472 266 136 532 390 165  97  40 485  95  66  38 169 331  39
 543 101 174 289 365 378  46 412 308 448 115 539 276 208 191 481  44 386
 339 456 534 253   6 260 350 512  15  23  89  56 287 255 261 355 435 475
  76 459  21 179  45 527 228 424 298  20 402   5 171  12   8 152 425 533
  47  29 493 121 147 528 280 258 343 542 251 257 103  30  59 446 519 292
 457  34 408  67 265 330 229 366  57 202 233  19 460 295  79 176 411  88
 304 218 311 182 413  41  24 514 536 508  16 328 238 237 371 194  80 333
 177 336 187 109 168 316 511 400  92  65 151 209 319 537 167  94 224  11
  22 116 471 349 388 383 139   1 117 444 335  13  42 387 186 302 414 102
 490 540 274 530 173 241 205 399  31 403  28 445 438  91 360 114 180 277
  33 337 352 541 269 198 447 119 118 267 256  75 175 154 374 465 373   9
 270  98 440 354 163 113  64 192   4 183  96 126 361 227 262 236 178 153
 100 451 538 489 458 157  74 313 320 492 409   7 325  60 140 497  78 345
 479 518  61 504 392 272  27  72 271  55 268 172 155 246 141 196 264  82
 469 203 104 110  26 338 441 300 217 450 416 442 206  87 327 415 453  63
 296 290  71  69 449 526 134  83 310 303 525 463 379 358 150 243 309 468
 235 226 494 510 486 149  93 484 341 215 406 500 462 281 207 499 242 501
 326 142  58 476 214 159 506 398 395 279 130 120 170 240 393  62 498 181
 488 370  90 478  68 248  99 344 288 244 314 356 166 211 323 332 230 329
 200 193 185 367 125  84 124 405 503 473 334 407  70 394 404 231 342 385
 324 466 216 401 321 544 212 496  48 138 133  77  52 487  73 161 421 397
 364 199 474 160 201 363 291 372 505 294 301 480 353  17 368 158 197 357
 495 204 245 419 454 195 375 491 502  81 156 128 369 359 362 247 348 464
 470 467 524  49 127 461 132 129 131  43 455 521 420 522 423 422 418 239
 523 347 509 426 443]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.2214
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.2551
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.192
INFO cross_voc_dataset_evaluator.py: 134: 0.345
INFO cross_voc_dataset_evaluator.py: 134: 0.166
INFO cross_voc_dataset_evaluator.py: 134: 0.207
INFO cross_voc_dataset_evaluator.py: 134: 0.298
INFO cross_voc_dataset_evaluator.py: 134: 0.419
INFO cross_voc_dataset_evaluator.py: 134: 0.268
INFO cross_voc_dataset_evaluator.py: 134: 0.116
INFO cross_voc_dataset_evaluator.py: 134: 0.272
INFO cross_voc_dataset_evaluator.py: 134: 0.174
INFO cross_voc_dataset_evaluator.py: 134: 0.269
INFO cross_voc_dataset_evaluator.py: 134: 0.157
INFO cross_voc_dataset_evaluator.py: 134: 0.195
INFO cross_voc_dataset_evaluator.py: 134: 0.438
INFO cross_voc_dataset_evaluator.py: 134: 0.487
INFO cross_voc_dataset_evaluator.py: 134: 0.343
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.138
INFO cross_voc_dataset_evaluator.py: 134: 0.188
INFO cross_voc_dataset_evaluator.py: 134: 0.221
INFO cross_voc_dataset_evaluator.py: 135: 0.255
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.342s + 0.026s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.351s + 0.040s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.352s + 0.039s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.354s + 0.038s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.364s + 0.040s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.363s + 0.040s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.362s + 0.039s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.361s + 0.039s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.361s + 0.039s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.360s + 0.039s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.362s + 0.039s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.365s + 0.039s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.367s + 0.038s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.488s + 0.038s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.370s + 0.036s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.365s + 0.036s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.365s + 0.039s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.361s + 0.040s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.367s + 0.040s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.373s + 0.041s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.371s + 0.040s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.368s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.369s + 0.039s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.369s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.370s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.366s + 0.039s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.478s + 0.040s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.354s + 0.036s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.366s + 0.036s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.373s + 0.036s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.371s + 0.037s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.366s + 0.037s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.365s + 0.037s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.364s + 0.037s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.364s + 0.038s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.367s + 0.038s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.367s + 0.038s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.369s + 0.038s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.367s + 0.038s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.446s + 0.030s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.354s + 0.036s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.349s + 0.034s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.355s + 0.034s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.356s + 0.036s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.352s + 0.038s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.358s + 0.038s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.358s + 0.038s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.363s + 0.038s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.364s + 0.038s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.361s + 0.038s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.361s + 0.038s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.361s + 0.038s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.994s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [194 736 747 188 143 199 366 235 779 104 757 196 359 106 739 162 266 778
 800 214 311  43  90  39 173  62 325 201 595 569  38 740  71 573 378 247
 140 193 776 755 773 217 221 187  48 748 250 381 167 138 312 574  89 763
 222 675 262 245 376 111 237 368 584  37 403 792 346 238 369 341 575 567
 808 743 838 345  69 258  59 741 518  64 274 227 379 204 248 471 224 737
  34  75 334 256 571 490 309 239 370 442 324  95 275 243 374  55  80 633
 316  94 267 160 676 794 840 759 236 367 829 492 762 401 268 272 470 756
 582 713 446 604  63 264  10  60 244 375  87 656 320 768 628 716 820 711
  45 408 746 219 839 344 190 521 871 488  49 538 409  40 440 830 263 658
   3  58 372 218 241 796 593 172 607  42 599 436 310 827  66 821 572 270
 594 210 377 246 271 231 223  52 596 437 641 174 207 482 659 113 120 265
 771 789 340  82 117 329 269 581 823 780 273 249 380 336 257 832 183 788
  41 347 577 433 226  33 142 605 321 443 603 356 580 314 485 202 742  36
 129 597 434 313  50 339 735 585 156 165 308 867 681 189 407 402 203 338
  46 206  56 489 484 579 164 606 578 738 176 608 318 212  99 154 350 277
 787 168 513 612 549 255  53 473 833 519 438 745  67 517 801 806 197 723
 146 118 286 657 307 435  47 151 751 660 478  77 152  76 629 487  54 112
 232 331 589 412 405 185 242 373  65 553 192 494  79 240 805 371 640 539
 784 555 476 726 576 337  86 260 786 852 215 586 661 302 220 600 149 406
 510 472 474 480 798 727  44 441 479 103 677 602 477 439 283  51 481 332
 710 486 205 775 777 362 846 496 712  93 483 175 279 767 645 647 761 511
  61 670 404 828 475 752 184 825  57 208 493 609  68 725   6 158 851 536
 749   0  27  83 166 590 191 684 163 644 132  35 781 495 128 598 721 234
 591 541 760 512 758 803 186 322 501 195   4 568   1 491 276 774 822 213
 883 121 459 797 654 631 233 284 653 587 570 769  97 291 566  98 819 534
 150 525 799 259 389 652 330 101 870 198 343 790 655  84 826 528 323 869
 632 358 200 783 873 211 824 533 414 722 638  70 550 583 728 364 335 327
 290 611 228 522   2 714 718 610 123 588 363  92 671 546 793   5 617 673
 159 225 592 445  73 281 365 287 649 141 352 766 785 293 802 177 387 770
 444 209 724 847 216 297 295 125 557 844 753 288 854 814 105 807 868 772
 502  28 134 601 804 547 556 540 417 795 497 648 744 834 750 817 315 717
 355 643 682 131 848 147 135 729 499 119 180 845 292 317 636 353 835 500
 559 765  88 688 782 683 348  96 127 124  91 452 764 622 554 635 877 542
 674 698 850 639 280 791 880 354 720 637 730 731 618 110 565 875 754 630
 876 305 155 678 642 562 498 300 418 831 139 558 261 810  29 294 874  30
 289 503 122 650 413 662 455 669 620 879 841 383 651 666 301 732 680 460
 357 126 133 170 382 179 178 563  72 624 303 278 145 182 130 114 285 509
 634 397 551 646 148 564 144  78 516 349 878 836 849 115 100 811 719 733
 715 399 390 881 818 465 298 410 416 561 136 687 619 461 837 685 107 282
 535 815 296 430 181 812 423 679 686 506 449 326 667 809 385 153 425 448
 386  74 613 411 692 816 668 351  85 361 709   7  13 813 419 843  23 672
 505 625 304 853 108 734  81 504 391 109  26 456 842 454 157 415 508 102
 161 447 342 520 299 421 615   8 171 857 560 169 426 623 523 229 463 427
 457 453 450 531 514 319 398 530 708 116 529 360 328 396 432 395 388 621
 515 333 544 466 614 458  14 420 663  20 548 230 451 543 422 431 462 429
 424 616 468 137 467 428 863 392 469 527 552 707 384  15 855 524 394 856
 526 859 393 705 306 400 532 464 545  31 537  24  12  21  11 253 882  32
 251 627  19 701 694 507 702 695 872  25 866 860  22  18  16 626  17 861
 703   9 864 858 699 696 706 665 862 865 689 691 664 252 254 704 693 697
 700 690]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1485
INFO voc_eval.py: 171: [ 94 309  48  96  98 258 112  47 306 372 375 175 142 377 260 261 349 369
 281 232 305 259 381 382 383 143 113 227 165 373 354 267 378 289  64 107
  78 277 110  74 118  99  67 266 228 346 356 262 154 145 220  60  97 311
 264 323 319  49  81 114 263 231 308   2 257 374 119 146 106  50 177 379
 355 100 286 163 104 370 160 116 225 282  65 368 269 151 357 347 184 270
 256  18 283  53 182 144 340 183 150  52 152 149 344 324 371 329 189 376
   0 310 102 384  14 380 314 280 101  58 271  95 170 176 245  55   3 115
 235  21 313 173  51   8 174 268 288 364 321 360 345   6  15  26   7 352
 337 240 325  56  33 224 275 238 276 181 111 358 105 335 247 108 255 292
  27 109 366 298 193  19 157 331  35 221 342 103 237 117 180  66  17 273
 317 171 148 367 363 287   9 304 132 219 316 290 201 284 320 291 156  13
 301 153 167 332 322 223 233 172 185 178  54 243  57 195  28 164  16  31
 303 253 351 206 274 278   4 326 254 318 246 161 312 244 196 133 234 159
 127 333 295 338 194 272 236 279 248 126  11 299 265  10 239   5 365 302
  29 200 186 350 362 241 162 205  23  73 155  12 179  70 190  72 359 158
  32 294 168 343 242  91 285 222 330  34 128 361  63 131 198 293 250 249
 300  36 296 226 147  92 188 229 230  85 213 307 187 208  88  71 315  82
 169 141  61  30 210 204  80 215 341 216 140  22  86 207 125  59  25 327
  79 203 348 328 166 192  20 353 334 252 209  45 130  90 197 129  89  76
  87  42 214 191  39 339 199  84 336 217   1  69 202 211 212  75  24  38
 138  46  44  40  83 123 139  62 124 251 297  37  68  43  41 134  77 135
 137 218 136 122 121 120  93]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3496
INFO voc_eval.py: 171: [2091  801 3932 ... 5729 5731 5730]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1461
INFO voc_eval.py: 171: [ 604  602 1905 ... 2357 1630 3027]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1858
INFO voc_eval.py: 171: [502 309 512 464 332 301 259 497 628 595 562 591 232 305 242 291  14 334
 592 649 260  16  15 313 631 473 247 643 520 528 481 268 601  20 273 604
 500 304 265 519 302 258 463 238 310 791 240 793 251 699 333 632 274 576
 231 261 596 546 233 106 266 465 401 641 534 470 563 721  13 335 270 842
 498 522 527 246 524 253 314 820 612 501 553 115 609 271 541 589 478 547
 575 312 250 593 499 248 642 792 540 668 635 308 700 243 816 598 566 594
 606  21 272 299 627 475 141 467 529 241  22 724 295 293 133 484 392 610
 682 438 710 538 804 802 679 554 624 552 504  19 600 123 140 579 254 646
 544 509 479 640 565 503 307 515 661 827 533 655 506 796 602 599 568 797
 336 665 113 619 652 472 435 477 806  63  30 420 513 536 537 701 622 505
 219 539 648 720 441 525 252 828 184 474 222  25 107 264 760 298 603 755
 234 570 616 613 669 647 551 794 518 267 244  32 608 263 469 466 237 756
 607 818 535 620 545 644 660 220 759 235 303 841 139 770 732  17 306 590
 508 763 526 421 311 416 708 174 394  26 630 633 403  96 269 262 255 327
 510 480 597 623 611 383 654 523 638  28  92 548 736 124 402 823 670 507
 677  43 256 683 412 511 765 198 385 614 164 569  27  18 758 846 415 754
 428 711 564 629 636 211 425 521 757 205 384 634 517  77 419 514 427 144
 249 821 653 516 389 122 639 637 680 483 236  24  23 128 382 645 210 297
 824 840 142 581 798 330 530 618  49 617 819 281 549 835 814 167 666 125
 202 753 831 203 482  29 712 664 471 239 626 657 587 725 573 145 409 468
 245 650 817 542 180 204 532 615 707 406 621 667 656 119 114 550 217  31
 223 543 713 795 662 651 531 697 830 345 422 555 404   2 625 437 120 722
 153 447 143 292 586 418 166 567 658 605 148 127 135 328 362 136   6  44
 426 388 764 695  38 734  48 476 395 748 183  93 112  85 659  53 743 200
 685 580 766 688 807 436 439 704 285 663 771   4  33  54 221 179 355  61
 154 702 209 111 772 809 300 169  86 726 672 149 693 767 165 296 832 393
 121 417 181 214 423 410 294  76 381 731 150 582 218 813 137 206 126 146
  37 173 745 348 116 424 172 182 213 411 354 277 739 459 730  67 349 400
 706 790  41 156 359 151  95 130 207 105 815  47 185 275 572 440   8 825
 147 132  36 719 350 391 152 838 379 684  39  71 390 276 789 811 691 319
 176 829 343 577 286 728 155 703 451  81   3 102 131 117 735 801 803 196
  42 698 487 129 138 709 407 727 716 118 186 187 212 195 109 178   0 413
 449  91 845 673  90 110 715  78 738 584 676 363 686  35 742 493 578 347
 280 733 583 723 369 453  94 588 747 405 445 574 557 199 171  97 365 837
  79 692 175 103 847 836 208 571 356 714 585 744 170 290 446  83 705 168
 216 351 450 444 283 278 448 414 805 352 108  74 717  64 342 750 288  89
  70 433 494 408 323 844 225 378  46 822 718 397 843 729 370 749  80 740
 689 496 834 674 361 681 556 344 741 443  34 491  82 215  88 430 737 461
 346 687  56 226  60 678 224 364 201   7 746 455 826 340 690 462 812 360
 561 559   9 279  69 230 490 486 457 387 454  65 799  72 485 460 289 452
  58  40 458 380 833 839 104   1 558 456 696 257  73 800 329  59 431 134
   5 694  66  11 194 808 768 769 398 442 396 353 341 322 190 489 761 228
 762 162  68 193 399 386 810  51 432 751 671 282  62 326 784 492 160 675
 488  57  55 495 320 367 560 284 188 368 325 366  52 429 377 324 787 287
 159 161  87 316 321  84 318 227 192  75 317 191 785 357 197 337 177  45
 189  50 434 774 376 788 229 163 752 331  12 783 373 786 315 779  10 374
 338 776 358 339  99 158 375 780 371 157 777 101  98 100 775 781 372 778
 782 773]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.2906
INFO voc_eval.py: 171: [329 129 337  82 333  80 128  92 356  66 165 136 130  33  83  81  72  84
 132  40 152 217 115 328  98 162 334  69 288  76 338  78  37 140 123  67
  86 340 300 156  95 332  89 144 131 164 360 311  34 314 331  41  22 285
  30  38 289  71  68 344  27 339 209 135 306 348  75  79  25 215  21 330
 133  70  59 342  96  58  77 158  28  74 104 101 142 204 284 310 194 116
 287  26 148 240  73 211  43 366 296  32 341 103 364 270 303 286 214 220
 143 353  90 195 145 307 146 163 210   3  91 324 228 349  51 157 134 160
 167 159 363 355 213 301 335 161 188  85 199 313 315 203 312 262 216 155
 350  46 138 265 225 224 230 205 125   9 336  16 242 372 218  97  45 354
 244 299  55 184 280  93 169 367 323 302 343 345  42  99 174  49   8 281
 309  88 322  36 127 154 282 266 365 305   2 294 319 304 308 368 166 291
 207 198  23 139 206 292 102 320 370 298 316 100 114 268   6 264 290  94
 118  35  87  56   7 233  44 113 208 168 293 181  65 321 111 110 263 297
  31 176  53 283 361 317 295 237 187 105 318 196  20 250 369 362 261   5
 236 120 234 117 238 359 219 256 241 197 351 212 226 249 252 112 149 259
 243 121 201 245 173 106 229 357 326 352 260  47 232 235 171 193 223 227
 180 108 221 255 185 202 239 200  62 327 107  24 124  13 231 346 147   4
 177 109 273  63 222  39 277 325 272 253 274 150 246  54 151 122 179  60
 257  29  64 267 269  14 254 137 247  57 271  12 248 358  61 275 189 251
   0  10 141 170 258  15 347 153 186 371  17  48  50  52 278 178 119 126
 172 175 192  18 276   1 191 182  11 190  19 183 279]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4081
INFO voc_eval.py: 171: [1585 1501 2181 ... 1908  724  538]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2573
INFO voc_eval.py: 171: [ 83  82 202  72 157 223 106  78 107 170  75 168  20 153  76 203  85  10
 225  77 227  17  71 140 199 109 169  81 119 167  84 138 224 173  39  13
  69  54  31 141 143 213  51 139 108 110  49  19 212 215 194  98 120 158
  52  42 226  57  18 156 105  74 142 190 174 207 124 125  22  47  14 185
  11  62 201   8  33 121  30  56  15   6  25  32  89 177  45 211 189  12
 179 180 191 100 132  68 184  38 181 193 178 118  29  53  16 102  99  70
  55 165 159 182 221 144 176   9 195 220 210 188 217 160 198 145  90 123
  26 117 122  21 146  63  35 218 172 222 175 101 219 133  59 171 214  93
  41  34  58  73 131  65  87 103 166  94  50   7 186 187  66  44  95 164
 134 197 204  92 196 155 104  86  40  48  43  67 163 130 152 208 183  28
   5  27  91  79  88   3 129  36 209 136  61 150  60 147  46 114 111 162
   2  64 135  80 154 200 161 206   0  23   1 137 149 116  96 113 126  97
   4 205  24 115 148  37 112 151 127 192 128 216]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0750
INFO voc_eval.py: 171: [4391  536 3858 ... 2784 2786 5001]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.2486
INFO voc_eval.py: 171: [405 279 558 195 545 419 283 309 278 103 554 338 105 104 506 292 550 331
 227 682 174 414 508 514 444  21 468 290 198 426 222 200 120 433 102 329
 434 552 312 297 465 286 543  61 232 300 237 432 521  39 356 322 741 192
 542 686 291 280 520 692 680   1 429 827 353 107 442 420  84 512 330 109
 510 469 193 349 196 513 178 842 299  62 828 304 214 556 876 694 296 333
 703 294 681 183 413 321 417 559 122 546 488 340 277 173 418 210  78 108
 175 181 733 687   2 425 203   3 216 572 328 878 882 339 411 163 440 423
 302 547 844  65 363 282 472 544 689 560 241 427  28  63 194  81 179 768
 740 704 850 281  15 518 565 206 385   7 522 551 685 406 185 138 509  26
 362 288 439 895 474 306 197 248 829 515 422  69 182 342 819 246  60 170
 568 327 247 382 471 207 308 843 833 180 683 726 563 769 231 204 610 688
 875 324 289  17 157 301 549  64 879 144 782 696 221  77 371  55 834 524
  52 489 217 851 528 127 317 367 303 191 838 695 212 130 410 211 841 438
 313 567 184 364 495 527 884 470 580 575 264 335 443 424 311 830  97 459
 852 835  67 891  72 523 172 836 608 498 793 225 177 890 758 219 467  95
 352  80 240 285 168 845  87 365 323 849 142  29 373 745 630 171 899 146
 412 441 877 519 129 176 557 788 298 525 314 366 880   8 245 287 158 388
 320 431 261 376 358   5 743 566  32  36 224 797 771 452 190 548 676 213
  74 822 334 477 226 839 186 348 234 747 744 316  35 529 900 862 639 888
   6 454 555 697 165 901 482 124 490 571 273 553 847 893 824 677 848 578
 389 462 894 199 110 760 846 436 209 818 698 507 481 831  34 684 892 583
  12 137 437  73 229 710  42 569 837 368 435 773 516 250 728 355 428 823
 135 143 742 473 336 379  53 326 491 511 789 883 693  51 391  14 260 873
 401 149  22 840  43 478 832 318 112 817 770 259   4 746 151 257 690 897
 706 139 480 632 415 642  71 825 421  66 564 141 700 310 826 501 766 672
  45 500 293 675 886 896 783 254 147 701 691 599 705 646 243 534 613 133
 885 164 253 889 774 239 796 619 228 408 344  16 343 858 816 455  96 201
 387 393 860 638 319 305 307 430 150 383  44 223 315 270 778 140 407 416
 709 159 325 275 874 332  76 863 767 702 730 732 359 484 631 777 123 350
 854 596 347 496 238  25 859 907 636 673  57 466 674 494 145 295  50 881
 131 737 898 868  13 475 154 606 450 451 634 722 869  56 341 493 255 113
  19 409 284 801 909 378  20  23 718 268 162 601 579 864 865 244  11 517
 526 386 476 792 605 156 499 561 887 648 233 155 357 598  75 729  54 345
  59 762 775 719 486 600 650  82 483 166 346  58 586 263 914   9 577 351
 502 271  24 218 623 615 487 714 117 106 497 235 611 609 121 649 712  41
 215 759 375 458  10 390 161  40 380 790 337 372  33 167 479 614 370 776
 392 361 855 369 570 160 699 532 354 492 126 148 242 908 637 169 647 252
 360 394 485 679 267 153 643 396 384 772 871 911 803 205  27 872 791 711
  89 395 152  91 856 604 720 603 597  38 256 641 588 671 592 374 798 258
  88 398 251 265  86 857  92  31 748 595 910 607 678 202 866  93 780 266
 274 276 763 800 125 236 735 400  30 867 136 853  37 272 715  70 906 119
 786 208 802 262  18 861 761 230 765 764 100 633 870 784 402 635 731 640
 739  79 562 716 533 794 249  47 756 618 397 269 404 734 377 220 787 659
 617 795 574 779 403 453 132 134 736 582 645 820 457 461 381 116  98 644
 799 399 620 464 913 101 657 738 535 781 713 616 537 912 752 460 785 750
 539 757 622 725 585 749 707 602 621 652  90 806 111 530  85 754 189 538
 902 456 118  83 815 612 628 531 128  94 624 590 813 505  68 655 717 540
 581 805 904 723 804 504 905  46 536 811 666 660 576 751 755 573 807 753
 812 724 814 188 653 665 708 594 727 591 445 721 625  49 809 187 593 115
 463 654 808 626 810 668  48 656 114 589 661   0 662 587 503 651 584 669
 541 670  99 664 627 658 667 663 821 448 629 446 449 447 903]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1134
INFO voc_eval.py: 171: [ 904 2074 1261 ... 2304 2308 2310]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2618
INFO voc_eval.py: 171: [192 276 293 306  95 159 195 167 510 499  91 254 512 319 156 212  84 161
 309 329 164 257  85  99  88 288 107 270 232 304 286 283 250  28  92 287
 103 191 277  22 105 193 514 230 213 465 403 209 255 124 251 247 335 296
  87 199 463 313 297  35 202  83  15  21 109 334 272 300 256 333 231 326
 469 200 104 112 379 399  36 310 203 271 284 506 482 229 502 360 122 480
 474 471 248 374 135 278 472 461 369  30 395 126 470 101  24 476 501 346
 163 407 123 397  86  27 121 466 158 106 473 508 258 402 444 378  26 273
 372 367 127  72 509 478 373 311 162 252  29 322 489 327  96 166 239 303
 359  42 102 483 228 125 477 451 131  98 376 491 401 210 197 411 521  97
 242 111 244 387  48  49 134  34 157 243 462 292 137 253 490 377 468  25
 440 464 438 492 153 519 485 245  32 216 308 448 215  37 289 449  38  76
 479 246 406 493 148 275 282 488  74 175 353 165 181  17  64  51 496  46
 421 447 383 398 460 529 410 155  75 370 136  61  71 328 467 356  31 453
 337  73  23 110 279  94 285 172  90 404 355  93 237 305 160 481 408 100
 475  47 198 149 128 405 145 108 500 497 115 371 439 211  16 362 420 357
 194 384 495 503 484 412 396 129 386 238 365 133 201 188 527 274 424  89
 290 130 312 486 450 138 400 351  58  44 422  20 392 517  81 432 151 459
 363  79 196 354 352 176 364 190 316 233 189  52 487 413 180 331 225 423
 494 380 409 524 147  68 280 143 338 298  50 214 375  33 429 394 443 525
 332 391 227 132 504 522 281 415  80  77 350 507  78 436 139 425  63 168
 174  18 235 169 433  43 418  62 314   9 427 336 341 249 330 320 119 437
 206 150   5 184  82 179 361 152 419 452 154 516  45 513  19 142 382   4
 345 381 262  39 177 236 389 234  65 339 141 518 358 301  57 366 442  53
 171 267 218 219  70 178 417  69 120  54 116 140 523 528  10 416 340 520
 388  66 295 456 185  67 385 414 393 259 390 325 173  56 342 318 217  55
 205 117 511 170 204 526   6 146 315 266 446 208  40 515  59 291 182 260
 428 241 317 264 321 445 118 434 223 454 344 144  14 505   0 268 207 430
 265 426 324  11 299 368 343   8 240 457 261   7 307 187 302 435 294 263
 431  12 226  13 269 224 183  60 455 186 220 221   3  41 222 113 348 347
 441 349   2 114 323   1 458 498]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1073
INFO voc_eval.py: 171: [452 973 109 ... 339 338 244]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.1658
INFO voc_eval.py: 171: [198  31 108 103  16  30 184 201 105  29  60  34  32 193  39 109 203 137
 100 111 202 188 194 183 116 113  63  14  56  36 173 192  54 135  41 156
 110 191  73  55 205 114 106  17 174  33  95 189  65   7   2  44  35 185
 104 178  90  64 160 142 125  94  92  86 122  15 187 118 199 154  89 112
  19 196  50 143  68 145  70  69 148  40  21  83  46  20 107 206  42  62
 200 190  59 169 170 115 207  61 155   6  49 176  52 141  18 179 117  37
  12  48  91 147 195   9  13   8  38 102  23 150  87 126 136 157  84  67
  22 177 146 171 134   1 163  85  10 129 166 138  11  76 121 180 181 167
 186 197 204  58 124  43 172 120 168 164  24 159 130 175  45 152  82  80
 128  72  79   4 153 139 151  78 149  81 144  74   3 140 165  51  75  88
 133  57  47 158 131 123  53 161 162  71 127  66  27 119 132   5  77  93
  28 101  98 182  99  26  25  97   0  96]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4635
INFO voc_eval.py: 171: [ 4637 18973 17404 ... 10891 20374 20375]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.4234
INFO voc_eval.py: 171: [ 212  278  213 ... 2014 1213 2020]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.3197
INFO voc_eval.py: 171: [ 57 311  38 157  45  79 329  24 140 586 336 607 575 302 606 331 142 343
  52  33 325  27  75 146  35 145 307  40 576 596 580 148 147 348 534 310
 161 178 591 579 678 588 144 153 608 170 151 667 354 330 162 581 595 577
 349 112 113 602 639 155 480 304 642 430 309 317 312  63  72 173 175 133
 673 115 351 323 647 166 516 640  42 306 676 643 571 116 584  77 322  70
 320 598 590 557 555  81 109 658 365 664 334 149 431 321 353 141 233 645
 342 221 543 373 346 605 638 663 143 335 337 279 338 319 308 669  78 258
 651 428 681 665 111 160  49 594 328  54 680 518 641 176 150  32 301 515
 429 154 163 648 324 256 432 666 650 610  65 549 316 315 318 528 563 649
 152 519 222 652 660 583 344 313 374 661 327 487 340 558 339 482 303 546
 525 603 314 425 164 674 533 529 135 671 350 273 134 675 137 601 562 668
 259 659 110 332 524 644 171  97 333 672 382 656 378 550 520 526 486 646
  64 156 587 226 326 426 657 653 136 364 132 345  29 114 260 352 654 189
 542 670 210 341 677 179  47 517 208 209 213 347  28 545 139 169 552 305
 679 637 662 559 211  34 203  13 138 592 130 168  74 371 611 370 531  10
 483 582 359 585 553 182 655 481 568 188 485 484 180 185 181 288 458  67
 267 427 274 225 540  26 561 578 500 530  23  11 291 379 560 522 440 541
  99 384 287 441 539 200 527 224 289 129 292 468 693 174 118 356 255 172
 600 367 204 227 119  59 206 212 489 551 565 548  69  12 234  41  53  43
  98 275 538 465   9 547 502 207  68 570 205 202 383  25 103 514 537 523
 158  56  60 566 159 100 451 242 593  17 355 544  31 165 535 357 554 232
 253 616 228 230 467  21  37 360 131 597 409 122 512 477 186 366  80  76
 396 361 358  73 235 567 269 117 490 503 199 513 261   2 223 493 564 187
  48  50 445 556 125  55 599 220 184 257 510 469 263 101 262 532 281 505
 177  58 126 380  66 625 375 121 511 124 376 536 497 123  46 569 268 167
 437 521 284  20 183 423 229 476 231 434 444  61 102 372 498 452 488  62
 406 241 216 471  88 443 272 290 276 589 475  30  44 244 362 413  51 368
  39 280 277 218 499 381  71 283 377  92 501 414 282 449 507  36 508 215
 363 285 573 369 626 618 217 454 191 617 266 271 466 238 682 265 572 296
 630 479 254  18  84 403 286 249 470   7 496 264 386 128 509 278 397  87
   0 504 473 390 385  85  16 219 433 270  82 106 250 439 418 474 492 410
  94  90 615 387 237 472 295 609 491 438  15 627 628 105 436 629 624 127
 574  14 240 246  19 456 401 108 604 201  22 300 613 446 297  91 435  89
 405 245  83 612 236 459 457 442 239 120 417 243 460 506 104 247 614 251
   1  86   8 448 455 447 450 248 462 461 399   4 298 107 464 214 407 453
 394 400 416 392  95  93 404  96 388 391 193 389 252 422 415 633 395 463
 393 634 622 198   6 632 299   5 419   3 420 636 631 194 635 620 621 424
 421 402 196 398 412 195 411 408 197 495 623 688 494 686 190 687 692 619
 690 192 293 691 689 684 478 683 294 685]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2006
INFO voc_eval.py: 171: [ 77  11  83 281  82 398 185 282 279  12 498 400 265 405 144 278 151 403
 404 490 337 494 147 283 175  87 401 277  90  89 449 340  23  78 336  85
 205 280 108 485 453 276 339 239 248 210 146 334 179 402 399 116 397 193
 396 148 177 407 455  80 216 266 262  14 150 328 395 228  79 338 109 394
  86  16 406 366 454 364 233 393 156 157  22 124 143 264 363 149 300  35
 238 243 285  21 198 314 294 132 232 456 126  27 365 119 381  84 343 231
 229 107 357  81 436 184 271 142 488 342 201  18 359 493 341 189 106 370
 335 489 153 268 474 195 319 501 181 222 448 244 486 499 155 196 270 154
 128 487 361 211 237 125  33 263 386 389 287 131 491 140  41 180 227 152
 472 367 475 246 358 127 190 247 261 145  68  40 362 242 183  20 369 188
 230  61 356 446 269 360  47 112 304 139 171 387 473 295  17 368 254 315
 350 192 206 257 497 110 352 267 253 241 495  70  34  76 194 255 476  60
 240 502 129 332 130  53 462 299 351 214 316 479 478 209 225  37 186 298
 320 191 286 480  15 292 187 245 438 256  69  95 221 178 288 450 102 226
 496 207   2 500 182 437 220 447 291 224 451 213 296  10 169  91 322 176
 197 374 103  39 375 199 452 293 113  44 477 390  71 303  49 202 331 297
 117 200 170 373  46 433  19 392 249 250 464 111  75 458 484  92 333 463
 385  73 422  13 344 218 204  32   9 284 321 504  63 118 274 460  51 123
   1 174  74 384  94  93 208 466 120  72 307 325  29 219  45 289  30 290
 372 136 327 388 326 252 115 310 251 235  31  36 330 459 203 311 121 324
  50 465 318 306 101 114 135  48 481 391 461 467 323 133 301 141 217 212
  64 272 105 236 317  43 104 215 468 223 329 503 134  62 432  52  42   4
 469 430 445 428  38 122   0   6 305 492 173 309  65 380 483  66 172 482
 302 435 308 431  28 471 273 383 379 371 160 427 442 423  55 457 234 439
 382  88 424  25 378 377   8  54 429   5 440 354 161 419 470  67 441 260
 164 426 444  57 418 158   3   7 443 421 425 259 353  96 434 420 355  26
 159 258 163 168 162 275  59 165  24 100 413 166 313  58 312  56 412 167
 410  98  97  99 408 346 409 411 345 137 414 347 376 348 417 416 415 138
 349]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1143
INFO voc_eval.py: 171: [543 541 552 298  60 346 542 195 548 554 348 549 588  54 302 692 105  62
 544 109 154 349 299 723  53 273 782 308 261 573 547 363 690 167 172 697
 344 306 199 265 699 729  61 788 411 103 101 202 784 262 556 555 270 802
 157 796 297  57 266 102 359 545 263 198 564  65 311  74 203 551 158 483
 798 300 801  51 647 351 634  59 791  77  92 345 165  76 115 741 806 267
  22 186 696  71  26 452  91 350 737 305 106 282 734 117 792 193 456 693
  58 169 787 616 691 108  86 269 168 162 706 662 636 352 592 151 589 113
 276 410 129 622 793 197 159 797  64 208 725 184 321 328 215 486 412 371
 704 313 499 550 772  63 181  66 360 212 264 275  83 235 695  79 463 565
 280 661 307 112 567 248 473 546 156 562 735 733 171 358 786 100 366 287
 789 161 170 626 205 627 304  24 803 740 644 632 180 474 709 651 738 783
 201  69 778 209 355 598 649 721 242  88  72 621 445 177  20 609 160 581
  40 472 369 732 246 327 317 207  90 805  45 187 557 635 150 281 785 278
  82 361 726 147  39 229 457 288 731 724  68 559 631 794 799  80 439 155
 189 807 133 152 620 357 618 173 485 689 727 291 701 491 759 301 790 711
  89  78 163 591 331 114  52 178 800 164 249 310  21 716 244 216 498 148
 274 619  75 558 471 477 617  37 681 703 325 370  99 760 479 461 652 623
 230 140 638 245 111 763  87 217 104  96 268 804 149 414 373 381  84 231
 372 218  94 192 153 289 795 489 730  30 466 478 418 710 329 137  48 777
 398 194 174 587  85 766 496 225 233 468   7 420 524  32 669  18 384 214
 188 283 660 284  35 272  29  55 123 480 292 475 771 196 519 259 768 708
 705 571 206  47 343  27 462 533 256 687 183 433 444 816 347 817 126 211
 613 365 442 397 590 204  56 629 640 176 642 526 144 643 576 525 247 354
 166 600 459 599 814 685 707 252 353 314 665 191 713 628 507 450  93 522
 286 279 744 409 139 368 221 364  34  81 718 476 213 200  49 121 210 179
 602 356  95 400 403 107 132 241 143 374 633 402 684 251  33 779  17 448
  50 714  97 271 603 770 376 257 438 250 686 527 341 237  19 427 318 227
 815 130 657 290 488  43 712 322 529 504 670 481 594 182  70 650  67 490
 601 293 367 743 568 775 303 722 238 484 116 362 521 728 503 694 612 596
 405  73  98  25 260 146 324 668  23  44   1 392 467 508 232 819 224 679
 764 185 494 294  28 419 769 243 509 715 469 667 606 399 615 443 648 639
 277  38 254 747 719 765 671 395 630 523 401 377 220 296  42 818 776 432
   5 441 773 656 566 255 389 761 767 234 219 460 487 285  31 702 138 228
 295 666 454  16 532 380 605 664 190 762   4 752 625 739 560 175 464 780
 781 497 624 720 535 645 142 604 506 440 120   9 540 141 582 413 608 447
 319 611 637 698 774 646 431 593 653 700 654 253 375 421 434 258 641 339
 570 236 455 424 415 682 449 515 751 717 436 451 417 610 145   0 223 407
 614  15 110 561 586 425 458 500   6 131 404 222  41 453 408   8 423 595
  14 312 416 572  46 516 597 683 422 316 753   3 678 406 429 465 430 757
 528 492 537 342 680 386 136 663 240 396  36 580 755 736 746 226 673 510
 607 337 658 437   2 435 672 127 553 750 239 326 446 340 118 393 518 531
 655 745 517 320 808 812 758 742 688 536 493 332 379 756 134 333 470 495
 659 428 512 501 534 513 538 122 124 754 383 388 520 315 128 385 309 119
 511 482 125 575 378 323 338 391 335 530 539 502 334 514 387 135 394 426
 569  11 336 382 813 563 677 330 577  12 811 574  10 390 505 578  13 584
 585 583 748 579 749 674 676 675 810 809]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.1845
INFO voc_eval.py: 171: [406 368 410 352 493 481 209  17 390 136 367 121 491   0  97 394 403 407
 487 275 265 227 131 409 260 447 211 411 234 287 132  46 249  36 170 174
 451 255 414 236 389 356 384 421  95 358 492 499 311 258 199  18  50 322
 351 408 277 212  10  38  34 296 107 405 246 353 483 135 182  19  94 412
  49 292 372 262 404  51 259 378 502 213 148 208 220  14 257 235 293  96
 271 176 195 101 266 127 179 228 450   3 288 203  37 508  79 441  31 301
 307 310 150 454 191  82 133 240  89 122 218 109  71 225  58 113 366 161
 370  32 445  13 286 355 374 363  39  15 396 477 125 449  45 137   2 387
 151 336 215 362 314 210 354 458 471  86 141 506  92 369 244   6 321 512
 420  64 298 166  67 253 480 241   4 106 482 105   7 488  65 156 178   5
 157  56 169 364 284 261 303 501 245 247 163 393 485 224 104 183  44 270
 177 329  60  76  22  99 429   1 313  75 511 290 505  81 138 263 360 379
 401 375 446 237 430 272 509 306 361 274 242  43  41 503  77 285  66 171
  42  25 108 140  26 189 167 159   9  20 216 309  35  12 248 418 432 190
 155  48 193 280 300 507 205 204 392 281 158  80 152 344  11 254  40 238
 500  91 194 460 128 419 139  47 153 504 207  21  30 382 229  85 422 297
 160  55 316 413 440  90 498 423  73 373 305  61  83 330 239 129 470 325
 268  74 206 431 479  93 168 453  87 415 304 146 130 226 510 102 338  24
 222 380 332 371 111 464 308 328 326  78 452 149 273  52 276 318  33 435
 320 251  70  98 250 424 279  23 269 175 103  88 357 172 475 425 489 112
 267 365 291 252 294 165   8 350 465 289 463  84 438 197 377 120 231 243
 164 486 144 426 359 184 312 173 427 117 134 383 469 397 416 264 490 317
  54 474  63 472  69 315 110  53  62 198 299 201 455  59 219 417  57 295
 196 180 331 456 457 283 154 302 443 343 162 339 147 484 468 462 402 100
 398  68 217  72 376  27 192 334 323 433 232  28 278 187 381 221 124 126
 386 388 448 223 333  29 143 395 439 123 185 473 399 282 256 188 444 437
 442 385 497 200 142 145 391 337 181 214 186 340 347 461 459 467 202 348
 346 349 327  16 342 341 324 466 345 335 115 400 476 118 119 436 114 116
 233 434 495 319 428 494 230 478 496]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1685
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.2316
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.149
INFO cross_voc_dataset_evaluator.py: 134: 0.350
INFO cross_voc_dataset_evaluator.py: 134: 0.146
INFO cross_voc_dataset_evaluator.py: 134: 0.186
INFO cross_voc_dataset_evaluator.py: 134: 0.291
INFO cross_voc_dataset_evaluator.py: 134: 0.408
INFO cross_voc_dataset_evaluator.py: 134: 0.257
INFO cross_voc_dataset_evaluator.py: 134: 0.075
INFO cross_voc_dataset_evaluator.py: 134: 0.249
INFO cross_voc_dataset_evaluator.py: 134: 0.113
INFO cross_voc_dataset_evaluator.py: 134: 0.262
INFO cross_voc_dataset_evaluator.py: 134: 0.107
INFO cross_voc_dataset_evaluator.py: 134: 0.166
INFO cross_voc_dataset_evaluator.py: 134: 0.463
INFO cross_voc_dataset_evaluator.py: 134: 0.423
INFO cross_voc_dataset_evaluator.py: 134: 0.320
INFO cross_voc_dataset_evaluator.py: 134: 0.201
INFO cross_voc_dataset_evaluator.py: 134: 0.114
INFO cross_voc_dataset_evaluator.py: 134: 0.184
INFO cross_voc_dataset_evaluator.py: 134: 0.168
INFO cross_voc_dataset_evaluator.py: 135: 0.232
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.387s + 0.038s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.353s + 0.042s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.354s + 0.041s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.353s + 0.040s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.356s + 0.040s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.359s + 0.040s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.362s + 0.039s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.362s + 0.041s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.361s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.361s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.362s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.364s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.367s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.618s + 0.047s (eta: 0:01:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.403s + 0.042s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.384s + 0.039s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.361s + 0.039s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.362s + 0.039s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.365s + 0.040s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.367s + 0.040s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.365s + 0.040s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.365s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.369s + 0.039s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.368s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.368s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.366s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.356s + 0.029s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.358s + 0.035s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.368s + 0.037s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.377s + 0.039s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.375s + 0.039s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.372s + 0.038s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.369s + 0.038s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.366s + 0.038s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.368s + 0.039s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.365s + 0.039s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.366s + 0.038s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.368s + 0.038s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.368s + 0.039s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.631s + 0.046s (eta: 0:01:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.384s + 0.042s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.377s + 0.040s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.359s + 0.037s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.361s + 0.038s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.357s + 0.040s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.358s + 0.041s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.357s + 0.040s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.358s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.360s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.356s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.359s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.358s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.292s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [711 190 744 103 181 356 232 137 713 206 734 725 715 193 215 343 144  66
 211 370 246  99 264  67 302 195 243 367 216 776 581 169 298 357 233  42
 236 360  44 179 213 777 297 398  40 143  89  80 733 219 366 242 737  50
 561 361 237 717 727 557 185 720 749  75 571 568 716 134  65 333 783 108
 258 712  45 331 394 746 769 560 768  91  94 741 803 321 461  39 262 156
 199 255 617 480 315 653 214 595 335 731  68 478 351 563 271 299 267 359
 235 736  53 266 745 506 339  34 312 265 565 249 373  60  64 368 244 311
 439 527 202 228 808 358 234 440 591 613 212 763 460 743 268  76 167 523
 241 183 365 296 654 680 508 559 471 641 765 684 723  43 722  63 338 430
 682 330 102 586  97 160 186 802 263 337 336 269 626 772 270 164 300 584
 799  15   4 795 120 757 256 579 791 429 838 564 775  49 433 752 222 476
 328 427 188 196 205 735 301 640 790 135 428  37 760 322 142 317 404 800
 562 145 313  57 273 764 342 567  51 401 755 465 431  54 250 374 372 248
 402  90 207 221 596 363 239 127 325 323 474 152  72  73  47 245 639 369
 541 168 165 155 113  41 545 154 649 804 240 153 364 283  61 139 507 589
 180 438 714 695 688 307 400 633 580 729  52  92 598 217 198 569 766 306
  85  59 346 642 728 348 197 434 362 157 238 546 177 151 397  48 110 472
 758 779 587 709 159 257  98 482 464 748 499 192  55 126 395 614 732 638
 590 260 477 473 230 140   5 469  56  71 597 679 466 292 750  58 624 751
 189 659 329  46 210 693 274 259 399 184 467 462 698 793 726 710 805 585
 319 651 316 771 475 223 505 721 470 629 481 761 528  62 655 556 575 483
 498 251 203 375 209 780 572 593 516 201 218  93  95 582 324 117 797 200
 623 681  86 753 407 583 697  69 468  74 116 531 396 463 686 542 762 747
 740 778 178 479 592 594 511 794 371 247 840 558 573 782 798 435 724 231
  82 509 738 436  83 182 497   1  87 616 621 662 739 224 288 344 128 742
 809 781 277   0 194 347 773 630 448 204 282 620 220   6 634 849 767 792
 314 530 129 340 272  84 519 577 327 677 837 636 133  26 637 730  38 687
 490 383 796 547 405 488 536 806 789 191 841 484 839  27 759 118 114 770
 850 652 618 836 309 287 588 842 650 566 601 576 843  78  31 807 125 537
 818 104 122 208 489 719  70 187 171 437 289 578 685 173 821   2 718 822
 690 352 754 774 101 756 353 549 661 485 406 318 166 229 305 570 574 520
 812 552 354 820 381 599 115 408 692 123  96 811 538 303 132 107 106 432
   3 261 285 696 276 625 615 627 619 170 355 310 816 815 487 702 845 130
 149 148 281 119 332 699 286 492 334 610 280  77 689 691 700 656 124 704
 846 377 787 635 341 670 694 647 290 414 494 138 121 628  32 105 622 275
 451 548 444 844 378 703 632 412 486 308 847 100 350 376 631  36 417 278
  35 409 175 605 450 174 550 403 176 131 493 385 813 136 294 660 379 555
 657 814 410 604 658 420 345 524 683 279 705 503 456 452 533 387 554 162
 784 284 449 291 708 491 446 458 707 421 504 801 522 304 663 706 150 158
 678 785  88 539 648 551 415  25 553 382 513 172  79 603   7 326 534  22
 788 786 146 225 602 111 389 141 423 510 517 384 109 643 606 501  12 419
 608 147 320 810 161 416  81 447 349 442  24 411 413 163 518 824 823 393
 819 828 540 445 443 817 495 422 543 500  17 418 600 515 496 607 380 609
 529  14 514 502 390 454 833  20 391 392 293 532 512 388 521 676 386 457
 227  21 535 425 426 226 827 424  18 544 526 825  16 525 112 441 453 829
  10  13   8 295  11 674   9 701  29 644 459 848 672 673 455 611  23  19
 612  30 667  33 831  28 832 664 253 252 830 668 834 675 835 666 671 645
 826 254 665 669 646]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1351
INFO voc_eval.py: 171: [ 80 300  37 372  89 241  36 297  83  81 128 365 341 244 162 361 263 367
 209 243 374 296 206 366 364 101 371 373 242  65  59 273 127 344 151 369
  39  40  99 251  97  68 247  86 131 260 340 368  94 210 348 250 246  84
 302 130 299 208 307 248 240 215  85 346 133   3  69  96  90 375 245  87
 134 272 268 164 370  38 132 351 353 362  95  43 139 313  88 376 264 172
 176 363  15 169 349 335 136 301 255 168 129 257 269 211   0 281 303   7
 143  92  82   4 356 254  17 138 304 343 336 253 163 308 155  98  91 256
  54 278   9 229  48  12 332  28 159 350 274 338 331 188 267 359 290 217
  93 352 258 324  25 310 314 140  16  66 270  13 223 100   8 275 334 276
 187  11 249 306 158 145  42 333 181 265  44 221 156  55 360 218  24  41
 113  14 141 231 317   1 318 295 179 305 161  10 160 292 166 173  51 205
  27 280 137 220 259 157 226 147 320 342 279 309  62 294 150 311 219   5
 171 216 238   6 358 262 282 170 237 288 225 233 106 194 213 319 277  20
 286  50 252 293 214 287 355 108  73 228 184 326  18 239 227  23   2 289
 207 236 148 144  63 153 116 149 142 266 175  29 224 291 261 167 110 154
 271 222  46 115 354 178 203 196 285  30 234 327 212  21 328 192  67 357
 174 109 330 202 232 135 193  77 123 230 182  26 298  75 201 315  49 325
 199 111 204  60 165  79 191  78 126 105  53  76 190  22  72 195 329  19
 322 345 183 337 152 316 107 112 198 321 124 185 312 114 146  32 347 339
 323 197 235  33 180  57 177  74 186  64 200 120  35  58  70  71  34  31
 189  61  52 121 284  47 283  56 104  45 118 125 122 119 117 103 102]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3618
INFO voc_eval.py: 171: [ 831 2166 4030 ... 5895 3261 5886]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1303
INFO voc_eval.py: 171: [1867  605  602 ... 2225 3062 3063]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1273
INFO voc_eval.py: 171: [529 322 539 491 582 320 526 674 352 273 593 627 629 654 242 676 260 323
 513 699 311  13 255 504 354  14 278 548 274 283  16 288 545 637 561 558
 572 636  19 246 527 596 560 495 501 267 837 326 251 835 609 563 279 740
 272 578 678 681 496 275 262 493 321 117  31 585 549 571 769 890 129 355
 328 538 658 691 254 324 286 576 428  12 245 330 842 281 896 259 683 579
 646 261 284 528 329 643 648 599 503 663 556 282 622 645 863 670 634 511
 669 628  20 540 641  17 547 597 316 665 851 836 844 512 156 741 247 530
 515 659 573 565 757  26 415 635 498 250 895 661 492 287 594 133 554 122
 689 770 318  22 883 269 263 155 544 253 534  21 724 632 146 461 276 317
 551 313 701 575 252 684 566 531 653 686 664 644 639 568 611 325 564 843
 264 574 717 864 631 500 570 849  24 704 514 650 601 707 806 620 353 803
 535 356 640 542  37 872 507 280 695 742 327 649 638 801 118 440 804 888
 533 277 228 494 839 404 445 265 580 546 231 194 407 433 268 662 154 441
 862 248 243 633 625 569 285 417 577  15 427 186 876 808 595 244 229 127
  29 685 559 550 751 693 598 405 716 810  33 891 586 889 139 624 438 536
 677 815 552 885  32 682 532 655 583  23 706 668 647 762 510 697 802 553
 584 107 462 778  18 782 688 120 257 721 148 205 157 256 846 600 672 258
 652 543 687 763 630 878 567 884 541 605 130 675 446 694 696 868  30 346
 429 497 657 614 703  25 216 537 506 660  34 613 651 581 249 692 841 700
 555 671 702 768 759 557 893 180 619 266  27 886 690 508 805 772 453 210
 137 705 447 667  28 750 748   6 452 402 720 755 432 189 301 444 618 182
 349 211 865 800 873 126 306 680 213 463 562 791 141  54 811 411 788 698
 752 729 679 809  49 882 656 509 159 147 418 852 673 725 144 125 499 443
 771 212 812 164 151 472 666 434   8 736 315 206 642  91 451 880 347 416
 381 779 105 179 160 505 790 401 191  50 123 747 294 209 814 454 435 365
 233  41 853 502 195 167 312 100 834  53 728 217 230 150 877  38 187 777
 615 607 406  64 857  69  62 464 879 227 295 132 773 181 448 881 403 436
  79 190 218 319 185 833 430   5 131  99 143 232 608 861 715 314 344 169
 455 730 764 165  40 138 731 372 749 431 161 426 223 134 783 170 450 219
 894 366 414 376 869 485 140 476 616 226 477 412 419 867 193 184 710 760
   3 449 135 898 152 781 761  89 215 293 792 413 767 307 606 222 158 758
 858 398 439 753 525 746 756 775 128   4  39 368 780 897  77 183 604 623
 722 168 116 588 874 887 465 850 478  86 617  95  35 289 153 602   0 203
 166 162 298 388 121 124 197 786 847 610 300 776 202 470 442 270 208 396
 192 221 787  42 612 795 732 163 743 471 290 737 291 382 774 754 363 621
 519 214  92 104 304 765 207 136 855 892 899 145 733 119 309 367 305 106
 384  71  46 339 711 235 473  52 362 220 587 875 387  43 524 739 592 108
 225 474 718 370 103 237 794 373 425 744  84 796 738  88 479 603 719 745
 142  83 224 488 437 766  48 785  75  93 870 378 240 424 467 482 364 726
 589 856 712 360 483 860 114 466  81 793   9 380  90 361 521 458 487 475
 481 480  94 188 102   2 789 296 490 784 457 420 848 854 813 310 234 807
 516 359 115 369  76  44 838  87 238  74 400 149 486 590   1 859 518  73
 900 489 177 845  61   7  60 520 343  80 377 468  78 469 591 299 866 340
 840 358  70  36 522 484 871  45 829  66  65 271  11 341 422 523 714  72
 734 713  63 727 338 410 735 397 297 303 336 383  85 456 174 421 379 342
  82 832  57 723 423  68 709 399 308 292 348 375 198  47 302 385 175 517
 241 798  58 409 408 708 830  59 345 178  97 386 357 101  51 797  98 239
 176 337 335 199 333 204 371  67 459 828 196 827  56 236 799 392 831 351
 818 200  96 201 374  10 825 110 350 393 331  55 821 172 826 389 394 113
 112 334 332 823 171 173 395 111 391 109 626 817 820 390 460 819 824 822
 816]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.2654
INFO voc_eval.py: 171: [307 118  77 329 117  76  81 127  30  64 161 120  79 311 145 119  80  75
  84  85  67 338  32  89 157 318 133 122 128 209 319 104 289 316  68 124
  35  66 312 308 322 135 309 141  31  71 275 113  28 313  82 310 320  72
 159 330 129  39  70 123 296  60 125 144  42  27  86  90  22  94 276 339
 326 200  65 272 196  41 130  69 166  73 153 291 163  25 233  26  74 271
 332 165 206 284 279 270 314 202 189 286  44 274 158 295 282  78 188 317
  88 137 346 212  96 111 315  93 105 324  92 151 155 292 298  52  87 325
 345  95 126 273 283 142  51 162 156  37 305 140   2 195 205  50 334 347
 201  21 221 344 167  53 287 197 297 278 269 208 160  49   0  33 299  59
 217 112 223 148 341 184 232 290 147 259 337 239 218 331 288 215 255 300
 304 285  83 191 294 164 180 348 262 349 351 109 280 303 256 136   9 103
 246  91 235 353  13 134 281 229 173 301 293 228   5 192 102 327 277 343
 101 257 198 152  24 236 323 204 115 302 238  38   3 108  56  45   1 107
   6 247 350  18 216 106 336  46 333 199 190 181  61 183 252 234  34 100
 132 168 139  23   8 230 231 220 335 203 154 219 226 193 254 240  98 224
 253  58 222 321 227 241 210 251  62 177 121 110  40 213 194  97 225 171
   7  29 250  47 244 214 263  57 211 149  99 207  19 265 237 340  63 242
 114 245 249 116 306 261  10  17 146 264 243  55 248  36 175 131 172  43
 342 328  54 176  16 260 178 170 258  48 150 267   4 169 143 266  15 138
 174  11  12 187  14 185  20 182 352 186 268 179]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4147
INFO voc_eval.py: 171: [1531 2180 1454 ... 1783  523  704]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2379
INFO voc_eval.py: 171: [ 82 193  83 158 213  75 106  73 105  80  74  21 210 151 212  10  70  79
 164  76  24  84 214 161 107 209 137 165  78  14 134 120 167  98  20 191
  34 157 110  52  54 185 138  35 135  22 108  81 203  72 192 122  50 139
  46  58 184  26 211 168  18 136 121  25 198  12 128  29 194  65 202  19
 183  15 201  89  43 186  36 180  57  55 101  64  33  69 103   8 190  86
   6   9 174  53 181 159 187 176 173 109 131 170 140  30  56 129 175 100
 127 172 102 205  13  77 143  60 171  49  48 208  16 206 156 207  17  11
  39  37 169 154  90  71 142  23  42  66  59 160  93 166 195  38 188 182
 163  87  94  99 104 141 126  51  31 179  92 189 150   5 116 199 177 130
  68  85  47  88 178 149  62 200  45  32   1   2  63  44  40   7   3 152
  67 115 125 196 132  61 144 113 215  91 123 162   0  27 153 119 117  97
 133  28 118   4  95 146 111 148  96 155 197 145  41 114 112 124 147 204]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0775
INFO voc_eval.py: 171: [4152  494 3655 ... 2644 4727 4728]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.2238
INFO voc_eval.py: 171: [396 540  95 104  97 283 413 188 270 338 410 529 269 273 495 676 102 219
 537 490 342 282 169 431 499  23 405 115  96 542 417 216 428 318 312 182
  35 693 303 456 427 492 224 278 348 502 293 680 277 235 186 702 453 175
 426 535  98  53 493 528 309 289 221 501 407 343 424 421 173  78 498 815
 430 505 538 100   1 118 739 317 167 291 541 527 674 419 862 190 170 705
 686 206 404 320 425  56 457 409 534  73 292 281 411 530 822 257 202 333
 271 433 248 276 681 204 287 432 353 156 816 539 473 207 351 864 203   2
 168 400 503 418  28 288 675  63  59 322  17 682   3 497 679 280  69 213
 230  55 177 176  16 295 268 545 231 819 185 809 436   6 691 561 134  45
 357 839 494 532 166 200 460 549 272 399 435 706 880 225 307 403 189 434
 331 201 869 763 867 101 279 416 513 738 371 174 687 836 458 359 536 313
 205 155 214 165 172  27 827 876 551 126 239 461 695 208  44 512 724  71
 180 310 886 598 596 871 820 247 262 863 817 184 187 677   8 764 337 481
 178  68 125 780 474 834 837 159 546 196 464 692 209 211 358 192  76 768
 218 574  72 560 824 367 514 401 301 143 243 304 306 325 164 580 311  62
 341 275 336 531 321 234 112 861 429 826 171  31 217 103 825 553 459 743
 290 696 678 833  79 832 349 299  82 161 179 485 422  30 746  54 296 300
 133 478 466 451 339 782  26 455 153 212 835 364 500 183 316  99  58 468
 808 885   9 767 619 240 533 198 141 882  81 360 194 544  60 875 504 812
 878 414  12  29 865 380 324 119 846 868 828  70 668 552  46 701 810 874
 197 689 261 374 136 627   4 284 475 236 108  66 688 251 699 346 467 559
 776 408 722  21 444 323  42 249 556 326 508 548 415 354 829 831  15   5
 823 139 406 491 507 496 740  18  65  92 267 818 879 124 814 314 132 756
 398 327 148 484 420 883 258 830 838 866 813 482 700 222 806 402  67 412
 127 392 721 158 684 131 130 604 884 255 147 858  52 297 870 334 285  10
 881 145 744 777 807 109 315 140 142 369 704 264 761 710 669 347 397  36
 568 742  25 479 138 233 856 727 368 694 450  87 772 872 821 769 510 703
 622 319 731 378 328  19 129 762 584 487 340 226  49 602 849  37 162 771
 329 550 698 377 463 685 254 841 770 633 480 191  24 543 506 697 683 488
 847 667 462 250 470 305 308 256 227 121 509 597 160  47 642 149  13 582
 195 620 665 848 783 745 232 873 725 741 511 790 335 709 144 379  51  48
 113  80 199 794 606 445 137 302 877 852  34 154 350 330  20 298 123 423
 713  86 516 345 792 106 465 286  50 274 210 294  43 785 641 585 779 352
 454 626 486 362 150 344 855 729 163 376 592 720 242 252 372 151 850 851
  74 757 587 579 332 260  14 157  91 356 447 583 600 476 621  64 588 266
 634 854 595 355 690 363 572 365 630 625 384 845 590 483 844  38 246 244
 781 472 840 146 766 152  22 557 571 469 673  33 555 245 715 718  11 477
 566 640 671  75 471 373  32 237 564 577 120 789 672 593 581   7 586 712
 723 386 609 228 624 793 629 775 859 773 220 443 361 193  89 843 784 449
 760 765 215 229 265 755  90 591 241 128 732 366 888 253 857 594 707 135
 223 664 711 383 853 623 607 736 778 842  77 601 730 860 787 639 717 791
 381 786 370 788 259  88 519 666 663 385 774 238 670 889  85 610 263 605
 611  83 573 547 389 754 753 636  84 638 632 390 635 728 599 716 391 603
 393  61 613 448 181 394 759 122 375 614 631 520 637 395  57 890 382 758
 489 387 748 734 388 110 714 558 107 628 726 589 650 752 446 804 643  94
  40 111 608 612 797 657 522 525 521 887 747 648 795 658 518 801 562 517
 803 554 569 575 796 750 749 708 563 800 811 570 105 114 735 567 719 524
 565 660 802 652 805 751  39 644 645 737 523 799 117 576 515 733 798 649
 618 654  41 578 452 647 661 646 615 616 653 116 526   0 656 617 655  93
 651 662 659 442 437 438 440 441 439]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1044
INFO voc_eval.py: 171: [ 871 1959 1217 ... 2182 2169 2183]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2453
INFO voc_eval.py: 171: [202 298 329 163  89 318 542 205 560 269 556  81 340 160 102 164 331  91
 309 351 268 229  83 302 287 328 306  86 304 246 200 170 438 341 330  97
 342 267  92  27 265 274 312 504 292 212 203  84 222 209 230 321  79 104
 433 121 508 350 356 245 320 429 522 289  26 211 408 272 247 441  29 545
 324 505 507 266  95 288  37 549 513 512 521 358 270  18 551 517 105 214
  38  99 167 127 553 333 407 297 311 275 503 544  82 566  93  88 554 119
 525 509 523 335 136 514 169 168 123  35 400 409 418 161  69 305 126 518
 201  94 120 257 527 485  31 391 515 567 124 303 403 393 564 405 118 271
  43  24  98  25 273 460 310 406 398 122 244 534 345  39  52 132  33 260
  28 208 404 516  51  64 313  68 353 437 300 414  50 492  73 537 500 261
 506 307 213 138 432 559 440 158 477 415  71 263 262 445 535 511 576 538
 137 493 456 446 570 495 442 294 376 232 462 381 435 540 502 352 543  36
 529 326 510  70 431 519 101 380 361  87 383 520 478 317 143  32 423  76
 166 155  48 231 384 402  85 528 536 181  21  90  59 176 382  72 254  77
 103 355 430 443  49 190 533 425 125 108 463 177 401 491 150  34 301 293
 226 149 464 454 225 434 532 206 290 135  19 129 207 447 308 162 197 198
 291 374 157 204 100 397 539 476  96  30 524 568 497 165 210 444 547 531
 332 375 248 410  46 471 390  80 152 546 151 416 486 134 458 428 114  45
 128 479  66 255 379 483 194 526 133 140 574  62 530 453 336 199 227 455
 473 459 156 145 557  60 395 363 469 251 159 130  67 573 457 180 299 370
 359 241 394 420  65 185 188 569  47 439 461 365  22 357 343  44 196 144
 451 228 494 223 171 250 154 396  57 354 243 249 224 179 424 173 139 411
 413 142  74 490 338 362  63 264 465 449   3 283 296 387 412  53 153  40
 565 187 378 182 344  61  54 422 562 436 285 256 337 377 419  56 450 191
 575 466 489  78 327 480  23   6 216 314 360 468 295 148  20 178 392 555
 117 339 364 563 146 186 284 183  75  55 452 488  14 417 131 252 399 448
 276 348   4 319 315 572 325 571 552 236 109 189 366 368 421 215 147 577
 279 141 286 499 349 172   8 111 334  10 195 467 475 218 175 174 316 388
 561 235 116 192 259 193 550   9 233 470   5 184 277 253 282  13 280 258
  12 472 389 234 487 367 369 426 281   7 237 474 558 278 115 217  17 110
 385  11 322 548 347 386 323 496 498 113 239 427  16  58 112 481  15 238
   2 242 484 372  41 371 106 240 501   1  42 220 221 346 219 373 107 482
   0 541]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1005
INFO voc_eval.py: 171: [ 463  955  986 ...  841 1018  253]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.1099
INFO voc_eval.py: 171: [210  27 112 109 197  15  41  26 196 111 215  28  30 209 121 113 105 147
 213 202 156 203  39 123 195  66 117  29  12 182 124 204 143  58  40 207
 205 211 165  37   1 118 100   3 122 184  14  31  32  99 114  91  96 169
  16 199  56  87 110  88 126  62  54 219 127 154  67  43 200 152 134 115
 119  13 157  57  68 148  34 185  17 198 162 216 181 220 208  86  33  19
 217 144 116 120 155  81 180  18  11  80  59 201   2 108 183 192  42   9
 158  84 179  10  21 136  53  95 125  20  90  47 145  55  93 189 138 150
 206 159 107  73 166 212 190 175  49  74 188  35  72 178 163   8  71 146
 191  36 129 168 214 131  79 218  63  61 161 128 186  48 149 170  45 153
  77 187   5  76 135 160  75 164  89   7 171  38  44 151 176  70   4  97
   6  52 130 222 137 167  78 139  60  92 106 177 133 221  23 223  85  94
  64  50  25  51 174 132  46 103  69 104  65 194 172 142 173  83 141 193
 140  82   0 102 101  22  24  98]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4246
INFO voc_eval.py: 171: [17587  8428  3485 ... 20579 20576 20578]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.3866
INFO voc_eval.py: 171: [ 289  226  228 ... 1992 1990 1988]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2650
INFO voc_eval.py: 171: [ 71 171 358  72  48  56 376 700 178 385 374 351  70 673 159 665 701 395
 679  62  79  32  43 177 373 671 170  39  58 678  38 176 393 162 169 694
 129 664 381 166 189 127 182 685  44 173 357 675 407 682 212 405 772 164
 123 622 354 353 218 208 372 160 399 406 401 670 556 209 689 368 761 198
 699 422 433 727  86 702 126 355 120 367 179 175 730  54 213 402 180 192
  64 161 429 686 491 352 696 149 726  46 165 386 684 643 380 736 731  53
 366 183 766 642 410 388 379 306 383 301 403 602 757 704 653 755 758 197
 187 495 762  55 264 167 490 674 646 194 203 363  45 397  83 444 649 389
 741 732 398  35 725 154 364 747 277 124 196 633 263 492 599 698 349 375
 181 350  73 436 125 615 703 400 390 163 151 432 168 773 734 378 763 494
 705 618 360 770 174 560 370 216 669 172 625 211 601 362 408  42 325 740
 608 365 215 728 565 377 628 751 610 548 153 690 394 391 652 316 753 738
 756 605 745 302 131 130 493 121 693 626 423 759 265 382 384 558 760 752
 657 735 210 147 635 443 609 361 446 201 404 748 251 564  28 729 387 767
 737 765 195 304 563 409 253 193 356 371 424  66 256 612 359 746 156 489
 369 155 637 411 185 396 392 309 632 754 298 733 128  31 252 486 559 557
 739 413 148 627 672 188 217 638 769 122  76 647 157 227 613  36 744 749
 333 668  52 654  61 268 600 624 335 764 566 771 310   9 150  14 152  11
 270  47 768 750  84  74 245 204 158 614  40 604 742  69 681 743 683 442
 623 666 644 677 561 134 119 104 562 607 667 650 142 237 487 603 207 317
 145  80 426 135 272 247 220  60  33 579 266 255  30 267  59 437 254 238
 269  41 249 271 190 587 537 106 305  57 629 569  77 318 184 248  10  29
 118 205 447 586 226 336 240 239 200 307 191 504 541 634 631  75 598 611
 656  67  65 440 416 620 430 412 222 697 334 250 299 640  82 695 138 630
 619  13 206 676 275 246  24  12 421 580 280 518 606 243 108 420 132 539
 214 414 143 648 417  22 475 186 144  49 536 621  68 639 418 244 616  51
 199 202 224  37 655 645  34 785 617 419 313 439 415 651  50 500 488 581
 139 597   1 636 502 547 688 434 133  85 595 512 573 109 146  78 480 468
 641 428 515 105 107 140  63 531 219 279 137 274 221 328 435 594 592 323
 225 261 223 282 445 438  81 427 322 441 459 116 297 103 596 425  25 477
 588 308 319 582 320 786 431 543 472 568 567 281 513 589 538 553 659 715
 471 327 692 329 450 578 585 593 484 300 473 540 331 522 687 546 259 498
 691 774 303 260 289 542 507  19  89  96 550 141 544 102 590 286 287 276
 583 273 324 114  21 510 708 501 326 330 658 321 574 278 341 294 312  87
 505 110 661 591 555 332 463 284 242 451 448 584  23 526 545   0 503 575
 716  90 509 452 572 449 460 485 288 680 112 660 285 454 283 662 136  27
  17 293  99 552 551 554 506  15  16 466 528 719 241 315 464 570 262 517
  93 571 117 496 714 499  26  94  18  20 111 314 497 290  92 348 717 342
 525 718 340 258  95 455 343 706 465 115 524 476 511  91 311 257 532 113
 478 707 467 339 295 519 530 470 535 533 462 456 344   6 722 453   3 229
  88 534 345 292 521 100  98 291 236 457 482 508 527 529 101 479 721 235
 483 523 514 461 520 346 720 516   4 474  97 458 723 712 228 481 724 663
 296   8   5 469 233 231 710 347   7 775   2 230 576 232 234 781 711 780
 777 783 778 709 713 577 779 549 338 776 784 337 782]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1714
INFO voc_eval.py: 171: [ 72  11  74  76 262 164 365 366  81 261 373  12 263 449 367 249 133 441
 371 135  82 312 364  71 136  78 264 157 368 259  77 362 445 314 411 260
 242  95 227  23 414 370 316 134 372 363 313 374 369 106 159 173 436  73
 219  26 340 361 158  16 320 137 186 311 339 304  79 243  97 144 114 139
 130  75 178 338 344 244 141 193 131 224 315  25 319 217  17 121 342 212
 281 247  80 208 117 132 276 183 450 142  24 250 207 293 400 334  36 252
 354 163 143 225 128 265  18 331 336 165 444 241 140 167 341 138 246 228
  33 115 343 176 410 251 274 337 317 345 185 161 431 358  15  94 120 189
 160 216 226 452 318 248 202 116 335 127 330  58 430 101 223  42 168 220
 297 191 237 443 118  64 356 235 170 100 447 409 103 282  20 233 232 154
 229 438 332 221 428 119 446 171 222 448 296  70 210 177 201 357  48  96
 455 433 245 328 209 280 286  65 182 429 175 218 413  34 298 236 211  38
 308  19 169 453 434 192 412 238   3 454 234  40 231 195 307 401 200 166
 179 162  47 152 327 198 306 187  67 102  98  39 295 300 172 174  43  45
 420 129 333 268 180 432 270  86 109  91  92 397  14 437  99 206 350 309
  84 277  62 278 197   6 203 181 230 347  69  93  66 418 416 184 267 388
  68 111 348  21  83 287  22 271  30 266 153 360 303  90  32 107 108 124
 310   9  10  53 205  51  13 440   1  31 407  61 299 302 199 301  52 122
 355  37 422 279 424 294 269  41 273 359   2  85 421 104 272 423 156  59
 321  29  46 253 417 113 439  44 419 442  50 394 305 256 426 451 204 188
 349 105 396   7 123 456 288 390 255  60  49   0 112 110 395 393 399 155
 194 190 196 291 125 284 285 404   5 435 329   4 385 425 415 289 427  57
   8 353  56 387 257 402 290 275 283 386 346  27 146  63 403 392  35 389
 406 405  54 391 215 408 148 213 214 145 398  55 151  88  28 147 254 240
  87 150 292 149 239 379 377  89 258 376 375 384 383 378 324 381 325 380
 326 351 382 126 322 323 352]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1209
INFO voc_eval.py: 171: [531 548 539 304 532 530 358 540  53 309 195 536 677  50 100  54 716 533
 303 149 270 352 547 541 319 349 535 351 684 549 717 678 675  91 721 105
 766 174  96 272 772 767 306  57 268 538 278 203 357  56  99 561 101  61
 780  51 768 555 151  95 271 160 311 413  71 787 273  52 379  70 201 269
 719 204 302 153  97 350 642 560 788 124 354 777  17 197 478  84 687 630
  86 102 789 371 355  63 680 114 148 683 731  78 610 288  15 315  48 277
 674 778 163  81 190 167 282 334 708 412 111 617 312 156 781 786  87 166
 283 496  73 783 324 534 556 573 692 632 729 165  60  85 329 622 773 313
 449 654  74 103  89  65 367 172  59 545 239 279 146 571 310 214 121 553
 248 720 380 187 552 316 276  72 726 451 199 562 694 760 212 168 175 147
 154 291 205 118 628 774 621 682 155 470 782 362 207 454 162 184 209 627
 727 578 616  16 377 366 241 542 790  18 494 176 646 550 537  69 251 615
  75 435 320 363 723 631 601 287 473 647  80 117  76 611 681 710 652 325
 189 775 711 771 588  41 323 144 246 713 709  36 644 126 714 770 216 698
 164  64 227 145 285 439 705  49 294 143 612  37 193 332 461 469 488 614
 206 468 134 779 361 178 776  26 691  83 113 393 249 725 546 157 298 784
 236 158 676 690  82 116 150 490 307 785 543 218 613 769 280 135 152 198
 213  77 485 159 702  19 564 659  62 563 495 693 289 296 730 695 618 387
 484 696 453 131 580 305 635 274 688  92 489 650 372  23 231 261 247 297
 369 353 133 679 217  79 374 667  43 474 290 415 375 707 211  94 208 636
 373 608 514  25 475 356   3 624 629 253 250 467 458 243 188 359 215 471
 281 697 225 348 183 108 173 753 376  67 797 455  10 569  27 200 125  32
 110 479 210 757 202 196 322  11 233 752 476 123  22 748 516 433 660 411
 648  34  47  90 589 232 799 252 368 755 170  58 235 527 795 405 754 300
 441 626 457 378 438 244 518 257 344 559 242 185 408 706 762 640  44 418
 301  93 179  55 512 672  66 364 732  28 605 295 286 637 445 643 503 177
 764 221 370 365 593 568 228  33 460 169 426 161 328 579 341 657 259 404
 456 180 669 360 670 472  14  46 620 649  68 333  29 299 658 140 292 596
 416 224 487 599 623 107 407 715 141 625 704 263 275 763  45 594 499 570
   0 511 245 465 758 381 406 551 129 262   9 342  31  13 256 798 318 106
  21 525 194 585 653 619 520 800 104 255 522  88 444 331 182  20 600 386
 685 254 656 586 293 431 686 265 267 747 665  24 639 226 452 584 513 230
 464   4  12 192 742 385 751 181 761 739 266 237 450 633  30 171 337 701
 191 507 497 728 481 115  98 655 671 400 577 703 186 284 119 327 482 796
 587 430 645 517 220 756 326 122 749 741 651 592 759 722 634 409 382 462
 602   2 109  40  39 222 598 238 699 137 132 664 477 395 440 264 668 260
 432 641 223 591 437  38 258 480 448 750 700 590 521 491 689 765   8 443
 459 463 139 502 604 583 442 733 427 595 392 447 314 735 597 607 436  42
 428 434 130 500 142 483 738 446 501  35 229 718 519 743 508 638 734 410
 466 136 666 317   1 429 744 712 515 724 112 398 425 419 138 240 524 403
 388 673 390 424 492 127 509 529 576 234 661 340 582 603 120 336 493 746
 389 558 506 219 609 339 791 417 572 330 740 606 510 384 526 422 420 308
 414 421 423 575 128 498 391 321 347 486 346 338 544 794 523 528 574 343
 383 745 401 335 504 345 557   7 581 554   5   6 505 394 402 567 396 566
 663 565 397 399 736 737 662 793 792]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.1331
INFO voc_eval.py: 171: [432 391 431 515 373 503 228 147 514   0 413  18 389 132 105 512 227 141
 275 399 414 428 471 294 436 243 472 123  46 276 145 250 185 376 438 194
 372 268 280 435 411  35 516 273 113 325 302 252 475  49 144 341  19 146
 295 520 448 505 232  53 406  12 375 208 198  20 380 310  38 231 189 224
 531 221 264  33 201 523 218 394 274 288 277 103 427 251 234 111 160 143
 196 106 286   2  34 226  58 199 419 477 122   3 136 307  32 464 281 480
  14  54 521 388 272 415 322 374 430 434 134  31  59 469 397  17 412 303
 377 490 116 355   9  80 340 178 390 527 305  15 328 301 209 260 112  96
 285  28 439 229 184 182 326  37 150 235  83  13 504  10 222   5 502 121
   4 163   1  71 509 312 257 278 115  40 193 290  67 526 507  77 176 195
 225 324 133 204 161 500 258  55 530  99  50 168 511  45 446 279 349 107
 200 270 263 473  62 179 485 402 253 293 247 433  26 304 289 470 210 207
 186 230 299 429 242 336 335 416 104  78 223  22 444 525 117 313 476 455
 400 524 443 319 165  70 454 483 316 306 254 137  48 214  47  66  81 346
 177 528 164 149  23  84 291 215  11 183  42  21 148 327 332 510 171 318
 362  68 261 488 495  93  61 379  16 255  44 220   6 265 474 529 522 190
 426 187 245 356 348  51 181 445  41 114 139 126 393 118  75 256 350 259
 354 463 292 238 437  82 378 267 456 233 489 287 447 188 213  74 398 449
 110 311 345 407 155 315 167  88 162  39 192 206 368 486  64 508  89 100
 119 347  63 371 296  25  24 175 401   8 417 266  73  85 467 392  92 283
  57 151 513   7 298 244  36 323 120  52 331 385 499 308 329 140 440 339
  86  79 395  60 458 453 262 271 108 219  29 408 387 451 217 452 418 180
 109 269 497 369  98 212  43 330 422 494 519 423 403 239 211 358 321 342
  91  27 170 300  87 172 450 410 154 493 360 173  69 506 169 101 297 386
  72 284 166 102 246 337  95 205 174 135  65 334 237 381 314 442  94 142
 359  76 282 157 191 248 353 202 484 333 481 482 441 309  56 320 317 197
 216 158  90 409 351  97 420 357 396 421 459 404 361 491 478  30 466 203
 465 152 487 159 124 138 405 498 367 383 241 236 424 240 130 153 363 365
 384 382 127 343 156 352 460 125 457 468 496 370 461 129 462 479 492 344
 425 366 364 131 128 501 249 518 338 517]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1581
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.2097
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.135
INFO cross_voc_dataset_evaluator.py: 134: 0.362
INFO cross_voc_dataset_evaluator.py: 134: 0.130
INFO cross_voc_dataset_evaluator.py: 134: 0.127
INFO cross_voc_dataset_evaluator.py: 134: 0.265
INFO cross_voc_dataset_evaluator.py: 134: 0.415
INFO cross_voc_dataset_evaluator.py: 134: 0.238
INFO cross_voc_dataset_evaluator.py: 134: 0.078
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.104
INFO cross_voc_dataset_evaluator.py: 134: 0.245
INFO cross_voc_dataset_evaluator.py: 134: 0.100
INFO cross_voc_dataset_evaluator.py: 134: 0.110
INFO cross_voc_dataset_evaluator.py: 134: 0.425
INFO cross_voc_dataset_evaluator.py: 134: 0.387
INFO cross_voc_dataset_evaluator.py: 134: 0.265
INFO cross_voc_dataset_evaluator.py: 134: 0.171
INFO cross_voc_dataset_evaluator.py: 134: 0.121
INFO cross_voc_dataset_evaluator.py: 134: 0.133
INFO cross_voc_dataset_evaluator.py: 134: 0.158
INFO cross_voc_dataset_evaluator.py: 135: 0.210
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.556s + 0.039s (eta: 0:01:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.392s + 0.045s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.384s + 0.043s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.375s + 0.042s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.373s + 0.044s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.370s + 0.043s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.369s + 0.042s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.373s + 0.044s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.372s + 0.043s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.372s + 0.042s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.368s + 0.042s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.370s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.371s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.425s + 0.028s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.378s + 0.040s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.363s + 0.042s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.366s + 0.042s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.367s + 0.041s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.366s + 0.043s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.371s + 0.044s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.369s + 0.043s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.370s + 0.042s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.369s + 0.042s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.369s + 0.042s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.369s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.368s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.403s + 0.029s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.338s + 0.033s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.346s + 0.036s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.358s + 0.038s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.350s + 0.038s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.351s + 0.038s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.348s + 0.037s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.349s + 0.038s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.351s + 0.038s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.351s + 0.038s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.352s + 0.038s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.353s + 0.037s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.355s + 0.038s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.658s + 0.032s (eta: 0:01:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.374s + 0.032s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.369s + 0.033s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.370s + 0.036s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.374s + 0.040s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.367s + 0.040s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.366s + 0.040s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.365s + 0.040s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.369s + 0.039s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.369s + 0.039s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.367s + 0.039s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.365s + 0.039s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.363s + 0.039s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.470s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [636 180  84 220 327 663 164 637 123 640 651 657 188  83  52 176 206 228
 335 199 212 119 319  53 207  80 696 213 320 162 172 274 134 215 322 361
 193 674 655 242 321 214 133 201 270 641 526 222 329 120  36 268  72 142
 661  58 677 708  51 642 645 695  44 673  34 330 223 665 306  37  67 515
 502 698 638 216 323  76 237 285  92 315 337 230  75 516 523 707 701 167
 145 301 656 184 179 235 478 668 510 416 431  65  38 721  33 211 436 284
 686 243 281 537 194  42 126 660 646 150 191 283 560 276 324 217 280 244
 503 509 165 648 141 659 148 312 241  54 432 415 658 435 332 309 225 420
 553 682 720  47 520 422 481  49 680 512 592  50  32 697 754 507  77 456
 458 171 511 153 501 718 704 311  41  73  93 198 685 113 269 365 286 650
 236 681 310 619 428 570 652  40 190 524 149 173 146  48 719 224 595 331
 508 278 699 397 109 396 358  85 628   3 505 246 580 295 715  28 333 226
 390  35 234 388 144  15 387 425 175 122 616 389  43 205 360 359 683 620
 392 296 104 522 670  30 398 504 364 185 714  46 252 140 105  39  64 672
 706 702 534 692  74 154 687  78 528 617 662  45 623 691 457 424 491 693
 639 302 576 334 227 367 297 316 221 328 492 535 238 653 558 178 196 137
 476 525  79 760 292 676 317 643 430 200 326 219 314 338 231 579 724 363
 627  89 362  63 304  66 187 393 147 366 163 240 186  26 197 434 437 427
 336 229 357 138 170 275 615 675 325 218 679 272 684 705 647 419 664 649
 554 530 540 282 591 429 654 450 308 500 418 166 108 423  55  56 667   4
 169 506 690 632 578 161 703 700 459 192 195 421 110 736 713 128 417 426
 565 669 202  57 294 204 722 688 181 256 174 433  97 480 717 568  90 521
 372 463 118 574  70 716 484 394 449 259 460 288 527 250 735 531 189  60
 489   0 559 761 115 666 567 279 755 177   5 518 249 493 556 622 168  81
 287 299 127 671 111 740 631 469 529 596 765 345 563 601 455 513 678 106
 689 438 517 403 644 482 532 157 132 539 753 562 728 544  29 694  31 618
 573 709 260  82 183 182 614 538 494 514 734  98 439 730 519 291 203  68
  27 533 626 107 536 590 239 156 444 633 257 370 114 629 577  71 486 758
 630 762 391 117 395 112 495 757 467 726 258 477 344   1 273 723 313 625
 624 245 555 290 318  61 248 441 575 763 759 300   2 271  59 561 447  88
 116 571 725  16 756 125 764 155 373 557 339 733 588  91 340 341 621 305
 572 564 369 247 277 253 342 378 351 383 607 729  87 440 442 710 307 569
 634  86 473 410  62 135  94  69 347 474 407 402 262 454 124 377 589 541
 599 379  22 139 566 381  95 727 261 496 343 298 598 405 143 158 354  96
 159 160 472 711 635 414 594 547 497 731 136 475 462 546 487 406 490 368
 380 152 464 732 587 551 549 384 737 251 600 254 356 586 129 210 293 593
 404 382 468 738 483 375 543 498 371 452 445 121  25 739 255 355 499 550
 303 346 443 401 548 461  23 451 542 741 712 485 466 545 289 453 151 376
 352  10 208 131 488 353 597  17  13 267 349 471 446 130  14 745 465 350
 448 374  11 581 479 470 751 348  24   7 399 386  18 412 102 400 101 750
  20 743  99 385 411   9 264  21 408 266 552 100 103 263 209   6  12   8
  19 610 742 582 612 409 413 744 749 232 233 602 746 608 605 265 748 609
 584 606 747 752 613 585 583 604 603 611]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1215
INFO voc_eval.py: 171: [ 81 282  36 352 266  83  35  84  97 126 234 323 344 346 348 265 351 299
 232 158 281 345 231  47 203 205  64 347 100 326 239 353  96  37 285 125
  38 236 240 288 150  70 237 330 206 349  66  87 241 334 238 138  86 283
  99 337 129 204 230 229  94 128 209 235 354 228 259  90 233 250 329 101
  88   2 331 286 336 136 350  89 289  45 130 247 176 102 164 328 254 307
 132 161 316  95 290 139 207 292 312  12 202 251 260 212 133 284  40   5
 244 258 327  91   0 305  14 339  85 127   3 245 263 154 255 313 298 309
  98  93 257 294 168 315  41 267  27  42 184 252   9 157 160 248 243 162
 103  82 335 135 221  92   4 273  51   8  46 264  10 131  13 156  53 189
 141 217 178 322 325  15 116 155   6 303 308 341 291 249 262 215 211 301
  39  23  25  61  11 256 171 219 277 332 140  49 242 275   7  16 296 165
 300 293 214 261 223   1 213  58 134 106 144 342 210 246 297 295  20 304
 188 269  44 151 279 272 218  54 274 287 163  26 193  52  31 276 333 280
 253 143 180 169  73 145 226  18  48 227 147 208 149 324 118 114 278 159
 320 153 271 187  69 270 310 199 112 302 111 170 340  43 148  30 317 222
 314 220 177 195 216 306  76 224  22 146 175 321  79 108 166 123 137  17
 110 152 191  78 190 173  21 194 142  77  62 192  71 167 117  74 311  29
 124  72  57 183 318  68 197 107  19 200 172  80 196 119 338 319 174 182
  28 185 113 115  32 109  60  75 186 201  24 121 225 343  55 179 198 181
  65  33  34 122  63 268  59  56  50 120  67 105 104]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3588
INFO voc_eval.py: 171: [2271  865 2305 ... 6235 6232 6231]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1299
INFO voc_eval.py: 171: [1863 1669  617 ... 3013 3014 3016]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1040
INFO voc_eval.py: 171: [573 571 350 519 555 339 336 716 280 289 628 667 369 719  12 695 279 340
 668 743 295 740 629 302 327 533  13 614 261 564 541  16 344 273 599 370
 290 556 529 674 604 525 606 679  17 680 526 269 595 562 281 297 897 730
 581 262 575 636 260 788 894 647 141 347 700 291 817 618 954 611 586 692
 337 371 345 600 567 520 950 288 283 584  26 723 300 277 558 736 341 518
 118  11 559 538 918 702 605 787 686 266 348 276 633 303 705 729 342 282
 699 594 673 274  24 299 900 543 631 697 527 905 446 650  25 544 718 685
 542 678 720 914 714 687 292 613 947 271 593 159 701  30 335 129 924 726
 568 961 608 334 596 601 134 802 676 530 434  19 861 661 557 936 677 285
 793 671 158 333 929 304 713 670 619 710 563  21 294 296 275 635 746 689
 607 895 694 682 343 478 751 522 707  27 122 952 329 690 298 851 771 933
 675 301 540 732 572 953 450  34 923 598 630 855 945 293 349 459 727 898
 338 865 570 849 660 717 426 616 709 681 760 906 725 706 632 796  14 157
 270 948 862 693 919 733 278 435 926 615 521 364 583 242 331 638 346 427
 120 742 590 814 565 617 265 126 577 758 456  22 240 119 464 812 597 664
 946 591 871 423 937 536 131 524 465 684 715 483 217 850 869 194 534 150
 284 711 703 794 962 669 560 801 462 951  15 737 272 539 263 741 940  29
 651 765 267 744 691 612 654 578 917 901 561 148 425 735 634 748 704  23
 747 724 201 445 826 688 722 804 708 463 588 580 132 830 139 574 228 566
 721 658 728 857 587 332 963 222  31 904 523 531 592 579 582   8 777 480
 858 657 935 799 698 863 569 161 683 931 750 268 866 653 319 264 840 731
 189  20 537 800 769 145 223 481 663 609 532  18 749 460 226 447 576 734
 856 451 836 852 479 154 738 854 585 925 589 790 768 602 603 775 781 166
 934 610 803 739 712 824 125 827 528 672 656 146  28 662 424 745 696  49
 911 955 837 218 365 853 225 645 535 314 420 789 173 859 191   9 870 436
 958 492 244 168 468  68 178 807 759 152 220 825 808 160 467  36 163 452
  51 105 311 421 170 909 169 957 130 891 133 809 655 245 912 229  62 401
 193 916  61 241  98 123 198 438 177  41 188 196 192 810 454 644 190 142
 437 868 230  99  92 780 646 430 136 821 495 243 470 330 176 384 432 819
 921 455 466 312 128 458 469 431 204 798 143 453 892   6 797 838 811 449
 890 387 239 247 770  32 422 828  40 433 498 778 872  79 508 393 323 941
 237 397 164 448 831 197   3 309 135 960 461 845 444  44 140 767 155 419
   5 642 639 175 822 246 227  89 248 203 922 620 554 500 829 417   4 208
 138 407 328 753 214  95 156 121 219 640 360 763  39 907 903 305  75 649
 505 764 174 235   0 124 144 202 944 893 805 234 389 791  94 172 482 489
 834 648 659 286 490 813 404 307 171 792 486 321 641 250 231  86 394 137
 551 233 623 167 251 165 956 104 860 762 484 320 910 103 493 149 199 359
 625 200 622  38 322 443 406 255 497 549 355 920 643 515  45 213  64 842
 943 386 930 494 818 833 102 782 106 772 843 823 949  91 385 398 774 491
 381 652 841 795 820 864  74 457 379 754  90 205 416 232  80 637 806 816
 400 902 215 221 206 501   2  87 116 815 236  43 499 127 151 899 938 547
  37 621 162  88  65  60 383 382   1 316 896 506 363 380  84 101 377 507
  63 928 224 487 147 378 471 375 153 496 473  85 757 512 504 324 253 839
 832  93 485 867 441   7 315 249 908 835  76 511 545 310 927 939 503 403
  73 358 258 325 502 964 399 477 388  47 313 186 932 548 405 552 516  72
 942 959 440 439  48 402 913  77 915 318 326 187 418 374  67 373 844 361
  54 472 624 396  78 766 488 376 553 317  83 509 287 356 626 550  55 517
  82 514 513 755 182  81 212 510 308 429 752 238 776 209 883 756 252 117
 183 428 442  42  35 367 627  10 779  58 773 184  59  33  97  46  57 847
 761 390 372 783 888 885 846 357  66 666 546 185 210 100 354 391 412 362
  56 259 476 254  50 353 786 784 257 785 216 366 395 413 195 256 886 848
 411 882 108 211 207 875 368  71 392 415 889  96 887 880 884  69  70 114
  52 878 409 180 879 351  53 111 181 414 665 352 110 179 115 107 112 873
 113 876 408 109 874 306 410 474 877 881 475]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.2464
INFO voc_eval.py: 171: [296 113 317  73 120 112 314  79  72 136  28 158 116 149 156  81 301  77
  63  34 122  78  75 305 114  51 131 309  88  85  71 152 302 119  64  27
 310 128  33  98 282 115 303 308 297  21 307 298 300 125 134  42 299 132
  66 123 108 198 271  25  76 319  53  67  62  26 272 126 289 159 196 144
 200  86  41  40  68 154  82  91 313  56 304  90  70  87 204 274 117  69
 230  65 306 279 153  30 278  24 130  23 326 140 141 283 188 121 268 201
 208  89  46 275 267 233 333 312   0  49  84 157 270  35 151 106  74  80
 285  45 203 187  99 280  83 194 330 215 167 222 323 234 315 269  47 335
 221 324   1 277 193 332 137 202 240 107 242 161  54   2  48 133 213   4
 238 273 291 281 288 219 290 129 205 336  43 334 227 160 276 258 294 199
 292 103 155  50   7 293 195 329  61 226 331 287 286 255  10 235 325 316
 253   5 247 321 244  13 210  11  29 220 284  55 177 127  31 146  44  95
  97 145 250 138  60 100  96 181 239 101 197 164  37 102  38 163 231 241
 237   8 311 189 216 142 206 211 207 236 248 229 162  12 243  58 225  22
 190 118 327 249 214 109 217 245  57 148 224  94 232 228  32 246 105 223
 191  15  36  93  59   3 124 111 176  52  39 328 192 218 166  92 212 150
 147 110  18 262 104 260 135 295 183 320 178 256   6 165 139 257 318 143
 209 171 168 174  19 254 173 182 259 322  14   9 251 264 169  20 175 252
  17 170 172  16 263 266 265 261 186 184 180 179 185]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.3737
INFO voc_eval.py: 171: [1492 2129 1425 ...  669  677  510]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2245
INFO voc_eval.py: 171: [ 85 200  84 167 222 117  87  88  78  89  25 218 115  90 161  10  76  83
 164  24 151 106 119 172  79  86 144 220  14 147 128 176 217 165 173  35
  19 146  57 116 198  13  75 221 177 129  20  21 149 148  64  16 137 191
  11 143  68 208  51  31 150 181  17  58 145  46 219 199 212  60  73 187
  18 211  37 188   8  96  61  36 109 210  69  34 192 193 138  82 139 113
 166  43  92 118  22  56  32   6  48 194   9  59  52  23 175 152  74 111
  53  12 179 184 178 182  15 215  55 189 107 180  49 112  38 101  54 183
  63 214  80 195 201 130  33  65 136 190 196 159  39  44  62 186  94 168
  81  42 174   5  70 110 153  93  66 154 171 140 205  98  91 100  47 108
 185 114 216 197 156   3 141  45  71   1 160 135 202 105 162  40  72  50
  95   4   7 124 131 125  67 206  97 142   2  99  77 170 163   0 127  26
 169 204 126 155 207 104 103 157 102 203 120 209  41  30 122 158 121 123
 133 134 213 132  28  27  29 223]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0207
INFO voc_eval.py: 171: [ 412 3093 3493 ... 3970 3968 3971]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1816
INFO voc_eval.py: 171: [383 530 427  83  86 106 332 279 399 265 173 397 107 498 161 101 333  84
 425 211 503 667 264 480 423  28 685 521 203  93 488 275 389 346 531 392
 172 421 267 527 310 300 484 216 692  90 411 518 672 177 295 675  85 405
 109 288 492 164 442 415 159 699 334 272  21 268 331 519 499 215 428  52
 182 525  42 390  16 180 395  54 419 162   2 307 803 481 853 192 163 303
 186 344 736 391 283 195  45 441 517 249 326 409 416 146 520 698 848 263
 398 158  40 200 193 342 266 188 674 292 489 406  49 529 548 824 846 278
 809 212 270 190 404 487  81  89 444 223 166 403 454 490 315 384 804 285
 281 522 676   3 185 491 273 224 796  57  15 483   6 290 157 220 823 350
 119 668   4  23 669 385 198 276 477  50 277 540 768 502 680 681  80 688
 734  41 118 448 493 447  47 167 144 446 179 413 314 683 296 849 202 806
 417 590 321 867 856 313 210   1 418 859 505 176 670 217 855 532 422 751
  24 194 349 245 104 569 312 733 156 256 328 274 311  56 298 494 305 543
 407 750 237 293 468 189 819 396 528 208 712  59 424 845 362 847 352 496
 115 178 821 693  73 596 814  96 426 679 553 684 299 393  69 241 808 262
 160 412 807  48 386 197 165 361 123 443 552 154 318 209 325 242  62 591
  88 155 145 204 175 470 812 100  10 301 259 873 737 555 677 504 329  75
 294 813 802 187 168  36 458 457  67 364 811 269 231 205 545 309 150 526
 284  17  26 862 348 738 255 762 330 678 306 801 753  38 340 287  64  12
 464  65  44 524 130 820 870 817 523 797 795 451 595 771 869 359 469 691
 671 103  34 544 539  63 799 244 694 401 822  92  43 710 388 218 140  39
  58 122  53 356 319 463 600  20 863  35 598 174 829 535 345 316 440  25
 400 127 730 336 226 452 143 112 687 213 618 339 818 153  22 850 304   9
  51 420 495 601 402 735 471 485 562 497   8 857 868 815 466 550 826   7
 696 136 865 546 236 234 474 816 660 323 632 800 121  82 810 758 431 317
 542 347 450 482  91 387 243 864 247  95 872 455 232 184 131 365 134 141
 337 148 686 114 432 755 408 854  68 825 582 116 572 338 222 805 626 841
 722 261  11 673 117 280 379 248 866 475 739 229 414 221 320 394 858 500
 252 128 533  13 576 135 282  79 861 132  29  30 308 852 565 759 559 732
 486 324 343 501 289  19 206 716 196 637 478 851 286 297 871 258 410 149
 597  37 541 579 748 769 453 718 227 138 760  14 233 767 327 711   5 833
 844 322 860 770 133  77 113 777 831 729 240 782 752 354 577 661 239 257
 120 271 367 682 302 620 689 366 368 291 580  71 832 556 251 690 335 705
 353 436 142 369 731 578 766 581 567 449 621 151 774 183 374 701  78 225
 589 129 139 778 351 460  18 341 508 152 627 137 700  61  31 253  66 665
 746 623 836  32 658 835 147  27 606 437 604 181 659  55 781 238 616 472
 839 235 776 697 828 830 360 199 592 585 476 465 126 827 254 230 201 125
 372 703 371 756 761 459 246 586 434 250 754 214 228 537 219  76 713 445
 461  97 773  60  98 721 834 843 599 695 602  72 467 840 363 473 456 462
 207 666 764 765 775 260 111 747 124 838 191 635 779 842 625 837 435 757
 745  70 607 373 724  46 715 707 588 549 727 619 664 358 628 357 558 587
 719 704 439  74 772 663 603 624 574 631 536 378 433 610 708 375 657 662
 355 633  99 377 605 622 749 780 108 763 617 510 609 629 376 370 171 630
 382 380 479 566 551 608 381 634 593 636 438 723 594  94 875 583 744 725
  87 564 509 720 560 563 706 105 638 570 575 644 790 102 653 874 170 573
 786 561 110 568 642 514 515 534 554 741 547 513 785 652 783 640 717  33
 794 787 538 714 788 511 647 726 512 728 743 557 571 702 793 792 643 648
 584 639 641 169 709 789 791 784 798 740 611 507 654 650 506 742 516 649
 614 651 612   0 613 615 645 655 656 646 429 430]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0817
INFO voc_eval.py: 171: [ 728 1668  237 ... 1426 1854 1847]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2300
INFO voc_eval.py: 171: [184 301 264 148 290  78 313 242 186 515 531 304 272 145 259 152 283 273
 270 300 276 247 312  96 274 305 331  76 207  89 316 182 220 243 415  84
 193  90 481 291  86  91  29 237  79 240 241 409 187 500 200 221 190 494
  75 260 384 280  87 263 483 278 419 320 327 482 292 275 112 374 535 244
 245 497 383 518 410 222 406  24 524 540 151 297  77 493 382 526  63 219
  97 248 282 530 394 298  25 277 441 279  95 238 536 517 195 204 480 262
 486 499 183  23 418 246 527 489  33 495  85 381 231 113 147 109 299 239
 119 194 463 369 487  27 492 412  65 496 543 424  32 281  34 111 124  16
  57 414 218 377 442 146 437  66  37 472 390 365  28 232 505 350 512 267
 485  64 303 506 484  40 389 143 332 189 405 511 352 399 443 519  62  30
 150 502 319 380 513 509  26  71 458 498  83 473 356 423 416 510 516 310
  46 545 490 191 117 323 122 338  81 479 265  31 401 421 234  93 203 166
 425 142 358 462 359 127 378 501 266 115 348 165 229  52 459 114  70 118
 541 132 202 422 411 236 136  88 120 445 550 130 408 474 179 159 228  42
 110 420  94  99 289 185 439 488 116  43 149 235 491  74  20 376 349 128
  41 157 435 188 432 391 436  82 417 233 329 284  39 334 129 520 137 141
 261 223 542 192 311 169 108 125 379 180 372 508  56 164  92 328 398 507
 123 225 386 357 144 201 306 351 181 464 370 440 355 375 451  49  55 107
 413 167 126  17 438  80 455 315 534 395  53 456 388 354  47 539 205 434
 454 341 121 139 206 335  44 340 470 172 368 431 224 178 333 135 133 138
  59 457  73 173  38 428 153  68  48 551 230 444 330 362 371 532 268 308
 407 163 446 396 302 254 318 363  45 447 314  35 546 271  69 466  22 385
 269 170 286  67 449 295 317 433 538 467 353 258 156 469  50 309 427   4
  72 397 429 392 158 373  13 217 293 387  19 345 336 140 430  54 426 134
 522  60 257   3 249   8 547 548  58   9 393 103 160 325 168 210 162 155
 360 288 287 339 529 161 342 337 326   5  51 400  10 226 533 175 215 131
 450  21  11 211 255  18 549 227 364 544 196 176 476 171 537 343 100 448
 197 256 251 209 344 154   0 503 361 504  15 366 468  61 402 477 250 471
 367 525 252 104 101 307 253 452 403 521 208 106   6 460 105 465 102   7
  14 523  12 528 296 294 475 177 174 212 453 404 346 321   2  36 214 199
 322 285 216 324 478  98 198 213 347 461   1 514]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0822
INFO voc_eval.py: 171: [937 447 911 ...   1   0 809]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.1289
INFO voc_eval.py: 171: [193  30 115 113 195 202  28  17 192  40 114 147  29 124 206  32 107 116
 208 199 197  37 119 191  55 121 143 178 200 103 125  41 112 205 194  31
   1  14 210 104  54   2 204 122  38 162  59 207 101  16 180  93  92  34
  91 123 201 151 212 127 211 130 120 155 158  53  62 146 168 117  65 152
  49  19  66  51 160 118 134  35  47  79  15 182 159 213 196  90  89 177
 138  45 144  52  82 128  94 175  36 188  87  10  81  18 157 149  80 145
 187  13 110  83  20  69 184 156  33 139  42  48 126  46   6 172  11  61
 203  70   8 100   7  63 198 174  71 185  44  43 209 166 167 179 186  21
 102 161 176  64  98  67 111 164  75 129 133 148 181  97  95 154   4 132
   5 163  73  12  56  76 183 150   9  74 153  39  68 169 136  88  78  58
 165 219 215  99  50 131 218 173  96   3 137  77  57 217 214  60 216 135
  25  72  23 106 171 220  26 108 140 170  84 189 142 190  86 105  85 141
   0 109  24  27  22]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4068
INFO voc_eval.py: 171: [18508 20198  8851 ... 16080 21632 21630]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.3466
INFO voc_eval.py: 171: [ 289  234 1802 ... 1936 1922 1930]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2449
INFO voc_eval.py: 171: [184  70 384  72  37  55 192 400 427 172 719 422 407 728  60 393 707 721
 194 414  61  74 708  38 375 173 137 136 416  58 183 426  28 177 180 735
 405  50 706  79  32 195 186 185 726 208 710 206 424 717  80 410 179 399
 720 449 129 233 378 236 401 124 191  47 207 661 390 594 235 176  71 730
 385 460 231 714 458 741 773  41 800 372 733 695 128 686 423 765  44 167
 382 581 406 413 429  76 190 786 130 397 408 674 203 476 683 127 283 794
 174 391 775  51 763 790 133 768 432 474 284 464  43 380 222 798 456 527
 329 403 528  77 737 662 371 157 161 421 739 395 330 131 641 435 777 477
 470 376 656 796 181 676 791 782 168 668 638 776 810 597 209 431 404 188
 764 478 783 450 738  64 389 531 175 778 377 420 596 731 603 182 784 394
 163 430 599 655 169 650 669 373 298 602 417 795 712 640 793 193 419 201
 204 398 653 693 396 692 386 711 780 320  68 797 379 664 200 324 189 223
 806 226 412  24 125  33 381 160 663 678 646 187 788 471 672 434 332 411
 155 529 374 772 792 742 779 402  54 684 138 263 809 530  49 392 328 647
 774  53 729 771 409 428 205 337 418 734 681 165 425 526 691 766 273 789
 347 690 787 723  39 718 433 134 178  23 595 387  26 770  57 164 388 285
 272 415 383 159 354 135 675 142  42 276  56 799 154  34 153 769  66 667
 785 666 598   8  31 216 126 802 644 454 781 212 649 801 156 270 473 722
 437 808 353 616 132 166  11 677 162 657 452 451 213 713 459 202 171 266
 158   6 245 210 227 651 170  48 150 639 807 724 268 274 262 652 294 804
 601   7 217 767 803 199 665 805 215 267 242 709  73  52 673 292 225  69
 289  25 109 151 463 600  59 643 221 687  62 152 331 645 286 682  81 715
  29   9 338 467 685 287 637 620 679 321 145 122  45 336 626 220 293 141
 520  30 232 255 104 211 660 196 121 224 257 438  10 238 269 290 648 680
 302 256  46 101 468 740 299 689 552 573 261 275 448 457 539 727  27 658
  65 271 264 440 688 436 219  63 265 333 670 576 303 659 218 475 461 214
 323 732 228 439 466 465 571 443 621 636  78 123 736  35 654 444 510 234
 327 551 537 566 494 107 642 312 441 605 580  40 504 352 281 442  67 241
  18 309 229  75 139 627 243 197  36 511 469 230 522 295 445 631 105 694
 579 244 822 297 455 240 237 335 671 513 446 108 106   1 447 525 103 569
 140 635 198 342 102 301 481 572 239 341 300 519 146 508 625 506 339 611
 119 505  21 585 319 725  15 147 583 462 634 110 521 322 345 472 482 148
 703 535 541 348 562 340 304 325 355 453 547 144 523 632 296 524 577 606
 823 570 100 310 554 305 114 575 279 622 591 538 697 811  17 574 326 351
 705 507 587 820 619 349 702 344 306 346 308 149 118 755 484 542 629 343
 117 613 480 532 311 350 259 288  83 549 568 143 592 588 280 701 623  16
 307  95 483 633 291 593 497 696 586  20 589 116 479  82 630 812 745 546
 700 492 610 258 282   0 536 821 584 563  22 617 514 618 716 590  14 112
 624 754 316  19  13 555  12 699 360 560 501  92 533 488 502 545 698 540
 534 260 277 120 516  94 518 111 753 608 578 509 499 558 500 628 314 113
 743 334 612  90 607  98 512 557 515 503 278 604  91 487 744 559 486 115
 609 490  89  86 543 493 363 561 313  88 556 759  85 246  84  87 495 369
 357 565 364 496 361 315 544 254   5 762 362 517 489 317 751 564 704 253
 756  97 567 365 367 498 553 758   3 760 485 491 359   4 548 761 550 757
 749 247  96 813 358 318  99 249   2 366 748  93 250 368 370 614 750 252
 815 819 248 251 752 747 817 746 615 816 356 814 582 818]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1091
INFO voc_eval.py: 171: [ 65  67   8 248 246 333 161  74 342 247 335   9 340 330  79 127 134  64
 249 405  70  72 228 232 413 336  78 129 297 331 383 328 234  66  17 337
 343 131 341 154 332 151 408  91 378 101 339 313 294 231 201 168 314 137
 233  24 329 110  22 334  93 327 400 211 132 312 207 296  73 283 338  13
 140 345 152 126 344 237 171  10 326 174  68  25 198 311 295 291  77 415
 115 229 180 190 135 299 292  21 293 125 238 139 263 300 136 258 240 195
 316  75 310  16 368  71 138 250 208 130 114 272 108 241 239 172  69  11
 159 109 317 133 128  76 191 230 308 217 155 194 163 193 301 418 409 402
 306 298 321 395 212 122  96 309 218 148  14  30 377 315  57 203 226 202
 222 221  98 223 277 169  95 406 257  38 276 369 157 404 410 166 158 113
 325 305 111  32 235 200 210 206 165 199 225 220 396 156 278  92 242 205
 121 288 175  90 319 173 397 412  60 264  63 379 236 204 224 112  58 219
 176  18 183 184 307 417   4  43   0 255 192  97  94 323 162 216  44 164
  41 398  31 181 393 382 227 215 170 259 285 214  20 381  19 394 380  36
  59 289 209 105 160 167 265 153 102 123  62  35 178 284  82 213  54 366
  34  61 124   6  80 253 187  87 385 290  55 185 186 274 104 106 419 251
 146 103  45   7  88 182 252 324 100 254  27 322  28  53 275 279 399  42
 287  86 107  89 384  12  15  99 389  47 117 267 179 414  29 118 387   5
 282 281 280 365  26 188  81 388 245 362  37 286 273 256 260 401  40  46
 147  39 358 268 407 261 197 411 189   3  33 403 386 416 119 361 363 269
 266   2 373 392 360 318 372 177 150 149 243 270 391 390 356 354 262   1
  56 120 367 376 364 304 355  49 116 359  51 371  50  23 196 375 374 370
 244 142 357 143 144  48 141 145  52  85  84 271  83 348 349 347 352 346
 350 351 353 302 303 320]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1115
INFO voc_eval.py: 171: [494 496 513 288 492 493 295 503 511 189 339 677  49 302 504 495 343 633
  98  46 502 354 256 508 517 530  90 678 673 148 336  56 635 289 287 636
 188  51  96 291 721 100 723 259 514  91 109  81  58 499 172 263 103 725
  74 630  14  62 345 296 686 195 149  48 720 535 727 519 718  99 120 337
  69  47  78 646 498 194 334  54 257 255 569 258  92 191 102  45  84 599
  63 151 286 355 634 317 738 515 193  97 270 360 586 174  77 262 649 676
 689 167 146 444 507 582  67  18 730 537  53 683 632 736  87 577 737  75
  68 671 344 134 311 298 156 129  85 254 648 540 640 609 734 227 597 307
  76 500 512 133 118 297 501 163  73 453  55 679 351 684 497 588 267 119
 722 340 237 365  71 170 166 510 650 637 265 638 682  17  15 584 280 741
 315  70 352 740 729 153 719 268 274 197 726 436 347 575 505 735 672 159
 208  16 158 724 145 581 342  28 229 292 572 144 576  93 731 154 338 335
 728 124 739 419 659  60 320  50 238 217 201 125 428 585 409 304 175 732
 715 571  13 669 361 207 161 199 526 373 137 236 346 182 160 363 203 412
 601 558 668 462 523 202 665 130 506  79 654 693 573 152 641 173 422 631
 674 574  42 176 278 733 206 234 643 653  89 200 656 460  72 271 142 171
 269 110 114 204 516 186 220 155  25 553 374 147 614 534 353 106  24 598
 570 294 568 192 612 651  95  80  22 143 642 456 670 228 150 104  86  39
  64 362 198  61 240 437  10 308 478 112 610  33 210 454 439 341 578 283
 413 587 472 592 264 647 566 127 108 314 429 359 224  23 248 230 168  88
 209 433 605 714 326  43 235  40 162 538 667 190 310 589 179 461 260  36
 126 239 434 223 435 115 358 266 430 101  52 604 306 455 318 357 681 177
 205 746 116 451 652 621 580 480 350  44 644 442 662  94 392 226 583 457
 196 290 567 440 157 548   9 366 282 349 327 518 181 356 395 329 272 443
 658 408 348 401  11 639 432 611 491 316 593 184 123  29 231 445 438  82
  35 483 622 467 187 591 748 393 579 165  65 484 595 477 233 418  57 169
 528 488 367 690 664 219 389 416 712 424 623 364 624  41 562 390 244 215
 709 261 615 183 441   1 596 600  12 710  38  26 476 131 222 613 312 178
 590 285 450 421  21 527 214  19   0 275 482 549 105 300 251 405 711  59
 448 628 700 708 663 140  32  83 431 594 655 164 322 332 369 423 279 252
 281 185  66 627 691 747 532 688 606 474 249 135 273 277 301 602 427 225
 117 111 213 475 550 703 687 406 375 717 250 463 276 284 309 464 660 666
 625 410  34 402 716 552 713 701 556 253 560 246 180 371 707 620 399 425
 136 546 608  30 391 243 696   8 247 452 486 685 396 122 607 657 661   6
  37 616 139 141 107  27 619 241 382 539 645 218 417 509 603 400 121 547
  31 479 692 561 564 221 325  20 702 485 378 551 555 414 522 216 132 531
 706 698 313 411 626 470 481 404 544 524 426 533 212 680 459 397 407 420
 368 380 618 372 211 138 232 629 465 128 245 559 466 415 447 323 545 330
 333 113 385 705 565 403 331 458 394 449 675 377 554 324 471 386 563 699
 328 473 242 697 557 469 489 299 490 305 376 293   4 529 319 536   3 704
 487 370 541 398 446   2 521 303   7 468 742   5 525 321 745 520 543 542
 383 379 388 384 694 381 387 743 695 744 617]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.1078
INFO voc_eval.py: 171: [461 419 459 547 397 545 249 532   0  22 543 417 162 117 150 301 166 442
 458 501 439 496 456 205 252 394 320  51 218 163 541 435 164 137 275 304
 302 297 160 258 127 226 300 256 402  54 299 346 505  14 214 211 433 534
 247  76 434 241  38 315 298  21 396 563   1 416 464 250 395 386 222 549
 274 552 398 260 308 220 462 500 118 325 267 223 244 121 447 165 236 465
 313  37 455 504  53  16 288 303 273   8  18 232 116 177  36  40 238 403
 204  68 216   6  11 492 152 441 154 550 457 125 266  12 401 358 440 136
 399 306 406 333 248  15 367  27 221 224  35 253 230 200 129 554 466  31
  62  91 337 305  89 344 229  43 539 339 538 280   3  66 415 560 535  56
 268 319 361 323 207 329 559 531 334 122 243 263 170 533 261  13 239  94
 179 192 318 128 119 507  44  20 327 151 276  95 115  71  30 450 281 195
 426 203 472  19 443 354 133 350 290 182 460  65 503  33 366   2 561 194
  58 208  92 251 360 108 332 390 157 470 206 130 110 245 473 510 231 181
  75 527 212 185 225  57   5 484 553 499 340 234 555 326  47  42 284 156
  61 285 414  72 513 423 309  23 365 257  74 193 400 351 132 287 270 523
 187 279 341 167 209  93  80 317 483   4  73 201  24 186 109 551 296 289
 369 356 324 237 227 161 537 168 138 347  55  46 312 516 144  83 213 176
 126 331 370  99 240 471 407 292 282 310  17  87 265 391 491 558 335 418
 556 378 482 123 104 124 479 485 562 178 431  85 540 342 215 463 542  28
  60  70  96 477  50 235 480 557  64 314 349 389 469 293 246 420 184 444
  10 408 158 525 321 180 425 487  86 120 493 283  97 311   9   7 359 172
  84 316  49  25 233  41  48 432 269 374 286 336 255 474 436 508 307 546
 153 242 196 254 411 481 428 352 412 512 198 348 380 328 363 219 421 291
  29 536  45  78 169 102 410 427  39 277 294 295 355 155 190  52 278 202
 519  77 509  81 100 330 478 445  90 264 475  59 188 103 322 338 345 405
  79 113 476 217 106 159 526 131 373 430  26 101 183 371  32 343 522 197
 112  67 544 375 383 228 511 114  69 191 376 171 174  63  88 515 381 105
 422 173 382  82 189 139 140 494 199 353 210 149 377 147 486 517 135 514
 502 446 111 387 134 489 448 437 384 506 404 438 424 449 429 468 107 368
 495 520  98 413 497 145 141 388 488 175  34 364 490 148 451 262 409 393
 362 259 498 146 524 392 518 143 357 271 385 142 529 372 379 521 452 467
 453 454 272 548 530 528]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1582
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1884
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.121
INFO cross_voc_dataset_evaluator.py: 134: 0.359
INFO cross_voc_dataset_evaluator.py: 134: 0.130
INFO cross_voc_dataset_evaluator.py: 134: 0.104
INFO cross_voc_dataset_evaluator.py: 134: 0.246
INFO cross_voc_dataset_evaluator.py: 134: 0.374
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.082
INFO cross_voc_dataset_evaluator.py: 134: 0.230
INFO cross_voc_dataset_evaluator.py: 134: 0.082
INFO cross_voc_dataset_evaluator.py: 134: 0.129
INFO cross_voc_dataset_evaluator.py: 134: 0.407
INFO cross_voc_dataset_evaluator.py: 134: 0.347
INFO cross_voc_dataset_evaluator.py: 134: 0.245
INFO cross_voc_dataset_evaluator.py: 134: 0.109
INFO cross_voc_dataset_evaluator.py: 134: 0.111
INFO cross_voc_dataset_evaluator.py: 134: 0.108
INFO cross_voc_dataset_evaluator.py: 134: 0.158
INFO cross_voc_dataset_evaluator.py: 135: 0.188
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.519s + 0.041s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.380s + 0.035s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.360s + 0.041s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.355s + 0.040s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.367s + 0.040s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.364s + 0.039s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.359s + 0.039s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.357s + 0.040s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.358s + 0.041s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.362s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.360s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.364s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.364s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.425s + 0.028s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.374s + 0.038s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.375s + 0.037s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.373s + 0.040s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.371s + 0.039s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.375s + 0.040s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.379s + 0.040s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.379s + 0.039s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.375s + 0.039s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.375s + 0.039s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.373s + 0.039s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.373s + 0.039s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.371s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.502s + 0.059s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.420s + 0.044s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.392s + 0.041s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.378s + 0.045s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.373s + 0.047s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.370s + 0.045s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.369s + 0.044s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.370s + 0.043s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.369s + 0.043s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.366s + 0.042s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.366s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.368s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.367s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.426s + 0.028s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.340s + 0.043s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.349s + 0.039s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.353s + 0.043s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.354s + 0.042s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.352s + 0.041s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.355s + 0.041s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.354s + 0.040s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.356s + 0.041s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.358s + 0.041s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.358s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.358s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.358s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.315s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [227  82 182 336 628 653 232 341 121 162 629  76 632 643 191 648 328 219
  57 179 207 690 326 217  94 283  54 205  55 169 160 645 221 330 134 665
 275  77 218 327 370 634 118  87 202 196  89 333 224 208 667 129 139 684
  37 285 635 337 228 650 220 329  52 528 247 193 344  69 235 660  35 654
 637 289 183 148 663 276  34 503 316 691 245 519 677 630 175 213 419 279
  39 151 472  92 249 657 651 696  72 428 308 656 305 520 323 239 234 343
 709  65 197 164 167 147 409 317 153 291 647  68 526  44 699 742 640 512
 509 672 288 659  40 250 292 229 338 137 225 334 331 222 649 248 688 315
  38 473  91 511 427 339 230 178 192  50 412 154 671 373 689 287 149 408
 686 560 231 340  78 678 429 170 502 706 163  43 324 185 290 172 277 144
 552 518 413  51 418 707 655 680 673 318 127 523 278  59 142 394 652  33
 109 150 506 417 365 123 241 662 312 113 319 303 622  36 332 223 306 455
 119  66 171  85 367 505 507 457 204 251 309 293 242 368 565 753 679 424
 416  48 194 614 282 510 366 297   2 372 705 592 174  45 641 516 674 132
 458 321 536 587 681 750 392 281 687 642  47  49  86 126 371  46  41 136
 199 198 206  42 145 244 369  61 190 508 522 668 621 697 631 695 253 389
 617 423 186 269 490 426 576 107 644  18 524  96 636 529 335 226  98  90
 165 574 639 675 161 189 557 704 188  80 180 200 414 246 421 106 723 744
 711 390 128 670 286 345 613 236 110 422  32 703 683 375 661 411 364 415
 692 638 633 233 612 342 243 493 685  75 729   3 611 527 195 173 693 676
 168 513 299 393 420  73 176 410 184 104 530 124 669 724 586 515 261 430
 117 553 666 266 111 533 752 575 619 298 374 532  53 425 514 105 517 296
 460  26 445 488 664 468 478 710 125 566  84 461 531 700 682 658 304 444
 469 708 166 294 539 487 626 378 203 525 558 570 254 187 391 325 108 262
 646 625 616  58   1 201 755 694 722 313  64 698 747 504 322 353 537 439
 495 538 740 572 727 181 433 307 590 158 535 564 177 280 492 543 534  56
 112  93 627 310 610 387 240 712 267 466 470 403 562  31 521 739 749  95
  60  63 122 480  71 454  28 114 748 624 623 434 556  83 585 751 484   4
 265 398  70 595 609 216 554 300 395 494 745 714 115 720 135 479 157  27
 259 721 618 571 743 388 348 146 385 352 347 713 116 719 620 467   0 442
 383 257 255 754 746 477 377 284 349 382 567 583 140 471 489 346  30 156
 379 356 355 380 561  67 215 741 435 263 569 381  81 728 268 258 491 568
 589 264 555 138 615  29 399 360 573 252  74  79 459 559 214 320 311 563
 407 584 437 540 604 452 384 351 701 451 376 361 718 582 547 432 397 159
  16 302 155 591 476 496 152 581 726 499 301 716 314  88 462 453 362 143
 448 358 359  62 449 475 486 212 498  97 295 717 256 464 436 497 400 431
 446 548 260 363 501 715 447 450 465   7 596 485 481 500 350 725 544 594
 549 541  23 542 483 593 482  15 545   6 438 546 357 463 209 354 702 441
 588 456  13 131  22   8 474 120 550  25 440 141  12 443 551 386 736 396
 738 130 102 404 133  10  11 731 730 210  99 101  14 577 405  19 211   9
  20  24 100  17   5 273 271  21 734 406 103 606 733 401 579 737 732 402
 238 274 607 272 237 735 597 270 600 602 578 603 580 605 601 599 598 608]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0667
INFO voc_eval.py: 171: [ 74 273  32 348 257  76  27 232  89  75 347 237 341 117 343 317  56 342
 327 272 192 218  58 246 154 224 344 345 283 191  81 279 220  31 349  30
 230 116 323 274 226 128 231 223 346 222 329  79 295 194 221  91 350  90
 238 277 130 217 228  86 334 326 118 322  92  78 319  63 282  80 233  84
 195 168  82 225 245 219 339 302  37   1 239 311  93 196 127 275 320 291
 281  28 158 161 248 314 321  87 200 255 235  34 260 147  11 156  83 243
   4  77 121 252 304  85 151 287 297 289 120 332 309 261 247 324 250 253
 251 306  88 197   0 259 276 284 242 229 170  33 293   3 330 301 290 198
 201 227 331 280   7 153 299 164 263 258   8 103 207 160 249   5 172  48
 124   9  41 129 296 125 155  12 325 202  23 199  36  51 152  10 176 215
 131 241  57  29 119 212 292 256 204   2  61 178 333 286 122   6 181 269
 205  13  47 294 193  18  96 234 244 149 267 159 165 285 150 262 310 133
  38 210 206 265  26 271 335 336 270  99 240 108 126  39 298  42 236 203
 169 254  64 268 313  15 184 171  46 137 177 143 312 105 266 303  67 216
 189 316 102 162 174  16  25 139 144 264 278 307  17 318 300 186  72 180
 104 141 308 337 213  95  73 100  66 208 209  20 148 136  35 140  69 132
 211 112  54 134  50  21 183  65 182 157 113 146  22 135  43 185 145 305
 106 107 328  14  68  19 101  70 166 340  98 142  97 163 288 315  44 179
  71 115 138 187 188 190 173  53 123  24 175 114 338  45 109  59 167 111
 214  94 110  60  55  62  49  52  40]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3279
INFO voc_eval.py: 171: [ 824 2247 4178 ... 6112 6084 6092]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1264
INFO voc_eval.py: 171: [1881 1684 2730 ... 3091 2398 3092]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0759
INFO voc_eval.py: 171: [582 353 580 528 339 563 732 344 288 678 637 749 296 287 304  16 703 592
  24 552 370 564 347 576 676 345  17 711 543 282 535 330 270 583 664 610
 615 685 741 581 613 618 636 545  20 687 371 292 297 305 534 276 912 586
 619 131 309 351 349 280 342 709 803 272 965 971 804 299 286 298 605 626
 308 614 745 701 908 949 340 575 644  32 350  21 289 293 690 567 663 707
 529 285 695 593 547 686 726  15 587 712 748 372 728 551 674 283 801 570
 274 571 279  28 352 715  26 536 527 346 117 300 574 307 731 659 278 679
 617 639 548 616 688 940 705 661 620 922 708 333 742 302 692 572 683 944
 337 684 603 681 166 924 291 152 565  33 589 704 680 960 881 538 947 451
 691 729 969 600 301 303 163 730 719 609  25 966 440 930 627 306 284 762
 566 747 458 869 682 717 737 935 588 740 142 641 495 669 720 466 813 343
 607 978 753 697 970 884 277 689 871  38 877 271 638 918 738 866 549 942
 968 125 431 700  18 334 724 886 750 165 622 807 743 785 164 120 602 733
 718 818 910 810 348 569 751 546 973  30 830 889 698 882  22 727 341 647
  34 130 671 721 608 462 473 162 228 880 432 442 763 954 533 479 579 273
 601 251 768 428 770 136 147 755 710 752 713 155 714 725 530 596 951 578
 821 648 756 368 694 532 584 699 808 909  19 959 499 281 470 249 624 783
 677 118 640  27 696 550 430 542 625 706 819 649 735 594 657 734 722 739
 832 746 736 577 628 963 977 790 917 816  35 754 668 876 568 595 761 955
 939 672 138 232   8 867 234  23 598 611 135 843 573 604 497 873 590 666
 544 336 471 693 757 662 467 846 874 531 321 872 916 540 809  29 825 484
 840 858 290 591 868 204 623 723 275 760 786 585 496 958 759 612 879 230
 875 702  31 129 820 198 429 597 213 539 775 769 937 621 781 139 870 716
 498 599 962 792 758 802 744 201  12 239 170 229 655 827 856 606 665 976
 855 177 367 452 457 967 541 842 141 137 124 946 241 480  72 167 950 235
 961 656  50 450 238 888 153 441 537 481 182 180 172 179 154 187 822 507
 456 181 444 905  52 426 148 318 157 653 796 319 823 483 158 841 101 188
 443  67 485 793 463 925  65 106 920 338 834 122 250 126 243  46 844 887
 837 404 673 438 425  36 907 670 253 437 144 461 931 839 208 207 127 812
 643 891 335 436 332 439 145 797 202 149 151 857 454 906 512 474 317 173
 943  97 482 248 159 658 205 123 460 388 890 427 646  45 469 650 519   3
 791 199 185 777 398 838 475 645 472 132   7 478 952 214 252 424 203 453
 945 861 850 314 237  84 629 140 327 824 394 923 254 477 476  13 420 383
 225 502 240 211 449 316 133 817 772 161 787 218 932 915 200 245 848 557
 294 845 919 408 212 559 513  93 494 119 244 331 805 771 160 975 631 357
 325 778 121 186   2 258 878 183 150 459  66 406 468 247 176 928 184 174
 146 324 936 134 815  43 236  98 852 500 260 835 515 980 175 455 504 178
 189 224 828 934 510 883 806 505 831 128 312 465   0 231 829 448 933 107
 395 265  44 514 634  80 974 957 660 836 323 387 885 941 464 143  69 405
 913 654 105 399 788 811 386 361 780 233 419 503  90 814 851 558 501 826
  48  96 508 401  91 642 116 506 833 156  85  68 171  95 210 795 169 859
 382  94 632 385 104  79 911   1  10 518 651 667 242 511 525 209 381 652
 556 378 789 384 948 938 979 362 633 226   6 215 380 929 447  47 487  49
 407 853  64 103 377 376 262 313 363  92 956  89 403 445 328 489 379 400
 516 523  40 847 256 921 366 784 509  57 488 197  87 981 320 630 849 982
 268 329  77  71 402 493 914   4 526  78 374 964 446   9 315 326 926  56
 953 854 927 423 553   5 520 972 322  11 860 375 397  88 524 555 635  61
 364 560 773  86 522  83 517  82 246 486 359 257 521 764 222 196 261 779
  41 295  81 194  70 561 219 255 767 389 782 433 373  63  62  60 863 195
 562  42 434  39 794 862 776 264 100 899  14 435 360 798 800 864 390 365
 421 369  37 220 774  59 766 422 206 554 901 358 799 418 396 267  58 903
 269 414  51 491 102 765 416 168 266 110 227 412  75 221 114 392 865  73
 902  99 217 259 893 263  74 223 415 898 108 904 391 216  54 413 393  76
 900 192 115 675 111 897 190 417 355 109  53  55 356 113 112 191 892 354
 193 310 410 490 896 411 311 895 894 409 492]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.2186
INFO voc_eval.py: 171: [299 109 320 120  73 108 325  79 115  83  78  31 118 149 126 114  75  37
 116 317  40 111  77 310  55 306 147  80 301  34  68 309 125  92  74 303
 123  72 318 284 304  41 110  32 324 121  30 128 307 113 124  98 117 300
 112  43  49 302  69 192 148  91 313 127 195 106  70  82 152 293 273 274
  90 203 151 153 311  71  81 333  88  67 314  76 229 281 277  35  54 136
  28 154 328 119   2 196  39   0 138 206 155 286  48 315 242  89 189  64
 305 197 339  53 146 233 335 282 276  27 279  52 232 316   3 191 287 326
 285  42 271  87 278 220 207  86 336 168 270  47 280  85 329 289  84 323
 222 150 104  93 131  13 240 160 340 215 308  99 338 331 202  58 253  45
 283 208  51 188  50  57 201 105 272 246 341 156 130 334 251 294 337   6
 227   5 275  22  33 214 132 269 144 187 216 261 295  15 199 291 217 210
 248  29 254 296 103 139 122 235 292 321 238 211  12 243 290   1  62 224
 252 100 288 140 239 247 244  44 332 249  38  59 157 245  61  97  60 236
 319 137  36 218   4  56 250  46 161 101  66  63 204 158 322 212 165 102
 176  65 237 234 258 327   9 193 241 107 231 181 225 129 194 230 145 312
 133 221 226 141  94 259 142 198  95 263   8 134  96  20  24 190 135 228
 205 171 174 298 213 264 172 330 143 209 223  11 219 200 262 170 162 297
 164  19   7 167  10 169 267  14 256  18 257  23 260 166  25 175  16 180
 183  26 255  17 159 163  21 266 268 265 173 178 185 177 182 184 179 186]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.3549
INFO voc_eval.py: 171: [2132 1512 1515 ...  676  679 2411]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2112
INFO voc_eval.py: 171: [ 91  93  86 126  94 234 231  88  28 130  11  98 177  96 181  87 114 128
 232  19  83  13 156  23 129  92  90 230 189  60 139 158 192 212  95  22
 124  39  61  21 163  81  15 157 162 138  24  12 155  73 233 127 193  67
 161 179  27 151 159 223 180 235 213  62  36  18 201  65 125  79 206  85
 160  69 207 105  26 202 227 148 208  25  54  55  40  10  59 150  41 214
 100 199  48 122  72  80 178 196  38  66 226 149 115  14  16   9  47  20
  58  56 191 164 210 119 187  63  45  17 203 121 118  46  57 175 209  29
 111  37 197 200 195 120 194  64 204 101 152 215 141 116 173 140 205 211
 228  49 103 153  42  52   7 110   4 218 109 168 229  68 117 224  99 166
 198 165 146 102 174  50   1 123 225  51   5 186 104  77  43  97  76  71
  82 188 147  53  75  78  70 182 220   2 236   8  74 184 190 107 135 142
  89 108  84   6 183 106   0 176 172 167 137 154 185 219 217   3  30 222
 112 221 132 216 113 169  44 170  35 171 133 136 134 131 144 145 143  34
  33  32  31 237]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0372
INFO voc_eval.py: 171: [ 301  399 2995 ... 2897 3844 2894]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1586
INFO voc_eval.py: 171: [388 103  86 529  93 337 389 113 278 395 262 425 110 104 400 402 490 349
 162 336 172 697  27 419 201 415  87 679 196 525 480 261 271 701 281 408
 485 298 413 393 487 304 204 170 111 534 338 160 224 508 489 177 403  50
 165 863 294 286 707 302 266  54 684 326 486 441  89 422 264  20 443  40
 533 345 182 426 708 385 108  14 179 852 392 484 270 279 522 748  51 407
 526 335 527 300   2  37 678 186 418 161 159 191 259 148 248  98 396  43
 230  69 190 524 820 410 498 189 412 481 850 199 323 164 263 290 555 216
 167 520 265 399 835 233 685 452 314 808 269 207 530 276 184 778 424 283
   3  49   4 535 814 504 683   6 307 274 158 862 831  56 386 860 420 854
 176 198 218 327  62 597 217 483 523 292 812 442  67 686 350 488  13  63
 450  23 253 387 310 282 214 765 417 348 858 109 747 702 813 209 695 309
 468 536  38 121 258 575 698 416  97 497 561 423 246  25 247 334 851  66
 146 689 240 293 157 446 761 682 145 602 750  78 528 462 492  76 397  55
 195 291  92 681 181 409 306 102 287 360 163  44 414 194 192 444 881 166
 297   1 315  58 174 550 398 313 541 825 744 475 107 809 260 321 853 494
 873 232 125  17 352  61 691 299 598 507 127 464 719 319 211 202  59 542
 243 219 827 257  68 212  64 780 531 117 559 328 532 178  45 849 361  34
 603 303 317 288 749  99 193 500 295 818 506 771 197  53 453 521 305 834
 553 819  96 562 280 188 129 272 173 421 499 680 320 816  84 391 341 822
 764 833 700 549 244 200  19 880 821 347 363 457  65 495 149  32 155 255
 226  26  22 551 445 458 605 472 817 329 428 308 447  11 440 604 830 466
  48 811 781  33 832 285 206 208 404 268 743 223 401 355 815 242 471 690
 768 463 451 867 579 703 865 826 567 225 870 874 502 546  85  21 746 566
 343 588 235 706 875  73 187 823 615 116 312 289 250 699 573 133 221 205
  16 316  39 872 766 411 477 333 693  42 776 156  57 538 150 406 143 869
 151 751 836  46 612 139 234 864 120 344 493 171 878 824 505 857 358 449
  47 556 130 828 871 126 856 277  36 325 229 877 628 503 137 879 330 770
 432  83 210 346  15 183 318 275 249 609 859 273 829 124  94 687 296   8
 734 301 213 696 454  35 231 203  79  29  72 227 763 215 394 649 180 245
 142 688 672 722  60 855 496 152 846 119 630 779 482 724 311 616 256 134
 284 772 185 601 324  28 709 745  74   7 118 340 611 618 491 241 364 141
  18 876  30 704  10 251 390 791 131 548 136  71 599 267 789 351   9 433
 381 563 726 135 866 405  82 112  80 465 138 175 254 427 144 375 123 732
 238  70 742 572  77 222 568 587 777 354 331 332 154 868 847 861 586 608
 239 692 237 220 584 100 501 581 436 342 322 132 368 140 787  75 147 339
 790 369 153 629 729 627 585 620 438 544 252  81 762 759 792  24 841  41
 367 455 461 470 756 236 366 769 715 648 356 619  12 785 840 775 228   5
 552 843  52 710 474 676 101 782 786 540 596 459 122 793 632 774 757 705
 448 842 512 614 673 476 359 377 645 767 711 694 837 839 783 374 670 617
 460 637 537 574 844 626 784 721 467 635 357 539 128 376 473 845 848 788
 638 795 594 773 469 838 794 478 671 456 115 592 716 593 105 674 610 362
 570 723 353 607 675 578 595 435 731 564 606 733  91 631 633 613 378 370
 736 639 713 600 677 725 758 646 636 439 643 642 383 372 644 740 479 437
 365 382 380 434 373 641  90 384 647 371 169 640 760 576 582 565 589 379
 634 714 720 514 583 805 755 558 730 106 557 168 577 545 580 737 513 650
 735 114 801 547 554 802 664 560 569 571 799 665 655 728 590 803  88 516
 806 656 797 517 796 883 658 543 800  31 652 885 807 804 884 515 660 651
 653 717 712 754 739 654 718 798 591 727 741 621 738 519 511 509 810  95
 510 659 661   0 752 753 625 662 624 667 623 622 657 431 666 668 518 669
 663 430 429 882]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0798
INFO voc_eval.py: 171: [1516  666  214 ... 1665 1668 1663]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2295
INFO voc_eval.py: 171: [277 308 198 163 300  84 320 287 317 296 251 269 327 154 285 524 283 311
 290  95 101  85 229 336 199 432 103 205 255 195 487 230 271 107 293 216
 302 307 501 425 509 385 249 270 211 201 253  83 319 288  29 100 499  26
 257 488 508  88 431 333 396 123 203 542 506 526 162 546 323 533 292 228
  82 423 504 102 291 426  65 535 121 272 395 503 108 409 489 433 313 505
 312 215 540 527 254 252 428 502 314 256 250 305 394 486 536 206 496 196
 129  25 544 294 295 124  69 326  57 494  34 429 365  96 498 442 500 464
  68 492 161 335 493 273 245 550 165  92 435 528 324 391 337  27 476 275
 522  33 415 315 446 377 520 518 403  36  41 177  38 359 401 243 127  18
 104  66 477  35 227 361 122 152 448 422 214  64  74 512  43 378 434  28
  67 416  98 436 516 554 507 342 463 176 388  31 419 278  90 525 369 151
 437  48 194 545 289 390 158  32 444 427 174 449  53 519  91 141 553 274
 521 491 246 513 139  56  30 490 126 495 160 237 303 172 441 105 197 212
 133 282 558 555 430  72 440  81 367 358 131 356 304 381 128 334 204 497
 517 424 306 478 332 213  87  94 387  22 180 125 514 146 200 159 153 547
 150 389  39 325 231 130  42 233 119 443 445 137 202 247 170 248 156 244
 366 155 418 380 406 460 118 192 529 178  97 157 135 109 454  99  93 363
 232 284 239 457  60 386 515 459 400 188 472 392 466 111 147 360 309 164
  89 279 374  86 193  79  59 136 372 132  19 543 559 134 339 345 106 410
 548 138 344 142 225 234 447 185 470 453  70  40 330 145 455 338 166 439
 316 364 181 240  47 286  46 412 280 144  52 370 541 465 298 241 413 450
 352 322   4 149 384 357  50  44 148 140 411 561  54  78 438 397 169 469
 268 539   9 173 537 264  55 467 182  71  76 560 171 399  45 321 362 417
 226 552  73 281   8  24 179   5 343 347 557  49  75   3 549 340  62 451
 175 398 375 276 371  77 299 405  80 190 349 531 267 114 368 348 260 301
  58 346 341  63 207 407 350  21 266 452 510 408 556  20 167 143 511 402
 414 219 404 551 184  12 461 208 265 318  51 261  16 473 183 168  11 420
 220 482 235  14 223 474 475 115 117 383 187 534   0 258 382 218 112 530
 116   7  23 120 263 376   6 262 471 259 483 379 538  17 242 373 456 113
 238 236 191  13 189 468  61 462 458  10 217 186 532 310 481 479 328 222
  15 480 351 484 210 329 393   2 297 421 353  37 209 221 224 331 355 485
 110   1 354 523]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0964
INFO voc_eval.py: 171: [969 470 109 ...   0 347 837]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0913
INFO voc_eval.py: 171: [192  27 114 112 203  26  14 191 116  47  28 207 151 205 113 105 199 208
  25 198  30  31 118 196 147 115 103 193 181 126 124  36 202 206 104   4
   1  44 120 125 211  11  94  33  56 119  35 201  58 190 100  90  12 212
 122  87  89 197 159 150  54 166 129 170 127  59 152  60 117 157 163   7
 169 121 123 161  46  66  51  32  17  13 111 178  41  76 186 214 209 210
 164 195 131  53 185  52  15  85 189 200 130  77 148 179 154 162  34  84
   6 184  78  88 149  79 187 109 180  57  43   9  29  16 204 101   2 174
  45  38 194 141 139  65  95 213 183  49  37 156 188  63   8 176  61 165
  18  42  39 177  69  96 134 128 142  40 136  92   5 110 167  68 182 153
  98 133  70  48  74  93  62 155  10 158 217 160  91 168 102 221  67  71
 171  55   3 132 220  83  86 218  73 216  50 219  64 215 222  20 106 175
  97  72 173 140 137  75 108  23 146 172 138 143 135   0 144  82  21 145
 107  99  24  81  80  22  19]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4157
INFO voc_eval.py: 171: [18723  6884  3341 ... 21908 21914 16261]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.3126
INFO voc_eval.py: 171: [ 285 1756  236 ... 1875 1871 1879]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2425
INFO voc_eval.py: 171: [185  69 406  73  33  50 463 431 781 803 173 432  52 423 783 788  59  31
  26 771 196  71 394 772 134 195 457 433 181  46 462 395 183  70 452 800
 192 399 135 177 176 440 769 773 805 190 787  30  40 172 212 448 184 246
 397 125  32 743 780  66 779 628 396 455 844 435 210 798 459 717 417  39
 642 136 492 804 426 167 807 792 449 391 179 776  68 413 853 215 182 460
 403 240 124 482  63 438  74 231 416 862 741 499 207 294 738 390 799 122
 166 188 802 421 400 214 430 295 838 178 424 691 746 509  42 855 571 461
 852 839  35 189 159 572 398  54 782 495 454 859 488 651 835 750 504 439
 718 465 392 436 411 645 655 342 464 716 701 575 573  58  55 869 840 427
 236 502 168 863 481 162 213 647 333 348 129 736 453 715 205 422 161 753
 653 126 407 401 410  49 774 420 156 793 343 409 404 837 503 695 405 775
 731 441 458 208 230 180 735 785 860 175 437 414 349 742 858  21 748 191
  34  57 574 204 861 233 745 728 693 714 451 447 778 733  65 842 428 211
 425 648 444 874  28 850 408 721 856 419 284 699 412 466 446 652 732 209
 169 187 434 796 700  20 137 277 878 456 158  45 450 237 429 194 402  47
 287 370 723 121 153 843 876 836 445 643 151 164 501 442 186 312 131 418
  72 393 443 854 415  37  48 508 867 722 127 344 851 364 197 353 697 152
  56  43 165 174 710 132 703 857 217 709 846 694 226 257 245 193 845 279
 873 507 810 834 341 273  29 220 871 160 672   5 754 130 288 510 371 234
 864 841 870 847 123 347 222 154 334 206  60 483 724 296 744 494 727 163
 157   6  27  44  24 171 656  64 170 155 282  75 484 747  41  38 849 650
 654 254 202 770 865 872 150 491 330 141  36 808 868 712 232 242 875 749
 140 877 719 225  67 866 704   7 486 702 848 752  22 500 266 149 467 784
 272  10 147 104 305 646 675 730 216 644 139 235  25 275 203 301 497 649
 777 790  23  53  62 133 297 692  51 734 795 490 314   8 117 345  61 355
 255 806 128 696 148 339 118 689   9 265 725 298 470 323 224 707 302 498
 143 267 740 303 751 241 218 698 726 119 276 274 794 480 713 493 729  98
 280 352 199 243 489 533 659 311 529 708 613 221 569 283 120 268 286 592
 223 198 737 228 309 200 678 705 475 238 559 688 739 292 711 549 285 229
 239 256 281 278 720 801 468 478 244 315 313 615 627 219 622 706 506 582
  15 619 562 597 677 252 338 472 626 545  99 322 469 786 474 512 227 570
 248 476 496 369 250 471 505 307 473 791 684 554 565 253 201 487 108 623
 479 306 586 332 630 310 350 552 101 316 477 687 634 138 890 612   1 620
 319 331 485 686 544 102 563 354 317 564 361  96 617 584 567 249 513 357
 144 579 351 766 103  97 251 624 247 566  18 367  13 532 308 320  95 100
 324 113 546 358 146 888 657 360 365 145 321 768 664 636 366 621 658 685
 340 299 318 759 568 589 356 362 577 891 363 291 115 637 511 142 368 676
 594 618 599 293  17 625 765 304 640 583 337 756  89 825 580 629 663 516
 635 679 641 359  77 762 346 673 797 639 789 553  14 764 889  76 757 269
 588 593 300 336 879 681 557 690 335 555 578 755 674 548 683 111 514   4
  87  11   0 631 638 558 632 665 602 763 761 515 537 758 271 633  12 598
 110 109 814  19 760  16 581 542 528 601 540 826 521 576 270  85 329 815
 680 661 667 107 535 824 524 538 590 666 827 811 328 112 606 816 608 611
 550 522 609  92 605 264  83  94 114 556 591 585 530 116 289 662 376 527
 682 106 105 607 660 520 290  82 526 812 325 604  78  84 547 380 378 833
 813 614 374 523 327 832  81 536  80 534 616 531 541  79 560 587 831 372
 517 263 829 610 767 809 822 830 326 600  91 518 519 375 596 525 258 828
 539 543 551 561 383 381 595 385 388 382 881  90 603 819 373  86  93 818
 259 377 882 384  88 379 820   2   3 823 261 389 260 885 262 386 387 821
 817 670 884 880 883 671 887 669 886 668]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1101
INFO voc_eval.py: 171: [ 67  69   9 256 258 346 131 341  74  80  11 257 350 137 165 128  72  73
 403 259  78 236 345  68 243 337 405 303 344  66  15 349  77 135 384 353
 340 351 240 383 338 336 318 319 157  22  93 102  75 141 301 154 342 348
 111  95 380 239 206 175 133  79 352 172 355 347 317 334 212  20 217 354
 343 339  71 399 138  76 292 246 320 335  23 140 238 302 402 400 411  12
 197 143  70 179 155 204 118 127 237 300  14 306 248 315 201 142 268 132
 129 110 116 144 299 222 185 130 261 112 176 145 321 244 199 245 214 305
  13 304 139 208 136  19 307 322 279 134 104 415 314 241 177 230  60 233
 270 163 158 313 250 372 316 228 227  96 200 394 328 223 101 169 311 283
 209 226 407 196 379 198 100  29 150 408 124 115 242 205  36 271 327 224
 267 219 251   5 229 413 113 220 333  30 247 221  97  94  61 168 324 161
 282 210 213 211 170 207 114 396 294 249 272  17 395 173  33 117  64 186
 381 234 174 216  59  65 215  63 218  41 290 164 108 159 414 329 382 225
 167 178 288 151  62 235 397   0 180 295  92 404 232 231 181 162 126 105
  35 156 190 312   6 103 171 370 106  40 191   7 160 273  27 125 297  56
 331 166 285  54 264  18  32  53  81 291 194  83  85  31 109 260   3  26
 192  25 386 107 369  82 262 254 410 416 293 276 265 332 280 263  90 296
 388 193 183 330  44  42 391  16  28  10  89 266 184  39 363 195 398  99
 289  88 286  47  37   4  84 409 275 120  91 281  98 406  38 149 412 121
   8 269 287 366 390 274 389 189  45  43 284 188 387 277 377 367 323 401
  46 153 152 393 385  34  57 182 203 187 326  52 365  55  58 392   2 362
 122  49 278 253 378 255   1 252 368 374 119 123  24 309 310 373 364  51
 376 148 202 371 298  50  21 375  48 147  86 146  87 357 356 361 358 360
 359 308 325]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1062
INFO voc_eval.py: 171: [489 496 288 487 488 295 505 491 187 499 490 502 669 356 493  50 627 101
 306 494  51 670 666 255  52 630  95 148 351 362 289 358 712  58 492 191
 715 292  62 714 629 103  79 510  73 265 102  98 116  49 297  12 186  71
 260 711 126 349 717 347 672 149 623 104  80 346 661 508 169 710  85  47
 322 503 564 193  46  76 254 361  66 512 526 258 262 535 626  78 105 528
 341 724 287 580 501 498 184 256 520 525 150 299 259 110 642 304 720  54
  69 726  55  15  70 147 343 599 497 725 677 168 121 713  74 664 439 500
  90  88 574 127 282 588 509 114 229 644 196 312 447 633 518 537 153 124
  87 165 671 372 718 660 639 170 161 305  48 506  77 240  14 138 523 631
 640 298 722 645 663 608 373 200 628 585 495 570 257 324 293 655 144 531
  16 527 154 253 267 668  61 625  68  94 354 164 728 139 263 231 270 360
 567 589 716 152 261 597 140 345 100  25 313 202 364  57 587 566 571 647
 730 729 723  65 673 129 433  63 650 205 565 290 107 348 237 426 217 380
 203 197 413 568 206 309 679 173 578 573 238 207 648 146 665 678 727 216
 151  11  89 192 271 421 584 157 569 551 721 370 611 350  82 123  56 507
 236 636 353 342 131 180 731 586 325 659 719 355 136  60 414 156 680 178
 183 166 624 583 369 315 163 119 239  96 145 185   7 308 532 198 547 382
 572 603 601  23 241 160  28 269  67 455 687 475 120 331 504 643 190 646
  41 118 634 709 344 428 416  31 158 135 125  42  22 278 171 189 448 657
 577  99 471  75 560 199 204 446 109 581 451 209 422 352 323 242 612 427
 232 188 683 291 522  91 605 465  17 330 536 434 319 656 222 363 248 130
 517 314 176 321 208 516  24 371 575 374 638  43  33 404 174 591 195 159
 235  53 106  18 266 223 281  81 397 582  45 365 736 284 201 598 576 368
 707 359 367  93 429 225 108  26 632 563 686 676 122 450 579  84 357 681
 228 111 425 449  72 133 431 335 366 264 194  83 484   6 432 227 473 280
 438 436 115 548 553 675 530 649 617 470 635 377 658 609 616  59 437 708
 224  64 112 606  37 435  97 613 317 682 276 338 275 558 219 514 167 283
 230 117 221 212 303  92 394  32 440 408 423 595 652 478 296 469 381 604
 444 181 593 250 602 546 542 162 277 696 559 424 607 155 618 213 393 273
 128 550 327  34 233 172  44 737 739 141 318 252 468 132  10   0 641 637
 302 143 590 244  19 458 182  86 417 175  30  29 430 697 177 511 409 486
 596 651 738 247 249 403   9 545 594 286 482 662 543 667 272 706 600 395
  39 653 692 285 539 406 142 329 134 592 251 418 705 544 610 620 211 619
 137  21 337 274 179 376 654 220 555 412 685 703 695 279 113 378 699 214
 704 339  35  38 320 561   5 621 334 340 407 268 218   1 472 481 243 698
 615 464  27 326 398 453 379 311  40 541  36 246 467 410 459 684 210  13
 328 701 519 234 415 332 674 307 474 702 457 557 215 226   8 462 420 445
 476 456 688 333 556 694 383 386  20 336 396 540 411 480 443 693 419 442
 385 614 549 622 554 405 401 562 375 529 466 460 524 400 463 316 399   4
 534 552 452 441 294 700 245 389 461 533 483   3 454 310 390 300 513 301
 479   2 538 485 402 477 515 732 521 387 384 689 391 392 388 734 691 690
 735 733]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0996
INFO voc_eval.py: 171: [444 443 410 523 386   0 233 508 521 408  20 154 522 427 189 285 151 140
 416 475 383 107 439 478 236 425 204  43 155 290 302 208 116 260 283 193
  16 482 199  46 241 287 511 390 423 420 240 284  12 217 226  83 229 535
 387 235 385 531 202 220  19 384 332 406 206   1 119 286 296  14  33 525
 244 209 143 213 123 288 250 201 259   7 341 438 297  45 445  17  75 156
 187   5 432  58 203 485 211 222  10 207 275 313 428   9 426 300 249 109
 442 258 343 176 231  32 382 479 391 471 105 338 509 238 266 310 190 224
  54 389 142  85  25 225  84 329 534 501  47 512  28  13   2 106 490 191
 118  11 483 186 355 246 291 232 167  61 314 251  18 431 407  70 529 185
 108 530 326 498 403 157 441 245  37 515   3 430 289 533  27 323 265 350
 169 261 415 219 520 114 404  53 269 194  34 452  88 188 312 175 321 124
  30 315  68  38  39  48 158 234 168 218 449 144 293   6   4  67  52 279
  87 120 489 102 327 354 331 440 462 460  89 215 177 223 333 242 253 146
 179 295 122 271 267  15 528 294  63 402 513 480 492 372  81 532 351 451
  21  78 388 133 303 357 413  60  22 153 174 379 196 526 197 499 517  76
 330  49 396 494 364  41 178  99 307 212 356 115 457 346 184 405  51 230
 121  40  55 470 418 272  71 113 152  50 164 358  86 149 463 299  91 163
 248 316 195 527 398   8 298 276 311 159 112 459  92 484 301 166 514 446
 148 412  98 393 504 458 360 228 467 453 461 320 306  36 411  77 318 252
 365 147 345 117 434 361 473  82 128 409 183 422 448 172 273 170 399 337
  59 214 110 481  26 292 126  64 239 210 281 304 400  42 335  66 125 486
 371 516  90  80 339  65 414 450 456 349  95  44 264 268 417  74 237 510
  23 497 336 344 395 454 280 487 263 262 322 305 282 198 221 491 139 278
 180 270 455 340  69 227  62 368 274  57  35 366 324 277  24 319 328 496
 103 419  72 150  56 477 171 192 162 317 334 374 309 325 216  94 519 243
 130 137 394 377  79 127 353 474  73  93 308 101 518  29 161 181 104 160
 256 468 111 173 205 424 502 200 182 429 362 500 421 433  97 131 373  31
 378 495 129 476 100 503 465 401 435 397 376 145 135 141  96 469 370 381
 352 380 472 138 348 392 165 254 363 136 247 359 255 134 347 466 436 507
 342 132 464 367 369 375 488 506 493 437 447 257 524 505]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1417
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1751
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.067
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.126
INFO cross_voc_dataset_evaluator.py: 134: 0.076
INFO cross_voc_dataset_evaluator.py: 134: 0.219
INFO cross_voc_dataset_evaluator.py: 134: 0.355
INFO cross_voc_dataset_evaluator.py: 134: 0.211
INFO cross_voc_dataset_evaluator.py: 134: 0.037
INFO cross_voc_dataset_evaluator.py: 134: 0.159
INFO cross_voc_dataset_evaluator.py: 134: 0.080
INFO cross_voc_dataset_evaluator.py: 134: 0.230
INFO cross_voc_dataset_evaluator.py: 134: 0.096
INFO cross_voc_dataset_evaluator.py: 134: 0.091
INFO cross_voc_dataset_evaluator.py: 134: 0.416
INFO cross_voc_dataset_evaluator.py: 134: 0.313
INFO cross_voc_dataset_evaluator.py: 134: 0.243
INFO cross_voc_dataset_evaluator.py: 134: 0.110
INFO cross_voc_dataset_evaluator.py: 134: 0.106
INFO cross_voc_dataset_evaluator.py: 134: 0.100
INFO cross_voc_dataset_evaluator.py: 134: 0.142
INFO cross_voc_dataset_evaluator.py: 135: 0.175
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.576s + 0.041s (eta: 0:01:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.361s + 0.038s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.378s + 0.042s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.375s + 0.042s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.387s + 0.043s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.380s + 0.045s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.374s + 0.046s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.376s + 0.045s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.376s + 0.044s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.374s + 0.043s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.373s + 0.044s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.371s + 0.043s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.373s + 0.043s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.410s + 0.045s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.343s + 0.046s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.352s + 0.045s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.358s + 0.042s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.350s + 0.043s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.357s + 0.041s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.367s + 0.042s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.372s + 0.042s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.370s + 0.041s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.371s + 0.041s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.371s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.373s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.370s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.528s + 0.033s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.391s + 0.034s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.373s + 0.039s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.372s + 0.038s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.369s + 0.039s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.370s + 0.039s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.366s + 0.039s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.367s + 0.040s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.367s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.366s + 0.041s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.366s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.367s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.366s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.524s + 0.029s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.392s + 0.037s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.374s + 0.043s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.368s + 0.042s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.368s + 0.041s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.360s + 0.040s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.360s + 0.039s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.359s + 0.039s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.361s + 0.039s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.362s + 0.039s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.362s + 0.039s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.362s + 0.039s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.362s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.824s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [330 224  94 185 612 334 228 635  91 122 163 613  52 619 195 667 322 216
 634 615 207 178 181 279 320 214  59 629 136  88  53 167 324 200 218 159
 148 654 271 617 666 623 120 321 215 369  68 203  65 333 227 188 131 179
 217 323 681 222 328  57  36 620 637 246  49 192 149 332 226 653  34 663
 288  67 285 274 291  62 522 282 313 317 245 425 648 465 173 234 340 211
 645 101 248 273 202 682 142 515 502 614 630 154 431 143 296 306 633 166
 230 336 680  74 326 220 658 295  37 238 298 231 622 337 413 674 669 416
  93 696 372 516 250 733 646 247 289 150  47 155 299 138 146 189 225 311
 331 621 249 624 636  40 668 508 286 513 433 638 162 180 509  46 140 287
 657  38 507 170 421  85 644 426  80  35 304 327 221 308 153 171 642 314
 641 499  48 169 418 272 182 545  39 510 363 430 161 112 553  50 229 389
 335 366 747 303 206 127 338 232 664 512 693 711 124 673 518 607 121 695
 243  55 319 115 168 219 325  44 240 640 422 631 275 477 503 315  43  63
 251  33 184 292 297 414 672 187 277 371 659 365 741 370 290 459 649 241
 329 223 460  41  45 415 196 367 198 368 462 500  95  42 531 556 239 364
 676 428 197 193 137 186 201 133 412 678 606 284 339 505 233 194 656 312
 235 341 616 419 417 486 628 424 686 599 580  99 632 110 399   1 183 679
 191 627 689 108 626 732 501 643  81 652 244 362 432 429 601 397  51 584
  56 423 427 651 281 420 639 734 523 565 625 528 190 618 109 611 491 165
 393 392 172 549 519   3 504 717 506  76 160 709 602 576 164 128 261 527
 253 490 683 176 675 174  84 743 294 204 566 517 671  21 687 521 575  90
 374 595 305 145 524 242  69 691 205 701 125 100 119 661 665 520 692 604
 463 670 301 264  75 526 530 605 710 525  78 557  92 546 660 175 650 594
 478  32 498 469 114 111 475 126 199 464 113 655 698 529 677 662 597 116
 596 373  96 647 480 511 130 690 177 390 448 377 551  70 443 395 514 310
 600 449 563 688 697 316 749 276 728 729 398 439 348 740 561 394  72 438
   0 714 256 610 482 694  26 489 265 574 555  82 117 471 254 135 738 404
 123 386 408 532 700 737 735 473 609 263 739 562 472 458 152 300 608 385
  98 535 158   2 213 342 293 278 744 147 603 302 347 708  86 354 554 345
 468 381 343 598 391 380 258 748 560 309  54  29 382 383 742 440 283   6
 447 487  79  31  61 479 736 572 212 746 118 349 707 384 257 266 731 356
 550 548 376 461 411  27 396 699 552 559 730 157 703 346 457 716 357 582
 481 745  97 436 593 403 488  30 571 318 702 492 280 712 139 307 401 570
 441 252 470 573 483 156 547 402  60  66 684 705 361 558 352  58  64 564
 485 533 351 484 496 375 134 540 255 581 493 437 151 452 353 103 141 259
 713 350 359  83 590  71  77 494 435 360 467  89 451   7 358  87 102  22
  16 466 453  73   8 355 455 434 454 539 450 704 715 583 456 405 260 476
 497 536 495  13 534 379 579  17 578 474 344  15 544 210 537 262 706 541
 542   9 442 387 538 577 726 144 132 129  14 543 208 409 378  20  24  12
 104 444 106 446  19 445  23  28  10  11 388 685 719 567 107 725 209   5
 407  18  25 720   4 723 410 591 727 400 724 270 268 718 568 406 105 237
 721 269 722 236 267 585 587 589 592 569 588 586]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0768
INFO voc_eval.py: 171: [ 69 267  29 250 227  26  71 341  70  83 342 232 337 338  47 340 112 314
 188  49 324 269 213 266 241 217 339  28 187 219  80 268 149 111 225 343
 221 313 122 226 215 286 218 233  72 344  74  85 216 222 189 283 212  77
  79 270  81 230 317  57 320 114 126  75 214 170 220 240 237 335  27 274
 276 191  84 192 228 303 316  86  78  25   1 275 247 309 196 229 325 246
 238  73 121 146   4 302  32  76 319 253 322 154 254  82 280 115 328 278
 155 248 123 298  46 272   8 326 243 168 156 295 245 244 306 277 151 116
 236  30 321 223   0 193 273 331 294   3 197   2   5 224 231 327 242  39
 256  97 148 289   6 282 195 194 312 179  18 171  40 258 202 239 284 252
  38 147 125  34 198  31 210 285   7 113 157 118 150 163 249 190 251 318
 201 307  35 207 144  13  43  12 173 261   9 264 178 159 199 235 255 293
 234  89  10 161 128  91 288 145  23 305 130 117 299 259 271 260 101 332
 208 279 265 205 304 310 297  11 257 124 200 262 263 166 203 119  98  24
  17 301 127 186  22 290 164 169 181  62 174 287  36 143  96  42 281  99
 296 291 134  66 162 211 142  61 139 315 182 308  88 329 103  67 204  48
 300 167  64 137  20  21 311  93 138 108 136 183 153  16  44 140  58 176
  15 206 158 180  59 133  95  33 336 292 129  52 132 110  68 135  65  41
  63  90 102 131  92  50 152  60 120 330 165  14  37 160 141 104 109 333
 184  55  19 100 175 185  94 107 172 177 323 334 106  87  45 209 105  51
  53  54  56]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3329
INFO voc_eval.py: 171: [2252 2232 5564 ... 4743 6066 6062]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0860
INFO voc_eval.py: 171: [1756 1573 1577 ... 2860 2232 2239]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0798
INFO voc_eval.py: 171: [619 613 563 ... 340 934 337]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.2170
INFO voc_eval.py: 171: [299 103 323 116 104 110  65 135  74  71 115 109  28 147 107 117 318 305
 152  47 302 122 105  36 310  68 145  70  69  85 301 120  39  80 309 328
  32  78 303  67 114  31 308 317  61 283  30 112  38 108 307 315 123 106
 300  26 319 113  76  84 304  44  92 133 335 193 121  73  83  75 127 325
 194 150 293 146 271  95 201 273  63  42 130   4   0 313 153 278  66  81
  62  64 228 111 205  33  50 337 240  49 274   5 280  77 230 330 195 149
  40 340 320 129  82  48 204 143  51 231 314  79 284  58 326 306 229 196
 189  45 166  86  23  72 247 217  54 275 211 276 277 331 192 252  24 128
 338 154 272 336 286 316 239 291 151  52 282  16  43 341 343 125 159 339
  14 279 220 285 269 191  99 342 248 321   1  93 148 281 311 268  53 144
  29 139 238 199 188  27 243 207 270 212   7  20 134  15 289 223  35 237
 131 222 156 137 118 294 200 187 244 292 334  57 219 101  59 226  37 246
 262  25 233 142 236 324 214  34 198  94 126 209   3  97  56   2 164 234
 242 100  55 295 206   6 138 322  46 251   8 235 288 287  41 241 210 102
 290  91 141  12  60 332 158  96 329 124 227 155 221  11 245 296  98 232
 249 136 132  87 119 312 215 250  18 169 218 197 259 327  88 140  21 224
 216 157 171 213 264 225 170 297 179  13 175 203 263 202 208 168 160 172
 176   9 165 190 161  17  22 298 266 163 256 333  90  19 257 261 258  10
 255 254 162 253  89 260 265 267 167 178 173 185 174 182 181 186 177 183
 180 184]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.3360
INFO voc_eval.py: 171: [2133 1500 1503 ...  669  667 2416]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1709
INFO voc_eval.py: 171: [ 99 104 105  97 133 241 107 235  13 100  30 108 186  96 132 237 236  15
  21  25 161  94 234 106  98  67 101 169 240  24 212 131  68 187  18 200
  23 168 238  91 196 162  41  73  12  26 160 142  80 239 167 130  70 164
 143 199 214  69  16 155 225  31  75  28 166  27  77  62 163  95 208  89
 204  79 201  38 165  66 156 189  52 113  11  90 213 154 216  72 215  51
  22  29  43 206  40  17  59  20  14  42  63 205 182 153 229 210  10  82
  71  19 193 124 170  65 114  61 209 126 207 102 129  64 198  46  60 127
  81 119  50 183 217  58 128 144   4 141 123 221  54 203  45 125 184 157
 232 116  74  39  44   8 115 227 233  55  57 185 111  92 172 194 202 109
   6 152 151  49   1 158 117 178 121  78  53   5   2 197  47 122 112  86
  56 195 222  83 110  87  76 103 192 118 179  88 219  93  84   7   9 190
   0 180 145  85 139 171 218 224   3 177 223 181 188 140 159 120 211 191
  32 134 226 220 173 230 176 231 135  48 175 228 174 137 138 149  37 136
 150 147  36  35 148  34 146  33 242]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0341
INFO voc_eval.py: 171: [2092 2862  279 ... 2769 3675 3676]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1528
INFO voc_eval.py: 171: [389 103  84  92 518 332 422 390 108 270 102 106 257 416 392 331 481 301
 161 398 697 171  85 418 413 406  23 400 110 266 193 196 276 157 675 514
 107 379 470 292 256 393 336 299 473 478 169 307 421 701  53 689 495 105
 865  87 397 226 522 259 200 323 164 702  55 699 176 703 258 524 434 476
 698 304 403  35 306 435  39 179 273 188 409 159 854 485 520  21 330 180
 268 387 264 288 420 148 391 190 511  20   1 679 515 395 247 223 158 254
 298 545 165 489 812 109 776 742 853 825 284 516 208 521  88 261 318 396
  47  86  44 513 183  52 415 492 202 445 211 835 234 308 862 677 471 857
 589 160   4  60 279 765 510  64 821 163 553 205 345 305 204 321 302  38
 683 682  50   2 296 439  22 341 221 529 104 564  91  13 333 244 455 444
 519 339 417  80 855  70 174 219 119 218 818 166 858 781  10 380  65  95
 467 758 242 472 147 197 512 597 145 156 269  43 412  96 819  72 286 681
 874 486 162 383 462   3 832 693 260 278  31 282  54 285 289 517 475 124
 680 385 414 530 181 280  57 290 743 856 438 126 488 539  26  81 468 241
 676 275 496  14 294 678 464 814  46 436 315 590  58 209 777 839 316 215
 291 688 759 189 830 718 479 245   0 770 121 419 550 297 388 220  66 838
 740 483 346 687 595 222 310 170  29 833 534 423 271 203 381 494  79 116
 353 173 227 824 186 233  45 152 523 552 191 448 300  62 474 356  28 402
 538 458 122 283 274  36 694 831 576 441 149 877 542 408 175 556 834 185
 684 779 771 477 206 820  63 449 267 852 598 460 826 192 873 582 497 864
 328 817 837 603 541 450 772  78 410 557 265 311  42 829 813 823 880 767
  48 608 250 704 281 405 440 255  69 437 562 433 231 322 404 217 869 700
 394   7 262  18 177 442 695 312 112 493 335 214 563 746 535 459 828 199
 761 111 151  49 836 782 150 303 287 155 130 881 615 744 329 790 142 741
 526 607 411 182 446  77 871 320 399 456  11  15 763 344 337 487 745 491
 240 543 822 325  41 612 870 128 327 382 547 868 827 720 295  33  68 309
  71  73 859  24 879 201  17 342 729 138 747 224 627 840  61 878 860 314
 293  76 136 178 153 114 355 340 139 354 861 184 360 313 882 249 604 334
 238 236 195 602 229 386 685  37  74 132  67 198  56 463 324  89 773 277
 384  98 123 786 775  32 554 876 754 248 591 143 626 319 691   5  25  19
 482 622 137 127 172 272 372  51 762  16 594 670 338 480  30 129 144  34
 600 872 581 134 216 875 565 146  82 787 401 253 407 135 243 783 140 263
 795 536  12 154 484 120 141 343 540 210 696 728 580 863 237 117   6 573
 866 490 785  40 867   8 131 187 317 359 766 644 376 366 546 133 614 579
 326 532 792  75 349 252   9 625 427 784 748 453 774 674 527 752 352 709
 601 606 623 228 115  99 578 528 235 791 723 239  59 461 212 636 525 364
 561 755 851 849 768 610 351 843 788 764 232 348 846 465 751 363 251 451
 611 757 350 842 430 358 503 692 373 466 760 443 730 706 844 426 793 714
 769 753 640 794 370 841 847 555 246 347 113 635 225 361 568 432 454 447
 452 118 213 721 194 710 605 780 230 778 207 620 609 850 669 845 716 848
 707 593 690 457 101 624 357 639 672 671 686 596 599 642 613 673 789  94
 588 431 365 717 634 633 425 727 724 374 125 712 574 577 368 641 711 643
 592 631 575 630 586 587 638 731 570 371 377 469 429 167 584 637 367 378
 369 362 756 375 734 806 629 533 551  90 428 719 632 571 750 619 705 885
 621 628 566 549 650 559 544 560 501 572 726 548 537 168 715 802 569 558
 567 801 803 804 662 648 809 884 646 507 886 811 798 504 531 800 739 733
 652 807 808 722 663 657 810 805 708 797  27 647 796 651 799 509 725 713
 659 649 645 616  93 653 583 732 738 100 505  83 506 585 735 736 502 737
 500 816 655 617 499  97 498 618 658 666 656 424 664 668 508 665 660 654
 667 661 749 815 883]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0781
INFO voc_eval.py: 171: [ 649 1452  201 ... 1592 1585 1250]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2073
INFO voc_eval.py: 171: [276 307 155 196  79 296 286  77  86 316 313 277 191 251 265 323 194 308
 281 279 148 287 229 528 255  85  82 333 195  95 230 188 291 436 493 101
 515  93 267 103 510 298 432 430 252 216 254 217  26 266  78 511 284 516
 544 400 250 514 495 201 330 228  76 513 200 549 292 399 537 531 285 253
 152 426 496 100 283  62 416 318 209 268 398 538 310 494 151 508 512  31
  64 320 231 504 211 104  83 306 532 427  47 118 289 500 120 492 309 447
 506 332 547 540 433  96 501  29  56  25  66 210 322 509 435  84 273 533
 474 203 393 366 319 247 301  88  41 269 439 485 169  35 551 226 395 244
 429 415  63 552  90 445 503 522 559 407 119 197  32 122  65  92 334 342
 199 405 202  80 213 363 524 166 360  39 507 453  27 146 312 314 394 423
 189  68 521 381 451 290 548  81 288 186 377 168 444 486 331 505 529  19
 272 145  75 455 382  55 102 523  52 499 473 214 237 497  33 526 376 329
 305 498 282 164 556 212 137 153 502 446  28  94 241 448  30 450 190 385
 370 422 106 245 271 123 233 236 121 418 536 198 150 392 321 420 125 179
 519 248 105  37  91 116 115  89 134 391  87 193 359 336 126 249 185 172
 128 428 518  67 357 425  72 384 144 246 434 449 525  23 369 520 299 147
 161 215 170 232 468 141 192 411 154 390 467 561  97 280 364 344 465  36
 149  40 187 346 475 335 480  98 302 463 227 361 130 225 479 362  99 460
 545 368 234 417 437 138 553 124 413 156  38 275 133 352 365  71 348 374
 129 431  20 132 337 372  58  74 343 345 270 388  51 135 295   3 140   9
 175 442  54 139 358 438 562 274  53 558 452 162  43 441 454 373 160 163
 378  44 543 240 278 367   7 557 127 404 443 414 459 419 478 421 183 402
 326 171 349 476  46  70   4  45  73  42 184  69 136 304 546 440 383 457
 165 542 379 109 555 340 173   2  61 167 315 347 131 410 261 406 371 350
 554 389 143 466 157  57  50 311 401 303  59 258 206 108 205 158 142 264
 341  24 535 403 517 409 412 408  17 220 159 481 550  14 375 114 470   8
 112 462 110 117 180 262  22 386 297 174 259 263 534   6 181 113 111 482
 424 238 338 339 317 176 387 560 484 219 483 256 380  21  15   5 242 204
 530 487 539 541  60 260 300 182 257  12  16  11  13 458 464 490 235 218
  10 477 469 224 472 243 239  18 461 178 456 324 351 488 177 489 491 471
 207 325 208 222 293 397 353  34 327 396 221 354 328   0 223 107 356 355
   1  49  48 294 527]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0812
INFO voc_eval.py: 171: [940 121 465 ... 652   0 813]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0763
INFO voc_eval.py: 171: [193  28 118 116  27 119 190  14 207  47 155  29 206 109 117 200 208 199
 195  31  30  26 129 151 192 106 105 128 120  38 181 203 205 131   3 123
 101 130  12   1  89 124  41  60  34 212 202  15  58  35 189  37  93 126
 213  39 160 102 171 210 198 125 139 156  91 165  53 121 132 168 127  59
 209 159  13  61 191 177 133 122 196  63 154  78 185  48 163 115 215  51
 201  46 194  52  65  77  64 158  33  87 197  90  92  81   2 178 164 214
 152  80 166  94  16 103  79 186 183  36 153  86  17 204  10 137 180 175
 146  32  55   5 182   9  54  68  44 162 211  45  40 167 114 187 179  43
 147  71  62 184 170  95 142 138 136 169  69 135  96  97 217   7 104  11
 157 144  42  73  57 134 220  98 161 218  50  75  99  66 216 172   6  74
 219  85  56   8 145   4 149  49 222  67  19  88 107  72  70  76  23 141
 174 140   0 176 173 188 150 143 221 111 100 113  84 148 108 112  83  25
 110  24  82  20  18  21  22]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4092
INFO voc_eval.py: 171: [ 6387  2407  3708 ... 16457 16456 22216]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.2578
INFO voc_eval.py: 171: [ 281  237 1761 ... 1871 1872 1504]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2297
INFO voc_eval.py: 171: [195  80 438  41  49 496 835 203 486 463  51 856 454 836 842 185  40 823
  77 420 459  68  37 822 190  43 422 140 493 141 849 136 200 193 497 483
 201 475 425 455 444 824  72 186 820  47 205 183 847  36 256  32 417 664
 224  75 521  82 469 206 192 129 485 176 832  48 435 897 478 421 846 901
 221 852 416 833 137 191 491 828 770 432 844 681 829 468 187  50 314  73
 173 788 490 447 914 891 787 445 257 785 540 740 512 797 481  56 248 530
 446 853 461 442 219 214 899  66 426 315 423 127  60 795 692 235 418 165
 197 175 239 172 198 697 908 603 786 229 851 439 369 882 768 480 798 436
 684 834 518 467 471 921 456 419 517 482 754  65  52 452 492 132 604 440
 918 881 773 431 534 135 450 495 536 885 437 910 916 170  42 370 605 429
 764 883 789 433 774 825 213 606 460 528 510 462  39 831 837 449 781 443
 466 683 362 794 488 775 189 476 895 514 441 696 374  27 220 780 465 434
 524 826 487 457 451 693 791 182 906 751 298 782 607 245 222  26  79  54
 484 178 893 427 532 489 763  58 161 188  35 243 184 157 458 164 743 539
 922 397  59 905  45 750 900 913 902 424 479 366 473 477 430 291 472 230
 247 223 767 139 894 474 231 926 180 448 779 464 742 887 494 772 848 202
 171 158 428 529 777 920 453 796 177  67 232 470 297 159 682 884 526 759
 196 375  46 145 888  63 126 204 746 898 758 753 912 538 143 766 372 793
  74 330 240 160 168  70  53  44 199 290 138   3  34 857 890  78 249 771
 194  81 130 907 234 799   4 889  61 388   6 169 167 757 377 364  25 827
 216 163 896  55  57  76  30 233 915 904 843 296   5 266 691  29 238 747
 166  33 523 854 368 142  31 303 688 917 821 760 531 359 131 841 155 286
 685 133 174 317 911 880 342 181 341 162 784 302 179 283 215 690 892 792
 349 211 749 146 293 909 886 924 144 919 716 263 498 925 244 227 698 755
  28 515 134 209  69 783 304 325  71 535 288 527  64 903 212 153 718 329
 276 108 217 525 923 502 246 686 284 154   7 855 318 687 218 689   8 741
  83 520 237 695 694   9 253  38 360 299 128 121 537 252 519 745 522 561
 250 295 326 242  62  10 845 500 748 228 264 367 254 335 122 505 148 588
 321 646 840 566 734 339 761 513 380 156 289 765 300 275 312 756 105 236
 255 363 123 776 151 838 207 323 595 285 762 287 241 345 628 580 769 511
 125 733 251 327 277 701 396 506 292 542 507 752 650 778 336 294 113 278
 614 260 152 719 379 662  21 533 226 744 225 261 265 509 790 508 633 301
 499 600 665 649 332 623 663 671 328 347 656 340 344 516 654 124 596 648
 104 320 585 259 343 361 333 395 729 720 589 503 676 732 319 582 338 346
 731 655 597 504 811 210 736 941 258 101 659 657  24 102 939 543 618 334
 501 107 262 652 149 612 393 735 858 563  14 150 337 103 378 371 384 737
 819 106 675 313 805 392 658 699 679 599 308 386 147 609 389 672 602 382
 726 942 586 818 702 850 387 381 593 640 373  23 348 631 624 636 706 730
 116 940 591 804 394 119 601 590 331 613 598 547 806 555  96 721 376 365
 541 621 802 839 938 705 322 610 391 810 617 680 569 390 324 674 801 653
  17 383 738 879 630 660 545  85 385 584  11 574 937 872  84  22 546 669
 280 668 670 666 279 316  16 803 625 717 739 673 572  18 577 118 282 830
 722  12 812 813   2 724 725  15 667 611 800   0  13 807 311 863 111 115
  20  19 707 808 709 638 809 608 309 874 587  98 703 615 639 571 708 281
 644 860 544 861 637 873 635 594 310 306 576 862 110 109 558 112 723 554
  93 864 678 551 677 565  91 629  92  95 357 100 641 114 553 557 627 548
 305 616  88 626 619 208 647 117 642 120 704 568 583 700  86 401 273 550
 651 404 354 575 877 350 620  90 556 878 643  89 406 875 661 307 816 562
 579 727 402 570 573 578  87 564 876 414   1 549 817 645 559 353 592 815
 581 560 859 552 871 356 814 567 728 399 398 358 634 351 930 352  97 400
 869 866 632 409 355  94 408 268 622  99 407 411 868 934 870 867 403 405
 271 410 269 270 933 272 274 865 415 412 413 267 936 713 927 714 929 928
 932 931 710 711 712 935 715]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0929
INFO voc_eval.py: 171: [ 70   9  69  72 264 265 355  86 140 266 354 143 344 352  66  74 134  10
  76 416 254 267  67  16 350 345 346 248 396 175 309 423 361 343 249  23
  81 325  77 357 356 395 146 326 351 186 100 107 114 363 342 148 150 166
  99 349 307 419 136 113  84 252 359 324 246 149 391 364 410  85 155 340
  80  73  71 224 347 147 327 219 328 297 212  20 360  25 251 204 358 362
 353 348 341 411 171 365 124 424  68 141 306 168  22 187  24 137 139 167
 121  13 115 308 231 260 256 153 211 311 190 144 320  78 240 154 208 151
 428 310 286  82  79  75 275 269 215 108 195  83 152 138 236 319 312 323
  61 104 142 305 145 322 221 321 170 238 118  11 384 135 103 234 213 239
 290 318 431 185 162 421  19 383 209 207 242 405 119 317  15   4 250 253
 232 205 120 334 229 247 217 216 101 393 245 206 179 417 235  39 116  98
 123 117 422 291 337 339 289 259 255 131 110 227 193 330 333  44 122 274
 283  33 278 222 183 392 226 218 407  63 214  64 173  14 301 223 241 276
 427 406 394 220  65 280  46 105 163 177  62 302 243 257 178 228 430 181
 130  45 316 133  30 237 282   6 414 109 295  37 277 233 244 380  36 270
 418  40 230  34 225 408  35 381 196 180 106  18  57 258 176 303   7   5
 132  43 112 268 169 102  97 174 199  58 203  87 189 335 296 292  56  31
 192 111 188  88 172  89  29  28 300 182 194 262 398 425 400 293 273 287
 281 201 200 338 432 184  32  47   3 402  12 336 202  48  92  27  96 426
  17 288 429 271 272 420  41 376  42  91 298  95  90 294   2 413  51  59
 329 415 127 412 126 165 164 279 299 401 371 404   8  38 399  60 197  50
  49 409 161 284 377  55 397 331 198 375 191 389 403 369 261  53 285 263
   1  26 390 314 160 304   0 378 125 379 372 128  54 386 129 315 374  21
 382 210 387 373 388 385  52 370 159  94  93 157 158 156 368 367 366 313
 332]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1050
INFO voc_eval.py: 171: [534 536 540 315 535 532 542 539 317 533 546 319 376 740 551 205 683  56
 537 201  58 739 110 538 206  59 281 725 314 780 316 375  68 783 103  83
 385  54 690 320 782  65 162 389 552 114  10 785 685  77 322  55 285 108
 132 113 557  93 687 200 724 125 373  15  84 349 289 742 123 612  63  73
  88 734  52  71  53 163 573 556 572 790 370 545 778  82  16 388 544  61
 182 209 682 395 730 550 555 548 284 630 286  62 116 313 570 781 788 799
 784 329 111 326 288 795 391 283 737 297 372 345 541 567  79 649 164 133
 161 180  57 701 616 543 308 793 796 584 251 779 217 342 746 637 686 184
 491  13 625 553 547 688 565  81 722 560 743 619  96 684 700 321 350 213
 112 712 480 401 174 681 167 613  29 635 727 657 263 726 697 155 171  72
 791 568   8 623 738  78  92 390 371 252 282  91 574 170 168 150  64 614
 374 382 703 151 706 680 109 335  70 293 472 618 218  69 733 177 287 238
 412 640 166  67  34 549 290 647 107 179 222 221 787 602 582 792 137 159
 148 227 379 387 347 298 296 463 622 794 128 259 186 260 624 237 451 563
 141 333 615 130 744  89 188 797  74 786 798 800 662 639 134 741 413 629
 789 291 160  25 262  27 352 226 264 380 689 214   9 583 381 358 655 337
 620 801 341 457 195 135 324 723 394 292 104 636 131 398 353 621 452 384
 142  85 178 215 634 650 453  80 198  26 642  18 698 280 617 721 383 357
 169 715 225 564 494 581 118 754 748 216 228 591 399 714 165 628 516 729
 402 566 194 633 318 223 696 331 173 295 204  60 207 139  45 440  32 514
 256 183 261 266 348 377 713  40 143 611 458 305 258  30 433 119 475 127
 339 507 307  76  17 490 775 576 561 265 192  97  98 203 610 747 294 694
 140  94  14 693 464 467 777 641 115 632 212 638 461 666 735 121  31 244
 806 648 190 145  51  46 117 129 627 487 224 397 187 364 528 449 175 674
  37 631 197 626 716 202 220  22 246 604 661 596  43 360 750 102 489 303
 479 306 522 408 250 211  90 673 704 176 257 579 465 208 343 327 774 249
 468 672 652 708 146 478  19  86 120 243 105   7 219 301 247 126 429 502
 311 671 392 400 444 396 300 460 414 275 378 699 587 210 346 488 705 658
 368 510  11 474 241 439 764  33 592 654 473 254 653  95 196 523 718 471
 459 245 147 152 513 745 598 354 659 608 356 304 393 720 644 651 276 558
  49 445 138 477 403  75  23 144 667 181 595 484 524 492 810 695 486 430
  99 476 344 496 511 277 732 761 386 172 101 719 328 691 717 571 278 759
 728 512 675 365 136 124 100 808 302 455 525 763 122  66 442 409 312   0
 185  87 334 232 660 646 106 235 709 466 656 158 462 645 807 678 149 762
 299 711  50 157 663 443 676 193 643 366 355 199  28 191 664 677 481 154
  48  36  39 707 279 518 240 515 692 338  47 273 448  12 590 588 310 406
 606 153 242 454 234 469  35 359 768 362  21 710 410 752 363 702 415 239
 753 575 554 809 605 506 504 609 309 749   4 767  38  42 369 772 773 189
 509 441 776 351 470 669 450 407  44 517 456 589 770   6 233 270 367 405
 431 731 559 446 253 771 577 229 404  41 736 586 580 248 330 607 255 505
 769  24 361 569 751 332 236   5 268 668 272 485  20 493 432 434 601 766
 594 500 411 679 521 760 597 603 422 421 447 593 436 531 600 267 501 508
   3 498 156 323 340 497 585 416 765 482 599 526 519 426   1 562 437 231
 670 527   2 435 230 336 499 520 483 495 530 325 424 269 271 578 802 503
 438 529 423 419 417 755 428 425 420 418 427 804 757 758 274 756 805 665
 803]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0752
INFO voc_eval.py: 171: [490 454 488 491 422   0 258 579 453 565 577 472 175  23 215 314 251 481
 529 174 159 457 420 320 262  47 526 466 423 176 259 121 332 239  16 531
 225 288 223 246 132 312  52 316  12 568 462 245 267 313 465 588 254 450
 221 424 231  24   1 240  13 421 438 233 122 363 235 581 315 177 269 324
 238   7  37 489 326 162 211  17 287 317 329 372 242  51 476   9  61 261
 536 330 530 249  14 250  10 142 483 470 471 275 374   5 304 214 340  95
 213 252 286 485 484 343 428 566 200 292 130 125  58 551 433  62 521 369
 241 593  49 133 120  28 543  92 362 425 535  11 569  31 548  38   2 266
  70 189  21 265 318 448 473 386 209 319 571  18 435 356 134 218 244 587
 591  32 276 475 123 486  40 289 271 179 444 272 212 257 456 430 345 586
 191 199 161 447 297 431  78 510 247 575  19  81 500 291   6 170  34  43
  56 504  53 358  44 346  41 163 342 129 164 190 487 323 260 181 104 385
 443 391  99 361 482   3 365  64  85 498 325  50 539 166 117 446 203 300
 202  88 224 355 513 367  36 293 570 533 135 222 383 143 311 585 219  74
 198 445 237  96 555 389   8 102  25  67 303 151 382  26 226 574 546  20
 582 549 590 279 503 534 281 217 128  84 387 208  63 461 408 377  55 114
 220 173 514  15 169 255 520 333 256   4 399 131 344 584 137  45  57 376
 532 341  76 390 339 328 283 305 439 393 469 186 509 493 364 180  97 171
 474 518 458 188  90 511  82 559 499 327 140 196 194 527  69  98  46 437
 101 210 187 463 395 321 508 207 290 497 302 350 127 451 375 109 248 512
 515  79 347 167 404 541 505 193 331 537 524  86  72 334 197 253 502 441
 351  71 567 230 309 145  42 583 368 589 592 550 370 299  68  30 349 406
 506 308  60  89 545 296 205 228  66 100 538 295 449 354 492 572  73 264
 310 157 411 229 360 301 464 401 216 427 459 107 278 496  75 294  94 192
 381  29  27 139 359  54 501 172 119 306 206 273 178 298 573  87 243 277
  39 168 452 405 507  83 371 227 432 103 561 268 148 576 353 282  59 155
 337 366 392 547 412 528 234 434 460  80  93 440 116 138 336 468 380 558
 136 307 236 467  33 204 110 118  65 557 185 201  77 495 352 357 578 165
 124 560 348 106  91  48 183 184 232 160 556  35 144 335 147 553 397 338
 384 108 322 409 105 403 396 115 442 195 263 552 112 113 517 111 416 477
 436 523 540 126 455 378 141 154 407 394 398 417 150 388 519 379 182 402
 415 152 418 400 270 414 564 426 156 274 429 149 373 280 146 522 419 479
 516 480 413 153 554 158 410 562 525 544 542 284 494  22 478 285 580 563]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1267
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1613
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.077
INFO cross_voc_dataset_evaluator.py: 134: 0.333
INFO cross_voc_dataset_evaluator.py: 134: 0.086
INFO cross_voc_dataset_evaluator.py: 134: 0.080
INFO cross_voc_dataset_evaluator.py: 134: 0.217
INFO cross_voc_dataset_evaluator.py: 134: 0.336
INFO cross_voc_dataset_evaluator.py: 134: 0.171
INFO cross_voc_dataset_evaluator.py: 134: 0.034
INFO cross_voc_dataset_evaluator.py: 134: 0.153
INFO cross_voc_dataset_evaluator.py: 134: 0.078
INFO cross_voc_dataset_evaluator.py: 134: 0.207
INFO cross_voc_dataset_evaluator.py: 134: 0.081
INFO cross_voc_dataset_evaluator.py: 134: 0.076
INFO cross_voc_dataset_evaluator.py: 134: 0.409
INFO cross_voc_dataset_evaluator.py: 134: 0.258
INFO cross_voc_dataset_evaluator.py: 134: 0.230
INFO cross_voc_dataset_evaluator.py: 134: 0.093
INFO cross_voc_dataset_evaluator.py: 134: 0.105
INFO cross_voc_dataset_evaluator.py: 134: 0.075
INFO cross_voc_dataset_evaluator.py: 134: 0.127
INFO cross_voc_dataset_evaluator.py: 135: 0.161
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.515s + 0.028s (eta: 0:01:07)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.364s + 0.041s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.369s + 0.042s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.370s + 0.042s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.376s + 0.042s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.371s + 0.042s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.372s + 0.043s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.372s + 0.042s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.370s + 0.043s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.371s + 0.044s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.368s + 0.043s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.369s + 0.043s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.368s + 0.043s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.472s + 0.028s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.396s + 0.044s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.398s + 0.040s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.390s + 0.040s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.383s + 0.040s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.389s + 0.041s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.396s + 0.041s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.393s + 0.041s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.391s + 0.040s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.391s + 0.041s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.390s + 0.041s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.391s + 0.041s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.384s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.321s + 0.037s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.337s + 0.041s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.341s + 0.039s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.352s + 0.039s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.352s + 0.040s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.350s + 0.039s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.353s + 0.040s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.356s + 0.040s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.356s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.358s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.357s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.357s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.357s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.398s + 0.029s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.373s + 0.042s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.360s + 0.043s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.361s + 0.043s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.362s + 0.042s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.362s + 0.041s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.361s + 0.041s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.363s + 0.042s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.361s + 0.042s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.361s + 0.042s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.357s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.359s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.359s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 59.897s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [235 344  92 198 332 223 633 660 124  89 634 167  52 638  74 689 677 211
 222 331 658  84 636 659 288 139 189  58 678 188 209 337 228 280 173 637
  53 645 122 686 160 345 236 333 224  73 347 238 142 187 335 226 643 133
 196 155  64  36 336 227 233 342 200 301 346 237 679  34 282 298  50 665
 256 291 681 328 708  99 380 352 178 243  60 690 446 635 477 439 293 258
 671 642 144 655 252 326 146 305 688 306 217  70 702 535 668 314 641 706
 283 151 384 334 225 246 307 308 150 166 426 296 140 259 257 373 340 231
 530 240 349 339 230 428 765 191 687 723  90 518 324 696 322 692  37 435
 182 657 663 527 234 343 315 297  46 163 639 441 661 656 531 156 177 646
 299  47  55 175 781  35 436 313 445 212 143 129 524 377 174 162 181  45
 376 528 281 165  41 125 523 667 303  40 447  48 113 254  38 529 183 697
 515 560  43 325 701 250 286  79 519 673 232 341 739  86 720 427 172 145
 319 704 123 525 284 444 135 207 573 241 350 351 242 699 186 309 194 539
 249 653 778 683 628 295 197 117 722 487 206 425 190 682 210 383 248 382
 654 473  44 433  57 443 137 261 193 713 591  49 192 707 208 685 110  88
 195 647 429 438 169  33 651 476  80 545 199 575 650  39 378 251 747  42
 763 532  62 170 675 434 472 289 734 108 542 338 229 664 521 626 348 239
 423 767 168 498 662 374 161 442 666 379 674 437  71 132  65 304   1 431
 440 775  51  82 179 533 115 312 507 411  96 516 109 381 649 703 317 375
 201 715 565 272 652 604 180 424 432 581 620 202 695 405 430 409 517  94
  85 676 203  69 274 719 632 621 506 520 693 114 594 404 691 712 700 640
 578 271  83  91 311 292 131 541 503 735 684  78 185 522 205 127 648 714
 204 625 670 593 327 669 318 623 253 644 111 116 694 118 269 482 112 536
 617 475 544 538 184 717 128 514 164 285 121 680 728 294 463 569  23 605
 725  93  56 672 152 386 561 490 171 759 176 534 698 136 489 126 526 417
 537 705 462 543 738 505  32 247 493 716  77 461 772 455 574 407  81 402
 566 770 260 504 359 300 761 302 718 540 785   0 245 741 148 330 119 771
 774 255  72 502 389 576 724 385 592 776 721 275 631 410 413 452 221 740
 398 399 577 406 265 501 481 496 451 743 598 273 353 219 290 488 618 572
 485 316   2 619 766 364 777 762 483 456 622 629 287 268 158 624 395 567
 220 630 784 568 393 627 358 394 356 548 471  54 368 453 276 357 396 329
 321  98 369 782 480 469 397 546 580 764 460 563 773 508 422  30 218 360
 403 392 355 486 745 614 780 365 570 616 138 564 760 499  97 310 408 154
 149 589 320 768 372  31 494 474 779 141  87 323 742 769 120 157 595 601
 587 726  63   6 415 450  76 362 729 412 101 586  59  29 495 388 732  67
 510 731 270 370  68 414 602 159 562 709 579  66  21 366 465 367 512 571
 371 590 588 466 478 599 363 102 267 509 100  14 547 264 555 737 736 262
   5 387 615 484  61  95 468  75 354 464 727 266 511 470 448 744 500 467
 550 361 479 553 391 449   7 559 147 513 612 491  11 130 600 757 497 558
 418 263 153 492  27 400  12 733  18 730  13 746   8 454 459 134  19 103
 556  22 551  28 596 390 107  15 554 552  20 557 549 106 603 710 597 420
  17 214 457 458 582 756 749  26 213  10   9 416 401  16   3   4 583 215
 753 711 755  24 758  25 750 104 611 748 216 278 419 754 105 421 279 751
 752 244 585 606 584 608 613 609 277 610 607 783]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0636
INFO voc_eval.py: 171: [ 76 280  34 227  52 240  30  79 359  77 360 259  88 354 355 228  58 202
 357 330 229 339 119 235 279 282 358 281 253 234 356 200 236 118 237  85
 329  32 245 361 158 298 231 124  78 266  31 293 283 243  86  81 238 247
  90  83 201  87 334 336 232 180 120  82 239 345 350  35 286 144  36 244
  29 254 321 314 288 332 205  91 287  89   2 269 315 257  84 252 204 233
 267   4  80 292 208 242 268 346 338 156 307 251 312 265 131 260 248 289
 168 342 166   9 130 165 264 121 347 285 284 304 319 337 179 125 256   5
 122 246 327 161 142  33   7 255  26 258   0 207 328 262 296 344 301 210
 263 143 295 303 241 164  46   6 104 191 218   3 157 209 318 261 206 297
 223 132 203 249 271 134 333 324  48 226 211 155 217 230   8 335 135 127
 162  28 159 250  10  47 317  99  12 126 175 190 185  55 183 152 300 273
 299  95 320 277 311 278 139 224 153 341  37 272 325 151 106 154 348  11
 128 313  19  41 213 308  25 274 136  16  18 275 199 181 123 276 270 302
 138 340 306 174 150 219 133 100 290 184  69 193 212 149 221 309 310 294
  21 216 108  49  71 316 331 322  13 167 169 214 147  68  74 215  45  96
 194 176 192  94  24 145 353  72 173 115 222 323  98 220  65 196  97 305
 103  66  23 140  17  53 195  60 178 146  73 172  50 137  75  15 117 109
 129  43 343 107  70 102 326  67  22 186  51 141 291 351 163  93  27 105
  56 352  64  38   1  42 110 177 116 189 170  59 113 182 187 349 197 188
 101  14  40 112  20 171  92 148 160 198  62  44  54 111 225 114  63  61
  57  39]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3138
INFO voc_eval.py: 171: [2304 1098 4330 ... 6312 6289 6308]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0873
INFO voc_eval.py: 171: [1824 1638 1689 ... 2964 2966 2969]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0867
INFO voc_eval.py: 171: [601 596 580 ... 194 196 695]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1842
INFO voc_eval.py: 171: [288 111 311 100 107 127  67 106  73  70 110 140  29 114 306 103 132 146
 298 292 102 139  99 291 290  84  69  32 118  71  42 297  39 312  65  77
  37  33  30  68 295 296 104  54 300 307  26 119  44  34 105 304 101 294
 308  27 322 120 276 289  83 109 116  63   5  66  75 222  35 313 123 148
  89 145  82  52   3   1 129  72 141  43 186  92  78 190 283 189 264 112
 235 302 143 224 108 271 266  64  41 226 131  50 273  79 320 135 318  74
  59 268  62  46 242 316  45 142 149 326  24  76  80 245 161 327 233 187
 223 321 277 267 301 244 133  28 293 196 299   2 124 198 212 137 115 138
  31 305  36 303 270 125 144 154 275  81 269 221 188 184 147 324 265 239
 152 241 191 121  25 201 204 281 136 328 319 278 325  10 310 272  53  14
   7 231  61  56 274 185 209 199   0 134  58 229 240  95 160  90 113 214
 206 130  38  21 315 200 210 128 183 284 217  94  55 280 195 227  48 182
 216 228  51 282 194  40 309   4 230 202 193   8  60 122 237  57 197 236
 285  96 243 207  47  49 166  91  15   6 126 151  13  98 279 314  88 317
 117 232 234 225 215 238  97  86 286  23 153  93 150  16 218 165   9 167
  85 253 192 258 211 323 213 257 205 203 155 208 219 220 158 168 156  18
 261  22 287  11 170 175 262 173  20  17 249  19 164 256  12 247 252 250
 248 163 162 259 260 251 246 263 254 157 159  87 255 174 169 171 180 178
 177 172 181 176 179]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2927
INFO voc_eval.py: 171: [2132 1514 1072 ...  682  681 2410]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1689
INFO voc_eval.py: 171: [102  95 106 107  99 231 129 104  12 180  33 100  96 130 233 232  14  98
  28  22  66 103 229 101 155 156 235  18  70  25  27 207  75  93 127  67
 234  26  89 161 154  30 191 128 210  71 236 163  11 182 157  31 139 149
  15  41 202  29 190 201 181 160  88 230 151 195  65 162 159 158  87 111
 211 174 209  97  61  72  32  17  52  24 197 213 204 148 208 212 192  16
 196  91 146  19  68 147 109 164  60  40 205  23  73 105  64 198 203 176
 184  58  77  69  78 227 120  51 122  21  63  62 179  13  20 175 199 125
  59 219   4 189 124 221 200  46  45  43 194 112 178 145 126 121  80  57
 123  55  44 216  50 119 226 114  54 153  42  79 152   6 228   9 168  90
   1 118 172   5 193  56 115 185   2 113  81  83 144 150 110 188  85 116
  53  47 108  74  76 171  92   7 183 173  49  82 215  39 186  94   0 177
  10  86  84 167   3 237 214 220   8 222 223 141 165 138 140 135 166 137
 187 217 170 206 169 218  34 117 225 131 224 132  48 134 136  38 133 142
  37  36  35 143 238]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0329
INFO voc_eval.py: 171: [ 283 2113 1599 ... 2774 2776 3675]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1391
INFO voc_eval.py: 171: [372 100  78  89 406 499 103 316 373 102 257 401  99  90 154 461 315 242
 397 388 375 287 367 670  79 165  21 107 675 384 187 104 251 263 405 108
  87 285 162 653 380 450 364 280 476  48 496 460 322 192 841 294 320  45
  81 387 451 681 176 290 218 245 311 504 169 241 243 455 273 159 276 153
 286 503  92 417 157 178 395  30 396 240 474 678 260 459 186 184 183  33
 826  12 138 790 463 283 418 253   3 177  19 757 502 399 370  61 271 277
 249 382 174 212 379 524 453  83 671 160 232 501 493  62 200 151 288 497
 152 658 824 182   1  59 531  40 267 196 810  47 195 248 836 429 477  14
 722 566 837  72 378 173 744 305  37 803 208 259 679 145  66 498 543  41
 156 495 672 158  20 404 830 293 655  46 827 324 306 738  56 264 434 326
 235 376 762 509 247 168 422 492 799 289 430 446 207 318 577 292  88 369
 155 150 365 829 328 209 660 398  77 500 191 442  43 849  73 116 300  53
 797 106 231 188  29 389 403 246 123 400 522 749 255 270 466  28 269  36
  13 494 813  76  50 301 262 809  97 721 811 172 575   8 828 663 673 265
 121 368 654 438 518 297 197 752  39 737 203 291 386 796  91 791 278 468
 407 281   5  98 659 527 147 514 233 381 217 554 313 148 279 656 665 211
 254 421 695 848 535   0 560 569  84 284 393 437 190 426 475 244 719  38
 214 664 856  25  32 758 420 118 847 223 676 741 427  17 234 806 530  57
 215 219 205 221 519 753 167 454 166 266 143  60 808 578 473 390   4 540
 250 139  52 840 109 340  31   6 256 584 295 807 298 544 536 163 801 515
 336 439 114 120 385 746 825  26 567  58 843 795 465 805 424 258 392 812
 680 185 394 312 431 798 592 661  69 802  16 574 472 282 748 593 720 804
 299 850 794 521 274 452 725  93 842 674  74  42 101 402 319 423 517 443
 374 742 180 456 268 323 144 416 383  55  15 507 128 296 137 855  68 310
 800 325 371 252 307 851 470 275 723 724  70 761 230  67 149 838 193 339
 854 171 335 125 302 581 699 740  27 303 831 110 170 272 425 181 464 834
 755 216  64 709 309 366 317 775  71 126 506 767 765 580  51 467 239 224
 133 435 327 605 532 135 726 751 833 130 136 462 845  35   7  63 760 763
 140   2 343 739 759 814 471 545 314 579 261 835 175 839 583 134 432 559
 572 357 419 533 189  23 743 308 141  11 441 377 391 321 127 433 337 111
 444  75 542 132 557  54 662 206 458 591 457  44 769 122 511 304 602 131
 832 844 846 766 119 198 852 331 194 607 732 440 756 469 731   9 238  34
 768 730 750 146 129 142 734 772 201  65 561  49 549 853 505 568 164 520
 236 508 727  10 652 556 586 648 525 696 657 773 614 555 204 222 350 516
 604 684 754 113  18 729 344  22 117 771 613 823 361 558 776 585 329 618
 588 601 534 770 822 733 228 747 587 445 213 668 202 448 589 227 512 447
 764 210 774 570 348 703 436 483 112 547 346 619 745 199 820  82 817 333
 538 225 816 359 571 342 414 428 330  85 818 179 237 736 229 576 410 669
 353 697 700 347 621 690 624 691 573 551 220 226 821 552 553 815 651 590
 649 650 360 677 683 819 667 666 582 338 603 332 334 611 341 411 647 599
 620 415 124 354 704 565 351 362 548 115 363 685 513 682 610 563 349 550
 608 606 705 688 707 617 161 356 694 352 708 345 787 687 710 529 600 358
 615 523 449 355 598 546 622 609 715 859 413 728 612 735 539 630 616 623
 485 537 528 526 412 782 541  80 789 858 785  94 510 693 783 781 692 786
 627 718 639 689 860 488 701 778  86  96 484 632 626 487 780 788 706 625
 784 777 702 628 640 712 631  24 698 594 490 686 629 562 779 635 633 486
 637 711 713 481 717 634 482 716  95 105 595 714 564 480 793 478 479 597
 491 596 636 645 409 643 489 646 644 641 642 638 792 408 857]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0734
INFO voc_eval.py: 171: [1452  645  209 ... 1243 1595 1600]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2277
INFO voc_eval.py: 171: [237 272 169  69 249 256  67 277 238 282  71 213 271 164 281 166 241 225
 243  77 127 246 196 217 258 250  74 468 167  85 197 435 294  72 455 374
 161 269 226 228 377 278  93 216 247 451 349  68 214 215  24 452 485 376
 454 180 441 252 288 199 436  65 348 380 173 439 280 490 361 244 477  53
 370 132  27 471 242 273  56 174 434 186  58 442 227 450 290  91 478  90
 232  79 472 446 198 433 499 443 245 107 387 379  41 371 109  95 130 444
 233 480 448  51 372  73 488 360 415 362 317 171 175  32 194  76 283  55
 453 149 208 172 426 183 299 342 391 427 147  81 449 321 184 262 465 494
 229 170 108 251 168 464  54 381 181 125 355 289 343 276 293 209 447 287
  78 440 318 500 394 185  57 491  94  36 159 438  86  70 385 487 275 388
 393  60 162 400  64 445  28 437  47 263 365  25 206 123 148  30 270  80
  29 327  34 458 462 204 469 202 354  83 131 201 461 345 144 315 211 378
 110  46 235  26 335 386 154 332 497 240 459 117 414 212  18 323 163 474
  33 476 248 344 126 493  75 475  37 369 463  62 133 158 301 231 333 182
 129 122 104 401 346 105 151  87  84 373  89 165 366 501 363 124 341 303
  61  88 498 210 460  92 410 409 291 265  59 200  82 116 389 382 334 313
 302 403 142 292  35 340 417 422  21 357 390 421 111 295 502 128 112 402
 300 195 160 193  66 407 324 264 260 329 392 114 316 234 267 319 187 320
 338 307 486 121 325 314  45 134 143   6 367 358 496 504  19 312 418 157
 375 359  38 479  39 113   2 255 296  63 239 150 395 304 236  40 205 259
   3 118 141 484 416 230 330 115  49 420 481 119 404 383 253 305 396  98
 399 328   4 339 353 503 120 274 350 139 326  11 322 356 146 489  44 364
 298 176   1 145 218 351  13 136  97 495 138 177 155  99 336  50 423  16
  52 152 352 103 268 221 492  48 189   7 135   9 331 223 106  23 279 207
 266 337 473  10 140 219 101 425 222 153  20 224 424 456 100   5 470 156
 137 102 483 368 397 482 257  14 297 188 457  12 220 428  15  22 203 405
 408 384  17   8 398 430 429 261 284 411 406 306 431 432 419 178 347 285
 412 413 192 308 179 286 310  31 254 190 309 311 191   0  96  42  43 467
 466]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0658
INFO voc_eval.py: 171: [918 106 893 ...   0 337 787]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0697
INFO voc_eval.py: 171: [209  33 131 129 132 219  31 207 164  15  47 124  32 215 218 130 210 216
 213 169  35  34  30 227 162 208 141 138 118 134 119   2  54  12  99 198
 133  64  11 114 212 135  13  68  42 174 102 226 139 136  39  62 223 113
  40 143 185 142 137 167  65 101  14 192 170 140  58 230 222  86 229 220
  18  66 214 108 201  43 232  69  85 168   6 182  88 100  96  89 115 224
 146 175 176  49 231  55 194 221  72  44 228  16  87  38 206  90 202  19
 211  17 104 163 225 177 171 217 200  84 197 156  97   3 191 178 155  73
  41 199 181  37  53 196  57 180   9 153 203  77 128  46  56 195 193 105
 148  10  67 107  59  36  51 154 157 117 179 145 183 106  76   1 172  79
 233 165 147 150   7 111 173  74 166 184 144  82 109 236  70   5 235  95
  78  61 238  52  22 187  45  60 234  81  75 159  71  50 237   8  63  48
 239   4 121 186  83  80 103 123 205 161  98 152 110   0 189 120 149  94
 190 112 151 125 188 122 126 127 158  93  24 116  27  28  92  21  23 160
  91  29  26 204  20  25]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4129
INFO voc_eval.py: 171: [    0  6940 14367 ... 16409 16412 16411]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.2258
INFO voc_eval.py: 171: [ 275  227  237 ... 1873 1872 1869]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2164
INFO voc_eval.py: 171: [ 74 445 197  46  61 204 831 487 474 464 427  50 852  33 425 186 841 133
  42 423 191 134 818  39 835  69  35 816 135 199 844 479  43 194 784 462
 457 451 485 203 494 205 825 419 187 814 498 661 535  44 470 846 477 492
  76 177  29 123 889 838  54 476 182 437 827 855 897 189 193 223 418 127
 790 440 424 493 450 465 828 173 458 435 310 848 436  57  68 495 220 176
 829 677 819 781 735 786  45 172 823  72 908 429 886 466 438 782 471 690
 195 459 420 849 131 449 528 891 695  59 794 236 212 373 163 431 518 434
 901 853 693 311 121 125 761 826 830 216 481  62 198 914 442 426  67 817
 484 876 522 472 854 421 851 446 433 748 444 500 909 847 422 473 787 821
 130 607 439 202 772 249 902 489 529 893 453 167 454 525 538  49 774  38
 171 230 877 456 789 369 762 879 428 608 483 611 447 875 752 463 129 242
 443 780 793 368 540 441 692 491 359 791 771  52 452  55 832 488 850 211
 224 183 680  24 430 375 179 888 178 511 221 537  40 490 516 770 776 157
 527 448 291 899 192 694 200 136 824 912 201 768 745 455 469  58 887  37
 475 779 231 517 609  36 524 376 497 286 239 907 480 162 141 610 468 501
 496 158 185 742 783 365  75 911 533 432 890  78 188 773 132 736 760 499
 534 196 166 885 170  77 164  73 486 184 467 225 169  66 190 460 245 461
  41 769 900 482 478 795 765  34  56 898 175 777 892  60  79 222 737 792
 360 778   3 285 161 531 174 880 372 294 180 747 903 159 137 678 764 905
 253 822 226  65 895  32 165  48 750 766 389 521 753  27 219 843 749 520
  23 878 401 882 160 374 181 120 536 884 904 124 329 341  28 214 218 378
   5 687 297 820 746  31 788 339 755 839 213 681 283 689 910 168 154   4
 883 281 217 751 288 785 265  64  30  47 290 696 505 280 232 523 250  25
 894 128 345 714 913  26 356 815 881 210  71 740 243 153 744 713 126 691
 313 298 682 139 896 321  53 140 252 240 209 906   8  51 833  70 513 207
 138 340 244 683 519 227 342 142 237 262 684 152 215 255 686   6 314 758
  63 562 685 102 502 532 834   7 326 278 679 738 688 840 292 296 251 234
 539 367 279 856 357 122 775 842 273 754 741 322 504 228 289 155 247 507
 330 739 763 293 563 150 145 836 258 759 659 509 254 287 117 743 721 282
 648 317 343 308 277 757 318 767  98 542 295 651 619 235 323 264 151 107
 332 756 730 512 515 380 233 731 587 284 381 206 580 259 156 238 632 508
 364  17 530 386 344 241 248 346 636 336 358 246 650 702 510 118 325 506
 119 229 664 324 660 503 257 260 604 526 603 597 328 514 337 334 624 669
 596 726 600 585 331 400  97 591 335 658 398 315 333 653 924 390 309 717
 732  94 716 263 656 394 543 560 927  21 623 733 727 395 655 728 631 581
 729 370 586  96 144 147 347 670  95 616 146 256 813  12 675 208 845 261
 634 303 812 363 798 366 657 148 805 379 143 592 544  99 384 925 612 100
 101 654 622 382 722 391 114 605  19 388 928 397 554 613 546 338 697 618
 399 803 703 627 800 396 837 807 601 672 874 923 320   9 614 595 115 149
 598 602 734 633 362 582 106 668 926 387 392 796 305 718 393 327 361  89
 383 599 802  22 873 666 545 312 676 541 593 621 319 111 663  20 589 371
 385 575  15 704  80 872 316  13 797  14 275 628 615  81 570 671 799  18
 665  16 720  11 801 866 699  10 274 109 588 105 620 110 804 667 276 806
 306 579 629 705 572   1 302 662  90 584 617 867 641 104 103 108 645 859
 577 574 548 715 858 594 698  88 557 304 553 551 640 583 642 647 673 555
 300  82 550 573 639 871  83 558 113 674 112 299 719 700  93 578 652 272
  86 649 565 576 116 568 701 870 549 377 630 408 355 869 638 571 868 625
 301  85 606 626 569 567 556 811 810 352 637 561 409 547 644 552  84 411
 646 349 566 590 809 351 643 857 724 559 564 864 808 404 350 860 354 348
 917  92 415 863 405 635 353 725 862  87 412 406  91 723 865 861 416 268
 403 407 410 307 269   2 919 270   0 402 413 271 417 414 266 915 711 267
 918 709 710 712 916 707 708 921 922 706 920]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1050
INFO voc_eval.py: 171: [ 54   6  50  53 179 178  57 247 180  13  89 246 238  46   7  55  93 278
 244  52 265  87 280 242 240  69 124 239 220 209  47 250  95 222  64 245
  56 248  19 167  77 116 243 236 219  63 208 272 174 252 237 168  97 251
  51 249  48 114 235 165  86 241  99  96  98 277 264 224 221  92 106 273
  79  49 126 169 102 139 201 282 151 149 155 207 171   9  91 225 170 218
 144 112 223  82  78 104  71 111 110  90  17  94 211  67 103 210 101 287
 184 143  88 100 163 105 142 212 191 183  68  80 117 125 146  45 160   8
 159 109 129   3 162 153  81 281 131  74 121 145 196  66 187 150  12 216
 283  62 141 115 154 217  29 148 270 123  31 231  65 279 227 140 230 166
 147 134 197 156 186 195 284 173 190 234 119 157  14 233 120 215 164  26
 188  70 161  10 269 152 175  24  73 185 158 172 276  85   4 118 271 261
  15 286 204 205  76  35 262 122 199 275  59  30  75 182  33 113   1   5
  27  60  16  28  11  61  25  34 181  58 285  41  22  72  38 232 202  23
 189  42   2 192 203  43 198 267 268  36 133 194 193 177  44 274  40 259
 128 127  37 135 200 266 137 226  32 130 138 136 257 260 132 108 214   0
  83 206 254  20  21  84 263 256 258 176 255  18  39 107 213 253 229 228]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1138
INFO voc_eval.py: 171: [511 517 513 512 300 520 510 509 516 303 522 688 362 305 525 197 198  56
  53 646 514 689  55 200 109 302 515  71  83 299 301  11 683 529 360 656
 277 121 100 193 740 533 744 373  70  60  52 743 528  80 375 159 551 308
  15 363 334 271  64 748  50 680 113 692 648 374 553  88 118  51 107 691
 739  75 524 116 650 583 274  58 160  89  16 202 532 521  61 701 601 749
  84 526 313 649 359 546 203 180 644  77 518 269 738 742 519 134 298 376
 660 272 752 545  54 270 268 326  96 693  63 127 356  12 355 292 530  91
 371 161 698 745 587 123 741 675 158 283 331 755 711 310 238 589 543  79
 617 685 645 111  93 523 596  31 131  10 594  62 651  86 661 664 584 181
 214 358 469 606 586  73 747 681 605  81 623 208 165 647 149 164 588 562
  66 539 679 671 550 563 369 252 329  59 151 559 169  72 243 250 643 173
 357 391 549  68 674 279 162 124 227 585 289 179  36  67 140 458 608 527
 642 215 273 751 108 690 536 133 754 687 135 210  18 561 452 379 431 168
 593 753 226 574 609 122 372 542 604 163  30 332 750 126 625 392 317  27
 697 335 607 157 182 444 614 618 211 112 703 315 433 621 757 756 366 544
 560 183 207 275 367  90 248 654 309 381 278 590 249 129 599 104 746 591
 316 652 184 306 322 217 592 166 175 176   9 699 125  57 218 612 339 432
 167 212 106  28 653 595 709 254 602 710 245 598  85  99 219 189 678 304
 537 370 557 114 702 132 377 378 206 368 330 136 361 472 201 438  32 280
 696 418 142 338 676 287 246 251  23 213 195 314 321 253 706  74 364 138
 247 282 196 365  78 178  94 170 177 619 411 566 290 498 667 255 144 120
  17 101  25 597 668  37 288 137 199 380 704 763 281 145  87 439 430 603
 582 673 194 611  33 231 665 658 128 493 205 115 216 705 187 634 657 626
  45 548 633 454 440 632 185 395 628 708 455 457  20 234  49 148 624  46
 600 387 736 684 466 567 235  35 443 616 237   8  13 677 143  47 501 344
 492 232  65 497 346 734 576 244 416 139 534 631 579 558  95 412  26  21
 341 552 285 204 662 407 327 481 171 620 318 286 209 230 328 737  76 102
 613 295 475 485 456 436 153 724 350 174 448 424 541 147 146 568 655 682
 610  97 263 192 119  24 725 462 507 233 441 264 453 191  48 265 468 336
 531 571 105 434 325 241 291 627 490 735 447 767 266 491 141 408  69 152
 721 342 565 666 117 713 765 311 228 188 239  14 347 465  98 695 629 464
 320 284 502 639 103  92 564 110 615  82 700 715 622 190 130 425 635 659
 155 422 172 672 150 474 669 340 449 388 421 223 489 722 333 437   7 555
 294 348 426 446 636 535 354 353 154 312 267 240 482 349 345  44 556 670
 663 186 242 429 707 343 764  43 486 686 726 450 445 471 578 297 547 386
 442 229 694   0 766 570 496 337 261 730 352 351 470 451  38  39 716 638
  40 435 156 723 712 296 460 637  34 420 383 393 580 389 494  41 581 390
 293 276 409  29 495 459 324 385 236 382 732 577 428 714 630 427   4 488
 319 423 417  42 729 731 419 641   5 225 483 506 733 307 384 487 394 540
 222 398 727 477 410 260 640 224 573 259  19 575 484 569 401 463  22 479
 572 503 257 414 538   3 728 500 413 467 473 478 220 323   2 461   6 480
   1 258 256 221 404 554 508 476 415 499 758 262 505 504 400 396 717 397
 406 399 405 402 403 720 718 719 761 762 759 760]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0668
INFO voc_eval.py: 171: [502 500 462 435   0 271 601 461 600 485 586 182 325 229  24 268 494 543
 433 181 166 474  49  17 272 541 184 476 255 332 241 545 340 240 345 261
 301 128 589 274 267 142 608 432  13 441 335  58 234 459 249 262 236  56
 254 339 282 326   9  25 251 434 129 501 333 481 602  18  99 257   3 276
 223 263 300 169  15 259 544 376 247 328 331  50 151   7  37 264 269 496
 277   2 228  10 439  60 548 482 488 102 442 329 287 227 235 480 443 356
 386 497 495 567 388 587  61 316 299 209 563 145 446 549 564 138 534 370
 260  57 327 590 440  32 436  22  19  53   5  30  78 127 358 279 265 198
 486   1 224 278 595 350  43 457 130  65 136 609  98  82  34  20 312 498
 487 226 341 607 140 302 284 520 208  38 364 131  90 465 400 197  11   4
 597 139 458 294 309 168  52 176 454  87 499 516 187 230  35 373 149 170
 407 199 303 605 270 408 280 289 173 273 349  51 509 591 103 258 550 212
 233 513 210 106 347 189 252 305   6 203 124 291 232 238 310 111 547  12
 554 134  46  45 456 551 381  59 150 598 455 231 295 357 560 524  54  16
 571 222 322 606 361 546 506  55 141 483 156 315 403  28  26 180 565 472
 366 397 517 314 396  73  89 398   8 526 533 418 511 194 121 604 451 104
  14 478 503 351 489 105  77 107 323 393  48 296  71  95  86 399 109 507
 450 175  83 447  72 540 135 522 552 308 411 603 521 201 484 452  42 466
 220 391 508 579 377 383 470 177 205 188 530 237  21  94 218 449 313 266
  93 379 405 593 415 348 137 523 389  80 527 352 588 518 174 337 380 202
 537 318 206 112 422 569 519 371  41  66 108 245 144 215  68 118 426  76
 221  70 369  63 409 242 525 512 165  47 553 317 384 610  69 133 359 338
 246 342 596 311 285 304 464  88 207  44 200  67  85 293 362 413 178 275
 256  74  31 171 126 448  79 306 582 460 471 363  96 321  81 336 196 211
  29 225 213  91 185 100 320 475  64 330 354 444  92 288 292  62 490 244
 160 575 505 186 467 114  84 334 477 395  39 250 438 123 599 406 479 193
 580 385 239 360 344  27  97 514 353 324 577 542  75  36 179 561 101 116
 219 172 427 375 153 394 143 248 214 555 581 374  33 167 578 346 372 365
 576 378 117 125 355 573 319 132 307 110 469  40 515 217 297 152 195 510
 147 368 192 367 424 570 558 343 382 216 253 243 204 536 410 122 594 566
 162 429 148 592 420 183 191 286 473 119 115 421 419 463 468 568 414 390
 158 146 157 431 425 155 290 412 190 528 532 113 416 120 453 572 445 159
 392 281 585 428 417 402 401 437 283 559 163 154 161 535 556 387 430 404
 423 531 539 164 562 538 574 529 493 557 492 504  23 491 298 584 583]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1307
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1539
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.064
INFO cross_voc_dataset_evaluator.py: 134: 0.314
INFO cross_voc_dataset_evaluator.py: 134: 0.087
INFO cross_voc_dataset_evaluator.py: 134: 0.087
INFO cross_voc_dataset_evaluator.py: 134: 0.184
INFO cross_voc_dataset_evaluator.py: 134: 0.293
INFO cross_voc_dataset_evaluator.py: 134: 0.169
INFO cross_voc_dataset_evaluator.py: 134: 0.033
INFO cross_voc_dataset_evaluator.py: 134: 0.139
INFO cross_voc_dataset_evaluator.py: 134: 0.073
INFO cross_voc_dataset_evaluator.py: 134: 0.228
INFO cross_voc_dataset_evaluator.py: 134: 0.066
INFO cross_voc_dataset_evaluator.py: 134: 0.070
INFO cross_voc_dataset_evaluator.py: 134: 0.413
INFO cross_voc_dataset_evaluator.py: 134: 0.226
INFO cross_voc_dataset_evaluator.py: 134: 0.216
INFO cross_voc_dataset_evaluator.py: 134: 0.105
INFO cross_voc_dataset_evaluator.py: 134: 0.114
INFO cross_voc_dataset_evaluator.py: 134: 0.067
INFO cross_voc_dataset_evaluator.py: 134: 0.131
INFO cross_voc_dataset_evaluator.py: 135: 0.154
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.373s + 0.041s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.359s + 0.041s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.357s + 0.044s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.359s + 0.042s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.367s + 0.042s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.365s + 0.042s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.364s + 0.042s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.366s + 0.042s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.365s + 0.042s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.364s + 0.041s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.362s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.365s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.365s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.542s + 0.039s (eta: 0:01:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.388s + 0.040s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.381s + 0.046s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.374s + 0.045s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.369s + 0.044s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.372s + 0.043s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.380s + 0.043s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.379s + 0.044s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.376s + 0.043s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.377s + 0.042s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.375s + 0.042s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.377s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.373s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.542s + 0.050s (eta: 0:01:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.372s + 0.037s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.366s + 0.038s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.374s + 0.040s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.367s + 0.040s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.367s + 0.041s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.365s + 0.040s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.368s + 0.040s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.368s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.367s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.366s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.368s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.367s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.508s + 0.038s (eta: 0:01:07)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.371s + 0.038s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.363s + 0.036s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.360s + 0.037s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.365s + 0.040s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.355s + 0.039s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.357s + 0.040s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.355s + 0.040s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.354s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.353s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.354s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.356s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.357s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.801s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [216 316  83 306 206 197 584 607 119 152 585 590  80  50 637  61 624 305
 205 586 132 606  89 605 264 164 192 182  56 210 310 623 594 162 588 177
 157 257 592  51 114 137  93 219 319 138 634 320  62 220 149 178 308 208
 189  72 317 217  33 276  70 629 144  34 311 211 272 614 659 139 267 321
 221 300 609 617 315 215 587 643 259 325 225 638 141  47  59 653 235 358
 135 165 362 400 408 231 591 285 236 439 301 283 599 281 612 271 610 615
 282 118 656 279 226 642 292 200 134 657 131 212 312 209 309 237 234 641
 179 295 492 399 349 207 307 714 620 291 298 646 391 273 595 393  35 671
 171 489 166 603 355 403 277 161 191 115  84 731 313 213 650 589  43 163
 354 478 269  41  36 485 630 627 611 406 494  44 167 229 107 193 297 640
  39 153  40 655 280  92 120  53 398  45 619 230 527  37 725 631  82 392
 187 151 488 263 514  38 289 604 258 302 181 185 635 481 480 576  86 474
 407 464 123 172 487 228  81 361 486 668 160 360 658 195 262 110 596 296
 662 483 170 405 449  77 633 545  46 544 194 484 649 180 434 401 404 685
  42 222 322 286 323 496 223 600 397 502 270 601 613 214 314 224 324 670
 176 265 694  57 713 651 184 278 390 143 574 625 359 396 104 109  32 716
 622 394 156 402 318 218 723 350 436 356 140 395 175 188 173 497 457 169
 616 351 150 122 287 482 105 639 117 154 168 248  60 669 268 238 108 645
  69 454 477  85  64 159 378  71 155 524 666 648 353 519 582 636 433 294
 467 572 284 475 602 679  48 473 290  79 598 644   1 476 500 196  49 250
 593 533 597 146 608 174 352 183  76 466 106 190 667 186 227 721 128 552
 626 654 158 531 621 111 570 565 376 618 661 303 664 233 363 381  87 546
 357 695 425 126 490 479 499 628 260 647 568 443 374 515 495 424 451 493
 652 719 632 526 663 534 239 665 121 718  68 274 547 520 686  73 708 386
 563 365 501 438 498 116 463 241 332 680 717 722 491 379 142 573 417 232
 734 577 673 711 553 535  75 113 529 326 370 462 348 437  25  31 456 465
 251 366  78 201 724 446 249 336 377 112 203 275 383 414 329 733 579 342
 710 364 461 442 371 580   0 330 418  91 204 368 712 571 687 338 720 245
  74 583 293 333 343 672 266 455  63 369 148 411 304 299 252 375 691 202
 380 328 715 569 423 261  55 564 549 127 288 145 347 522 728 726 689 441
 709 444 578 567  58 133 346  96 130 528 448 335 575 415 581 729 460 518
 345 136 409 246  52 543 389 384 562 453 339  29 517  21 561 730  95 447
 340  30 382 505 147 459 688  90  26 541  12 468  97 337  94   5 327 540
  65 471 530  67 682 341 548  88 458 428 334 693 427 542 532 452 551 513
 536   9 440 512 523 470  66 676 684 410 344 675 674 516 521 525 331 244
  54 503 124  11 472 508 677 683  13 435 450 506 430 367 413 243 426 511
  18 678 240 509 125 681  14  20   7 247 445 469 550 558 566 242 504 429
  22 412  17  23 705 432 431   6 690  15 727 422 387 692 372  19 510  10
 253 254 129 538 420 421 101   8 507 416 199   3  16  28  98  27 419   2
   4 660 255  24 704 100 697 385 373  99 102 701 198 559 537 706 700 388
 702 256 696 103 703 707 539 699 698 554 560 556 555 732 557]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0525
INFO voc_eval.py: 171: [287  78  39 234  54 233  33 264 371  81 369  80 364 347 231  89  61 211
 288 366  93 345 286  32 289 128 236 238  65 367 237 256 242 248 365 126
 209 338  34 305 368 245 370 132 269 166 290  87 259 253 239 246  82  85
  91 241 339 186  88 342 210 252 353  79  84 214 341 243 235 325 247 250
  83 127 293  31  37 322 356 263 273 215 294 321 297 271 261 300  92 274
  90 295   9 316 153  86   5 298 232 272 343 240 173 164 333 254 213 249
 270 139  11 292 175 355 351 317   7 251 257  36 291 189 334 262 349 268
 134  25 172 308 332 142 182 313 217  35  44 129 169 133 299 267   1 258
 151 301 304 266   8 352 165 303 312 196 344 221 329 118   6  57 244 212
  51 227  49 260 354 216 314 222 131 144 152 265 114  29 147 255 277  12
  10 225  14 143 162 230  41 218 159 170 167 180  38 285 328 193 284 283
 160 228 350 190 309 306 161 278 282 335 116 145  58  13 115 163 157 138
  26 324  20 136 135 307 315 219 191 323  45 319 188  19 105 140  17 358
 281 276 326 280 275 106 320 223 340 302 137 296  22 224 279 311 119 203
 310 181 348 192 130  77  69 327  15  43 220 185 337  74 363 103 111  76
 155  27 330 148 174  46  68 176 226 197 204 183 113 149 336  18 104 318
 202  67 123  66  71 178  56  24 331 200 141 120 201  16  70 110 124 109
  23  72 179 154 108 150 194 361 112 362  75  64 125  73  28  59   3  50
 156 102 195 360  60  48 146  40  62 122   4 184 117  21 359 187  30 107
 158 205 346 207 206 168 171 177 357   0 208  97 121  47  42 198  55  63
 229 199  96  98 100 101  99  53  52   2  95  94]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3170
INFO voc_eval.py: 171: [2187  627 2170 ... 5866 5827 5851]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0508
INFO voc_eval.py: 171: [1776 1596 1685 ... 2268 2856 2858]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0763
INFO voc_eval.py: 171: [631 386 575 ... 207 211 203]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1736
INFO voc_eval.py: 171: [293 111 318 103 109 122 143  30  68  70 301 117 114 142 133 106 104 303
 124 108 296 141  33 325 102  31  79 294 107 101  44  34 295  41 300 115
 309  84  37  26 305  47  74 299  40 105 314  28 310  71  67 312 116 118
  54  38  36   4 298 119  89 110   3 146 151  78 281 123 313   1 225 321
 121  92  83  80  76  66 145  45 144 227 246 192  43 327 188 236 331 306
  72  69 113 228 138 201  63 276 131 191 287 277 326 148  29  82 235 270
  49 272  27 324 134 240 152  24  77 137 275 243 274 140 130 129  56 273
 332  75  35 316  81 149  73 226 139 150 307 315  61 147 280 163 297 278
 202  46 334 238 189 127 304  57 155 200 190   0  58 311 216 320 271 197
 230  64  60 239 195 157 224 193  25  94 282 285 136 164 333  96 132 199
 207 279 185 288 232 244 317 187  39  90 217 283  55  65  59  11  48 112
 322  50 126 186  32  16 241 214   7 125  62 302 213   2 128  53 242  42
 308 237  52 229  19 286 330   5  15 206  51 210 135 231 284 170 183 120
 269 100 233 184 220 323 289  97 219 245   8 234 194   6 196 262 154  88
  95 290  86  99 171 168 203 212 218 261 208  85 319  98  91 153  93 209
 215  23  87 328 221  17 156 258 205 265 172   9 254 169  10 223 160  21
  20 198  13  12 222 204 211 268 167 159 329 292 264 291 248  14  18 263
 251  22 266 249 176 267 252 166 259 260 178 257 174 247 165 162 250 255
 158 161 256 253 173 177 179 182 181 175 180]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2820
INFO voc_eval.py: 171: [2222 1105 1571 ...  692  691 2519]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1669
INFO voc_eval.py: 171: [107  96 233 128  14 181 104  94  35 100 235 234  16 129 101  32  25 232
 106  69 153 105 238  31  29 154  20 212  76 125  30  75  70 161  93 126
 213 160 240 152  33 214  13 155 158  18 191 147 102 182  81  36 139 204
 159  43  91 203 196 236  24 239 144 150  71 231 201  68 157 237 130  99
 175  90 156  64 190  27 110  19  28  78  44 127 124 211  95  34 206  21
  54 215 146  79  73 108 192 145 174 176  66  17 210 216 199 163  82  62
 194  12  83  22 198  59  74  61 151  23  67 205  60  15 200  63 120 121
 178  48  65 186  26 223 122  46 224  53 123 197 171 202 112  49 148 195
  45  58  92 221 162 103   5 115 177 172  57 119  47 230  56  10 116 187
 220 179  52  84   2  55 183   3 113 167 170 143  86 189  97   1 111 149
  80 109 193 117  50  98   6  77 185  88  72  85  42 188  11   0 217  89
   9 166   7 225 173   4   8 226 222 180 218 164 165 141 136 138 140  87
 137 118 208 209 207 169 168 219 184  37 132 227 229 228 133  51 135 134
 131  39  38 142 114  40  41 241]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0358
INFO voc_eval.py: 171: [2177  379 1646 ... 2856 3785 3786]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1044
INFO voc_eval.py: 171: [368 104  77  90 404 106 315 491 401  92 366 369 100 251  79 385 394  96
 154  91 373 457 236 279 370 667 379  17 108 164 670 400 184 498 280 260
 243  43 160 383 374 843 282 677 472 360 273 652 278 447 317 458  44 188
 488 324 309  80 313 497 403 239 189 321 448 150  89 174 217 792 395 262
 267 676 237 158 454 269 828 396 255 183 755  67  25 215 414 235 495  30
 167 455 181 214  10 136 247 275 467   2 465 311 460  55 242  14 264 425
 523 378 531 200 494 362 377 179 492 744 234 147  42 840 826  53 838 171
  58 201  86  11 195 319 450 281 812 194 564 149  37 431 152 486 259 153
   1 283 657  62 829 471 284 539 298 736 489 252 760 177 155 833 720  39
 182 430 675 148 245  34 365 156  51 170 445 669 258 145 322  15 855 575
 157  54 490 294 505  41 371 674 192 292 151 299  71 107  68 805 659 832
 415 653 398 241 438 748  98 487 122 246 387 168 420 813 473 361 316 678
 325 173  70  23  56 208 399 750 285 263 485  47 521 845 493 814 204  45
  52 116 735  40 718   9 830 240 850 212 436 574 548 375 310 121 256 288
 253 191 560 468 190 811 507 517 261 250 141 511 276 801 860 496  29  35
   7 527  36 390  63 222 662 532 793 101 364 847 673 144 205  85 231 290
 434 655 754  82   4 380 248 692 286 277 464 569 751 593  21 291 169 664
 739  33 767 274 233 582 437 803  28 470 423 537 393 658 428 671 238 272
  20 540 221 746 654 469 257  26 416  81 842 418   0 800 716 198  57 827
 588 534 509 382 391 572 844 386 663 717 389 392 202 199 451  38 672 119
 807   5 810 249 565  64 143 207 165   3 137 331 550 166 852 424  13 120
  50 757 213 724 859 333 835 797 265 210 520  12 115 308 515 809 402 320
  97 740 289 856  87 519 815 808 503  61  72 307 802 806 799  66 268 131
 462 725 440 578 229 162 660 752 287 858 837 668 836 337 367 376 804 421
 580 759 514 529 762 798 413 220 293 244 363 449  75 753 738 758  46 723
 722 134 305  22 453 846 306 129 226 728 126 592 312  59 841 525  24 296
 839 332 694  32 426 300  69 295 538 848 441 741 775 501 135 530 303 314
 834 463 111 372 381 526 124 163 765  60 773 302 388 266 541 559 353  27
  74 466 138 270 187  48 254 502 461 504 419 737 422 133 831 127 132 571
 323 216 761 397 218 697 318 857 340 611 384 427 764 853 561 555 271 730
 180  73 328 446 172  31 459 705 816 456   6 584 727   8 417  65 146 301
 851  18  49 747 729 130 142 704 849 589 197  16 452 139 756 128 140 854
 691 297 766 125 576 219 112 769 304 609 661 203 772 123 209 605 556 749
 185 821 326 334 719 553 770 536 721 518 516 345 568 558 583 118 341 745
 433 619 586 175 732 224 614 196 554 656 193 587 607 768 477 771 825 227
 102 557 232 117 743 443 684 620 774 211 444 615 161 698 823 665 225 442
 114 435 742 579 647 585 535 624 545 355 687 223 763 570 543 178 206 228
 327 335 329 591 566 666 819 650 603 549 103 699 230 622 186 113 822 429
 330 342 432 439 343 347 679 176 338 818 734 703 820 693 731 649 348 479
 411 817 613 354 350 581 590 648 573 621 651 522 616 577 680 824 606 339
 612 506 344 357 352 336 547 567 546 356 499 702 407 685 604 683 551 682
 408 351 790 159 617 524 358 681 610 359 618 608 513 349 346 512 544 695
 625 542 726 478 623 602 412 533 783 601 528 863 711 633 733 785 508  78
 788 700 706 409 787  88 688 784 777  84 500 696 628 410 862 641 632 782
 791 627 781  94 701 708 864 105 786  99 510 789 626 629 631  95 563 562
 778 634  19 643 689 483 780 594 690 630  76 776 709 686 707 779 715 109
 475 476 638 712 639 480 110 635  83  93 595 484 600 710 714 713 796 636
 596 474 598 597 599 640 406 637 645 644 646 482 642 552 481 794 795 405
 861]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0708
INFO voc_eval.py: 171: [ 624 1416  202 ... 1566 1569 1568]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2274
INFO voc_eval.py: 171: [257 290 186  61 267  59 276 296 256 270 230  64 301 288 302 182  70 185
 245 260 263 299 184 214 133 234 268  76 469  66 504 402 211 404 403 505
 179 286 244 247 295  60 277 264 485 233 487 468  80  56 232  88 521 231
 486 196 470 390 399  63 134  46 188 512 308 483 300 535 139 261 475 509
  89 189  86  49 467  81 202 476  50  91  23 105 524 246 107 405 406 251
 479 489 472 484 215 262 417 137 210 254 389  27 132 391 477 269  75 341
  65 190 481 446 513 401  87 473 191  72  48 160 409 515 200  71 309 466
  45 157 519 187  34 106 322 488 421 458 226 459 197 408 471 482 271 130
 201 381 136 530 529 478 344  47  55 480 370 528 460 372 227 527 474 416
 497  83 310 248 394 138 177 537 278  73 418 496  30 431 128  77 424 369
 287 170 318 495 491 353 219 225  32 180 158 508 502  67 294 223  52  79
  41 313 212 371 259 542 498 340 293 317 153  26 118 228 415  28  24 199
 217 494  22 140 407 198 291 253 229 104 101 281 181 280 392 510 492 511
 127 445 349 395  54 363 176  57 534 265  38 314 425 311  69  78  85 360
 400 183  84  29 539 493 429 430 440 163  16 398 203 419 213  90 368 326
 423 420 324  53  82 383 439 109 122  58 129 453 151 367 216 192 173 361
 315 250 348 437 448 354 454 533 432 131 422 343  62  51  68 178 114 135
 325 412  20  74 316 338 413 350 266 541 339 297 387 396 123 345 108 323
 141 352 388 312 152 115 540   4 102 411 538 517 252 449 365 112 331 113
  39 274 523  17 110 356 357 162 150 222  31 174  33 327 282 531 320 258
 116 346 342 337 126 447 434   2 328 351 355 154 452 450   3 283  19 522
 171 156 209 520 347 120 386 393 518  37 249 410 366 532 384 124 148 119
 426 377 362 142 275 292 382  95 373   8 117 125 193 255 380 155 121   9
 159 145 168 536 236 111 385 289 378 103 224 436 455 166  12 161 503  42
 194 507  94 100 147 175   1 205 364 279  40 526  98   6 242 143 379  15
 149 457  96   5 204 240  99 146 359  44 235 144 516 285 298 514 220 525
  18 284 456 506 490 165  13  97 358 321 319 428  11 241 414 218 397  21
  14 238 435 164 237 461  43  10 221 438 169 167 239 243 329 441 463 433
 462 195 303 172 427   7 330 443 465 451 306 375 464 208 305 304 307 444
 332 334 374 376 207 336  25  93 206 333 442 272   0  92 335 501  35  36
 273 499 500]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0630
INFO voc_eval.py: 171: [969 116 942 ...   0 358 840]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0638
INFO voc_eval.py: 171: [217  33 135 133 136 226  36  31 213 172 221  15  34 125 214  32 227 220
 134 176 216 225 169  35 127 215 138 140 120 145  70 223   3  30  98 144
  43  13 181 141 203  62   1 232 102 142 137 229  63 143 116 158  19 192
 146  64 234 199 139 174   6  40 222  14 212 179  87 109 228 117  66 236
  89  99  18 108  86 219   8  97 150 175  68 231 151 202  55  53  90 183
 201  46 189  71  69  20 182 235 233  37 193 208  88 184  39 104  91 177
  17 178 224  16 230  72 170 206  56 162   2 159   4 218  54  38 188 204
  50 200  49  73 130 209 198  47  42  44 131  77 121 205  10 157  74 106
 187 180  65 173 110  57  45 149 171 124 163 160 107 186 238  48 100 190
  76 185 147 207  51 101 154  67 129 148 242 115  81  11  83  41  60 111
 239  22  58  59   5 243 245 240 103  96  52  61 194 165 105  75  79 123
 191 241 112 237   9 161 244 247 167  85  80  84 114   7 211  12 113 196
 152 126   0 132 122 195 246 153 118  82  78 197 155  23 119 164  94 156
 128  28  93  27  26  21  95 166  29 168  25  92  24 210]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3901
INFO voc_eval.py: 171: [ 7761     0  3647 ... 16579 16578 22382]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.2323
INFO voc_eval.py: 171: [ 230  246 1761 ... 1867 1865 1864]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2328
INFO voc_eval.py: 171: [ 73 474 205  62 201  47 881 520 505 456 496  51 902 455 494 138  34 186
  42 895 867 139  41 448 508 189 898 487 887  40  71 479  37 206 528 135
 864 202 460 512 445 507 194  43 513 484 700 885 530 446 497 862 900  75
 504 567  33 182 875 828 904 466 188 187 889 941  31 444 526  55 476 469
 126  57 228 225 451  67 132 468 452 136 901 880 179 949 465 459 198 489
 863 781 837 328 523 447 177 810 501  69  44 736 462 176 826  64 870 882
  60 391 959 515 461 499 872 492 185 720 903 724 454 879 470 166 891 502
 511 486 829 827 876 233 937 944 557 874 478 449 518 471 192 131 509 482
 865 517 218 529 450 640 553 877 525 804 831 480 488 951 464 473 125 493
 899 200 137 222 960 954 514 475 926 729  50 256 644 472  39 641 458 174
 467 141 890 234 181 561 516 199 251 170 833 481 527 522 457 820  53 947
 382 393 818 524  56 796 204 519 500 521 498 227 398  54 956 394 550 830
 249 190 399 642 819 927  25 140  77 217 543 133  58 483  76 798 180 569
  61 195 925 836 226 506 559 938 835 159 823 547  46 723 463  78 813 562
  65 477 556 490 308 239  38 203 878 963 563 964 510  48 789 824 817 388
 495 950 794 491 171 555 307 645 936 962 961 246 453 503 160 184 896  72
 800 485 238 397 383 948 883 566  59 892 643 803 229 822 167 197 178 253
 173 869 395 162 164 196 304 646 807  70 940 783   4 379 169 782 737 224
 259 221 825 929 832 165 223 262 411 191 811  24 942 955 888 957  35 868
  27 809  49 363  30 894 721 219 549 302  79 952 834 558 551 928  28 939
  45  29 943 359 571 161 734  74 793 168 158  68 220 193 163 568 792  36
 277 248 245  63 933 306 966 366 535 350 310 183 871 934   5 300 799  32
   6 725 958 965 402 953 930 240 548 762 312 235 735 257 175 812  52 785
  26 884 738 733 156  66 595 313 946 931 209 866 124 172 128 795 341 265
 134   9 730 932 298 935 544 216 945 554 759 142 788 309 797 808 252 243
 360 258 784 242 330 727 570 731 230 791 299 893 215 538 155 814 396   7
 873 331 531 361 130 534 728 129 732 295 263 260 726 787 802   8 100 273
 790 801 722 296 593 805 289 345 267 210 380 343 815 150 786 368 537 127
 539 153 305 347 120 390 237 264 297 698 311 147 154 821 211 247 806 816
 688 625 301 303 352 575 123 364 336 685 342 387 151 546 545 669 655 241
  98 365 348 334 250 294 106 207 236 254 768 152 157 326 268 540 405 622
 367 232 244 404 261 255 623 776 424 778 356 362 687 288 381 536 542 122
 564 276 704 121 630 533 357 290  18 231 675 346 638 560 709 637 565 541
 339 354 744 269 659 271 634 607 422 344 699 552 327 532 590 353 351 426
 270  95 694 413 978 629 355 421  97 897 416 386 392 148 410 777 145 610
 690 696 760 689 274 275 333 146 710 981 763 861 860 389 266 115 667 627
  22 272 574 775 844 779 660 105 385 671 321 415 213 143 848 859  99 576
 651 212  96 586 423 578 979 149 692  13 665  20 619 691 403 414 849 845
 214 843 657 621 647 739 774 425 419 324 654 358 636 408 718 886 769 648
 924 340  10 839 695 635 632 407 406 711 982 384 349 417 977 670 745 337
 573 332 605 418 706 633 329 980 650  23 338 420 412 631 923 606 572 976
 853 693 702 108  90 719 764 922 712 577 707 626 921 400 612 335  15 118
  12  21  17 840 649 708 697  11 291 780 409 852  14 292  88 705 666 746
 624 717 838  19 767 850 653 104  80 114 613 293  16  83 614 915 846 747
 602 117 112 701 842 580 319 111 107 584 652 102 847 851   2 741 907 841
 615 740 916 678 582 617 668 761 703 592 680 906 587 616 591 583 611 103
 679 661 116 656 628 320 101 208 323 318 113 322 604  85  84 920 908 713
 684 620 676 110 109 609 371 597 742 603 581 401  94 376 918 714 686 596
 919 858 716 588 598 599 314 618 658 287 857 436  87 743 378 917 715 639
 119 600 677 579 435 748 608 766 317 765 316 585  82 683 663 682 601 431
 589 370  81 662 594 144 854  92 856 369  86 373 315 905 681 773 970 372
 910 664 855 443 377 770 432  89 674 912  93 772 375 374 673  91 672 771
 433 909 281 438 437   3 280 325 441 434 974 430 285 914 284 427 282 283
 429   0 286 279 428 911 439   1 913 440 442 758 278 971 968 753 752 751
 969 757 972 973 756 975 755 754 749 750 967]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1033
INFO voc_eval.py: 171: [ 63   7  61  62 199 198  70 283 200   8 319  55  64  69 111 146 277  83
  60 269 271 278 321 270 303 276 114 248  78 273 107  56 250 281 312  94
  80 233 285  95 247  68  22 189  59 275  10 272 235 192 286 135  67 268
  57 313 117 280 188 284 119  65 266 274 185 148 282 108 267 279  66 120
 249 318  87 116 251 134 118  58 302 252 327  84 162 234 125 175 102 115
  85 122  97  19 193 187 173 190 171 112 246 237   9 101  96 124 147 209
 130 109 232 236 110 222 113 239 167 238 121  98 132  88 133 123 195 165
 335 333 166 325  12 242  99 331  79 183  54 169 100 214  77 129 210 203
   3 144 182 184 142 181 205 168 170 180  38 164 156  16 191 245 226 219
 179 261  35 260 172 308 254 155 178 206  81 174 316 326 163 265  34 221
  30  29  15  17 243 186 158 138  40 263 315 211 320 194  20 139 177  32
 264 143  71 307 176 297 208 151   4  13   5  33 244 229 323 298 309  90
  93  18  14  82 141 137 106 334 230 202 225   1  72  11 224  41 145 204
 140 332 136 216  92  86 314 150  48  73 262 201  76  26 207  28 218  39
  25 329 215  31  89   2  36  91 322  27 330 324  51 328 153 131  45  74
  42 197 217 228  47  75 212 253 306   6 227  52 317  46 305 223 220  49
 295 293  37 311 310 149 304 160  50 159  53 154 152 161 231 296 128 157
 241 294  23   0 196  24 301 291 288 256 292 255 105  21 287 289 290  44
 104 299 103 300  43 213 127 126 240 257 258 259]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1164
INFO voc_eval.py: 171: [457 465 463 461 279 462 458 472 281 460 634 456 278 475 338 635 459 629
 468 182  49  63  48 184 464 280 593 177  56  97   5  73  62  52 484 277
 493 285 500 466 259 110 333 680  44 310 672 675   9 503  71  43  41 178
  61 350 346 639 471 253  50 673  98 289  76 470  99  11 143 117  79 480
  40 115 670 496 256 348 467 587 589 598 474 529 291 621 594 627 628 144
 596  55 591 101   6  65 547 603 476 681 618  39 186  42 102  67 354 669
 685 674  45 633  46 477  74 161 292 302 251 185 497 205 334 107 331 276
 349 271 252   7 250 678  54  82 288 307 254   4 616 343 679 179 642 335
 352 336 641 200 533 258 535 287 263 300 113  81 539 224 469 620 684 145
 605 337 570 534 305  84 671 506 562 611 303 552 638 142 530 123 632 127
 114  60 359  20 542  13 610  70  53 136 532 588  75 473 615 367 495 595
 676  92 494 128 551  57 148 103 226  58 213  69 510 208 509  64 235 631
 650 146 122 126 630 197 162 531 147 236 306 194 491 590 555 132 424 487
 683 311 592 160 100 488 368 212 550 154 689 507 124 394 341 304 396 652
 682 151 119 553 255 538 572 358  24 540 332 195 340 688 486 537 554 353
 344 104  47 112 121 536 196 234 293 286 504 599   3 342 140  94 118 563
 204 297 150 409 640 149 656 190 274  19 677 284 206  80 355 415 560 230
 181 239  27 541 613 602  96 686 609 402 548 545 511 191 687 624 395 586
 558 597 165 158 521 164 622 308 282 544 257  78  31 626 502 203 316 125
 105 512 130 505 345 266  21 237 173 231 501 138 238 232 202 111 157   8
  88  17  89 298 481 357 478 573 351 267  91 155 398  12 198 425 347 129
  15 644 170 269 134 646 692 649 606 543 174 614 569 384 356 108 116 400
 518 106 546 131 574  85 579  23 315 339 370  95 201 260  16 183 159 564
 608 363 549 294 393 449 489  22 264 133 219 187 189 188 120 221  14  87
 575 566 508 647 180  72 262 379 312 557 580  28 483 156 225 199 380 207
 515 443 176 222  51 568 482 135 228  25 607  93  77 324 479 444 193 383
 668 301 261 571 408 137 166 654 528 559 321 401 617 643 139 450 192 412
 561 378 109 403 498 385   2 218  10 317  33 451 411 442 556 600  38 431
 405 389 223  90 623 323 441  59  35 410  86 248 268 404  66 437 604 619
  36 296 667 233 217 565 399 249  68 172  83 246 567 662 247 695 418 313
 413 325 663 522 386 612 214 397 637 216 454  26 422 168 693 661 653 514
 407 581 318 329 152  37 665 275 326 362 517 314 309 169 625 171 327 520
   1 265 175 319 601 648 273 328 406 330 364 452  30 655 421 440 153 320
 220 417 513 322 387 299 229 694 163 434 645 427 270 215 390 167 290 371
 272 227 369 636 211 576 664 651 436 365 392 295 388 499 391 366  29 445
 420  32 360 584 524  34 666 526 361 492 439 583 245 519 141 527 578 585
 283 438 582 435 516 446 447  18 525   0 414 423 429 426 523 485 373 490
 244 430 577 419 432 416 448 375 209 433 210 243 428 382 381 241 455 242
 453 376 690 240 374 372 377 657 659 660 658 691]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0540
INFO voc_eval.py: 171: [502 501 464 434   0 603 273 602 461 486 586 186 327 232 493  24 466 545
 182 432 253 169  50 546 244 274 187 475  18 240 343 328 270 254 588 261
 276 252 245 263 611 431 239 144  13  59 258 306 439 168 338  56 251  12
 339 131 478 459 330 235 541 267  25 255 283 101 271 132 433   2 334   4
  16 547 224 604 340 250 278 437 589 303 374 228  11  52 495 266 106 269
   3  60 441 440 550 279 289 227 568 485  39 496 494 304 231 561  62 566
 386 553 260 587 180 302 444 500 329 172 209 141 349  58 438 534 136  81
  17  51 388 350 595 268 281 280  65  23 150 435 487 262 201  34 612 457
 369   1 226  93 356 259   9 498 286 100 233 367 520 362  85  21   7  10
 211  87 489 463 598 499 294 134 311 609 229 305  43  54 325 199 458 521
 453 173  35 516 107 346 408 147 548 592 282 481 405 237 264 142 383 513
 313  40 590 202 189 234 373 171 275  53 207 555 551 497  45 549 210 296
 108  48 272  63 213 293 236 129 552  61 560  15 345 391 154 120   8  19
  92  55 392  20 482 177 454  57  46 160 332 318 298 333 570 337 355 139
 317 365 109 563 608  22  29 511 480  26 110 610  86  14 508 540 605 526
 111 533 143 456 450 519 525  80 238  32 326 503 322 176 404 607 359 352
 348  75 445 455 126  97 591 138  94 402   6  69 175  96 449  74 265  49
 222 554 221 397 204  84 191 220 196 484  72 414  90 529 316 451 465 606
 382   5 181 578 314 331 206 572 536 418 223 377 412 447 112 151 247 140
  71 230 115 243 469 523 117 409 376 203  66 216 146 113 190 474  37 379
 509 205 387  70 242 320 517  89 145 315 116  68  73 246 510 225  42  88
 161 600  67 470 324 351 256 601 514  76 422 594 460 384 446 287 105 378
 292 178 212 179 581 309 515 174 170 556 312 208 524 473 443 130  64 448
 336 407  99  36 200 335 419 188 102  83 596 164 360  47 353 368 321 104
  79 277 401 505 421 342  82 295  91 214 249 364 341 579 241 347 599 415
  44 198  78 483 476  41  30 477 354 361 472 576 344 488 307  98 127 371
 300 159 424 573 118 149 323 156  31 357 580 215 308  95 426 544 257  77
 103 430 319 219 507 310 577 559 522 358 380 462 471 575 512 398  38 543
  28  33 375 184 597 381 183 428 518 148 195 153 135 128 291 506 166 155
 537 565 121 218 217 185 133 167 593 363 479 468 297 385 370 390 372 366
  27 137 569 427 194 288 114 248 413 410 158 467 152 197 452 162 399 165
 124 396 290 567 393 423 558 532 562 119 284 442 193 584 192 395 400 122
 394 157 411 420 406 163 125 527 574 417 416 436 389 429 123 564 285 539
 425 403 531 535 542 299 530 528 538 301 492 571 557 504 490 491 585 583
 582]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1156
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1464
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.053
INFO cross_voc_dataset_evaluator.py: 134: 0.317
INFO cross_voc_dataset_evaluator.py: 134: 0.051
INFO cross_voc_dataset_evaluator.py: 134: 0.076
INFO cross_voc_dataset_evaluator.py: 134: 0.174
INFO cross_voc_dataset_evaluator.py: 134: 0.282
INFO cross_voc_dataset_evaluator.py: 134: 0.167
INFO cross_voc_dataset_evaluator.py: 134: 0.036
INFO cross_voc_dataset_evaluator.py: 134: 0.104
INFO cross_voc_dataset_evaluator.py: 134: 0.071
INFO cross_voc_dataset_evaluator.py: 134: 0.227
INFO cross_voc_dataset_evaluator.py: 134: 0.063
INFO cross_voc_dataset_evaluator.py: 134: 0.064
INFO cross_voc_dataset_evaluator.py: 134: 0.390
INFO cross_voc_dataset_evaluator.py: 134: 0.232
INFO cross_voc_dataset_evaluator.py: 134: 0.233
INFO cross_voc_dataset_evaluator.py: 134: 0.103
INFO cross_voc_dataset_evaluator.py: 134: 0.116
INFO cross_voc_dataset_evaluator.py: 134: 0.054
INFO cross_voc_dataset_evaluator.py: 134: 0.116
INFO cross_voc_dataset_evaluator.py: 135: 0.146
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 5499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.373s + 0.040s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.353s + 0.041s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.364s + 0.039s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.369s + 0.040s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.373s + 0.040s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.372s + 0.040s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.368s + 0.039s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.366s + 0.039s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.366s + 0.038s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.367s + 0.038s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.367s + 0.038s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.367s + 0.038s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.372s + 0.038s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.425s + 0.039s (eta: 0:00:57)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.358s + 0.043s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.360s + 0.041s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.360s + 0.040s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.360s + 0.040s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.362s + 0.040s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.369s + 0.039s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.364s + 0.039s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.363s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.365s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.365s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.369s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.367s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.536s + 0.049s (eta: 0:01:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.383s + 0.040s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.369s + 0.040s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.376s + 0.041s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.375s + 0.040s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.373s + 0.041s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.367s + 0.042s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.370s + 0.041s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.368s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.364s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.364s + 0.042s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.364s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.364s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.549s + 0.038s (eta: 0:01:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.354s + 0.043s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.359s + 0.046s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.360s + 0.048s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.358s + 0.046s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.356s + 0.046s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.359s + 0.046s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.358s + 0.045s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.360s + 0.044s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.358s + 0.044s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.356s + 0.043s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.357s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.354s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.254s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [318 219  83 209 308 593 597 119 604 153 171 628  80 602 594 598  50  61
 208 307 133  89 614 265 166 194 186 312  56 213 636 633 164 180 631 159
 258 114  94 600 222 321 138 139  51 224  62 323 211 310 181 150 192  72
 319 220  33 644 279  71 147  34 642 313 214 140 273 663 322 223 666 595
 620 269 218 317 303 615 622 623 596 648 260 142 647  48 168 360 660 136
  59 364 236 405 285 234 442 284 237 304 618 272 277 118 281 283 228 621
 606 617 411 203 135 661 649 132 293 314 215 212 311 182 238 309 352 210
 723 402 266 501 629 297 301 292 650 625 395 357 603 274  35 397 678 605
 174 169 498 115 193 278 408 315 216 155 741  84 163 639  41 165  44  36
 599 655 486 494 616  45 233 640 167  38  40 641  37 299  87 503 120 197
 662 108  92 282  53 358 152 403 735 627  39  46 396 643 535 264 188 123
 612 190  82 489 290 184 259 412 470 523  81 497 675 305 585 482 363 231
 496 198 488 175 362 665 495 162 263 635 356 607 584 298 173 111 693 669
 454 410  47 492 196 438 183 552 490  42 406  77 409 656 225 324 232 401
 511 226 288 325 702 610 505 316 217 611 677 271 179 619 267 721 187 657
 645 227 326 144  57 280 394 239 320 221 725 353 361 637 105 439 400 191
  32 399 733 110 359 141 632 158 407 398 506 366 151 463 176 122 178 172
  68 156 117 624 646 676 287 270 161 491 106 170  60 229 157  70 249 437
 109 404  64 458 485 653 688 295 654 355  85 673 591 291 528 483 199 251
 533 474 481 286   1 509 145  49  79 608 651 484 541  43 731 601 177 185
 129 659 354 609 195 107 189 638 560 160 674  93 296 473  86 626 306 630
 703 579  76 668 574 365 613 384 539 671 380 127 428 381 499 456 553 652
 261 447 427 154 577 508 121 502 691 240 389 378  73 729 658 524 670 444
 487 504 554 728 634  69 276 230 672 116 542 716 534 529 510 469 143 572
 664 732 727 689 507 368 680 587 421 235 382 544 581 468 718 561 493 500
 440 334 461 745 113 374  75 472 537  25  31  78 350 734 204 588 275 327
 206 252 450 112 338 369 250 446 744 589 695   0 467 422 247 417 386 207
 580  91 245 367 345 700 375 590 294  63  74 452 730 583 679 719 268 330
 372 340 331 300 592 149 460 346 333 531 415 383 205 371 724 373 379 578
 128 253   2 722 289 262 146 720 557  55 445 698 736 426 738 329 573 134
 131 387 351 448 527 335 586 453 717 466 349  97 526 739 302 137 536  29
 576  52 475  58 726 582 459 393 413 337 694 514 418 740 551  90 148 451
 465 348 571  30  21 570 343  96 549 699 385 556 548 432 244 476  65 464
 555  98  95 687  12 457  88 479  26  67 543 692 540 538   5 339 550 559
 344 430 682 443 683 532 530 342  66   9 512 478 336 246 414 347 525 685
 684 124 521 522 332  54 328 480 243 341 370 241 455 441 686 126 517  11
 434 471  13 690 515 242 568 416 558 513 701 429 449 518 462 696  18 520
  14 713 477 433  22 681  20 436  17 697   7 575 435 419 425  23   6 390
 376 431 248 130 737  15 202  19 254 546 519 424 103 420 104  10 255  99
 423 562 516   8   3  28 125  16 667 256 712  24  27   4 388 705 377 709
 200 567 715 101 545 708 201 392 711 257 704 391 102 710 714 100 707 547
 706 569 563 565 742 564 566 743]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0321
INFO voc_eval.py: 171: [ 74 275  35 222 229  50  29 253  77 359 357  76 352 336  84 220  57 199
 276 354 227  88 335 274  28 277 120 225  61 256 355 236 226 233 353 118
 326 197  31 293 223 234 125 356 358 260 278 160  82 240 235  78  81  86
 230 177 327  83 332 345 342  80  75 198 202 331 232 313 224 164 247 238
  79 281  27 119 244  33 252 258 310 159 203 309 282 285 255 249 287 303
  87  85   5 283 286 145 262 221   2 322 246 165 241 333 157 228 131 237
 344 280 201 239   7 340   3 243 305  32 279 323 180 251 338  20 321 127
 296 175 134 261 301  41 205  30 259 121 248 288 124 292   0 245 191 289
 257 158 142 341 300 318 290 110   4 210 216 200 334  48  53  46 250 231
 343 204 172 254 302  24 106 136 242 123 139 143 264 206 214  37   8 152
 213   9 135 219   6 317 126 156 272 273 171  34  11 186 271 329 162 339
 294 153 217 181 108 265 154 297 324 269 149 138  54  21 155 107 130  15
 314 304 128 270  10 295 185  42 179 207 182 311  98 189 268 346  26 306
  14 315 263 267 308 132  99 291 111 129 328 284 299  17 266 212 194 298
  73 184 174 337 209  65 316 122  40 351 208  72 103  70  96  22 319  43
 146 140 166  64 215 211 168 137 195 190 105  12 176 307  97 193  63 115
  62  67 196 169 320 112  52  19 133 101 144  66 312  68 102 187 116 147
 350 141 173 170 349  18 104  60 325 100  71  69 117  23  55 167   1  47
 330 188 348  95 163 148 183  56  45  58 114 347  16 109  36  25 178 150
  13 161  92 113  44 151  39  59  51 218 192  90  94  93  91  38  49  89]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3158
INFO voc_eval.py: 171: [2033  618 2149 ... 5821 5837 5848]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0527
INFO voc_eval.py: 171: [1616 1799   20 ... 2901 2902 2900]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0803
INFO voc_eval.py: 171: [607 602 766 ... 204 209 206]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1671
INFO voc_eval.py: 171: [311 116 106 112 126 304 332 146  33  73  71 310 120 118 319 136 107 108
 128 111 145 105 315 305  36 316  82  34 109 104 308 306 119  46  37  44
 328 321  88  40 309  29  77 113  49 142 152  31 329  74  70 121 323 110
 314  58  41   6  38  57   5 123 114 290  92 156 149   1  81 330 232 324
 125  95  87  84  78  69 255 234 147  47 199 341 244 195 289  45 236  75
  72 117 210 198 134 285  50 141 297  67 335 320  83 243 279  86 281 318
  32 153 249 137 154 157  52  80 122  30 252 339 133 138  27 283 343 131
 127 284 336 327 286  60 282  79 331 144  85 155 194 233  39 338  76 325
 168  65 211 288 143 313 150 344 160 247 196 209  61   0 151 140 224 322
  48 197 205 238 248  62 280 201  68 200 162 292  64 215 296 169 148  28
 208 135 253 240 191  97  99 287  13 293 225 298  63 291 115  59  42  93
 312 235 130  18   9 250 129 221 193   3 192  35  51  66  53 132 251 222
 317  56 245  43 237  22 139   7 218  17 295  55  54 214 246 189 294 175
 124 239 206 278 190 241 334 254 103 228   8 100  10 242 202 203 299 227
  19 326   2   4 301 159 270  91  98  90 174 102 216 177 220 101 226 269
 158 217  94 340 223  96 229  26  89 213 337 161 267 173 263 178 273  12
 166 231 164  23  14  24 207  15 212 219 333 307 172 230 204 277 302 303
 342  11  16  20 272 257  21 300 271  25 260 275 171 180 184 258 182 276
 261 274 266 268 167 170 256 259 163 176 264 265 165 262 183 179 185 188
 187 186 181 345]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2971
INFO voc_eval.py: 171: [2225 1115 1579 ...  696  698 2526]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1650
INFO voc_eval.py: 171: [104 112 108 110 136 254  14 195 109  98 103  35 255 256  16 137 105  32
  25 253 113  73 161 111 259 192  29  31 162  20 228 132  78  80  30 170
  97  72 133 229 261 169 160 231  33  13  77  82 205 155 163 165  18 106
  36 196 147 219  43  95 168 158 153 257 211  23 260 138 252 102 216 258
  71 218 166 204  66  94 164 187 167  27 116 135  28 131  19  44 227  85
  99 221  21 134 232  34 233 154  54  81 206 114  75 226  69 230 186  17
 188  63 214 159 172  12  22 208  60  86  70 213  76 220  24  15  61  64
 215  62  65 200  68  67 126 128  26 190 241  49  53 129 222 212  46 242
 130 183 217 157 118 210 239 152  59  45  96 107 171 127   5 189  58 184
 250 125  57 251 201 209  10  47  48 238  52 191 121 100 198 178  56 156
   2  89 151 182  83  87   3 203   1 101 119  84 115 207 117 122   6 124
  79  55  50  88  74  42  91 202  11 197   0 234  92   7   8 177  93 243
 185   4 240 175 235 173 193 194 244 174   9  90 148 149 146 123 145 180
 224 225 223 199 179 237 176 236 141 144 142 247  37 245 246 248 249 181
  51 139 143 140  39 150  38 120  40  41 262]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0327
INFO voc_eval.py: 171: [2146  286  376 ... 2808 3726 3727]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1131
INFO voc_eval.py: 171: [376 109  82  95 411 111 323 410  97 504 374 377 105 259  84 403 393 161
 102  96 381 466 245 288 378 677 387 173 113 682 409  19 190 511 290 253
 269 168  48 391 382 688 660 482 851 291  20 368 455 281 467 324  47 194
 332 501  85 317 196 412 321 510 156 456 248 227  94 182 801 286 404 273
 223 246 166 462 327 405 277 279 685 835 226 263 767 423  73 175 464  33
 188 508  35 225 244 142 255 283  11 477   3 550  63 474 469 319 679 302
 435 535 386 252 274 208 187 243 370 385 507  46 505 154  49 756 833 179
  59  91 847 201 845 458 199 204 574 289 821 326 499 157 159  42 230 663
 268  68 160 250 680 434   2 481 306 546 502 837 184 293 260 748 732 189
 439 840  39 773 162 373 164 155  44 152  58 197 178 861 453 330  61 379
 165  45 518 585 301 158 686 267 307  76 503 300 112 665 667  77 425 814
 103 406 176 447 839 822 427 395 671 760  22 369 500 128 333 251 212 217
 483 181 407 163  27  74  62 498  16 762 533  57 272 823  50 852 729 506
  53 747 858 121 222 271 836  12 249 562  14 444 383 318 572 584 195 126
 520 478 868 258 147 297 820 529 285 232 261 265  41  40 523 810 205 399
 669 539 687 151 372 270 509  18 106  69 214 802   8  87  29 855 240 388
 441   9  90 673 256 298 766 705 865 578 287 177 242   5 664 473 295  38
 763 601 751 282 661 779 424 542 812 231  24 402 683 549 480  25 445  34
 247 662 591 479 759 426  86 206 264   1 262 727 849 809 544  64  32 522
 294 672 850 834 596 398  15 400 728 211 390 581 401 394 207 174 684 575
 172 459 148 215 124   4 668 224 257 149 819 432  70 564   6  43 816 340
 736 342 220 859 867  56 769  31 218 120 532 125 841 316 328 527  17 329
 583  13 100 807 554 408 863 531 275 806  92 818 284 752 516 470 824 137
 238  79 429 170 815  60  72 817 448  67 315 737 384 678 808 866 430 346
 811 813  28 278 559 764 428 587 844 843 540 375 254 771 589 765 526 422
 140 371 457 734 299 775 234 750 735 770 131 313 314  81 461 304 740 209
  52 135 308  26 854 600 537 449 341 706 846  65 681 303 689 129 171 556
  30 320 848 389 541  75  37 141 856 436 380 310 538 515 753 296 777 472
 322 675 311 785 397  66 360 842 193 276 266  80 571 292 117 580 514 517
 144 138 132 475 139 431 774  55 838 471 619 186 860 228 749 573 331 862
 325 349  78 392 666 337 864 465 555 280 454 776 437 396 568 180 136 716
 593  36 758   7 468 309  71 153  54 203 717 305  10 229 715 150 825 853
  23 744 597 134 857 739 704 145 130  51 460 146 741 617  21 116 200 768
 312 334 210 476 778 143 219 133 614 780 463 569 343 127 191 761 577 783
 731 566 233 782 528 670 123 350 733 547 829 592 354 530 183 570 202 442
 628 198 757 742 595 622 107 615 241 730 235 236 567 490 122 623 696 755
 221 832 119 781 169 784 451 629 674 786 452 699 708 831 545 450 594 655
 336 633 754 579 588 213 185 443 363 772 338 344 239 237 611 599 563 108
 676 631 118 192 576 690 827 351 446 352 658 348 553 339 355 216 830 438
 440 433 828 826 743 746 713 492 335 582 361 419 657 357 598 621 586 362
 691 659 630 620 590 534 613 347 551 519 353 656 345 365 359 560 364 512
 612 697 415 711 693 695 416 626 358 625 167 536 800 692 366 627 524 618
 367 356 616 525 561 558 557 552 491 420 632 707 609 610 543 738 548 641
 792 798 871 624 723 710 521 714  83 745 709 794 718 797  89 417  93 703
 513 796 793 787 418 649 636 110 640 870 421  99 635 720 104 790 694 791
 712 795 634 799 637 101 701 651 788 642 638 702 602 789 721 496 698 639
 726 719 114 489 488 646 724 647 700 493 115  98  88 643 603 608 497 722
 644 725 805 604 484 606 605 487 607 648 485 486 653 652 645 414 654 495
 650 565 494 803 804   0 413 869]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0711
INFO voc_eval.py: 171: [ 614 1399  882 ... 1550 1552 1551]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2290
INFO voc_eval.py: 171: [251 283 176 260  61  58 269 289 250 263 224 294 281  63 295 171 174  69
 256 253 239 292 173 206 126 261 228  74 503 399 467  66 203 401 400 504
 168 279 238 241 258 288  59 270 481 466 227  86 520  55 484 226 225  77
 483 188 385 395 468 127 180 293  46  62 511 533 132 302 254 508 473 181
  87  49 465  84 194 474  79  48 486 523  22 240 403 402 101 103 470 207
 245 413 478 255 130 488 480 202 248 384 386 125 265 482  25 262 475 335
  85  65 445 397 512 182 471 303  71 149 178 179 192 514 464  70 147  32
 417 191 518 317 177 219 485 102  42 487 457 479 469 458 189 129 264 529
 376 124 175 528 193  47 338 477 527 305 389 220 476  54 526 472  81 496
 131 242 412 363 365 414 536 271 166  72 420 122  75 495 423 280 158 490
  28  30 169 507 542 494 501  51 204 287 362 212 313 148 218  76 216 346
  78 334 312 286 222  41 491 364 411 144  26  64  24 387 404 133 493  21
 113 285 209 190 273  97 247 100 223 509 275 170 392 510  53 356 121 257
 421 309  68 444 306  83 534 165  56 342 539  39 492 205  82 426 396 172
 416  27 415 195  52 321 419 152 394  16 497  80 439 319 123 353 361 378
 438 310 161  88  57 221 452 447 183 429 142 208 116 341 418 532 427 184
 360 244  60 128  50 354 541 290 453 337 105 436  18 320 167 347  67 308
 259 332  73 311 540 391 343 407 210 382 333 307 304 383 318 408 339 345
 117 109 143 104 448 246  98 406   4 537 358 516 134 325  38 322  17 268
 151  29 162 274  31 323 522 108 252 107 110 215 349 398 106 446 530 449
 141 331 430 336 315   2 159 390 276 344 146 348 350 111 340   3 201 120
 428  35 519 115 451 381 517 521 284 531 355 379 359 243  91 405 114 422
   8 366 372 112 185 118 377 249 119 145 535 282   9 375 150 135 230 137
 140 432  99 374 217 454 502 388  40 357  12 155 156 506 380  90  96   1
  92   5 164  94 525  36 236 163 272 186 138  15  43  37 456 197 352 234
  20  95 229 373 301 136 196 513 515 139  45 278 291 213 538 431 505 277
 524 351  13  93 455   7 314 211 489 393 425 410 370 154  19 433 232 235
  11 371 153 316 435  14 437 459  10 231 214 157  44 237 461 233 460 160
 440 424 296 187 434   6 463 442 409 324 450 299 462 368 298 326 297 443
 328 300 369 200 330 367 199  23 327 266 441 198   0 329  89 500  33  34
 267 498 499]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0662
INFO voc_eval.py: 171: [937 114 909 ...   0 354 812]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0590
INFO voc_eval.py: 171: [206  31 126 124 127 216  35  28 202 162  13 210  32 116 203  30 215 209
 125 167 213 204 159  33 205 129 131 118 136 112  67 212   3  92  29 135
 172  42 132  11 192 221  59  96   1 128 133 218  60 134 149  61 109  17
 182 223 137 188 130 211 165   6  39 225  12 170  83 201 103 217 110  63
  93  16 102 164  82 208  84   7  53 191  91 166  65 220 141 142  51  85
  68  45 190  66 179  18 173 195 222 183 224 174 168  98  36  48 214  15
 169 219  14  69  54 197 160 154 150   4 207  70   2 198  52  37 189 178
  47  72 187  46 194 121  40  44  71 113 146 122   9  75 193 196 171  62
 104 177  55 100 163  43 115 140 151 161 227 138  41 176 180 101 175  94
  95  74 145  49 232  64 108 139 229  57 105 120  10  20 231  38  78  97
  56 234  50 148  79  34 228   5 230  90  99  58 106 156 114 226 184  77
 158 233 235 152  73 181   8  81  76 107 200  80 186 117 153 123 143   0
 144 185 111 147  21  88 155 119  26  25  23  87  89 157  19  27  86  24
  22 199]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4006
INFO voc_eval.py: 171: [ 7771 11193 19121 ... 16593 16594 16595]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.2232
INFO voc_eval.py: 171: [ 244  227 1745 ... 1842 1847 1845]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2331
INFO voc_eval.py: 171: [ 72 490 209 ... 774 995 769]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1051
INFO voc_eval.py: 171: [ 66   9  64  65 216 217  76 303  10  58 339  75 159  68 124 295  62  92
 290 341 288 289 297 322 126 267  87  59 332 269 119 323 218 292 301 103
  89 252 300 104 305 266  23 203  73 210 296  63 333 291  12  55 254 206
  56 294 149  72 202  60 129 299 287 161 131 200  70 285 293 268 132  96
 120 270 302  71 128 286 298 338 146 304 130  67 321 271 347 176  61 253
 112  93 136 189 127 106  94 208 204  21 354 187 137  11  69 111 256 185
 145 160 105 265 135 123 242 255 251 107 142 222 258 125 144 211 133  97
 257 122 162 345 181 134 108 121 179 353 197  14 109 261 180 351 231  88
 157  74  86  57 183 141 198   3 155 221 195 196 151 110 194 245 168 182
 178 264 224 193 273  18 205 336 237  38 328 192 174  90 223 188 239  36
 284 280 186  31 177 355 148  30 279 346 225 262 171 201 340  17  19 226
 335  39 184 228 283 212  78  35 153   6 327  33 118 156 191 190   4 316
  15 170 329 263 199 152 248 102 343 317 213 154 158  91 117  99  20 220
 243 207  16  79 249 244  34 209   1  13 233 101 334 282 357  85  41  80
 147 150 352  49 236  95 219  28  29  26 281  77   2 240 100 232  40 348
  27 349 350 356 166 215  32 342  82 234  42  98 344 247  52  48 330  46
 246 241  83 326 235   7 337 143  47  44  50 325 238 227  53 272 311  81
  84 314  43 163 331  37  51 324 173 165   8   5 172 167 164 175  54 229
 315 169 250 140 260  24 313 214 308   0 320 309  25 275 312  45 274  22
 307 113 310 115 114 318 319 116 139 230 138 259 306 276 277 278]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1114
INFO voc_eval.py: 171: [470 478 476 474 288 471 484 475 295 654 473 469 346 490 649 480 655 472
 189  51  49  64 607 191 290 477 184   5  75  63 101  54 299 292 287 289
 497 617 498 515 260 479 697  57  89 690 113 693  47  73 318 341   9  46
  44 519  62 112 185  52 360 691 658 348 485  79 104 105 120 483  11 150
  82 688 509 493  43 508 609 265 487 481 544  56 359 342 641 302 612 272
 647 648 151 268 118   6  68 698 563 298 351 488 193  42  45 638 611  69
 687 702 692 172  48  76 653 603 192 311 127 510  60  55 212 111 286 619
 262 261 695 339 282   7 106 696 273 316 635   4 356 355 186 661 660 267
 124 548 309 207  85 550 701 554 482 689 152 231 586 640 623  87 578 347
 314 149 103 549 629  21 569 657 312 520 545  65 605 652 132  58 128 117
  13 613 558 694 628 566 547 620 486  72 142 633  78 507 377  96  77  59
  70 156 236 352 506 173 133 215 512 244 669 221 525  66 344 524 651 650
 245 126 315 170 154 153 546 204 300 571 436 700 608 201 162 159 499 504
 319 501 699 706 379 408 521 567 264  83 568 123 587  26 353 411 313 670
 129 553 220 365 500 606 202 570 557 704  50 102 350 340 363 552 604 303
 358 615 203 125  98 423 158 551 579 518 243 263 429  20 188 357 420 211
 659 576 674 175 122 293 197  29 242   3 537 168 174 703 157 284 213 631
 627  84 136 198 155 291 526 574 248 556 614 410 561 705 555 564 644 238
 317  81 131  33 266 560 210 646 642 324 522 209 516 130 109  22 134 116
 138 144 517 278 354 166 246 362 418  17 163 343  92 489 239 114 364 247
 240 100  93 179   8 367 588 307 494 183  15  95 279 413 361 205 663 437
 345 182 415  12 665 710 610  25  88 169 668 533 139 632 559 366 107 398
 562 119 323 121 320  99 271 135 110 580  23 590 190 381 585  16 196 626
 595  74 349 304 565 502 171 195 572 523 392 208  14 187 194  30 407 141
 274 321 372 224 270 137 462  91 530 666 229 591 495  53 582 597 492 596
 214 176  27 394 584 200 232  80 206 237 296 140 622 230 416   2 491 686
 575 577 391 310 625 397 329 331 441 419 463 456 426 455 662 672  41 143
 145 589 199  94  35 511 636 417 425 616  10  61 226 464 325 573 424 449
 108 335 181 637 624  67 399 453 454  39  38 621 306 643 539  90 257 403
 256 225 258  86 685 297 414 639 427  71 115 581 713 228 178 259 285 332
 431 241 583 327 467 180 165  97 656 630 222 679 400 529 160 435   1 538
  28 681 514 422 535 711 671 233 532 227 326  40 678 598 683 371 333 164
 337 161 322 277 334 634 645 618  32 283 276 374 434 421 336 673 667 338
 465 527 308 439 452 177 430 330 234 328 712 664 446 401 280 592 301 404
 601 223 218 305 275 281 148 448 513 378 380 167 235 541 432  31  34 457
 402 376 682 269 375 254 406  36 405  24 505 409 534 600 368 684 451 390
 602 370 146 412  37 450 393 594 294 599 458 531 447 543 542 459 369 680
 428 147 373   0 442  18  19 540 438 219 528 253 536 443 433 383 496 503
 593 461 444 386 216 217 252 440 445 250 396 251 395 707 466 387 468 249
 460 385 255 382 389 675 388 384 677 676 709 708]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0545
INFO voc_eval.py: 171: [524 523 484 456   0 633 285 632 481 508 618 341 193 244 571 515  23 486
 190 453 263 175  51 572  17 286 194 264 496 356 252 343 282 619 274 261
 288 257 275 269 641 149 273 452  12 351  60 136 319 251 460 353 260  11
 344 268 174  57 479 569 499 247  24 265 295 454 137 105 348 283 573   2
   4  15 354 634 234 259 391 316 291 458 238  10 255 620  53 110 577 278
 159 517   3 279  61 463 290 461 595  37 301 507 237 518 317 588 406 516
 243 360  66 579 593 315 404 187 617 219 342 465 281 522 178 559 140 459
  59  16  86  52 363 280 626 153  70 292 455  22 384 210  33   1 545 509
 477 642 144 236 370  99 306   8 297 382 520 532  75 245 218 377  20 358
  90   9 148  94 483 511 324 318 521 208 628  43 339 639 138 241 416 478
  54 424 422 574 111 179  34 474 540 151  38 276 293 145 248 327 503 622
 211 401 157 389 246 197 581 216 177 621 287 302 576  42 529 147 222 575
  55 112  65 519 296  47 284 220 531 305 578 587 249 309 134  64 411 124
 158 359   7  14  97  18 184 504 475  58 346 332  56  19 166  48 369 142
 307 331 598 352 113 491 591  28 638 114 549 379  21 635  25 115 548 558
 501  13 420 146 640 471  92  31  78 336  84 525 543 340 103 637 250 373
 183 102  41 311 131 362 365 182   6 476 466 430 415  89  73 213 101 232
 231 580 470  63 230 277 539 330 410 399 199 616 608 205 188 472 214 328
 506 554  91 636 417 561 345   5 116 435 599 233 394  85 347 426 143 117
 468  69 120 122 225 258 242 544 396 393 489 408  36 212 215 368 407 535
  71  95 198 254 533 334 547  74 528 150 567 550 329 421 256 551  77 235
 118  93 338 441 429 536 630 266 582 631 612 217 185  88  79 400 625 541
 109 395 530 480 180 298  50 221  82 467 186 326 176  67 304 322 490 538
 464 209 436 135  46 366  96 253  87 469 546  35 374 610 270 495 349 104
 383 240 335 527 357 289 195 106 355 108 170  44 403 361 443 207  29 223
 364  62  39 431 562 376 600 132 350 604  98  81 505 321 629 152  83 438
 510 168 239  30 494 367 497 196 371 107 337 446 267 498 320 611 224 388
 161  80  76 229 164 449  72 603 372 570 586 534 605 537 323 333 568  68
 100 398 419 485 607 624  32  27 160 492 413 325 609 300 392 606  49 542
 390 189 191 228 133 397 590 139 627 204 564 125 380 172 378 173  40 192
 227 167 448 402 381 386 387 623 500 440 596 226  26 121 156 493 488 141
 310 314 181 427 119 299 425  45 272 203 585 271 502 154 385 375 473 487
 163 594 171 303 414 262 129 206 418 155 294 423 482 127 589 462 433 126
 123 557 602 428 437 202 614 412 445 201 162 169 432 552 447 434 451 566
 165 409 457 592 128 450 439 130 312 560 565 444 556 405 442 597 555 308
 553 563 514 313 583 584 601 526 512 513 615 200 613]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1259
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1468
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.316
INFO cross_voc_dataset_evaluator.py: 134: 0.053
INFO cross_voc_dataset_evaluator.py: 134: 0.080
INFO cross_voc_dataset_evaluator.py: 134: 0.167
INFO cross_voc_dataset_evaluator.py: 134: 0.297
INFO cross_voc_dataset_evaluator.py: 134: 0.165
INFO cross_voc_dataset_evaluator.py: 134: 0.033
INFO cross_voc_dataset_evaluator.py: 134: 0.113
INFO cross_voc_dataset_evaluator.py: 134: 0.071
INFO cross_voc_dataset_evaluator.py: 134: 0.229
INFO cross_voc_dataset_evaluator.py: 134: 0.066
INFO cross_voc_dataset_evaluator.py: 134: 0.059
INFO cross_voc_dataset_evaluator.py: 134: 0.401
INFO cross_voc_dataset_evaluator.py: 134: 0.223
INFO cross_voc_dataset_evaluator.py: 134: 0.233
INFO cross_voc_dataset_evaluator.py: 134: 0.105
INFO cross_voc_dataset_evaluator.py: 134: 0.111
INFO cross_voc_dataset_evaluator.py: 134: 0.054
INFO cross_voc_dataset_evaluator.py: 134: 0.126
INFO cross_voc_dataset_evaluator.py: 135: 0.147
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 5999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.560s + 0.045s (eta: 0:01:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.349s + 0.041s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.359s + 0.040s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.364s + 0.041s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.373s + 0.041s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.371s + 0.042s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.369s + 0.043s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.368s + 0.043s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.369s + 0.043s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.371s + 0.042s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.370s + 0.042s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.371s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.371s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.473s + 0.039s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.387s + 0.052s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.395s + 0.048s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.380s + 0.045s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.376s + 0.043s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.377s + 0.043s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.381s + 0.043s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.381s + 0.042s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.375s + 0.042s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.376s + 0.042s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.375s + 0.042s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.374s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.372s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.385s + 0.035s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.377s + 0.036s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.377s + 0.040s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.380s + 0.042s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.374s + 0.042s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.370s + 0.042s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.372s + 0.043s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.372s + 0.043s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.370s + 0.043s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.364s + 0.043s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.363s + 0.042s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.363s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.365s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.451s + 0.035s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.376s + 0.037s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.368s + 0.040s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.366s + 0.041s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.362s + 0.044s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.358s + 0.044s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.359s + 0.044s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.359s + 0.044s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.362s + 0.043s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.363s + 0.043s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.359s + 0.042s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.359s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.358s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.606s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [317 219  83 307 209 592 118 596 153 602 171 626  79 593 600 597  49  60
 306 208 132  88 612 264 165 194 185 213 311  55 635 632 179 163 629 159
  93 598 113 257 137 320 222  61 138 224 322  50 180 211 309 149 192  70
 220 318  33 643 277 146  69  34 641 312 214 139 272 662 665 594 321 223
 617 268 302 316 218 619 613 620 595 647 141 259  47 659 135 167 359 362
  58 236 404 285 234 283 237 443 615 303 271 117 276 280 228 282 410 604
 618 134 614 203 648 660 131 292 313 215 212 310 181 351 308 210 723 401
 627 501 265 296 649 300 291 622 356 394 601  35 273 678 395 173 603 193
 168 114 497 408 278 646 155 162 216 314 743  41 638 164  84  43 654  36
 486 233 494  40  44  37 640 166 639  87 298 119 197 661 503  91 107 281
 152 357 736  52  38 151 625  39 402  45 396 122 642 189 610 534 263 191
 469 289  82  80 183 489 411 522 304 258 675 363 498 482 584 361 198 231
 161 174 663 488 496 495 355 605 634 262 583 297 170 668 110 455 182 409
 196  46 551 439  42 405 492 490 655  74 407 232 324 226 287 412 225 323
 400 702 608 609 505 616 677 178 315 217 630 266 227 721 325 270 143 186
 657 279  56 508 470 393 360 221 319 238 352 726 637 734 631 399 104 440
 398 406 158 109  32 506 140 365 358 397 190 175 150  67 121 176 172 645
 621 156 116 286 269 676 491  68 169 105 229 157 248 108 403  59  63 438
 459 652 485 687 354 653 294 672  85 144 527 644 590 199 290 483 250 474
 481 106 532 284  48   1 509  77 651 540 484 606 624 732 599 177 658 636
 160 128 353 689 195 607  92 184 674 188 295 558  86 703 628 623 473 364
 305  75 611 538 575 667 126 574 578  81 670 379 383 429 650 457 380 499
 552 448 428 260 120 691 154  71 388 502 510 187 730 377 576 729 445 656
 669 523 239 633 230 275 115 504 487 671 541 553 533 717 528 733 142 511
 728 664 571 688 507 235 680 586 367 421 468 543 719 493 559 381 441 673
 500 333 580 536 462 472 747 373 112  73 735 349 204  31  76  26 206 326
 274 251 111 587 337 451 249 746 447 246 694   0 422 467 588 699  62 207
  90 344 385 579 416 374 293 244 731 366 720 453 589  72 582 679 371 339
 330 329 299 148 267 591 461 345 530 332 370 205 414 378 127 372 382 725
 145 252   2 722 577 261 696 446 288 368 555 427  54 737 740 350 328 130
 386 133 526 334 450 136 585 465 718 525 573  96 454 535 348 741 301  29
 724  51 693 475 727 392  57 738  89 581 466 336 742 460 419 147 514 698
 550 452 548 347  21  95 342 570  30 568 384 547 433 686 464  64  94  97
 476 243 458 569 554  12 479 692 539  66 537  25   5 542 431 549 529 343
 444 338 557 682 341  65 413 683 346 478   9 531 524 245 335 449 512 520
 684 331 123 521  53 242 340 327 480 690 442 125 369 456  11 417 516 685
 700  14 566 471 435 240 515 556 241 430 517 463 513 519 695  18  13 434
 477 713  22 697  20 681  17 701 437 418  23 426 415 436   7 389   6 572
 129  78 739 375 247 432 545 518 253  15 202 102  19 424  10 103 254 420
 423  98 560   8   3 124 712  28 666 425  24   4 255  16  27 387 376 705
 100 709 565 200 716 544 708 201 391 711 256 101 704 390 710  99 715 546
 706 707 714 561 567 563 744 564 562 745]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0321
INFO voc_eval.py: 171: [274  73  36 220  50 227  30 251 358  76 357  75 352 218 335  83  56 198
 275 225 354  85 333 273  29 280  60 223 118 254 355 234 224 231 353 116
 326 196  31 295 221 122 232 356 258 277 157  81 238 233  77  80  84 174
 228  82 327 338 278 345 342  79 201  74 197 330 314 230 222 245 161 236
  78  28 281 242 256 117 250  34 313 156 202 312 253 285 247 306   6 331
 282 261 289 276 322 219 163   3 133 244 239 154 226 128 235 344 237   4
 241 200 340   8 308  33 323 177 279 296 284 288 249  21 337  52 321 298
 124 303 172 259 304  41 203 131 257 246 119 294   1 290 243 188 255 291
 318 155 108 292 302 341   5 214 199 208  48  53 332 248  47 229 343 252
 169 283 305  25 103 240 132 260 121 136 263 204  32 140  38 212   9 149
 211  10 316 217 271   7 272 153 168 123 286  35 183  12 270 329 297 339
 159 150 215 106 178 264 151 324 299 145 146 268 135  22 139 152  43 105
  16 315 307  11 182 269 125  42 179 176 127 205 186  96 267  15  27 309
 346 311 195 262 293 266  97 129 301 109 126 328  18 265 210 192 300  72
 181 171 336 317  64 207  40 351 120  71 206 101  23  69 320  94  44 142
 137 162 213  63 325 209 165 193 187  14 134 104 310 173  62  95 191 113
  66  61 194 319 166 110  20  99 190 130  65 141 184  67 100 350 114 349
 143 167 138 170  59 102  19  70  98 115  24  68  54 287 164 185   2 348
 180  93  46  57  55 160 144 347 112  17 107  37  26 175 147  13 334 158
   0 111  89  45 148  39  58  51 216 189  92  87  88  49  91  90  86]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3136
INFO voc_eval.py: 171: [ 630  625 2052 ... 5899 5897 5922]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0545
INFO voc_eval.py: 171: [1809 1626 1682 ... 2908 2907 2909]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0706
INFO voc_eval.py: 171: [604 376 549 580 373 599 763 705 757 308 561 378  19 329 699 312 615 631
 553 552 607 596 371 776 603  20 680 321 601 294 732 654 382 306 637 311
 635 633 330  29 581 712 703 383 653 377 379 961 359  25 317 613 316 619
 642 298 632 372 777 587 333 716 724 402 332 310 986 773  21 714 783 322
 168 323 403  22 824 554 704 608 384 570 784 929 728 825 557 677 590 324
 582 385 740  32 295 628 663 741 592 949 743 583 589 691 915 326  35 955
 708 746  18 548 715 713 327 735 301 770 381 709 325 978 727 331 958 827
 318 701 566 707 774 937 584 652 980 625 722 734 990 618 674 314 747 297
 984 948 363 744 140 128 621 586 147 593 328  24 780 710 617 977 630 943
 658 683 782 616 711 738 594 611 638 726 375 179 718 894  27 754 634 313
 690 898 981 742 482 300 624 934 753 636 725  31 620 657 759 779 471 684
  36 991 374 172 758 751 568 380 656 178 748 362 923 158 762 889 983 764
 556 891 822 305 723 831 768  39 835 902 605 515 606 772 307 787 164 940
 245 490 832 737 315 897 756 720 562 309 992 760  30 892 453 717 150 595
 733 486 765 138 993 161 609 565 736 478 839 944 177 507 689 904 551 800
  34 293 855 842 643 455 887 767 623 134 987 769  23 785 559 454 270 662
 811  28 721 155 963 394 159 919 491 588 571 660 749 706 512 781 745 149
 885 612 838 668 852 976 567 828 367 739 518 719 469 154 778 303 254 729
 497 598 775 641 752 639 920 345 890 864 850 550 157 473 251 456  37 627
 679 761 516 793 979 888 884 700 900 600 845 265 678 702  26 893 682 816
 614 676 246 602 755 950 750 982 472 585 496 665 640 771 788 766 973 730
 810 247 917 896 878 302 731 622 786 296 563 591 834 130 560 856 823 841
 685 918 833 629 299 799 304 873 610 396 166 669 218 144 807 466 857 369
 597 876 498 675 965 626 905 564 886 141 509 794 511  33 142 686 213 517
 901 971 464 190 844 250 347 503 483 798 129 195 477 830 452 260 806 862
 136 858 666 975 480 170  40 501 171 673 193 912 258 504 927 257 854 395
 152 861 197 189 215 502 196 110 952 355 146 945 192 951 470 481 194 664
 184 681 113  49 672 555 266 175 558 529 808 895 843  17 341  78 859 936
 506 271 489  81 968 256  76 174   9 930 492 913 397 135 465 185  55 151
 867 837 488 228 366 809 339 253 522 319 344 423 914 569 447 219 500 848
  15 357 169 495 802 252 932 508  43 468 972 493 487 877 163 467 510 160
 156 533 451 216 494 579 424 849   1 412 485 947 988 966 268 969 803 505
 261 499 956 484 267 167 667 863 985 340 463 796 903  97 942 946 269 272
 162 826 871 536 353 273 797 957 105 540 899  46 143 187  79 214 645 521
 933 249 223 217 234 523 186 476 176 847 411 916 188 132 387 358 354 244
 964 479  47 259 262  77 970 574   7 446 875 191 655 921 361 535 278 133
 433   6 248 922 925   0 445 519 348 829 836 239 131 866 427 183 953 370
 180 421 520 360 137 865 282  48 960 840 860 939 114 350   2 646 542 368
 962 104 338 153 846 851  96 227 365 229 264 145 112 474 853 644 545 528
 225 532 661 289 872 670 671 182 804 874 392  75  45 959 527 364 814 115
 101 255 429 524 148 181 530 343 450 526 349 659 425 538 449 226 534  98
 537 413  80 688 393 126  44  60 240 649 996 531 410 954 414 111 342 938
 386   3  94 139 879 408 173 687 212 236   4  16 870 578 356   8 989 165
 967 650 407  12 941 401 576 995 428  90 285 431 926 648 994  13  14 924
   5 351 935 546 928 815 931 974 346 400 352  11 211  10 443 544 475  95
 795 882 647 868  99 102 409 224 525 277  91  41 103 575 817 792 513 881
 100 460 541  92 577  93 869 543  82 276 880 415 539 820 448 320 284 457
 279 651  64  63  61  70 275 547 818 405 274 280 389 291  42 789  72 805
 117 388 235 801 263 283 422 406 458 883 430 813 791 233 220 290 390  73
 426 398 461 444 286  53 812 404 432 399  85 462 459 819 910 572 209 125
 106 514 821  38 391 416  67 437  51  54 221  68  74 123 292  71 287  69
 109 440 231  50 420 790 127 222 242 911  88  52  65 573  66 288 237 441
  62 232 419 909 692 281 335 418 243 118 417 107 121  86 230  89 439 108
 908 442 116  84 241 119 124  83  58  57 122 238 907  87 697 434 334 436
 120 695 210 438 435 198 906 337 207 204 199  59 696 693  56 201 202 698
 336 694 203 205 206 208 200]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1667
INFO voc_eval.py: 171: [316 118 113 108 126 309 337 148  33 315  74  72 120 128 323 138 116 107
 109 130 147 320 106 310 321  36  83  34 111 105 313 311 333  46 326  37
  89  44 314  40  29 115  78  50 144 152 334  31  75  71 121 328 112 110
 319  41   6  58   5  38  57 124 117 295 157  94   0 150  81 335 236 329
  97  88 237 260  85  79  70 149 202  47 344 248  45 198  76 249  73 292
 213 238 119 201 136 291  68 143  49 302 339 325 287 246  84  87 285 324
  32 254 139 158 154 155  52 122 257  30 135  82 140 286 133 346 340 289
 127 293  27 290 336  60 288 332 156  80 342 330 146  86 197 123  39  77
 169  64 214 294 161 252 348 318 145 151 212 199   1 125  61 129 227 327
 251 142 208 153 253 247  48 317  62 200 204  69 284 242 235 163 297 203
 245 170  65 258 137 211 301   3 243  28 347  98 194 100 240  13 228  63
 298 296  59 303  42 131  18 132   9 239 255  95 224 195 196  67 114  35
  53  51 256 134 141 225 322  56  43   7  22  55 221  17 300 218 250  54
 177 192 241 209 299   2 244 338 193   8 259 283  66 104 231 101  10 206
 205 230  19 304 331   4 306 160 275  93  99 175 219  91 103 179 223 102
 274 229 159 343 220  90 226  96 217 232  26 216 341 162 272 174  92 268
 278 180 167 165  24  14  23 234 222 210  15 215 173 312 233 207 282 345
 308 307  11  16  12  20 305  21 277 262 276 265  25 280 172 186 263 184
 182 281 266 279 271 273 168 261 171 178 264 270 164 166 269 267 176 181
 185 187 191 189 188 183 190 349]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2795
INFO voc_eval.py: 171: [2214 1109 1569 ...  692  694 2515]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1667
INFO voc_eval.py: 171: [103 111 109 107 256 137  14 196 108  97 102 257 258  33  17 138 104  31
  25 255 112  72 261 163 110 193  29 164  20 229  78 134  30  77  96  71
 230 135  27 263 171 162 232  32  13  81 167 206 157 165  18 105  83  35
 197 148 220  94  42 170 259 160 154 262 212 139  22 254 101 260  73  70
 217 219 168 205 166  93 188  65  28 169 228 115  19 136 133  43  98  21
 222 234  34 156  80 207  54 227 113 223  75 235 231 187  68 189  84  16
  62 161 215  12 173  23  85  59 209  69 233  76  60 214  24 221  15 216
  63  61 201  67 129  64 191  66  26 128 243  48 131  45 213  53 244 184
 132 218 158 116 210  44 241  58  95 153 106 172 130   5 190 120  57 155
 185 252 211  56 253 127  10  46 202  47 240 192  99 179  51 121  88 159
  52 198 183 118   2  55 152 100  86   3   1 204  82 114 208   6 200 122
 126 117  79  49 125  87  74  41  90 203  11   0   8   7 236  91 178  92
 176 186   4 245 174 242 194 195 237 175   9 246  89 123 149 150 147 181
 124 225 146 226 224 199 180 239 177 238 249 250 145 248  36 142 143 247
 251 182  50 140 144 141  38 151  37 119  39  40 264]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0315
INFO voc_eval.py: 171: [2095  591  275 ... 2745 2749 3647]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1156
INFO voc_eval.py: 171: [373 107  79  92 409 110 318 408  94 501 371 103 374  81 259 390 401 159
 100  93 378 463 286 244 375 678 385 171 112 406 683 189  19 507 288 251
 268 166  47 388 379 289 689 661 479 851  20 365 453 279  84 464  49 193
 319 329  82 195 498 410 154 316 454 506 247 283 801 227 180 402 272 223
 245 164 403 323 460 277 276 262 767 226  72 835 686 421 172 461 187  33
 108 225 505  35 140 243 254 281  11 474 546  64   3 222 471 326 300 310
 466 382  83 433 531 680 242 252 273 209 186 383  46 367  48 502 756 504
 152 177  59 833  89 847 201 199 845 456 204 571 287 821  42 157 322 155
 230  68 266 496 664 249 158 432   2 478 681 183 291 304 542 837 748 499
 258 732 437 370 772 840 188  39 162  44 160 862  58 153 150 196 176  62
 451 514 327 376 156 163 299  45  74 584 267 305 298 687  75 111 666 668
 500 101 423 404 174 444 822 814 839 393 760 126 211 426 672 366 179 330
 250 405 497 215 480 161  63 581 529  73  27 762  15  57 271 495 729  51
 823 852  54 747 858 836 503 248  10 120 221 270  14 559 380 569 441 314
 516 194 868 125 257 145 294 525 475 284 820 231 397 519 205 264 260 535
  40  41 149  69 269 670  13 369 688 810  17 214  86 802 239  29 855   8
 439 296 255 384  88 674 766 241 706 575 285 865 175 292   5 763 665  38
 751 470 280 399 779 662 593 539 422 545  24 812 477 684  25 442  34 246
 476 663 207 424 759 582 263 197  85   1 261 727  61 518 849 541 850 595
 809 673 396  32 834  97 728 398 210 387 173 400 206 392 685 572 170 669
 146   4 457 216 561 106 256  70 147 819 224 430 867  43   6 219 736  56
 859 123 769 816 523 528  31 217 337 324 841  98  16  12 313 124 527 325
 236 552 863 407 119  90 274 806 512 752 807 168 818 295 135 467 282  76
 824  71  67  60 428 815 737 381 817 866 679 342 764 808 811 813 556  50
  28 587 427 843 537 844 771 339 253 293 765 372 579 452 774 522 138 750
 420 297 455 368 233 770 734 735 130 740  78  52  53 312 133 302 583 311
 600 446 854 459 208  26 533 846 306  65 169 386 707 128 554 336 690 682
 139 377 301 538 753 856 785 510  37 776 848 315 308 534  30 434 395 469
  66 357 309 676 317 265 568 513 842 511 338 192 275 578 290 136 131 116
 773  55 142 425 472 137 185 749 860 838 429 228 570 586 618  77 468 864
 861 553 278 389 394 777 345 328 320 590 565 391 462 667 435 758 134 178
  36 465 716 151 307   7 596 445 229 717 715 148   9 303 132 853 203  23
 743 857 825 129 143 739 705 585 144 768 616 458 741 200 331  22  18 778
 115 321 218 473 141 781 549 613 566 761 784 731 574 782 524 182  21 127
 190 563 232 733 526 592 671 351 346 829 122 567 628 104 597 757 202 577
 198 622 594 449 742 240 235 220 564 333 730 234 610 755 167 623 487 697
 780 832 783 121 629 335 118 448 786 754 700 591 634 709 332 447 450 675
 831 184 588 576 213 656 440 340 599 775 360 237 611 560 334 105 191 632
 573 677 691 827 117 348 443 551 349 343 659 212 353 181 438 830 436 238
 431 744 828 746 826 713 489 580 598 358 355 658 417 692 620 619 347 589
 631 530 660 359 515 548 350 657 341 362 344 356 557 508 361 612 711 694
 698 413 696 626 608 414 625 354 532 165 614 544 800 363 627 520 693 617
 352 521 364 615 558 555 633 488 550 547 540 708 621 609 418 738 630 543
 792 536 798 642 624 871 723 517 714  80 745 710 794 797 718 509  91 796
 415 704 793 637 787 109 416 650 641 102  96 419 870 636 790 720 791 872
 695 795 712 799 635 638  99 702 643 788 652 703 601 639 721 789 493 699
 640 726 114 486 719 485 724 647 648 701 113 490  95  87 644 602 607 494
 722 805 645 725 603 605 481 604 606 484 649 482 483 653 654 646 412 655
 492 651 562 491 803 804   0 411 869]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0711
INFO voc_eval.py: 171: [1386  608  873 ... 1536 1538 1537]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2301
INFO voc_eval.py: 171: [253 285 179 262  65 291  62 252 271 265 226 296 283  67 297 174 177  73
 255 294 241 258 210 176 130 263 230  78 402 503 468 207  71 404 403 504
 171 281 240 243 260 290  63 272 484 229  91 467 486 520 228  59 227  82
 485 191 388 398 469 132 183 295  49 511  66 533 303 135 482 508 256 474
 184  92  52 466 198 475  88  83  51 131 407 406  25 523 242 106 108 471
 247 417 257 488 479 133 483 206 250 389 267 387 264  90  28 338  69 476
 446 400 512 185 472 480 304 195 181  75 182 152  74 514 465 150  35 518
 421 318 180 222 481 470 192 107 487 458 266 459  45 529 178 196 380 129
  50 528 341 478 305 392 223 527 473  80  86 526  58 477 496 134 416 244
 367 369 418 273  76 169 536  79  53 424 127 282 495 136 161 490 427  33
  31 507 208  55 366 172 501 494 288 313 215 151  81 350 221 337 219 289
 224 312 491 368  84 390 408 147 194 415  44  68  29 193  27 137 493 287
  24 225 118 249 212 103 105 509 275 395  57 277 510 173 541 360 259 309
 126 425 306 445  72  89 168  42 534 209 539 492  60  87 346 428 399 322
 420 199 175  30 419  56 397 423 497 440 320 155  17  85 128 439 310 357
 382 365 186  93 164  61 453 144 448 432 211 345 121 339 422 542 187 364
  64 292 532 246  54 358 438 321 454  20 110 170 308 540 351 261  70 335
  77 311 394 347 213 307 411 319 385 336 412 386 342 343 349 146 122 114
 449 248 410 104 109   5 362 516 197 140  41 327  18  32 323 270 154  34
 254 165 276 113 112 324 522 139 401 353 115 111 334 530 405 434 447 218
 340 450 143 316   3 393 162 348 145 352 149 354 278   4 344 430 205 116
  38 519 120 125 452 384 517 286 521 383 363 359 531 409 245  97 138 119
 117 370 188   9 376 426 123 381 251 535 284 148 142 153 379  10 124 232
 435  43 220 537 502 455 378 391 361  13 158 159 506 437   1  96 102 238
   7  39 167  98  40 100 189 166 525 274  16 141 457 356 325  22 236 201
 231 101   6 377 513  46 515 200  19  48 216 280 293 538 505 355 524 279
 314 214  14 489  99   2 414 375 456 396 429  21 157 234 436 237  12 317
  15 315 156 217 460 233  11 431 160 433 462 239  23 235 441  47 163 190
 461 298   8 464 413 326 443 451 463 301 373 300 299 328 330 302 444 374
 371 332 204 203 372 329  26  95 202 442   0 268 331  94 500 333  36  37
 269 498 499]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0646
INFO voc_eval.py: 171: [933 110 904 ... 350 351 804]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0600
INFO voc_eval.py: 171: [210  34 129 127 130 220  38 206  31 166 214  15  35 207 120 219  33 213
 128 172 217 208 163  36 209 134 132 139 122 138 116  70 216  96   3  32
 137 176  46 135  13 196 225  62 100 136 131   1 221  63 152  64 226 113
  19 140 192 133 215 169   7 228  43  14 174  87 205 185 222 114 107  66
  97  18 106  86 168 212  88   8 195  56  95 170 224  68 145  54 144   6
  89  71  12 194  48  20  69 199 177 182 186 171 178 227 102 218  40  52
  17 173 223  16  73 200 190  57 157 153 164   4  74 202 211   2  55 193
  41  50 181  76 191 198  39  51 124  75  49 118  44 149 125  80  10 197
 175 201  65 108  58 180 119 104 167  47 143 154 230 165 141  45 183 105
 179  98  99  78 148 235  67 112 142  60 232 101  11 231 236 109  59 117
  42 123  83  53 151  37  30 233   5  94 110 234 103  61 159  72 229 238
 187 161  82 155  77 184   9  79  85 111  23 204  81  84 189 121 156 126
 146   0 188 237 147 115 150  92  29 158  28  25  91  93  26 160  21  90
 162  24  27  22 203]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3902
INFO voc_eval.py: 171: [ 7088  7789     0 ... 16611 16613 16614]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.1903
INFO voc_eval.py: 171: [1727  242  226 ... 1826 1824 1823]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2198
INFO voc_eval.py: 171: [  75  215  496 ... 1000  776 1008]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0878
INFO voc_eval.py: 171: [ 66   7  63  64 214 215  75 302   8  57 337  74  67 156 122  61 295  91
 290 289 288 339 321 297 124  58 267 322  86 269 292 117 330 300 102  88
 305 294  21 251 103 266 201  71 296  62 291  54 206 293  55  10 331 203
  70 253 146  59 299 127 200 287 198 116  68 158 285 129 301 268  95 304
 126  69 298 118 270 336 286  65 128 143  60 271 303 345 173 110 320  92
 125 185 252  19 104  93 352 205 184  72   9 182 109 256 265 134 157 105
 142 121 133 250 258 211 123 240 139 219 120  96 208 255 132 106 141 343
 257 195 177 131 261 351 179 119 108 178 107  12  73 349  87  56  85 159
 229 181 196 154 254 138   3 218 130 194 193 152 192 190 148 180 176  16
 221 273 243 189 165 235 326 264 186 202  37 334  35 280 171  89 220 237
  29 175 284 183  28 279 353 222 262 344 199 145  17  77 338 168 223  15
 333 225  38 283 325 115  34 209 150 188 153 174   5  13  32 315 187   4
 327 197 263 246 316 167 341  98 101 151 149  90  18 210 207 155  14  78
 217 241 242 114 204 247  11 191   1  33 100 231 282 350 355  79 332  84
 147  94 144  40  27  48  24 216  25  76 281  30 234  99  26 230 347   2
 340  81 346 348  39 238  31 342 226  97 354  51 232 163  45 213 328  41
 245  82 244  47 233 140 239   6 335 324 224 272  52  46  43 236  49 323
  80  83 310 313 329  42  36 160  50 170 162 169  53 161 164 172 227 314
 248 166 137 260  22 311 307 212   0 319  23 308 275  20 249 274  44 312
 113 306 309 112 317 111 318 228 136 135 259 276 277 278]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1113
INFO voc_eval.py: 171: [471 479 477 475 289 472 485 476 296 474 660 349 470 491 481 661 655 473
  51 188  49  64 612 190 291 478 183   5  65  76 101  54 300 293 288 498
 290 622 516 704 261 499 480  57  91 696 112 700  74  47 320 344   9  46
  45 521  52 111  63 184 362 697 664 351  80 487 104 119 105  83  11 484
 151 694 494 510  44 509 614 266 488 548  56 483 361 345 647 305 617 273
 653 654 152 269   6  70 117 705 567 353 299  43 192 489 644 616  71 710
 698 693 170  48 659  77 609 191 314 127 511  55  61 213 624 287 121 263
 656 702 262 210 342   7 283 106 703 274   4 318 641 358 357 185 268 667
 124 552 312  86 554 559 709 695 482  58 232 153 646 591 628 583  90  21
 350 316 549 553 636 150 103 522 573 663  66 610 304 658  59 132 116  14
 128 618 701 562 570 634 551 486 625 508  73 142 639  78  60  79 378  97
 156 236 507 171 354 133 675 245 528 216 172 513  67 222 657 526 347 317
 126 246 301 169 550 575 155 204 154 437 201 613 500 163 708 160 503 321
 505 707 380 523 712 408 571  26 264 572  84 411 676 665 558 221 122 355
 315 592 367 501 129 611 574 202 561  50 102 352 608 343 556 365 306 620
 360 203 125 424 520 584 555  99  20 244 159 430 265 212  29 557 187 580
 359 680 175 420 666 167 243 294 196 123 197 540   3  85 174 215 633 711
 638 158 285 292 198 136 157 529 249 578  13 565 410 619 560 568 650 699
 239 149 319 131  82  34 706 267 564 652 209 326 518 208 648  22 524 130
 109 134 519 115 144 138 356 279  17 247 180 364 418  94 490 166 366 240
 346 113   8 248 369 164 241 495 100 310 593  96 182  24  68 280 413 669
 363  87 181 205 716 348 438  12 671 536 615  89 415 368 674 139 107 168
 563 399 637 118 120 566 325  98 211 322 135 585  16 110 382 600 595 272
 502 590  75 189 307 569 195 632 576 194 525 393  30 207  15 407 186 323
 140  93 193 137 225 672 374 228 230 271  53 275 533  28 463 493 601 596
 587 496 602  25 200 589 206 395 233 492  81 297 238 627 141 581 231 214
 416 392 579 313 630 442 692 398 332 419 464 335 678 457 668 145 143 594
  10  42 458   2 199 512 427  62 227  95 621 642  36 447 417 327 451 465
 426 425 577 108 179  69 334 643 629 401 455  40 456 309 626 649  92 542
  39 404 517 257 258 414 298 259 645 226 691 114  72 586  88 428 177 719
 229 260 336   1 286 432 588 662 329 242 402 223 635 468 302 178 582 515
 685 677 532 717 538 436 541 234 687 535 161 328 423  41 689 603 337 684
 373 640 340 330 338 324 623 165  32 162 651 277 284 375 440 278 341 339
 679 435 311 673 466 176 530 422 454 333 431 331 173 235 718 448 631 670
 421  33 303 405 403 597 606 281 224 148 219 308 276 450 282 514 379 381
  31 544 459 237 433 377 376 688 270  35  37 406 506 255 537  27 370 391
 409 690 605 146 453 372 394  38 412 607  23 452 599 295 604 400 460 449
 534 461 527 547 686 546 371 147 429  18   0 443 543  19 439 220 539 444
 545 531 384 254 497 504 434 598 462 387 445 441 217 218 253 446 397 251
 396 252 713 467 388 469 250 386 256 383 390 681 385 389 682 683 715 714]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0574
INFO voc_eval.py: 171: [522 521 482 454   0 630 283 480 629 506 614 339 193 242 513 568  22 451
 483 260 188 569  48 175 261 284  17 494 250 355 341 280 272 615 259 286
 255 266 271 638 148  13 450  58 249 317 265 349 135  12 258 458 351  54
 174 273 342 477 566 245 497  23 262 293 452 136 281   2   4 570 346 102
  16 232 352 631 389 236 314 456 253 289 616  11  50 107 276 574 515 277
 158 461  57 459 288   3 592 235 505 299  36 585 516 315  18 241 514 403
  61 576 590 313 613 187 219 401 279 463 340 520 178 361 139 557 457  56
  84  49 362 278 623  66 152 290 453 639 209 383  32 507   1 542 475 234
 143  95 304 370   9   7 518 295 243  20 530 218  71 381  90 270  86 376
 147  10 481 509 519 322 316 207 625 239  40 335 636 138 476 415  51 571
 424 108 179 421  33 358 472 538 150 274 246 325 619  37 291 210 501 244
 144 399 156 177 388 617 578 215 196 285 300 573 572 222  43 527 146  52
 517 294 109  62  46 220  19 282 247 307 529 303 584 575 133  59 410   8
  53 357 157  15 122  93  45  55 184 330 502 473 344 165  47 338 305 595
 329 369 141 110 345 350 490 547 309 586  27 635 111 112 546  24 632 378
 499  14 556  88 469 418 145  30 637  80 337 248 523 541  74  99 634 100
 360 128 365 182  21   6 464 474 373 183  85 230 414 217 229  68  97 429
 275 468  60 577  72 328 407 612 604 189 397 470 204 213 198 212 185 504
 326  87 552 633 559 231 343 416 113   5 596 434 392 114 466  81 142  65
 240 256 225 117 119 544 404 211 214  35 405 487 391 395 368 537 252  91
 332 533  67 545 548  70 531 197 254 564 327 336 149 526 233 549  39  73
 419  89 364 115 334 627 438 628 263 428 534 579 608 622 216  75  83 186
 106 539 478 180 398 221 528 465 393 296 208 324  78 433  63 462 302 176
 251 536 488 320 423 366 467  92  82 435 267 606 238 543  44 348  34 493
 101 347 287 103 105 525 374 194 169 382 333 354 400  41 223 353 206 379
  28 319 359 440  38 503 560 626 597 600 237 167 131 356 151  77 264  79
 508 375 607  94 492 367 496 495  29 104 318 195  76 228 160 444 163 387
 599 447 371 567 601  69 311 565 321 372 583 532 331  96 535  64 603 621
 396 489 479 323  31 605 159  98  26 298 412 540 602 191 227 190 390 587
 624 394 132 137 561 203 172 121 192 173 226 363 130 166 446 377 620 409
 380 385 498 491 140 118 155 593 224 437  25 386 618 308 129 485 408 181
 268 427 297  42 425 116 269 500 202 582 420 484 471 153 384 162 301 171
 257 591 126 413 205 292 430 417 154 486 460 422 588 598 124 120 555 123
 426 411 610 201 436 443 161 200 134 168 431 432 550 445 449 164 563 406
 455 589 125 442 310 448 558 127 562 441 402 554 594 170 439 553 306 551
 312 512 580 581 524 510 511 611 199 609]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1262
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1420
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.314
INFO cross_voc_dataset_evaluator.py: 134: 0.055
INFO cross_voc_dataset_evaluator.py: 134: 0.071
INFO cross_voc_dataset_evaluator.py: 134: 0.167
INFO cross_voc_dataset_evaluator.py: 134: 0.279
INFO cross_voc_dataset_evaluator.py: 134: 0.167
INFO cross_voc_dataset_evaluator.py: 134: 0.031
INFO cross_voc_dataset_evaluator.py: 134: 0.116
INFO cross_voc_dataset_evaluator.py: 134: 0.071
INFO cross_voc_dataset_evaluator.py: 134: 0.230
INFO cross_voc_dataset_evaluator.py: 134: 0.065
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.390
INFO cross_voc_dataset_evaluator.py: 134: 0.190
INFO cross_voc_dataset_evaluator.py: 134: 0.220
INFO cross_voc_dataset_evaluator.py: 134: 0.088
INFO cross_voc_dataset_evaluator.py: 134: 0.111
INFO cross_voc_dataset_evaluator.py: 134: 0.057
INFO cross_voc_dataset_evaluator.py: 134: 0.126
INFO cross_voc_dataset_evaluator.py: 135: 0.142
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 6499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.401s + 0.029s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.381s + 0.038s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.367s + 0.039s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.365s + 0.039s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.373s + 0.039s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.372s + 0.039s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.373s + 0.039s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.370s + 0.040s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.370s + 0.041s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.372s + 0.041s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.372s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.373s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.376s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.580s + 0.044s (eta: 0:01:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.380s + 0.039s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.377s + 0.039s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.370s + 0.038s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.368s + 0.038s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.367s + 0.039s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.373s + 0.038s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.375s + 0.038s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.371s + 0.038s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.372s + 0.039s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.368s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.367s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.366s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.528s + 0.029s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.359s + 0.032s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.355s + 0.038s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.362s + 0.039s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.358s + 0.039s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.357s + 0.038s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.353s + 0.039s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.355s + 0.039s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.358s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.357s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.358s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.361s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.361s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.551s + 0.047s (eta: 0:01:14)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.390s + 0.042s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.384s + 0.043s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.372s + 0.042s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.365s + 0.042s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.362s + 0.041s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.365s + 0.041s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.363s + 0.041s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.362s + 0.041s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.365s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.362s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.362s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.362s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 59.248s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [320 222  86 212 310 592 118 596 155 602 173 626  81 593 600 597  49 211
 309  59 132  88 612 267 167 197 188 314 216  54 635 632 181 165  93 161
 598 629 113 137 323 260 225  60 325 138 227 312 214 182  50 151 195  74
 142 223 321  32 643 279 147  73 642  33 315 217 139 665 661 275 594 226
 324 617 271 305 221 319 613 620 619 595 647 141  47 262 659 135 362 169
 365  56 288 405 237 239 286 445 240 615 306 274 117 231 134 285 283 281
 604 411 618 614 648 205 660 131 316 218 295 313 215 183 354 403 311 213
 723 627 649 299 502 294 268 303 622 359 396 601  34 277 679 603 397 175
 196 170 114 409 498 164 646 280  41 157 740 219 317 638 166  43 653  40
 236 487  36 495 640  44  87  37 168 639 301 200 119 662  91 504  35 107
 284 360 154 735  52 625  38  45  39 153 404 122 641 610 398 191 534 193
 471 266  82 293  85  62 490 194 522 184 412 307 459 366 675 364 261 483
 584 499 201 358 663 234 605 176 489 163 497 496 634 265 583 300 669 110
 172  46 551 441 199 185 410 655  42 406 493  84 491 229 235 327 408 289
 402 413 700 228 326 608 609 616 328 677 230 506 630 144 721 180 318 220
 269 187 273 657 282 509  55 363 472 355 725 631 242 322 224 395 637 104
 401 733 400 442 177 407 368 109 361 140 507 160 399  31  68 192 152 178
 121 174 645 116 272 621 158 290 676 449  71 492 232 105 171 251  72 108
 159  57 440 651 357 461 654 673 486 297 145 526 644 590 253 484 482 532
 106 292 287 475  48 540 510 652   1  80 485 599 606 731 624 636 179 356
 162  92 658 128 607 198 689 186 674 298 190 623 701 367 558 628 474 308
 538 575  78 668 611 126 686 578 667 574  83 382 671 430 650 386 454 460
 500 429 552 383 451  75 264 120 690 390 189  70 156 503 511 729 380 728
 656 576 448 523 241 633 670 233 250 278 115 541 488 505 672 533 715 553
 527 732 143 512 664 727 688 238 571 508 680 470 586 422 543 717 536 370
 494 443 384 473 464 559 580  89 375 744 336 501  77 210 112 734 352 206
  79 329 111  30 254 587 455 743 276 340  27 252 450 698 423 249   0  61
 209 469 588 347 387 579 247 730 417 719 296 376 342 369 457 582 589 678
  76 374 149 208 332 333 302 270 591 348 463 530 207 687 335 373 415 127
 458 146 255 381 385 695 720 722 724   2 577 263 736 291 428 371 388 555
  53 133 130 331 737 353 525 379 453 716  96 337 535 467 585 136 524 351
 738 304 573 693  58  28 726 468 476 394  65  90  51 462 581 739 339 420
 697 718 148 456 515 550 548 350  20  95 345  29 568 570 547 435 685 150
 466 477  97  63  94 246 537  12 539 554  67 569   5 691 480 542 346  25
 446 549 432 341 344 528 682  66 531 681 248 557 414 349 452 529 338 479
 520   9 513 692 683 123 521 447 334 245 444 343  11 418 125 372 481 330
 516 565 243 517 699  13 437 684 556  69 244 431 465  64 694  18 514 696
 518  14  17  22  19 478 439 436 712 419 427  23 438   6 391   7 129 416
 377 572 434 545 256 433  15 204 102 425 519 103  21 421  98  10 257 560
   3 424   8  24 666 426 124   4 711  16  26 258 389 100 378 566 708 703
 713 202 544 203 707 393 259 710 702 392 101 709  99 706 546 704 705 714
 567 561 563 741 564 562 742]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0321
INFO voc_eval.py: 171: [271  72  35 217 225  49  29 249  75 355 354  74 350 215 331  55  82 195
 272 352 223  85 332 270  28 277  59 221 251 118 231 353 222 229 351 116
 323 193  30 291 219 122 230 254 274 156  80 236  76 218  79 173  84 226
  81 328 335 275 339 343  78 198 310  73 194 325 228 242 220 234 160  77
  27 278 253 248 117  33 309 155 308 199 250 282 302 245   5  83 326 257
 279 285 273 317 216 327 162   2 133 237 232 152 224 129 341 233 235 240
   3 337 197   7 304  32 319 176 276 292 247 281  20 284 318  51 243 294
 124 255 299 171 300 200  40 131 244 340 289 119   0 241 286 187 252 314
 287 108 211 288 154 298   4 196 338 205  47  52 330 246  46 168 227 280
  24 301 239 256 132 103 334 121 136 238 201 260  37  31 258 209 140 148
   8   9 208 312 268 214   6 269 151 123 283 167 182 267  34  11 293 324
 336 212 158 106 177 261 145 320 149 295 144 342 153 265  21 135 150 139
  42 303 311 105  15 181 125  10 266  41 178 175 127 202 185  96 307  26
 264  14 344 305 259 290 263 297  97 109 128 126  17 262 207 190 296  71
 180 170 313 204 333  63 322 349  39 120  70 203 101  22 316  68  43  94
 142 210 137 161 321  62 206 164 191  13 186 306 104 134 172  61 189  95
  60 113  65 315 110 192 165  19  99 130  64 348 183  66 100 114 347 141
 166  58 138 102 169  69  23  98  18  53  67 115 184 346 163   1 179  45
  93 143  56  54 345 159  16 112 107  25  36 174 146  12 157 329  89 111
 147  44  38  57  50 213 188  92  87  88  48  91  90  86]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3129
INFO voc_eval.py: 171: [2045  632  627 ... 5884 5852 5879]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0500
INFO voc_eval.py: 171: [1625 1808  642 ... 2293 2894 2892]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0753
INFO voc_eval.py: 171: [609 770 377 ... 209 211 204]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1674
INFO voc_eval.py: 171: [119 312 334 114 109 128 340 150 317  34 121  76  74 130 325 116 108 110
 132 149 323 311 107 324  37  35  85 112 315 106 313 335 327  38  45  90
 316  41  46 117  30  80 146 155 337  32  52 122 113 330  73  77 111 322
   6  42   5  62  60  39 124 118 160   0 297  94 345 338 153  83 238 331
  57  97 239 262  89  81 151  72 204  48 318 347 125 250   1  47 251 200
  78 240 215 294 138 120 203 293  75  70 144  54 343  50 289 304 248  86
  33 287  88 256 140 161 157  53 158   7 137  31 259 142 135  84 288 326
 344 141 349 295 129 291 339 346 292 159 290  28  82 332 123 148 314 199
  40  87 172  67 216  79 296 164  61 254 350 147 214  58 154 321 127 320
 201  64 253 328 131 255 229 145 249 210  65  49 319 156 206  71 202 244
 286 237 299 126 166 205 260 247 173 139  68   4   2 245 213 152 303  29
  99 196 101 242  66 230  14 298 300  63 134 305 133  43 241 257  10  19
  95  69 197 198 115 226  55  36 143 258  51 136 227 341 336  44   8  24
  59 220 223  18 252 302  56 179 194 211 243 342 301   3   9 246 261 195
 105 285 102  11 233 208 207  20 306 232 333 308 163 277  93 100 221 178
 104  92 103 181 225 162 276 231 222  96 218 228 219  98 234 165  91 274
 177 280 270 182 169  25 224  26  15 236 212 217 176  16 209 235 168 284
 348 329 309  12 310  17  13  21  23 279 264 307 278 267  27 282 175  22
 184 188 186 283 265 268 281 273 275 171 174 263 180 167 272 266 170 271
 269 183 187 189 191 193 190 185 192 351]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.1773
INFO voc_eval.py: 171: [2210 1112 1571 ...  694  692 2511]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1690
INFO voc_eval.py: 171: [105 113 109 111 256 139  14 197 110  99 104 257 258  33  17 140 106  31
  25 255 114  74 261 165 112 194  29 166  20 229 135  80  30  79  98  73
 136 230  27 263 164 173 232  32  13  83 169 158 206 167  18 107  85  35
 149 220  96 259  42 161 172 155 262 141 212  22 254 103 260  75  72 171
 219 217  95 168 205 189  67  28 170 228 117  19 138 134 100  43 234  21
 137 222 235  34 157  82 207  55 115 227  77 231 188  70 190  16  86 163
  64  12  23  71  87 215  61 209  62  24  78 233  15 214 221 216  65  63
 201  69  66 192  68 130  26 129 243  49 132 174 223  45 213 185 133  54
 244 159 218 118 210  44 241  60  97 108 154 191   5 131 122  59 156 186
 211  58 252 253  10 202 128  47  48 101 193 240 180 160  52  90 123 184
 199 102  53  46 120   2 153  88  57   3  84   1 204 116   6 208 124 119
  81 127  50  89  56  76  41 162  92 203  11 198   8   0   7 236  93 179
  94 177 176 187 195 242 196   4 245 175 237   9 246  91 125 150 151 182
 148 225 126 226 224 200 181 239 178 249 238 250 248 144 251 147  36 247
 145 183  51 142 146 143  38 152  37 121  39  40 264]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0315
INFO voc_eval.py: 171: [2108  598 1593 ... 2769 2766 3664]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1168
INFO voc_eval.py: 171: [374 108  80  93 411 112 320 413  95 504 373 104 375  82 257 392 404 100
 159  94 379 467 287 242 376 681 387 408 170 113 686 187  19 289 510 249
 266  46 165 390 290 692 380 484 664 856  20 457 366 281  85 468  48 321
 191 331  83 193 501 412 155 318 458 509 245 283 805 179 405 225 270 243
 221 406 464 276 275 771  72 260 840 424 224 689 171 465 185 109  33 223
  35 142 508 252 154 282 241 327  11 550 478   3  63 220 301 328 474 384
  84 312 436 470 239 535 250 271 683 184 207  45 385  47 368  90 760  58
 838 153 505 176 507 852 199 197 850 460 573 202 288 825  41  67 157 325
 264 228 499 669 247 435 482 291   2 158 546 684 752 305 668 842 181 502
  79 256 371 441 845 776 736 162 866  43 186  38  57 151 160  61 194 455
  74 156 300 175 517  44 163 329 586 377 299 265  75 111 306 690 103 671
 309 407 426 826 448 503 173 844 395 127 764 429 274 208 818 410 367 332
 248 178 500 161 211 583 766 483  62  73 532  15  27  56 269 734 827 751
 858  50  53 498 841  14 246  10 383 563 571 121 268 506 444 219 316 872
 143 519 126 192 294 255 400 479 284 148 528 824 262 229 539 523  40 258
 203  39  68 150  13 267 675  17  87 370 691 297 666 814 212 443 236 861
 296  29 253 238   8 806 770  89 386 677 577 285 869 709 857 401 767 755
   5 596  37 174 782 670 280 475 543 549 425  24 816 480 481 687  25 446
 244  34 293 427 665 763 205 584 261  86 195 259   1  60 732 521 854 595
 545 855 733  99 399 839 676 402  32 403 389 688 209 813 172 393 204 574
 674 461  69 107   4 565 169 871 214 254 149 678 773  55   6 823 433 740
 217  42 863 222 527 215 820 124  98 326  12  31 533 339  16 846 867 125
 556 315 531 235  91 756 272 409 120 514 810 137 822 471 811  71 286  77
 167  66  59 828 870 382 431 741 768 682 819 344 821 589 560  49 775  28
 817 430 251 541 849 848 815 812 581 769 456 341 778 372 754 774 526 140
 231 459 298 423 738 744 369 739 131 314 585  51  52 323  78 135 450 295
 303 860 313 851 388  26 537 307 206 463  64 789 338 710 378 129 558 693
 279 141 780 862 757 168 685 542 398 302  36 513 853 310 538 317  30 437
  65 473 359 263 570 680 273 515 530 340 319 580 190 777 138 292  54 132
 864 476 145 139 753 117 572 428 843 588  76 183 396 865 868 432 781 592
 620 391 522 557 277 472 227 347 394 568 466 322 762 438 381 672 397 330
 278 598  70 136 308 177 152 449 469   7 718 720 226 719 859 304 134   9
  23 130 847 772 747 201 587 743 708 829 147 146 618 462 333 745 198 477
 783 324  18 116 144  22 133 216 553 784 765 569 786 615 788 311 735  21
 576 737 567 232 673 128 188 594 180 529 105 348 353 834 579 123 630 761
 599 453 200 624 746 597 516 335 196 218 234 237 667 612 759 233 625 700
 490 785 166 631 758 593 787 837 119 452 337 122 790 636 334 703 590 454
 240 451 439 836 578 679 712 213 658 601 182 445 230 779 342 564 106 362
 336 613 634 575 447 555 694 189 350 118 832 345 351 442 355 210 662 440
 659 835 434 748 750 833 831 600 716 492 360 582 357 420 591 622 633 661
 534 349 621 695 518 361 663 552 364 343 352 830 660 346 358 561 511 363
 614 714 701 628 697 699 416 536 356 610 627 548 417 164 616 629 804 365
 696 524 354 525 619 559 562 617 635 554 551 544 491 742 632 611 623 711
 421 547 796 626 644 803 540 717 520 875 727  81 713 749 798 801  92 512
 721 707 418 800 110 797 639 791 102 652 643 419  97 422 794 874 638 795
 723 698 876 799 715 802 637 640 101 705 645 792 654 706 603 641 793 726
 496 702 642 731 724 115 489 488 114 729 650 649  96 704 493  88 646 728
 609 604 497 722 730 725 647 809 605 607 602 485 606 608 487 651 486 415
 655 648 656 657 495 653 566 494 807 808   0 414 873]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0712
INFO voc_eval.py: 171: [1401  613  887 ... 1557 1551 1556]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2241
INFO voc_eval.py: 171: [252 285 179 262  65 290 251  62 270 264 295 282 225  67 296 174 177 293
 254 257  73 240 210 176 130 261 229  78 398 207 497 463  71 400 399 498
 171 280 239 242 260 289  63 271 478  91 228 462 480 514 227  59  82 226
 479 384 191 394 464 132 294  49 505 183 302  66 527 135 476 502 468 255
 184  92  52 461 469 198  87  51  83 403 402 131 517  24 241 106 466 108
 482 246 413 256 473 206 133 477 249 385 266 383 263  90 481  69  28 336
 471 506 441 396 467 185 474 303 195 152 182  74  75 181 508 150 460 194
  35 317 512 417 180 465 192 221 265 453 107 454 178 470 196 523  45 376
  50 129 386 305 472 339 522  80  86 222 520  58 521 490 134 412 365 243
 367 414  76  79 170 272 530  53 420 127 281 162 136 489 484 423  33 208
 501  31 364 366 495 488  55 172 287 151 214 312  81 475 335 348 485 288
 218 223 404  84 147  68 411 193  29 487  44 286 137  27 118 248 224  23
 503 105 391 212 103  57 535 274 504 309 276 173 358 258 306 421 126  72
  88 209 169 486  42 534 528  89  60 388 321 344 424 416 395 199  56 415
 393  30 319 419 175 491 436 128 219  85  17 156 435 310 165 378  93 186
 363  61 448 428 144 343 536 211 418 443 121 337 291 362  64 187 434 356
 320 533 526 308  54 245 259 449  20 110  77 349 311 333 390  70 345 304
 307 407 318 389 381 347 408 382 340 334 122 146 341 444 114 406 247   5
 510 109 104 360 197 140  41 326  18  32 269 322  34 155 166 253 323 113
 516 275 112 401 139 111 332 351 430 397 115 442 387 217 524 445 338 143
 315 145   3 346 350 149 342 205   4 352 426 277 116  38 513 380 120 284
 447 511 361 164 125 379 515 357 525 368 405 119 117  97 138 244 188 372
   9 123 377 422 154 283 529 250 142 153 148  10 354 375 124 231 431  43
 496 220 374 450 531 160 159 359  13 500 433  96   1  39 237   7 168 102
  98  40 189 167 100 519 355  16 141 324 452 273 235 507 230  22 201 373
   6 101 509 200 532 215  19  46 279 292  47 353 499 313 213  14 278 410
 518  99   2  21 483 233 451 158 392 425 432 236 314 157  12 316  15 216
  11 455 232 427 161 457 238 234 429 437 190 163 456  48 297 459 409 325
   8 439 446 458 370 300 298 329 327 299 301 440 371 331 369 204 203 328
  25  95 202 438  26   0 267 330  94 494  36  37 268 492 493]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0632
INFO voc_eval.py: 171: [933 110 828 ... 353 354 801]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0630
INFO voc_eval.py: 171: [210  34 129 127 130 221  38 207  31 167 214  15  35 208 120 222 213  33
 128 223 173 219 164  36 209 134 132 139 122 138 116  71 218  96 216   3
  32 178 137  46  13 135 197 227 100  63 136 131   1 217 224  64 152 212
  65 113  19 140 193 133 215   7 171 228  14  43 176  87 206 114 225 187
 107  97  67  18  86 105 170  88 196   8  56  95 143 145  69  54   6  89
  72  12 195  20  48  70 200 183 179 188 229 172 180 220 166  52  17  40
 226 174  74  16 201  57 153 158   4 211 203  75   2 194  55  41 182  50
 106 175 192 199  39  51  76 124 118 184  49  44 149 125  10  80 202 177
 198 168  66  59 108 119 144 231 155  47 169 103 165 141  45 185 104 181
  99  98  78 148 236 112  68 142  61 101 233 232  11  60 117 237  42 109
  53 123  83 151  30  37 234   5 110  94 235 102  73  62 160  58 230 162
 239 156 189  82  77   9 186  79 111  85  22 205  81  84 191 121 238 157
 126 146   0 190 147 115 154 150  92  29 159  28  25  91  93  26 161  21
  90 163  24  27  23 204]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3902
INFO voc_eval.py: 171: [ 7080  7782     0 ... 16633 16635 16634]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.1789
INFO voc_eval.py: 171: [1735  243  225 ... 1830 1832 1829]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2111
INFO voc_eval.py: 171: [  74  490  212 ...  775 1001 1004]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1031
INFO voc_eval.py: 171: [ 64   8  61  62 214 213  72 305   9  71 340  55 156  65 121  88  59 298
 293 291 342 292 324 295 268 124  83  56 333 270 325 215 117 303  99  85
 100 308 251 302 267  22 200  69 334  60 299 294 297  53  11 205 202 253
  67 146  57 199 301 127 158 197 290  66 116 296  92 269 288 129 304 271
  68 119 300 126 339 307 306 289  63 272 128 143  58 348 173 108 323  89
 252 125 185  20  90 102 133 355 204  10 184 256 107 157 266 182 101 142
 122 134 250 258 210  93 139 240 103 255 219 123 207 132 346 120 118 141
 159 257 177 104 194 131 354  13 261 105 178 352 179  84  82  70 154  54
 229 181 254 195 138   3 130 218 152 192 193 106 191  17 176 148 221 190
 180 265 243 165 337 274 235 330 189 201  37 186  86  35 237 281 171 220
 287  30  29 175 356 183 280 262  18 222 347 145 341 198 336  73 168 223
  16 225  38   5  34 114 208 329 150  14 188  32 153 318 174 115   4 264
 187 246 196 319  98 344  95  19 167 286 155  87 151 149  15 263 209 206
  74 283 217 113  12 242 241 247 203  33   1  97 335 231 285 358 353  75
 282  81 147  91 144  40  27  25  48 216  28 284  96 234  26 350 349 230
   2 343 351  77 238  31  39  94 345 212 226 232 357  51  45 331 163 245
  41   6 244 338  47 140 233  79 328 239  52  46  43 224 273 236 327  49
 313  76  80 332 316  36  42  78 160  50   7 326 170 162 169 161 164 227
 172 248 317 166 137 260  23 314 211 310   0 322  24 311 276 249  21 275
  44 315 309 109 312 320 321 111 110 112 228 136 135 259 277 278 279]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1175
INFO voc_eval.py: 171: [464 472 470 468 285 469 465 478 291 653 467 463 344 484 648 474 654 466
 186  49  47  62 605 188 287 471  56 181   5  73  64  99  52 296 289 284
 491 286 615 509 699 492 256 473  89 691 694 110  72  45 316   9 339  43
  44 515  50 109  61 182 657 692 356 346  78 481 102 117  11  81 103 688
 477 488 148 503  42 502 606 480 261  54 541 476 340 301 609 640 646 355
 647 268   6 149 264  68 700 115 559 348 295  41 190 637 482  69 610 706
 687 167  46 690 652 698  75 602 189 310 504  59 126  53 209 617 119 283
 696 649 258 206 257   7 338 279 697 104   4 358 269 634 314 352 351 183
 660 123 263 545 308  84 547 552 689 705 213 475  55 639 228 583 150 621
 575  88 312 345  21 542 513  63 546 630 565  57 656 147 603 300 101 651
  14 114 132 611 695 562 127 549 628 554 618 543 501 479  76  71 632 141
  58  77 373  95 232 153 168 500 120 668 133 521 241 169 212 506 650 218
  65 519 343 313 297 125 166 242 704 200 567 493 152 151 607 544 432 159
 157 496 317 364 498 375 703 516 403 563  82 406 564 670 658 551 259 494
 311  25 121 361 584 217 349 566  48 199 128 604 553 601 347 302 100 550
 627 359 337 613 354 124 702 514 576 419  97 548 240 156  20 208 185 572
  28 164 674 425 260 415 172 290 659 194 239  83 122 130 353 195 171   3
 211 534 288 196 155 281  13 522 244 154 405 557 570 612 643 560 693 234
 131 315  80  33 701 262 645 556 205 511 204 323 517 641 134  22 512 106
 129 143 137 113 350  17 274 243 178  91 483 413 360   8 111 341 163 363
 235 487 160 236 306  94  66 180 585  98  23 275 408 662 201 710 664  12
  85 179 529 362 342 357 608  87 667 433 410 138 165 631 555 394  96 118
 558 116  16 322 135 207 495 318 577 377 107 187 267 303 587  74 624 592
 582 561 568 192 388 626 518 193  15  29 203 665 184 319 136 191  92 369
 226 139  51 402 221  27 486 266 526 489 593 579  24 594 457 270 202 588
 198 581 485 229 293 390 276  79 573 437 210 140 309 411 387 571 620 623
 227 393 686 672 661 414 328 331 144  10 458 142 586 197 505 451   2 412
  60  40  93 223 177 445 614 635  35 452 324 459 422  67 421 569 330 420
 622 105 636 395 450 642 449 619  38 305 510  90 252 536 399 108 409  37
 253 638 222 175 294  70 112 254 578 685  86   1 713 255 225 423 655 332
 320 580 508 219 176 671 396 629 282 574 298 237 462 427 525 711 679 230
 531 224 325 174 535 681 528 633 418 368 431  39 333 678 683 595 326 334
 158 616 321 644  31 272 435 280 370 307 673 335 336 666 173 460 329 523
 430 417 448 273 170 327 712 625 426 231 663 442  32 416 400 299 589 599
 398 304 220 146 271 216 444 277 278 507 374 161 376 238  30 539 162 669
 233 454 372 428 371 682 265 401  34  36 499 530  26 386 684 365 404 597
 145 250 389 447 367 600 446 407 292 591 596 397 443 453 520 527 455 680
 366 540 438 538  19  18 537 424   0 434 598 533 439 532 524 379 497 249
 490 590 429 456 382 436 440 215 214 441 248 392 246 391 247 707 383 461
 245 381 251 378 385 380 675 384 676 677 709 708]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0545
INFO voc_eval.py: 171: [518 519 480 452   0 630 280 504 629 478 614 190 338 239 566  21 510 482
 449 257 172 185 258 567  47 281  16 491 354 247 340 277 615 270 255 283
 252 268 263 638 146  12 448 246 348  57 133 350 315 262  11  53 256 171
 456 341 271 475 242 564 494  22 259 290 278 134 450 568 101   2 345  15
 229 351 631 387 233 454 250 312  10 286 616 106 274  49 572 275 512 156
 459  56 457 285   3 590 232 502 296  35 583 513 238 313 511  17 402 357
 574  60 588 269 311 613 184 399 216 339 517 461 175 137 554 455  55  83
 360  48   8 276 623  65 287 150 451 639 206 381  31 538   1 503 473 231
 141  94 301   7 368 240 515 292  19 527 215  70 267 379  89  85 374 355
 145   9 479 516 506 321 314 204 625 236  39 334 636 136 474 412 569 421
  50 107 418 176 470  32 535 272 148 243 323 619 241  36 288 207 498 397
 142 154 576 212 174 617 386 282 570 193 297 524 219 571  42 144 514  51
 291  61 108  45 244 428  18 217 526 279 300 303 582 573 131   6  58 356
  14 121  92 155  52  44 328 181  54 471 499 163 337 343 302  46 109 327
 367 139 593 349 306 544 487 110 585  25 111 635 543 377 496 553  13 632
  87 467  73 415 143  29 245  79 520 637 336  98 540  99 634 359 126 179
 363  20 472   5 180 462 371  84 214 227 273  96 411 429 226  67 466  59
 575 326 426 405 612 395 186 603  23 468 210 209 182 324 501 556 195  86
 201 112 228 549 633 413 113 342 433 594 390 344 423 464  80 237  64 140
 222 253 116 118   4 401 541 211 249 366 534  34 208  90 403 484 389 530
  66 393 330 251 562 545 542 529 230 325 194 335 523  69 147 546  38 416
  72  88 333 114 627 628 438 577 260 427 183 532 608 622 213  74  82 536
 105 476 177 525 218 463  77 396 391 293  62 248 460 173 322 205 299 364
 533 485 539 318 434 235  91 264 419 465 605  81  33 490 100  43 284 522
 104 372 346 102 167 191 332 380 376 398  40 353 220 352 441  27 203  37
 595 234 317 557 358 261 599 626 500 165 361  76 129 149 347  78 373 505
 606 489  93 436 493 365 103 158  28 492 225 316 385 192  75 161  71 446
 598  68 308 563 600 369 370 528 565 329 531 580 319  63  95 602 604 621
 486 157 394  97 477 320 295 601 409 537 188 224 187 388 584 392 624 607
 559 169 135 170 189 120 331 200 164 445 128 362 223 620 375 442 407 378
 488 383 152 495 437  24 178 221 138 591 265 117 618 127 305 384 483 310
 425 406 294 266 422 581  41 497 115 481 469 417 382 199 160 151 168  30
 254 298 589 289 410 125 414 153 420 440 596 202 458 586 130 123 122 430
 119 552 424 408 444 610 159 435 132 166 198 197 431 162 561 432 547  26
 587 453 404 124 443 447 307 555 560 592 400 551 439 304 550 548 558 309
 509 579 578 597 521 507 508 611 196 609]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1256
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1367
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.313
INFO cross_voc_dataset_evaluator.py: 134: 0.050
INFO cross_voc_dataset_evaluator.py: 134: 0.075
INFO cross_voc_dataset_evaluator.py: 134: 0.167
INFO cross_voc_dataset_evaluator.py: 134: 0.177
INFO cross_voc_dataset_evaluator.py: 134: 0.169
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.071
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.063
INFO cross_voc_dataset_evaluator.py: 134: 0.063
INFO cross_voc_dataset_evaluator.py: 134: 0.390
INFO cross_voc_dataset_evaluator.py: 134: 0.179
INFO cross_voc_dataset_evaluator.py: 134: 0.211
INFO cross_voc_dataset_evaluator.py: 134: 0.103
INFO cross_voc_dataset_evaluator.py: 134: 0.118
INFO cross_voc_dataset_evaluator.py: 134: 0.055
INFO cross_voc_dataset_evaluator.py: 134: 0.126
INFO cross_voc_dataset_evaluator.py: 135: 0.137
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 6999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.377s + 0.029s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.347s + 0.038s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.347s + 0.038s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.354s + 0.039s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.367s + 0.043s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.368s + 0.042s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.366s + 0.041s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.370s + 0.041s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.368s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.370s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.369s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.372s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.373s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.523s + 0.039s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.384s + 0.040s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.379s + 0.038s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.379s + 0.039s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.376s + 0.039s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.378s + 0.040s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.389s + 0.040s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.386s + 0.040s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.381s + 0.039s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.382s + 0.040s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.380s + 0.040s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.379s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.376s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.374s + 0.042s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.369s + 0.038s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.363s + 0.042s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.362s + 0.043s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.362s + 0.041s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.362s + 0.040s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.359s + 0.040s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.362s + 0.040s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.363s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.361s + 0.041s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.362s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.362s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.361s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.391s + 0.030s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.366s + 0.040s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.360s + 0.044s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.364s + 0.042s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.362s + 0.043s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.359s + 0.042s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.363s + 0.042s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.360s + 0.043s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.360s + 0.042s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.356s + 0.042s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.357s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.357s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.358s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 59.063s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [221 321  84 211 311 596 592 116 153 602 171 627  79 593 597 599  47 210
 310  58 130  86 614 166 268 195 186 315 215 633  51 179  91 598 163 159
 630 135 111  59 324 224 262 326 226 136 313 181 213  48 149 194 140 322
 222  72  31 641 145 282 640  71  32 316 216 137 664 660 594 277 225 325
 272 619 617 611 320 220 306 595 621 615 643 139 648  45 659 133 264 364
 367 167  54 290 288 237 408 238 445 239 307 276 115 132 230 287 286 604
 414 646 620 284 649 616 658 129 203 613 317 217 297 214 314 180 355 406
 212 724 312 300 296 501 304 622 360 601  33 399 279 603 680 193 400 173
 168 162 412 112  40 498 281 741 155 645 218 318 636 164 653  41  39 235
 642 647 274 487  85  36 638 495  35  42 165 302 198 637 661 117  89  34
 629 361 285 503 105 736 152 625  37 120  50  38 639  43 610 401 534 407
 189 151 471  80 191 295 269  83 192 522 182 308 490 366 368  61 415 459
 676 359 263 483 605 199 584 662 499 233 174 161 489 632 496 497 267 301
 582 669 551 441 108  44 654 170 197 183 409  81 328 413 228 234 291 493
 411 491 405 701 416 227 608 327 618 229 329 609 142 678 723 270 505 178
 319 219 185 656 283 275 508 665 472 356 365 631 726  53 223 323 241 734
 175 102 635 404 403 398 410 370 506 442  66 107 138 362 402 158 150  30
 176 190 119 172 273 114 292  69 623 677 156 449 231 169 250 492 106 103
  70 157 650 358 143 440  55 461 673 652 486 299 526 644 252 532 590 289
 484 540 482 104 475 294  46 509   1 651 600 485 732 634  90  78 232 160
 606 177 357 626 196 657 607 690 126 675 369 702 184 624 628 188 558 538
 474 574 124 309 612  76 668 687 667 432 573 577 384  82 671 454 460  74
 388 431 451 385 392 500 363 118 691 552 187 266 154 729  68 448 502 730
 382 655 510 523 113 575 670 280 541 240 533 715 249 488 733 527 672 504
 141 554 511 728 663 236 689 681 587 507 470 571 543 424 586 536 718 473
 674 494 372 464 443 378  87 559 745 386 337 579 209  75 735 353 110 204
  77 744 330 109 253 341  29 278  26 251 698 455 450 248 425 717  60 208
 588   0 469 578 348 720 731 389 246 298 379 420 343 376 581 457 146 589
 207 679 334 333 371 303  73 530 271 349 463 688 206 591 336 144 375 125
 254 458 418 205 377 695 383 721 725 387 576  57 390 265 737 430 373 131
 293 354 555 128 525 738 332  94 524 134 716 535 467 453 352 694 338 585
 739 305 468 572  56  27 697 583  88 727  64 397 462 740  49 580 340 719
 722 147 423 456 351  19 346  93 548 550 514 686  28 570 549 148 568 547
 466  62 435  92 539 537  95 476  11 245 553 692 480 542 569   4 446 347
 434  24 683 528 344 350 557  65 247 682 342 531 529 452 417 339 684 520
 478   8 512 521 693 335 447 121 244 345 699 444 422 331  10 481 123 374
 565  12 242 517 437 243 515 685 556 465  63  67 513 696 433 518  13  16
  18  17  21 700 439 712 436 438  22 393 421 477 429 127 419   5   6 479
 380  52 255 256  14 545 519 202 427  20 100 101  96 516   9 426 560   2
   7 428  23 666 122 711   3  25  15 259 391 544  98 381 704 566 200 713
 708 201 396 395 707 257 258 261 710  99 703 394 709  97 546 706 705 260
 714 567 561 563 742 562 564 743]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0322
INFO voc_eval.py: 171: [ 72 272  36 218  50 226  30 250 357  75 356  74 350 216 331  82  56 196
 273 224 353 332  84  29 271 278 222 252 232 117 354 223 230 352 115 323
  31 194 292 122 220 231 355 351 255 275 156  80 237  76  79 219 173  83
 227  81 328 335 339 276 343  78 199  73 325 195 243 229 235 221 160  77
 254  28 279 249 309 116  34 308 155 251 200 283 281 303 246   6 326 258
 286 280 318 274 162 327 217   2 238 133 152 233 225 236 241 129 341 234
   4 337 305   8 198  33 320 176 277 293 248  22 282 285 319 244  52 295
 256 172 300 124 201  41 301 245 131 340 290 118 242   0 187 253 287 315
 288 212 107 197 289   5 154 299  48 338 206  53 330   3 247 168  46 228
  25 257 240 302 102 132 121 334  21 136 260  38 202 262 239  32 311 210
 147 259   9 140 313  10 269 209 270 297 215   7 149 123 167 182 268 294
  35  12 324 336 213 105 158 145 177 263 312 150 296 153 144  23 342 151
 266 134 304 310  43 139 104  16 181 125  11 267 178  42 175 127 185 203
  95 307  27 265  15 344 291 261 298  96 264 128 126  19 208 108 190  71
 170 180 322 316 205  63 333 349  40  70 120 100 317 204  24  44  68  93
 119 142 211 161 321  62 137  14 207 191 164 186 306 103 171  61  94 189
 314  65  60 112 109 192  20 165  98  64 348 183 130  66  99  18 113 347
  59 166 141 138 101  97  69  67 169  54 114  17 346 284 163 184   1 179
  47  57  92 345 143  55 159 148 111  26 106  37 174 135  13 157 193 329
 110 146  45  89  39  58  86  51 214 188  91  88  87  49  90  85]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3136
INFO voc_eval.py: 171: [2050  630  634 ... 5880 5847 5871]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0569
INFO voc_eval.py: 171: [1627 1810 1205 ... 2297 2296 2901]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0694
INFO voc_eval.py: 171: [589 565 536 368 584 755 365 750 694 370 600 324 581 688 613  18 548 305
 301 586 767 641 316 539 722  19 540 591 363 618 596 304 288 325 374  29
 375 302 310 617 628 692 621 369 566  26 701 371 665 312 328 944 737 627
 572 352 597 705 604 364 309 292 616  20 629 317 327 703 714 393 541 764
 810 974 167 318 962 377 592 557 394 693  21 773 576 811 544 718 729 567
  31 376 319 289 920 677 663 730 732 577 568 575 933 959 321 736 697 899
  34 938 704 702 698 725  17 323 759 717 320 373 964 630 535 644 313 696
 707 640 326 690 940 295 766 311 553 919  36 817 569 724 957 610 165 706
 602 738 733 970 291 925 356 141 307 668 571 127 607 603 639 771 963  23
 615 322 699 578 727 623 809 927 669 772 700 138 647 595 579 716 367 709
 622 878 745 731  25 619 966 746 306 472 715 882 620 294 916  30 609 949
 606 748 643 769 366  35 751 461 973 555 743 740 372 671 176 355 969 754
 756 906 873 157 875 543 762 299 713  40 588 822 775 590 827 763 505 653
 163 886 240 300 923 480 308 831 975 881 752 814 749 711 651 648 549  27
 442 876 723 303 821 580 150 976 758 708 593 476 160 137 726 552 826 468
 601 175 679 930 888 498 789  33 777 871 444 538 287 608 133 761 776  22
 443 546 264 680 800 945  28 712 154 662 158 386 481 573 739 902 646 770
 674 734 695 869 148 502 825 598 360 728 554 961 815 768 508 710 153 818
 249 719 459 765 297 487 583 626 874 744 342 849 625 813 903 155 463 838
 445 246 537  37 753 670 506 611 965 872 868 689 782 673 832  24 691 585
 884 877 259 241 747 666 599 804 741 967 587 932 462 570 485 624 774 760
 757 720 848 799 242 605 574 863 880 742 900 721 547 550 842 290 735 296
 129 828 901 614 293 594 166 788 388 298 858 658 215 143 796 843 456 861
 488 664 889 948 612 582 551 499 140 652 870 819 501 783  32 676 142 507
 210 955 885 188 454 493 341 473 245 820 787 344 191 128 657 467 441 254
 795 847 655 844 136 656 491 960  41 470 170 494 190 251 252 387 911 169
 840 896 151 678 194 212 186 935 492 109 193 146 934 189 928 471 192 654
 667 181 833 173 112 661 260 542 518 545 797 879  76  16 496 336 830 265
 918  79 250 479 650  74 389 952 482   7 172 913 135 672 897  54 149 182
 455 823 852 798 248 478 359 512 335 314 412 490 334 836 216 898 436  15
 247 556 486 224 791 497 171 914  44 500 483 956 457 862 162 477 159 156
 484 214 520 835 440   3 413 931 402 475 971 950 953 262 495 792 255 489
 261 474 460 453 168 939 968 786 263 458 887 266 452 926 929 267  96 161
 812 856 349 523  77 883 528 942 104 785 347 185 144  47 211 244 511 632
 213 915 513 230 837 183 907 174 239 841 401 466 845 220 184 351 348 130
 947 675 380 469  75 253 256 954  48 860 187 435 642 904 560 354 272 522
 243 132 905   0 909 434 824 509 816 345 416 235 851 180 937 177 510 362
 943 410 829 850 353 339 846 276 922 113   2 530 946 131 633 361 333 839
 103 834 152 358  95 465 145 258 533 517 225 111 223 631 105 857 283 649
 659 519 179 660 422 803 793 418 859 221 385 941 114  73 357 516 178  46
 514 525 100 147 414 338 439 645 340 515   9 438  97  78 521  45 403 524
 526 384 125 404 979  59 337 398 400 134 936 921 222 110 236   1  11  93
 636 378 139   4 864 209 855 350 232   6 564 972 951 397 399 164  12 637
 924  14 417 978 908 392 977   5 910  89  13 421 912 917 562 279 534 346
 343 208 958 784 635   8  10 532 432 802 391 634  94  98 866 853 101  42
  90 464 781  99 503 561 102  91 865 529  80 449  92 405 854 531 271 563
 807 527 315 437 446 273 638 270 277  61  62  60 269 805  68 268 395  43
 274 381 285 778 379 116  70 794 790 396 419 278 411 231 867 257 447 229
 801 217 382 284 780 415  71  81  52 450 433 420 280  39 390  84 806 123
 451 894 448 205 808 504 106 558  38 383  65  53  50 218 426 121  66  72
  69 286 227 281  67 429  49 779 409 108  51 126 219  63  87 895  64 282
 430 233 559 408 228 893 330 117 681 407 406 275 238 226 107  85 119  88
 206 428 892 431  83 237 115 122 118  57  82 120  56 891 687  86 234 423
 329 425 124 195 424 207  55 682 427 890  58 685 686 196 332 203 684 200
 331 683 197 199 198 204 201 202]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1717
INFO voc_eval.py: 171: [118 305 114 325 109 128 329 149 310  34 122 120  76  74 319 108 110 130
 113 148 316 304 317 107  37  35 112  85 308 326 106 121 306 320  38  45
  90 309  41 115  46  30  80 145 327 153  32  52 123 322 124  73  77 111
 315   7   6  42  60  39 125  62 116   0 158 337 292  95 328 151  83 236
  57 127 237  98 260  89 150 311  81 203  48 342  72 303 247   1 248  47
 238 214 135 290 199  78 202 119 289  70  75  54 143 334 245 285 298  50
  86  33 255 283  88 139 159  53 136   8  31 140 257 155 133 335 138 291
 284  84 344 129 287 338 330 157 286 288  82  28 307 147 198  40 169  67
  87 215 162  79  61 251 345 213 146  58 313 250 152 200  64 252 321 144
 246 227  65 209  49 205 235 242  71 142 201 282 258 154 126 164 204 137
 170  69   3   5 243 156 212 297  29  99  66 240 195 101  15 228 293 117
 294 132  63 131 299 312  43  68 256 239  11  20 141 197 196  55  96 224
  36 254 134 332 318 225   9  44  59  24  51 218  19 249 296  56 177 324
 221   4 210 193  10 241 333 259 295 253 244 194 105 102  12 207 231 281
  21 206 300 230 314 301 161 273 219   2 100  94 175 340  92 104 180 103
 223 272 160 341 229 220 226 336 216  97 232 217 163  91 174 276 323 167
 181 269  25  16  26 222 211 234 339 173  17 331 208 233 166 343 280 178
 302  13  22  18  14 275  23 262 274  27 265 278 172 268 263 183 279 187
 185 277 266 271 261 171 168 179 165 264 176 270 267 182 186 188  93 190
 192 184 189 191 346]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.1756
INFO voc_eval.py: 171: [2208 1111 1571 ...  692  694 2507]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1674
INFO voc_eval.py: 171: [104 112 108 110 252 135  14 193 109  98 103 253 254  33  17 136 105  31
  25 251 113 257  73 161 111  29 190  20 162 225  79 132  30  78  97  72
 226 133 259  27 160 169 228  32 215  82  13 165 154 163 202  18  84 106
  35 145 216 255  95 258 168 157 151 137  42 250 207  21 102 256  74  71
 167  94 164 213 185 201  28  66 166 224  19 134 116 131  99  43  22 230
 218  34  81 153 203 223 114  55 231  76 227 184 186  69  85  16 159  63
  23  70  12  86 229  59 115 211  61  24 205  77  15 210 217 212  64 196
  62 188 127  68  26  67  65  48 126 239 129  45 170 219 181 209 130  60
 240 155  53 206 214  44  96  58 237 107 150 187   5 128 120  54 182 152
 208 117  57 248 249  10 197 100 125  46 189  47  51 236 176 156 180  89
 101 121 194 118   2  87  52 149   3  83  56   1 200   6 204 122  80 124
  49  88  75  41 158  91 198  11   8   7   0  92 233 174 173  93 191 183
 192 238 172   4 241   9 232  90 242 123 146 147 178 144 171 221 220 222
 195 199 177 235 175 246 245 247 234 244 243  36 140 143 141 179  50 138
 142 139  38 119  37 148  40  39 260]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0315
INFO voc_eval.py: 171: [2070  273  585 ... 2729 2728 3607]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1086
INFO voc_eval.py: 171: [376 108  82  96 409 112 322 412  98 505 375 106  85 259 377 368 392 402
 102 159  97 468 290 244 378 678 408 114 171 683  20 187 292 511 251 268
  49 165 390 293 382 485 662 849  21 284 367 458  87  50 469 189 323 192
  84 688  75 155 404 320 502 286 459 247 510 799 405 179 225 245 221 273
 406 279 278 765 465 262 833 324 424 224 172 466 686 184 223 109  35  37
 142 509 254 330 285 154 549  11 243 479  66   3 385 220 476 242 314  86
 433 535 252 471 274 680 185 208  56  48  93 754  62 370 387 831 846 506
 153 435 199 508 196 843 572 461 201  69 819 291  44 328 157 266 228 665
 206 249 500 295 746   2 483 545 181  81 835 158 258 307 681 861 503 373
 770 838 442  46 730 162  60  76  41 151  64 186 457 169 193 303 156 518
 160 176 301 585  47  78 380 558 163 267 331 111 308 105 687 669 667 407
 311 820 449 175 427 837 759 396 209 128 504 277 411 250 673  24 178 369
 582 760 161  65 333  77 211 533 501 812  59 484  16 272 745 728  29  52
 851 821 499 834  13 248 570 384  89  10 445 318 271 122 219 520 379 507
 143 297 127 191 439 399 562 529 257 287  55 539 525  70 429 230 148 264
 480 203 260  42  54  43 818 150 269  18 671 299 372 240 854 444 238 212
 764 255   8  31 800 808 596 386 862 850 300 675 576 400 750 288 761 776
   5 703  40 548 666 283 426 663 475 481  26 296 246  27 684 482 810 204
 447 757 393  36  61 428 263 584 664 194 522 261   1 726  63 594 847 848
 544 270 298 727 832  15 101 685 173  34 674 401 403 389 210 425 202  72
 865 394 573 807 767 564 170 670   4 174  58 256 214 462 734   6 217 856
 528  45 817 149 222 860  12 215 329 431 235  33 555 839 532 751  17 276
 125 814  92 340 126 410 515 317 137 803  73 289  79 167 121 472 816 864
 804 762 383 735 822 588 679 769 345  90 580 813 841 763 541  51 253 749
 842  30 302 772 815 748 768 811 809 527 231 374 342 806 738 140 460 423
 583 733 732 371 316  53 805 132 452 853 135 844 326 388 783 305 537 315
  67  28 774 205 141 381 282 557 309 855 130 752 464 168 704 339 398 845
 542  39 514 682 538  68 304 456 312 265 436 319  32 360 275 569 516 677
 771 474 857 531 552 138 579 747 294 133 341 477 587 190 571 836 592 321
 139 145  80 397 859 775 183 523 280 118 430 617 391 227 395 756 348 567
 597 438  74 467 281  38 473 325 136 668 152 450  57 177 310 470 332 226
   7  71 852 712 134 714 766 306   9 713  25 863 840 586 131 147 702 737
 615 741 858 146 200 334 823  95 327 197 777 739 463 144 780  19 216 758
 568  14 117 478 551 779  23 729 313 782 612 731  22 180 107 232 566 575
 530 129 593 672 578 432 354 349 827 437 755 454 627 434 517 598 218 124
 595 198 336 740 195 621 234 753 239 237 609 233 695 778 622 591 166 491
 628 589 633 784 453 560 781 120 335 338 697 830  88 455 123 600 213 241
 182 451 577 440 656 829 706 446 676 229 773 563 343 574 610 337 236 631
 363 448 554 188 689 350 443 825 119 346 351 207 356 441 657 660 599 828
 744 742 824 826 710 493 358 361 590 581 534 618 619 630 419 519 353 690
 659 362 661 365 352 344 347 658 359 113 512 561 364 611 708 625 692 694
 536 357 607 624 415 547 613 416 626 798 524 164 355 691 366 526 559 616
 614 543 632 550 553 736 492 608 629 620 546 705 556 623 790 420 797 641
 521 711 540 868  83 721 707  91 743 793 795  94 513 701 715 794 110 417
 791 636 785 104 640 650 788 100 421 418 867 635 789 717 792 693 709 422
 698 796 103 637 634 699 643 700 786 652 787 638 601 720 497 725 696 115
 639 116 642 718 490 489 648 723 647  99 494 644 722 606 602 498 716 719
 645 724 604 486 605 603 488 487 649 654 414 655 653 646 651 496 565 495
 801   0 802 413 866]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0715
INFO voc_eval.py: 171: [ 600 1375  193 ... 1522 1524 1527]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2248
INFO voc_eval.py: 171: [252 286 180 263  65 291 271  62 250 265 296 283 226 241  67 297 175 294
 178 258  73 255 211 177 130 262 230  80 399 208 465 499  71 401 400 500
 281 172 261 290  63 272 480  91 229 464 482 516 243 228  59  85 403 481
 227 385 192 395 466 253 295  49 132 507 184 181 527 303  66 478 470 135
 504 256 185 242 244  92  52 463 471 199  51  88  84 404 402 131 519 107
  24 484 468 108 207 475 257 414 133 249 479 386 266 384 264  90  68 483
 335 508 473  28 443 397 469 186 476 304 196  75 152  76 510 183 150 462
 398  35 317 514 418 467 193 222 267 455 472 197 456 179 387 523  50 129
 474 377 306 338  82 522 223  74  45 520  58 251 521 492 364 413 366 134
  77  81 415 171  53 273 282 127 530 421 162 486 136 424 491  33 209 363
 503 365 490  31 497  55  70 151 173 215 288 313  83 334 347 487 477 219
 289 224 405  86 195 147 194 412 489 137 287  29 118 392 225  27 505 248
 106  23 535  57 104  44 213 506 275 310 259 174 277 307 126 357 210 422
 442  72 488 170  89  42 534 389 321 528 343  60 425 396 200 417 319 394
  56 493 437 420 416 176  30 128  87 220 436 311  17 187 156  94 379 362
 182 536  61 342 144 450 429 121 292 419 336 533 212 445 435 188 361  64
 260 320 355  78 526 309  19  54 246 348 110 332 451 391  79  93 312 344
  69 408 390 318 305 308 409 346 382 339 146 122 383 333 340 407 446 512
 105 359   5 198 247 109 140  41  18  32  34 322 270 155 254 323 331 113
 114 431 139 111 388 350 167 115 518 276 218 444 337 143 447 524 345 149
 145 349   3 316 341 206 351 116   4 427 163  38 278 515 285 120 360 381
 513 449 525 380 119 125 517 367 138 117 406 356 189 112  98 245 373 123
   9 284 154 378 423 529 142 153 353 148  10 232  43 376 432 221 124 498
 159 375 452 531 502 358 434  13  39   1 239  97 161  40   7 103 169  99
 354 165 190 509 141 101  16 324 168 454 274 231 237  21 202 102 511   6
 374 532 216 201 352 280 501 293 214 160 314 411  47  14 279 372 100 485
   2 234  46 393  20 158 433 426 453 238 157 315  12  15 217  11 166 233
 457 458 428 430 240  22 236 438 164 235 191 461 459 410 298 325 440   8
  48 448 460 301 370 299 328 326 300 302 371 330 441 368 205 369 204 327
  25  96  26 439   0 203 268 329  95 496  36  37 269 494 495]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0517
INFO voc_eval.py: 171: [935 107 832 ... 351 350 805]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0629
INFO voc_eval.py: 171: [211  36 130 128 131 222  40 208  33 168 215  16 209  37 121 214  35 223
 129 174 224 220 165  38 210 135 133 140 139 123  72 117 219  97 217   3
  34 179 138  14 136 228 198 101  64  58 137 132   1 218 225  65 153 213
  66 141 114  20 194 216 134   7 229 172  15  46  88 177 207 115 226 188
 108  98  68  19  87 106  89 171 197   8  55  96 144 146  54  70   6  73
  90  21 196  12  48  71 201 230 184 180 189 173 181 221  52 167  18  43
 227 175  75  17 202 154  57 159   4 212 204 195  76   2  56  44  50 183
 107 193 176 200  77 119  42 125  51  49 185  47 150 126 178  81  10 203
 199  67 169  60 120 109  41 232 145 156 104 170 142 166 186 105 182  99
 100  79 149 237 113 102  63  69 234 143 233 118  61  11 236  45 238  53
  62  84 110 124 152  31 111 235  39   5 103  95  74  59 161 231 163 240
 157  83 190  78   9 112  13  80 187  23  86 206  82 192  85 122 239 158
 127   0 191 147 148 116 155 151  93 160  25  30  27  92  32  94 162  22
 164  26  91  28  24  29 205]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3902
INFO voc_eval.py: 171: [ 7825  7122 11221 ... 16717 16716 16715]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.2224
INFO voc_eval.py: 171: [ 239 1720  224 ... 1818 1817 1816]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2338
INFO voc_eval.py: 171: [  75  490  211 ...  773  766 1000]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1020
INFO voc_eval.py: 171: [ 65   8  62  63 214 213  73 301   9  56  72 333 156  66  60 121  90 291
 286 287 335 285 318  57 125 289 264  86 326 266 319 298 117 294  87 102
 303  22 263 297 103 249 201  70  61 288 292  53 327  11 293  54  68 203
 205  58 251 146 295 128 200 158  67 198 284 116 290 282  93 130 265  69
 127 296 267 119 302 300 332  64 283  59 268 129 299 341 110 173 144  91
 126  20 186 317 250 134 349 104  10 184 204 262 253 182 109  84 157 105
 248 142 256 135  96 122 123 210 139 218 118 106 120 207 239 133 339 254
 177 255 141 194 258  95  13 132 348 178 346  71 108 107  85  55 159 179
 252 228 195 154 181 138 131   3 217 124 193 192 152 190  17 189 176 220
 270 180 188 148 330 323 165 242 234 261 202 276  29  36  34 219  88 236
 281  28 175 171  18 350 259 221 183  76 340 329 199 334  16 222 145 191
 224 115  37 168  14 322  33   5 187 174 153 312 150  31 208 185 260   4
 196 245 197  19  98 313  15  94 338 206 101 280  89 155 151  77 167  12
 149 209 278 216 241 240 114   1  32 100 328 347 279 352  92  78 230 147
  83 277 143  26  27  39 343  30  75  25 215 344  99  74 337 229  80 342
  97 345   2  38  50 233 237  45 351 225 336  47 231 212 324 162 244   6
  40 331 140  81 243 232  51  46 238 321 269 223  79  42 235  48 320 308
  82  35 325  41 160   7  49 170 163 169  52 226 161 164 246 172 311 166
 137 257  23 305   0 309 211 316  24 272 307 271 247  21 113 304 310 306
  44 314 315 111 112  43 227 136 273 274 275]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1164
INFO voc_eval.py: 171: [467 475 473 471 285 481 468 472 291 470 656 466 345 487 650 657 478 469
  50 186  48  63 608 188 287  57 474 181   5  74  65  53  98 295 289 284
 493 286 618 513 701 256 494 476  90 695 112  73 691  46 316   9  44 340
  45  51 516  62 182 660 692 347 357  79 484 103 114 696 119  82  11 489
 104 480 507 505 149  43 483 609 261  55 542 644 479 341 300 648 612 649
 356 268   6  69 264 699 150 703 117 560 294 349  42 640 190 485  70 613
 167 689  47 655 690  76 698 606 310 189  60  54 702 209 620 120 706 283
 651 206 258   7 107 257 279 110 693 339   4 314 635 269 352 354 183 101
 263 546 663  85 308 548 212 553 477 708  56 521 643 227 585 576  21 624
 312 151  89 346 543 515  64  58 547 633 566 299  14 659 605 116 148 654
 135 563 614 550 504 137 631 555 544 482 621  77  99  59  72 140  78 373
 105  95 231 154 671 141 503 168 127 302 240 509 653 217 344  66 519 313
 169 296 124 495 241 166 707 568 200 545 506 153 610 500 152 317 434 158
 160 136 364 697 375 517 502 705  83 404 672 564 498  27 407 565  49 661
 552 311 259 365 216 567 361 350 586 199 125 607 554 301 604 348 629 551
 616 359 338 496 123 355 577 704  20 421 122  97 549 652 239 208  30 157
 676 573 164 185  84 427 194 290 238 662 172 260 417 121 171 288 196 353
  13 211   3 195 535 281 156 700 523 244 558 709 571 406 615 155 561 100
 694 315 233  34  81 262 646 514  22 557 204 205 323 129 518 645 142 524
 630 108 126 115  17 351  92 274 242 358 360 486 178   8 138 415 113 363
 342 234  25 243 306  67 161 235 163 180 587  94 665 275 409 128 362 713
 530 667 201  86  12  88 179 670 343 102 611 133 412 435  96 165 499  16
 395 556 634 559 131 207 130 322 318 303 578 377  75 109 187 595 589 562
 584 388 118 569 267 192 520 627  31 668 203  15 193 628 132 319  93 225
 184 191  52  29 369 403 220 490 491 596 580 202  26 266 488 597 528 641
 198 583 460 441 590 139 293 228 276 270 574 309 391 413  80 210 134 394
 387 572 626 226 623 674  10 664 416 688 144 461 328 331 106 588 143 414
 197 447   2 453 177  61 222 324  68 617  41 462  36 636 330 625 423 424
 422 454 570 452 638 396 451  91 305 111 622 252 400  39 537 411 175  71
 579 637 642 221   1 254  38 686 253  87 658 716 511 332 673 255 497 224
 320 582 425 397 218 639 632 575 297 176 236 282 714 429 465 229 581 223
 681 527 532 325 682 368 174  40 684 536 525 680 598 420  24 326 333 433
 437 159 619 272 647 321 370 307 675 280 173 336 463 334 669 329 526 432
 327 419 450 335 715 444 230 666 273 170 410 428  33 418 401 298 602 591
 304 399 219 147 446 378 271 510 278 685 215  32 540 374 237 376 162 512
 277 456 232 372 371 683 402 265  37 430  28  35 366 386 508 531 405 146
 389 600 250 367 449 448 292 603 408  23 398 594 599 687 445 522 455 457
 529 390 439  18  19 541 539   0 538 436 145 426 601 440 534 533 380 501
 492 249 592 431 438 459 383 442 214 213 443 248 393 392 593 247 246 710
 384 464 458 245 337 382 251 379 385 381 677 678 679 712 711]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0540
INFO voc_eval.py: 171: [522 523 483 454 283   0 634 481 508 618 193 340 241 514 633 568 451 486
  24 260 569  49 188 261 175 284  19   5 249 280 495 356 342 619 281 258
 286 254 266 270  14 450 248 642 149  59 271 265 349 352 317  12  55 174
 259 136 458 478 244 498 343  25 262 293 452 103 137 570 231 351  18 256
 353 635 272 235 390 252 456 315 620  11 289 273 276  51 108 277   3 574
 516 459 159  58 461 288 592   2 234 506 299  38 240 585 517 316 515  20
 405 576  62 591   8 617 313 278 187 521 218 402 462 341  13 178 140 457
 557 362  57  86 279 643  50 358 627  67 290 453 152  34 208 384 476 509
 541 233   1  96 144 304 242   9   7 519 294  22 371 217  91 531  73  87
 382 326 377 482 148 520  10 510 238 629 206  42 338 477 640 571 139 415
 110  52 424 421  35 473 179 245 359 539 243 623 151 324 291  39 502 209
 400 145 157 177 621 214 578 572 285 389 300 528 221 573 196  45 518  53
 147 109 246  63 111  47  21 320 430 307 219 530 583 303 282 575  60 134
  54  16 357  94 158  56 124 184 274 329 166 503 474 306 112  48 547 345
 328 491 309 595 370 350 142  29 114 587 639 546 500  89  15 380 556 247
 417 636 470 469  32  75  81 524 335 146 339 100 641 544 475 638  23 361
 182 101  41   6 464 129 183 275  85 229 216 414  98 374 228 431  69  61
 468 327 616 577 398 429 189 605 408 537 471  26 173 211 505 213 186 325
  88 549 559 115 230 204 198 116 637 552 393 416 596 344 436 346 239 466
 255  66 224  82 143 119 251   4 121  17 404 212  92 406 253 331  37 369
 210 548 232 534 565  68 487 545 392 113 396  71 533 527 150 197  90  74
 418 321 632 631 332 337 117 263 365 579 440 611 626 185 428 536  76 215
  84 107 479 540 180 250 220 465 529 394  64 463 207  80 295 323 176 434
 237 302 399 489 538 267 608 366 422 437 318 467  93 542  36 494  83 106
 102 104 287 364 526 169 347 375 305  46 379 401 194 264 222 205 236 630
 334  43 443  30 601 504 383  40 354 355 597 560 360 609  78 348 132 153
 507 497  79 336 493 367 105 161 376 600 227 164 496  95  77  31 388  72
 195 602 448 566 311  70 532  97 373 567  65 607 330 604 535 372 420 582
 490 625 160  99 480 322 603 397 314 297 543 191 412 190 226 628 391 586
 610 171 395 606 562 138 192 167 333 319 123 225 203 447 624 131 363 368
 410 268 378 444 492 381 155 181 439 386 499 622  27 141 308 223 120 593
 484 409 130 296 427 269 387 425 501  44 584 118 485 419 472 163 385 170
 301 257 202 172  33 154 413 292 128 432 589 156 488 598 423 442 588 460
 122 133 126 125 411 555 426 162 446 614 135 168 438 165 433 200 201 564
 435 550  28 407 455 590 298 127 310 449 445 558 594 563 403 441 554 561
 551 553 312 513 581 580 599 511 525 512 615 199 612 613]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1198
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1388
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.314
INFO cross_voc_dataset_evaluator.py: 134: 0.057
INFO cross_voc_dataset_evaluator.py: 134: 0.069
INFO cross_voc_dataset_evaluator.py: 134: 0.172
INFO cross_voc_dataset_evaluator.py: 134: 0.176
INFO cross_voc_dataset_evaluator.py: 134: 0.167
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.109
INFO cross_voc_dataset_evaluator.py: 134: 0.072
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.063
INFO cross_voc_dataset_evaluator.py: 134: 0.390
INFO cross_voc_dataset_evaluator.py: 134: 0.222
INFO cross_voc_dataset_evaluator.py: 134: 0.234
INFO cross_voc_dataset_evaluator.py: 134: 0.102
INFO cross_voc_dataset_evaluator.py: 134: 0.116
INFO cross_voc_dataset_evaluator.py: 134: 0.054
INFO cross_voc_dataset_evaluator.py: 134: 0.120
INFO cross_voc_dataset_evaluator.py: 135: 0.139
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 7499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.480s + 0.040s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.343s + 0.040s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.351s + 0.043s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.357s + 0.043s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.361s + 0.042s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.360s + 0.042s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.360s + 0.042s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.363s + 0.043s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.365s + 0.043s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.364s + 0.044s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.362s + 0.044s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.363s + 0.043s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.364s + 0.043s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.339s + 0.039s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.359s + 0.033s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.354s + 0.037s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.356s + 0.038s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.358s + 0.040s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.365s + 0.041s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.374s + 0.041s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.368s + 0.040s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.366s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.365s + 0.039s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.362s + 0.039s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.365s + 0.039s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.363s + 0.039s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.457s + 0.029s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.367s + 0.037s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.363s + 0.047s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.365s + 0.046s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.362s + 0.047s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.358s + 0.045s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.356s + 0.044s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.356s + 0.044s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.359s + 0.044s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.362s + 0.044s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.365s + 0.044s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.364s + 0.044s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.365s + 0.043s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.466s + 0.041s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.361s + 0.035s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.360s + 0.038s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.357s + 0.037s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.357s + 0.038s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.351s + 0.039s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.354s + 0.041s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.356s + 0.041s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.357s + 0.041s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.356s + 0.041s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.355s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.356s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.357s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.921s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [223 325  84 213 315 599 120 595 157 605 174 630  79  48 596 600 601  49
 314 212  57 134  88 617 169 197 274 319 217 189 636 183  94 602 166 162
 631 139  58 226 328 115 227 329 267 184 140 215 317 153 199 144  72 326
  31 224 645 149 644 290  73  32 141 320 218 667 664 597 330 285 228 622
 281 614 620 647 598 624 618 222 324 309 143 652 137 663  46 369 372 170
 269 298  55 294 239 412 449 241 242 272 136 232 119 311 650 653 607 283
 623 418 619 292 616 288 661 133 206 321 219 304 216 318 182 243 359 410
 724 214 316 240 305 505 278 365 625 308 276 604  33 403 286 606 681 196
 404 177 165 171 416  40 116 741 502 158 649 639 657 221 323 167  42  39
 237 651 646  87 642 491 201 499 168  43  36 307 665 640  91 121  35 632
  34 366 735 291 109 508  50 628 156 643 312 613 123 474  80 287 405  44
  37  51 194 192 538  38 411 303 155 195 275 371  85 373 527 463 185 494
 313 419 364 677  61 608 487 202 666 587 635 268 503 235 176 164 493 500
 501 586 670 445 556 296 660 273 413  81 186  45 112 332 200 230 236 295
 417 173 414 409 702 498 495 621 420 611 333 231 331 146 229 612 723 679
 510 181 220 188 322 289 659 668 370 475 634 513 360  68 282 726 178 408
 407  66 106 415 733  54 375 638 225 327 402 511 446 245 406 111 367 142
 161 299 179 154 124  30 279  71 175 193 453 118 626 297 678 233 159 254
 110 147 172  70 496 160 107 654 363 465  78 674 444 531 656 648 490 256
 536 544 488 108 293 593 478 560 486 637  47 603 514  93 655 730 163 489
 234   1 362 180 609 198 374 302 629 610  41 676 662 703 691 130 187 627
 633 306 191 542 477 577 128 615  77 688 388 436 458  75  53 190 580 576
 672 464 368  82 692 455 122 396 435 504 389 557 361 392 728 271 386 452
 658 729 515 507 528  69 545 671 117 578 537 715 532 145 244 732 509 673
 492 253 516 559 301 238  59 641 690 727 473 590 682 540 512 574 428 547
 589 675 476 718 447 497 468 382 340 506 745 211  89 734 357 390  76 582
 207 744 334 114 699 113 257 343 454 459 252 255 284  60 210 429 717  29
  26 472 591 352 581   0 720 345 731 393 383 250 424 338 337 150 380 584
 461 592 680 259 353 376  74 467 310 341 689 209 535 148 594 379  83 277
 208 258 462 696 721 561 381 129 387 422 394 725 736 391 579  56 280 135
 358 270 377 434 530 336  97 132 695 716 539 470 737 738 356 457 138 300
 471 529 588 698  90 585 575 401  27 466 739 342 583  64 479 719 427 722
 355 151 460 348  96  19 687 553 740 555 519 469 152 552 573 554  28 541
 349 571 543  95  62 439  98 693  11 558 481 249 351 546   4 484 450 572
 684  24 533  92 438 344 400 480 346 354 534  86 421 251 526 563 683  65
 456 685 483 525 451 125 517 339   8 350 551 694 347 700 426 248 127 378
 448  10 335 485 522 568 520 441  12 562 246 697 247  63  20 686  13  67
 523  52  17 518 437 443  16 701  18 440  22 713 482 442 425 433   5 423
 397 131   6 384 549 260  14 431 524 261 205  21 103   9   2 521  99 430
   7  23 432 126  15 669   3 712  25 264 395 102 705 548 101 385 105 709
 714 569 399 262 203 708 263 204 398 711 704 104 266 710 550 100 707 706
 265 570 564 566 742 565 567 743]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0321
INFO voc_eval.py: 171: [272  72  37 217  50 225  29 249  75 356 355  74 351 215 332  82  56 195
 273 222 353  84 333  28 271 278 221 251 231 354 117 229 223 352 115 324
  30 193 292 121 219 230 275 254 158  80 236  76  79 175 218  83 226  81
 329 336 339 276 345  78 198 323  73 326 242 194 234 228 220  31 162 253
  77  27 279 248 308 307 116 250 157 199 283 281 245   5 327 257 318 286
 280 164 274 328 216   3 237 232 154 133 240 235 224   4 233 342 128 344
 338 304  11 320  33 197   8 177 277 293 247  21 285 319 282 243  51 255
 294 259 172 300 122 201  40 301 244 290 130 341 252 118 315 241 287   0
 211 107 196 288  25 289 299 156  48 206 340  52 331 246 256 200  24 178
 239 302 227 207 102 132 309 258  20 120 335  38 261 238 180 167 311 149
 209 131 313   7 269   9 141 270 202  32 297 295 268   6 214 134 169 151
 183  35 105 337 325 212 147 160 312 262 179 152 296  22  34 174 155 266
 343 153 146 303 135 310 104  15  42 182 140  41 187  10 267 124 123 176
 186 203 126 306  95 265 298  14 291 346 260  96 264 127  18 125 263 108
 208  71 190 322 317 181 350  39 205  63 334  70 100 316 204  43  23 119
  68  93 321 210 143 163  62 137  13 191 166 103 305  61 173 138  94 189
  60  65 314 109 112 192  19  98 168 349  64  99 184  66 129  17 348 113
  59 142 170 144 101 139  55  97  12  53  69  67 114  16 185 171   1 284
  45  92  46 165  57 347  44 145  54 150 111  26 161   2 106  36 136 159
 330 110 148  58  90  47  86 213 188  91  87  88  89  49  85]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3147
INFO voc_eval.py: 171: [2041 1022 2164 ... 5812 5832 5814]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0606
INFO voc_eval.py: 171: [1679 1801 1620 ... 2288 2289 2887]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0624
INFO voc_eval.py: 171: [585 590 538 567 367 364 756 369 695 753 583 550 323 689 304 593  18 542
 300 614 602 643 362 315 541 726 587 624  19 769 303  29 618 597 373 324
 287 370 368 629 568 622 301 309 375 693 702  26 667 948 312 741 574 351
 327 599 617 628 706 290 326 606 308 363 393 766 316 166  20 704 631 376
 558 966 317 701 812 981 543 775  21 694 392 594 578  31 546 374 813 721
 731 569 318 288 732 665 924 570 679 734 579 903 740 577 937 319  35 697
 942 703  17 760 708 705 699 967 632 320 322 372 311 735 720 709 691 537
 645 945 642 555 767 294 310 819 923  36 571 962 725 612 164 716 325 742
 976 604 736 974 291 670 355 929 630 306 573 140 126 605 773 609  23 641
 968 698 623 671 321 931 729 616 580 811 774 700 719 581 649 598 137 707
 621 711 747 366 733 882  25 749 469 718 970 620 619 611 305  30 886 920
 293 751 953 608 771 646 365  34 715 458 978 371 746 557 354 673 757 175
 973 755 156 909 877 879 444 298  40 545 717 764 589 829 592 777 162 824
 762 505 655 239 299 927 479 890 307 979 833 754 816 752 713 885  27 650
 551 653 727 880 302 823 582 710 980 759 595 149 474 159 136 728 554 828
 603 465 442 892 681 174 497 934 791 779 875  33 610 540 286 763 778 132
  22 443 548 263 802 949 714  28 153 664 480 157 385 648 575 743 772 906
 675 696 739 147 873 500 827 730 556 359 768 965 600 770 508 722 817 712
 152 248 820 765 456 486 296 584 878 627 737 341 851 748 907 292 815 154
 839 626 460 245 539  37 506 872 672 969 613 690 876 677 784 692 834  24
 586 881 750 888 258 240 668 744 806 971 601 588 936 776 761 572 459 625
 484 723 801 758 607 850 745 241 867 576 724 884 738 904 845 549 552 289
 295 128 830 905 615 165 596 387 790 297 862 660 213 797 143 844 591 865
 487 454 666 893 952 139 553 498 501 654 874 821  32 785 678 507 141 960
 208 889 187 452 492 340 471 822 244 789 342 190 659 262 127 441 464 657
 798 849 253 846 658  41 490 964 135 169 467 493 250 189 842 251 915 386
 168 900 150 939 680 491 193 185 210 938 145 192 932 188 468 656 669 191
 180 835 172 110 663 259 544 521 547 799  75 495  79  16 264 559 883 335
 832 922  73 652 388 249 478 481 956 171   7 917 674 134 148  54 825 181
 855 901 453 800 247 477 358 512 413 489 334 313 838 214 333 246 437  15
 902 793 485 496 170 222 918 499  44 482 961 161 455 866 483 155 475 158
 837 212 523 414   3 935 402 957 473 954 975 261 494 794 260 254 488 472
 457 451 167 943 972 788 266 265 930 891 933 450 160  95  76 814 860 347
 526 887 532 946 103 184 787 144 345  47 243 209 511 919 211 513 634 229
 840 238 911 182 173 843 463 401 847 346 183 350 676 951 129  74 380 218
 466 255 958 252 864 186  48 436 644 908 353 271 242 525 910 131   0 435
 913 826 509 818 417 343 234 179 853 947 941 361 510 562 476 852 176 470
 831 352 411 848 338 111 275 926 534   2 360 950 130 635 332 836 841 102
 151 357 462  94 142 520 257 108 861 104 633 223 651 221 281 178 805 420
 661 522 662 424 795 112 384 944 863 514 356 565  99 177 519  46  72 528
 219 146 415 647 339  77 440 337  96 439 518   9  45 524 403 527 124 383
 405 529 984 336 925 400  92 398 516 133 940 109 515 235   1  11  59 378
 220 638 502 868 138   4 349 207 859 231   6 977 955 566 397 399 404  12
 163  14 419 928 982 983  88 912   5 639 423 914  13 391 921 916 564 963
 786 344 531 348 206  10   8 433 536 804 637  93 636 856 390  97 870  42
 280 100  89 461 517  98 783 503  90 869 563 101  78 533 406  91 857 535
 809 270 959 446 445 530 314 272 438 269 276 640 854 858  62 268  61 807
  60 395 267  43  67 381 273 379 284 114 780 796  69 412 396 792 422 230
 871  52 277 228 215 803 256 382 283 782 416  80  70 448 421 434 394  39
 278 389  83 808 447 123 449 810 898 203 105 504  38 560 428  53  64 216
  50 120  65  68  71 285 430 226 279 781 410  66  49  51 107  63  86 217
 899 125 561 431 232 897 227 418 329 409 408 407 274 682 282 237 106 115
 224 225  84 204 118  87 432 429 896 236  82 113  57 121 116  81  56 119
 233  85 895 688 328 122 425 427 117 194 426 205 683 331 686 195 687  55
  58 202 199 894 377 685 330 684 198 196 201 200 197]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1697
INFO voc_eval.py: 171: [304 115 327 111 106 126 146 312 119  33 117  74  72 320 105 107 128 110
 145 318 306 319  36 104  34 109  83 147 310 328 118 103 322 307  37  44
  88 311  40 112  30  45  78 153 329  50 120 121 324  71  75 108   7 317
   6  41  59 308  38   0 113 122 156  61  92 330 292 151  81 236 237 125
 260  56  95 199 150 313  87 201  79  46 342 305  70 247 148   1 248 238
 211 289 132  76  68 200 288 116  52 141  73 335 245 285 297  48  32  84
 254 283 134  86  31  51 137 138   8 131 257 154 336 136 290 344 127 284
 338  82 331 155 286 287 157  80  28  39 144 309 196  66 167 160 212  77
  85 250  60 249 143 198 345 210  57 152 315 142 251  63 246  64 323 225
 206  47 258 203 242 234  69 282 124 135 162  67 202   5   3 168 235 243
 149 209  65 291 197 296  96  29 240 226  98 193  15 114 130 293  62 129
 139 298  42 314 256  53  11  20 194 195  35  93 253 222  55 140 333 223
   9 133 321  58  43  49  24 216 295   4  19  10  54 176 326 123 219 207
 259 191 241 334 252 244 294 102 192  99  12 205 239 255  21 229 233 204
 299 281 218 228 316 301 159 273   2 217 173 340 101  97  90  91 179 100
 272 158 221 341 227 224 213 337 230  94 214 161  89 276 172 325 180 165
 269 215 220  16  25 339  26 171 232 208  17 332 231 280 343 164 177  13
 303 302  18 275  22  14  23 300 274 262 265  27 278 170 268 279 277 263
 184 182 186 271 266 169 178 261 166 163 264 175 174 270 267 181 185 187
 189 190 183 188 346]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2688
INFO voc_eval.py: 171: [2207 1113 1572 ...  695  697 2504]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1657
INFO voc_eval.py: 171: [108 115 112 111 253 138  13 195 113 100 107 254 255  34  16 109 139  24
 252  31 257  75 163  29 114 192  19 164 226  30  81 135 110  80  99  74
 136 228 260 162 171 230  36  32 217  33  12 167 156  17 165 204  86 218
 256  97 259 105 170 140 153 159 251  43  20 106 258 209  76 169  73  96
 166 187 215 203  68  27 168 225  28  18  83 137 118 134 101  21  44 232
 220  35  84 155 205 227 116  56 233  78 229 186 188  71  87  15 161  72
  22  65  11  88  63  23 231  61 117 213 207  79  14 214 212  66 219 198
  25 190  64  70  69 129  26  67  49 128 241 131  46 183 172 221 211 132
  62 157 242 208  45  54 216  98  60 239 189 152   4 130 122 133 184 210
  55 154 119  59 250 199   9 103 191 127  52  47  48 178 158 238 182 104
  91 123 196 120  89   0 151  85  53  58   5   2 202 206 124 126  82  50
  90  77  42  57 160  93 200  10   7   6  94   1 235 176 175  95 193 185
 194 174 102 240   8   3 243 125 234 244 148 149 180 147 173  92 223 222
 224 197 201 179 177 237 248 247 249 246 236  37 245 146 143 144 181  51
 141 145 142  39 150 121  38  40  41 261]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0315
INFO voc_eval.py: 171: [2077 2070  590 ... 2725 2726 3604]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1146
INFO voc_eval.py: 171: [379 108  82  96 413 113 322 415  98 504 378 106  85 259 380 395 405 371
 102 160  97 470 293 245 677 381 412 115 172 681  20 188 295 510 252 270
  49 166 296 393 385 850 485  21 661 286  87 460 370  50 471 190 324 193
  84  75 156 686 407 289 321 800 501 248 509 461 408 180 226 222 246 766
 275 281 280 409 263 834 467 426 326 225 173 468 185 684 224  37  34 109
 143 687 547 508 332 255 287 304  11 479 155 244   3  66 388 221 243 477
  86 315 534 435 254 472 276 186  56 211  93 755  48  62 832 847 390 373
 437 200 154 505 197 507 844 571 203  69 463 294 820 330 158  44 268 229
 250 206 747 666 499 543 182 836   2 483  81 258 863 159 771 308 376 665
 680 502 839 443  46  76 163 459  60  64 303 152 730 157 194 302  41 170
  78 288 187 584 517 557 111  47 177 161 269 383 164 334 679 105 309 312
 410 685 451 821 668 760 176 838 429 209 399 128 279 581 761  24 162 251
 503 179 372  65 335  77 212  16  59 746 274 500 484 854  52 813 728  28
 569  13 822 835 387 249 447  89  10 518 498 319 273 521 144 382 220 122
 560 298 506 402 441 191 127 257 529 290 538  70  55 431 526 265 232 260
  42 204 147 300  54  43 150 480 242 271  18 856 446 819 765 672 213 375
 240 256 595   8  30 663 852 864 762 403 801 389 750 777 575 291 809  40
   5 701 546 297 285 481 667 428  26 757 205 247 476  27 682 396 482  61
 583 430 449 811  71 264  36 195 593 523 662 261 848   1  63 542 849 299
 833 726 727 272  15 683 101 174  35 404 392 673 406 210 427 768 867 202
  72 572 562 397  58 171   4 218 671 175 858 214 411   6 394  12 674 735
 528  45 464 808 223 862 331 216 554 818 237 149 751 278 840  17  92 532
 433  32 137 514 267 414 126 804  73 318 125 342 815 292  33 763 168 121
  79 866 587 770 473 817 386 579 805 678 734 842 823 769 843 347 540 814
 253  90 773 764 749 739  51  29 233 301 812 527 582 816 141 344 810 377
 317 454 807 425 462 733 806  53 855 374 784 845 131 135 732 329 391 775
 536 306 857  67 384 316 142 284 752 556 207 846 130 703 310 401 169 466
 341 541  39 537 513  68 458 266 363 772 313 438  31 568 305 277 551 859
 320 515 139 475 531 676 570 591 748 478 578 586 776 132 861 837 343 145
 138 400  80 192 183 282 323 596 524 118 617 756 228 398 567  74 432 440
 283  38 350 453 327 469 153  57 311 474 136 669 178 227 853   7 767 134
 333 713 585 146   9 841 307 865  25 860 851 140 262 700 615 738 714 336
 328  94 781 742 148 198 824 778 201 740 759 566 705 550 217 465  19  14
 314 780 729 117 107 133 151  23 731  22 612 783 234 565 181 577 325 574
 592 670 530 754 129 357 455 828 434 351 627 439 594 219 597 436 516 338
 741 622 124 753 230 236 199 196 241 239 590 609 235 694 621 664 779 588
 628 558 167 633 491 456  88 120 599 337 457 782 340 696 215 576 831 452
 123 656 442 448 184 707 830 561 231 675 573 774 450 610 345 630 553 238
 339 369 445 709 189 688 352 826 208 119 348 444 360 598 758 353 657 702
 659 743 745 829 825 827 589 712 361 493 533 580 364 618 619 631 519 421
 356 689 365 660 658 367 422 346 355 354 112 349 559 362 511 611 366 563
 626 710 691 535 693 607 624 359 545 625 358 799 525 613 690 418 165 368
 520 616 614 548 632 552 737 629 608 492 544 790 555 623 620 798 704 522
 641 549 539 870  83 722  91 708 794 796  95 744 512 706 110 795 791 715
 419 103 636 785 640 100 650 788 423 789 635 420 869 793 717 692 711 424
 797 104 697 637 792 634 643 698 114 699 786 638 652 787 600 720 496 725
 695 639 116 642 718 490  99 723 489 647 648 494 606 721 644 601 497 716
 736 645 724 719 602 604 486 603 605 488 487 649 417 654 655 653 646 495
 651 564 802 803   0 416 868]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0715
INFO voc_eval.py: 171: [ 598 1381  189 ... 1533 1529 1532]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2262
INFO voc_eval.py: 171: [249 282 176 262  64 286  61 247 267 294 279 290 222 237  67 171 174 292
 257  72 253 207 173 261 128  65  80 226 397 254 204 465 499 401 398 500
 277 168 260  62 284 269 480 225 482 464 516 239 224  58  85 402 481 223
 383 466 188 393 250  48 507 130 180 177 300 527 478  66 470 504 133 255
 181 238  90  51 240  73 463 471  50 195  88  84 403 129 400 519 105  24
 484 468 106 203 475 131 256 414 384 245 479 382 263 288  68 293 333 508
 483 469 443 473 395 182  28 287 476 192 301  75 150 148  76 510 462 396
 179 315  35 514 467 418 189 218 472 385 455 193 175 456 127  49 524 474
 336 303  82 375  74 523 219 521  57 248 362 271 492 522  45 364 413  77
  81 132 415 125  52 278 167 532 421 134 486 491 424 252  33 361 205 503
 490 497  31 158 211  70  54 149  83 332 345 310 487 169 363 215 477 404
 220  86 145 285 191 190 412 135 489 283 116 390  29 537 506 221 104 244
  27 102  56 209  23 216 505  44 258 124 206 307 272 304 170  71 422 355
 274 442 488 166 388  89 319  42 536 341 529 425  59 317 196 394 392 417
 126  55 437 493 420  30 172  87 416 246 436 308 183  92  17 154 538 377
 360 178 340  60 142 535 289 119 419 429 450 334 208 445 435 359 184 318
 259  63 353  78 389 346 330 306  79 108 528  91 342  19 309 408  69 242
  53 451 387 316 409 344 305 302 144 337 380 406 120 331 338 381 446 512
 194 103   5 357 138 243 107  32  41 113  18  34 163 266 320 399 153 407
 251 329 386 517 112 321 109 431 111 348 137 214 444 273 147 447 335 164
 141 343 339 347 143   4 525 202 313 349 114 427   3  38 515 281 358 117
 379 513 365 526 136 378 115 405 185 449 110 123 354  96 371 118 241 280
 152 121   9 531 140 151 376 423 146 351 268  43 228  10 432 217 374 498
 520 122 157 373 502 533 434 452 356  39 235  13 161  95   1 509 162  40
 165 101 352   7 186 518  97 139  99 322 227 454 233  14 198  21 270 511
 100 212 534   6 372 530 197 350 210 411 276 501 311 291 370  47  20 391
  98  15   2 230 485 156 433 234 275 160 312 426 155 453  46 314  12 213
  16  11 159 458 229 457 428 236 232 430  22 187 438 231 461 410 295 459
 440 323   8 448 368 460 298 326 296 299 324 297 369 328 441 366 201 367
 325 200  94  25 439 264   0 327  26 199 496  93  36  37 265 494 495]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0505
INFO voc_eval.py: 171: [933 107 828 ... 348 349 802]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0683
INFO voc_eval.py: 171: [210  34 128 126 129 221  38 207  31 166 214 208  15  35 119 213  33 222
 127 172 223 219 163  36 209 133 131 138 137 121 115 218  95 216  63   3
  32 177  13 134 136 227  99  64 197  54  62 135 130   1 224 217 150 212
  72 192 112 139  19 215 132 228   7 170  14  86 175  43 113 206 225 106
  48  96 186  66  18 195  87 104  85 169  52   8  94  51 142  69 144   6
  45 203  20  88 194  11  70  46 199 229 171 187 179 182 178 220 226  49
  17 165  42 173 200  16 152  53 157  71   4 193 211  74   2  68 196 181
 105 191 174 117  58 123  47  40 183 202  44 176 148 124   9  78 198  65
 201 118 107  50  39 231 167 143 102 154 168  73 140 164 103 184  67  97
 180  98  76 147 236 111 100  56 233 232 116 141 235  10 109  59 108 122
  82  30 151 234  41  37 101   5  61  55  93 230 159  57  60 161 238  80
 188 155  75  12 110  77  22 205  84  81  79 185  83 190 120 237 156 125
  27 189   0 145 146 114 153 149  91 158  26  29  90  92  25 160  21 162
  89  24  23  28 204]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3843
INFO voc_eval.py: 171: [ 7848  7137     0 ... 16787 16786 16785]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.2167
INFO voc_eval.py: 171: [ 224  227  240 ... 1823 1822 1821]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2402
INFO voc_eval.py: 171: [ 75 484 207 ... 768 769 992]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1015
INFO voc_eval.py: 171: [ 64   7  61  62 210 209  72 296   8  55  71 328 153  65  59  88 119 280
 286 282 330 313 281  56 284  84 259 321 122 314 261 293 115 289  85 100
 298  21 258 292 101 246  69 196  60 283  52 322 287  10  53 288  67 198
  57 247 200 290 143 155 125 193  66 195 114 279 285  91 277  22 260 127
  68 124 291 262 117 295 327 297  63 278  58 263 335 126 294 108 169  18
 123  89 141 181 245 343 312 131 103   9 179 199 257 248  82 154 177 107
 244 102 251  94 132 139 121 136 214 205 120 104 116 130 235 201 118 333
 249 172  93  12 250 253 189 138 341 129 173 340  70  83 106 105 156  54
 151 190 224 174 135 176   3 213 128  16 188 345 187 185 149 184 171 216
 265 325 175 230 256 318 161 183 238 145  30 271 197  36  34 215  29  17
 276  86 232 170 254 167  74 217 324 178 334 329  15 194 218  13 186 113
   5 142 220  33  37 164 182 317 307 150  31 147 203 180  19 255   4 308
  96 241  14 191 192  92  87 152 202  99 332 275  11 148  75 163 146 204
 273 344 212 237 236   1 112  32 323  98 342 274 162  90  76  81 144 226
 272  26  39 140  27 337  25 338  73  97 211  28 331 336 225  78 339  95
   2  50  38  45 229 233 207 319 221 227  47   6 326 240  51 137  40 239
 228  79 264 316  46 219 234  77  42 231 304  48  80 315 320  35  41  49
 157 166 165 159 222 242 160 158 168 306 134 252  23 301 305   0 206 208
  24 311 267 266 243 303  20 299 111 300 302  44 309 310  43 109 110 223
 133 268 269 270]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1173
INFO voc_eval.py: 171: [468 476 474 472 286 481 469 473 291 655 471 345 467 488 656 649 478 470
  52 186  50  46 188 609 288  58 475   6 182  65  75 296 100 290 494 285
 287 618 513 703 496 257  92 477 696 692  73 114  48 317  10  45  47 340
  53 517 659  63 348 693 357  79 485 105 116 697  11 121  83 491 106 507
 505 480  44 152 484  56 262 610 543 643 479 344 301 647 341 189   7  70
 648 612 269  69 355 700 265 153 704 120 561 295 350 639 191  71 486 613
 168 654 691 690  49 508  77 608 311  61 190  55 209 702 122 706 621 207
 650   8 284 259 109 258   5 280 113 694 359 315 634  80 354 270 353 183
 103 264 662  85 547 309 214 549 554  57 707 642 586  23 699 339 229 623
 313 578 516 346  91 154  15 544  59  64 548 658 632 566 300 118 607 138
 343 653 151 504 564 140 614 551 630 545  78 556 483 622 101  60  74 143
 375 670 107 233 157  96 144 503 130 303 169 356 509 241 652 314 520 219
  66 297 126 170 497 242 708 569 506 167 546 501 318 156 366 155 611 160
 162 139 698 436 495 377 518 672 405 705  51 502 498 408  84 565 567 660
 312  29 368 482 553 363 568 587 260 201 218 127 555 606 666 605 302 628
 552 349 358  26 361 125 616  22 124 577  99 550 651 676 208 240  89  33
 165 195 574  86  14 159 185 661 292 239 422 289 196 213 123 197 173 172
   4 429 261 418 352 282 158 701 524 536 245 407 559 709 572 562 615 316
 695 102 234  36 515  82 514 645 132  24 205 145 263 324 558 525 206 519
 629 110 644 128 117 131 351  54 342 275 243 362   9 620 115 141 487 365
 179 416 235 244 307  67  95 236 163 410 589 181 276 364 164  28 665 531
 360 713 669 202  90  12 136  13  87 500 180  18  98 104 413 211 134 397
 557 633 437 166 560 133 323 304 319 112 347 379 187 596 590 579 585 367
 570 111 119 563 390 521  76 667 193  16 204 268 135  34 626  94 320 194
 227 492 184  32 192 371 627 203 490 423 222 581 597 489 598 267 640 421
 199 529 584 619 294 142 277 230 310 575 591 461 442 212 674 393 137 396
 414  81 625 573 389 271 663 228 146 329 417 462 332 588 129 200 689 108
 415 147 178 198 449   2 325  62 224 455 331  68 463 617 635 624  43 454
 637  38 571 426 398 453 425  93 424 306 456 412 253 402 176   3  72   1
 580 538  41 636 641 451 223 255 657 673  88 210 687 511 254 333  40 321
 499 256 638 226 399  97 576 583 298 631 220 237 283 177  31 427 714 231
 582 326 225 682 528 466 431 370 533 683 685 599  25  42 327  21 175 526
 681 334 537 273 675 308 322 420 646 372 439 161 174 281 337 335 330 668
 435 464 336 527 328 664 715  17 445 411 434 232 419 171 403 430 274 299
 305 603 592 380 150 448 401 221 272 510 686 238 512 279  35 671 376 217
 378 541 458 452 374 373 404 684 266 278  39  30  27 388 369 432 532  37
 149 406 391 601 450 293 595 409 604 400 251 523 446 600 688 457 530  19
 392  20 459 440 542 148   0 438 539 540 428 441 602 522 535 447 534 382
 493 593 250 433 460 216 385 215 443 444 249 594 395 394 247 710 248 386
 465 246 338 384 252 381 387 677 383 679 678 680 712 711]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0520
INFO voc_eval.py: 171: [516 517 477 448 280   0 629 502 475 338 613 192 239 628 508 562  22 445
 480 258 563 260  48 174 187 281  17   4 490 354 278 248 340 614 256 283
 252 273 264 268  13 247 637 444  58 263 146 347 351  11  54 257 452 242
 315 269 173 472 133 492 341 261  23 289 446 564 229 134 101 349  16 350
 630 270 233 251 385 450  10 312 615 286 106 274  50 275   3 510 568 453
 455  57 285 586 156 232   2 500 295  36 238 579 511 509 313 400  18 570
  61 585   7 612 276 515 186 311 216 463 397 339  12 177 137 360 451 551
  85 277  56 638  49 356 298 622  66 288 207 447 149  31 379 470 231 536
 503   1  94 287 240 141 301   6   8 513 267  20 209  89 366 525  72 377
 314  86 373 514 476 145   9 504 624 205 471  41 565 327 635 336 108 418
 410 271 136  51 343 416 243 241 467  32 179 357 618 325 533 148 496 395
  37 142 616 153 176 572 566 282 522 384 221 567 512  44 226 195 215  52
 303 107 245  62 144 109  46 425 318  19 217 524 577 300 319 279 569  59
 244 131  53  15  92 355  55 183 155 122 162 302 468 497 110 541  47 485
 326 342 589 365 348 112  26 494 139 581 634 540  88 375 246  14 550 234
 464 461 631 412 213  29 518  74  81 332  98 337 143 636 469 296 633 359
 181  99  21  40 182   5 272 126 457  84 227 426 409  97 225  68 534 370
  60 462 324 611 393 600 188 424 403 571 531 465  24 210 543 499 113 185
 553  87 323 114 388 632 197 237 546 203 431 590 411 344 254 458  65  80
 249 140 250 211 230  90 399 117 401 329 119 542 559  67  35 364 528 208
 111 539 481 387 163 214 391 527  70 521 147 196  73 627 413 626 335 573
 259 621 639 434 605 115 362 422 184  75 212  83 105 535 473 529 223 523
 178 459  79  63 236 389 456 429 206 265 292 603 299 175 483 321 432 363
 532 537 394 417  91 460 218  33 488  82 316 291 104 100 305 102 284 262
 167 361 371 520 345 235 374  45 625 396 193 595 220 438 204 498  27  38
  42 591 331 378 554 604  77 352 129 353 346 358 150 501 491  78 487 334
 103 594 158 224 372  93 489 161 383  76  28  71 443 194 560 309  69 526
 602 597 368  95  64 599 561 530 328 484 157 415 620 576  96 367 228 474
 320 606 294 369 392 190 538 189 407 580 623 171 386 172 601 390 333 596
 598 556 135 191 442 219 330 619 317 128 266 322  39 121 439 405 202 486
 376 180 617 435 152 381  25 306 138 493 222 165 127 587 118 478 404 293
 423 495 578 420  43 382 479 466 160 307 414 380 116 255 170 297 290  30
 408 427 151 592 201 583 125 253 419 482 436 154  34 582 454 130 123 120
 406 124 159 549 421 441 609 166 132 433 558 428 199 200 430 402 544 584
 449 308 440 169 588 552 557 398 168 164 437 548 304 555 545 310 547 507
 575 574 593 519 505 506 610 198 608 607]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1250
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1437
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.315
INFO cross_voc_dataset_evaluator.py: 134: 0.061
INFO cross_voc_dataset_evaluator.py: 134: 0.062
INFO cross_voc_dataset_evaluator.py: 134: 0.170
INFO cross_voc_dataset_evaluator.py: 134: 0.269
INFO cross_voc_dataset_evaluator.py: 134: 0.166
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.115
INFO cross_voc_dataset_evaluator.py: 134: 0.071
INFO cross_voc_dataset_evaluator.py: 134: 0.226
INFO cross_voc_dataset_evaluator.py: 134: 0.050
INFO cross_voc_dataset_evaluator.py: 134: 0.068
INFO cross_voc_dataset_evaluator.py: 134: 0.384
INFO cross_voc_dataset_evaluator.py: 134: 0.217
INFO cross_voc_dataset_evaluator.py: 134: 0.240
INFO cross_voc_dataset_evaluator.py: 134: 0.101
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.125
INFO cross_voc_dataset_evaluator.py: 135: 0.144
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 7999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.527s + 0.036s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.361s + 0.042s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.375s + 0.043s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.375s + 0.042s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.373s + 0.041s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.380s + 0.042s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.379s + 0.043s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.377s + 0.044s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.374s + 0.044s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.376s + 0.044s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.375s + 0.043s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.375s + 0.043s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.374s + 0.043s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.577s + 0.040s (eta: 0:01:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.386s + 0.040s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.395s + 0.045s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.382s + 0.048s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.381s + 0.048s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.381s + 0.046s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.386s + 0.046s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.386s + 0.045s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.383s + 0.044s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.382s + 0.043s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.380s + 0.044s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.380s + 0.043s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.375s + 0.043s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.418s + 0.030s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.342s + 0.042s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.354s + 0.041s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.364s + 0.041s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.366s + 0.039s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.362s + 0.039s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.361s + 0.039s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.362s + 0.039s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.362s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.361s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.361s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.361s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.362s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.459s + 0.041s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.377s + 0.038s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.359s + 0.036s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.364s + 0.039s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.367s + 0.038s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.361s + 0.039s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.364s + 0.041s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.362s + 0.042s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.360s + 0.041s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.359s + 0.042s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.357s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.359s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.359s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 59.364s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [325 225  84 315 215 603 599 122 158 609 194 634  80  48 604 600 605  49
 214 314 137  55  90 621 171 199 274 219 319 190 640 184  95 607 164 168
  56 141 635 328 228 118 229 329 185 317 217 268 142 154 202 146  31  72
 649 326 226 648 291  32 143 320 220 673 670 601 230 330 618 626 286 283
 652 602 624 629 145 622 309 324 224 658 669 139  46 372 369 172 270 299
  53 295 413 241 450 244 243 138 273 659 234 121 656 611 311 419 628 284
 620 623 293 667 135 208 289 221 321 218 318 183 245 359 304 411 729 216
 316 242 279 305 365 506 631 308 606  33 404 277 610 198 686 287 167 178
 173 405 417  40  73 747 117 663 159 655  39 503 643 169 239 223 323 657
  42 650  89 651 204 646 492 671 307 170  36  43 500 123  92 636 366 644
  34  35 741 292 109  79 509 647  81 475 617 125  50 157 312 193 406  37
 541 196  44 288  51 412 630 371  38 197 303 373 276 156 529  85 465 364
 186 612 495 313 420 682  59 203 672 488 639  96 589 177 166 237 269 504
 494 501 676 502 558 588  82 666 446 414 298 332 238 232 297 188 201  45
 275 114 707 418 410 415 625 174 499 496 421 148 615 233 333 616 728 331
 231 684 511 182 674 638 290 370 189 476 665 514 222 322 360 179 731  67
  65 282 408 409 416 375 739 108 642  70 327 227 512 447 112 247 403 407
 367 180 300 144 163 155 280 126  71 176  30 454 195 296 120 235 683  88
 149 161 111 256 175  86  69 162 660 110 497 363 680 464  78 445 627 533
 662 654 546 113 538 258 491 641 489  94 294 165 563 736 597 479 608 487
  47 515 661 490 236 362 181 374 200   1 708 681 614 633 613  41 668 302
 632 696 132 637 187 480 544 306 580 192 130 478 619 693 191  75  77 398
 459 389 697 437 368  52 456 124 436 466 361 678 578 582 505 390 653  83
 393 559 734 452 272 664 387 735 160 530 547 119 516 508 677  68 539 147
 720 738 246 645 679 510 240 493 255 517 561 301 474 584  57 542 733 695
 687 594 513 477 429 577 592 549 723  64 498 383 448 507 470 535 213 740
 751 345  76 357 391 209 704 750 334 116 344 259 115 455 254  58 460 212
 722 257 430 285 583 353  29 595 346   0 725 737 394 252 384 151 425 337
 336 381 462 685 260 596 150 211 694 310  74 376 341 537 468 701 380 395
 463 210 598 278 726 564 131 382 423 388 730 281 581 392 700  54 435 136
 358 271 473 540 378 721 703 335 532 472 744 743 134 140 458 591 356  99
 585 531  91 593 587 745  27 402 467 727 579 342 481 586 428 692 724 349
 355 461 152  19 377 471 746 732 557 555 590 153 520 545 543 554  98  60
 440  22 350  28 556 100  97 576 698 574 482  11 560  93 352 688 451 251
   4 485 401 548 534  25 347 536 439 422 354  87 343 575 528 351 689 340
 253  63 566 457 527 705 690 553 699 427 453 484 348 339 127 250 379   8
 518  10 486 129 449 338  12 571 702 521 248 442 565  61  66 469 249 524
 706 444  20 691  62  16  13  17 525  18 438 716 519  23 426 443 133 483
 397 434 424 742 441   5 385   6 522 551 261 207 432 262  14  21 105   9
   2 101 523 526 431   7 433   3 675 717  24 562 128  15  26 712 265 104
 396 550 103 718 400 572 107 386 710 205 263 206 264 713 267 709 714 106
 399 715 719 552 102 711 266 567 573 569 748 568 570 749]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0319
INFO voc_eval.py: 171: [ 70 269  36 215  48 223  28 244 351  73 352  72 347 213 328  54  80 194
 270 221 349 329  82 268  27 275 219 247 229 350 227 220 115 348 113 320
  29 192 287 119 217 272 228 250 157  78 234  77  74 174 216  81 224  79
 332 325 335 273 341  76 319 196 238  71 322 232 193 226 218 249  30 161
  26  75 243 276 303 246 304 114 197 280 278 156 241   4 314 253 323 282
 163 277 271 324 214   2 255 153 230 235 233 131   3 222 231 338 126 340
 334 300 316  10  32 195   7 176 274  20 245 289 315 279 281 239 251  49
 290 256 199 171 296  39 120 240 297 286 337 128 312 248 198 237 116 283
 209   0 105 284  24 285 295 155  46 204 336  50 327 265 242 252  23 177
 236 298 205 225 100 305 254 130  19  37 118 331 148 179 167 308 258 207
 310 266 129   6   8 139 291 200  31 293 267 132   5 212 168 182 146 103
  34 150 333 210 321 309 259 159 178 151 292  21 173  33 154 152 299 339
 263 145  14 133 307 102 181  41  40 138 175 264   9 122 121 185 201 302
 124  93 294 262 288  13 342 257  94 261  17 123 125 106 306 260  69 318
 206 189 346  38  61 180 203 330  68 313  98  42 165  22 202  66 117  91
 317 208 141  60 162  12 186 135 190 166 301 101  59 172 311  92 107  63
 188 136  58 110  18  96 345  62 183  97  64 127 344  16  57 111 140 169
  51 142  99  95 137  11  67  53  65 112 184  44   1  45  55 170 343  90
 164  15  43  52 143 149  25 109 104 160  35 144 134 191 158 326 108 147
  56  88  84 211 187  89  86  85  87  47  83]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3134
INFO voc_eval.py: 171: [4331 4238 2030 ... 5791 5760 5784]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1171
INFO voc_eval.py: 171: [1613 1672 1797 ... 2275 2872 2278]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0597
INFO voc_eval.py: 171: [578 573 746 526 361 358 556 743 363 686 295 571 680 580 590 602  18 315
 291 529 538  19 530 718 367 356 760 307 633 575 614 586 316 279 294 557
 607  29 364 618 612 362 369 684 292 300 345 694 658  26 936 303 357 587
 299 606 734 698 619 594 319 282 318 756 692 386 308 621 956 696  20 162
 370 547 531 310  21 685 803 582 534 766 385  31 724 565 712 311 804 368
 958 561 558 280 309 723 670 655 914 545 564 726 925 733 894 688 568  35
 930 700 312 314 690 697 695  17 622 570 750 317 682 954 711 636 727 632
 701 757 932 302 366 525 301  36 543 286 811 913 600 716 160 613 707 592
 966 732 283 349 963 728 137 662 620 560 297 566 593 764 122 689  24 611
 955 597 665 720 313 631 919 605 691 609 802 567 699 135 765 710 638 873
 725 360 702 737  25 610 740 959 709 608 599 462  30 285 910 296 877 761
 741 596 940 706  34 359 969 546 738 365 664 450 747 348 172 962 867 152
 870 900 693 435 753 708 533 577  40 579 289 768 752 820 816 158 494 645
 234 290 917 469 634 298 968 881 742 824 704 744 808 717 876  27 642 639
 871 539 970 749 569 293 583 815 146 703 156 466 134 830 719 542 591 433
 487 672 171 883 782 457 770 865 922 603  33 598 754 528 769 278 128  22
 964 434 536 937 257 643 793 705  28 149 656 153 585 470 759 379 763 562
 667 687 731 896 863 144 490 722 758 762 544 818 353 953 588 497 713 148
 809 243 755 812 476 448 288 572 868 807 729 616 841 334 739 284 806 150
 897 452 831 617 240  37 527 862 495 957 601 663 681 866 683  23 825 721
 574 775 872 879 235 735 253 659 797 745 960 589 576 924 767 751 714 559
 792 615 474 451 748 595 840 736 857 236 563 730 715 836 537 875 895 281
 287 821 123 161 604 584 381 781 650 852 788 210 140 835 581 855 477 657
 884 941 445 488 491 136 540 864 644  32 668 813 776 496 948 138 880 205
 183 482 443 660 333 463 814 780 239 649 186 336 647 256 432 838 455 790
 648 248  41 480 869 952 139 483 165 460 245 905 185 380 927 246 164 891
 481 671 147 189 182 207 926 142 541 188 920 184 461 661 646 187 177 169
 826 654 108 124 254 532 535 789 510  74  78 485  16 258 548 912 823 874
 327  72 641 155 471 244 468 945 168   7 666 907 145 131 791 817  54 845
 178 242 444 467 892 352 479 501 406 304 326 829 211 241 486  15 430 325
 475 893 784 166 489 129 472 908 133  44 949 473 856 218 446 151 154 828
 209 512 407   3 923 944 942 395 484 465 965 255 785 478 249 464 442 163
 779 330 260 449 961 459 950 447 259 918 921 882  75 441 157  94 850 805
 341 515 878 520 181 935 102 141 778 238  47 500 909 206 208 502 225 233
 624 832 901 179 170  73 834 394 669 180 340 344 837 456 939 125 374 458
 946 250 247 854 214  48 429 635 898 265 347 237 143 514 899   0 428 126
 903 819 410 810 498 176 934 229 843 842 499 929 355 551 822 173 346 404
 839 109 916 267 331 522   2 938 354 827 324 625 101 833 351 454 305  93
 851 106 509 252 796 640 412 175 132 623 273 651 219 110 416 653 511 217
 503 786 337 931 378 554 853 350 174  98 517 508  76  46 127  71 408 637
 332 431  95 329 215   9 507 513  45 120 377 396 516 398  91 915 973 518
 393 328 505 504 928 107 391 230 130   1  11  59 372 628 652 858 849 343
   4 227 204 216 167 943   6 967 555 397 392 390 933  12 427  42  14  87
 411 159 971 902 972 414  13 911   5 629 906 384 904 342 777 519 335 553
 951   8 338 203  10 339 425 524 795  92 846 626 627 383 860  96 272  99
 453  97  89  88 774  77 506 492 859 552 100 521 399 847  90 523 800 947
 264 437 436 266 306 630 263 268 798 844 848 262  61  43  62  60 261  66
 388 373 375 276 112 771 787 783 389 405 415  68 226  52 861 269 794 212
 224 376 275 251 773 409  79 413 439  69 426 387  39 382 270 799 440  82
 438 200 801 889 493 103  38 549  53 420 213 118  63  50 277  67  70 422
  64 403 271  49  65 772 222  51 105  85 890 121 423 223 888 321 550 401
 402 673 274 400  86 232 104 220 113 221  83 201 116 887 424 421  81 111
 231 119  57 114 228 117  56  80 320 679  84 886 417 419 418 115 202 191
 323 674  55 677  58 678 190 199 676 371 196 885 322 675 194 192 198 197
 195 193]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1643
INFO voc_eval.py: 171: [299 114 109 323 104 120 142 135 307 115  33  73  71 315 103 105 121 108
 110 141 313 314 301  36 102  34 107  82 143 111 324 305 101 316 302  37
  44  86 306 112  40 124  30  45 322 149  76 117 318  70   7   6 106  74
 312  41  58   0  38 303 152  60  90 147 287  79  81 231 232  50 256  93
 194 146  55 196 308  85 195  46  77 300 242   1  69 335 244 144  62 233
 206 284 126  67  75 113 283 137 240 329  72 292  32 280  48 250  83 128
 132  31 278   8  51 125 130 253  84 330 150 319 131 332 285 151 339 119
 325 279  80 281 282 153  78  39 192 118 304 140 156  28 207 163 245 246
  59 139 193 205 340  56 148 247 138 310  63  64 241 254 317 220  47 201
 136 237 229  68 198 129   5   3 277  66 158 164 238 197 230 145  65 211
 204 286 291  94  29 235 221  96 133 123 189  15  61 122 288  42  52 293
 309 252 249  11 191 190  35  19 217  91 134 327   9  54 116  57 218 127
  49  43  10 210   4  24 290  53 173 321 255 214  18 202 236 328 187 248
 239 289 337 100 251  97 234 200  12 188 199  20 213 228 224 243 294 296
 311 223 276   2 155 212 267 169  99 334  95  88  21  89 175  98 154 266
 216 336 208 222 219 331 225  92 157  87 168 270 320 161 215 176 209 264
  25 167 333  26 227  27 203  16 275 326 338 160 226 172 297  13 298 269
  22  17  14  23 268 295 258 260 272 166 274 259 271 182 178 263 265 180
 261 174 165 162 273 257 159 170 171 262 177 181 183 186 185 179 184]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2696
INFO voc_eval.py: 171: [2210 1121 1570 ...  698  699 2506]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1652
INFO voc_eval.py: 171: [106 114 109 110 256 140  13 200 112  99 105 257 258  34  16 107 142  24
 255  31 260 166  74  29 111 197  19 167 231  30  80 137 108  79  98  73
 138 233 263 165 235  36  32  33 222  12  83 170 159  17 168 209  85 150
 259 223 262  96 141 173 103 156 162 254 104  20 261  43 214 172  75 169
  72  95 191 220  27 208  67 171 230  28  18 139 136 100  21 117  44 237
  35 225  82 158 210 232 115 238 234  76  56 190 192  70  15  86  71  22
  64  11  62  87  23  60 236 116 218 212  78  14  65 219 217 203 224  25
 195  69  68  26  63 131  66  49 130 245  46 133 193 174 226 216  61 161
 134 246  45 213 221  59  97  54 243 194 163   4 155 132 215 188 135 157
  58 118  55   9 101 204 196  52 129 187 160  47 182 102  48 186 242  90
 122 201  88 120  84   0 154  53   5  57   2 207 211 123 128 127 126 119
  81  50  89  77  42 164  92  10 205   7   6  93   1 178 240 176 198  94
 199 189 113 177   8 244   3 247 183 124 239 248 179 151 175 152 149  91
 228 227 202 206 229 125 180 241 252 253 251 181 250  37 249 145 146 148
 184 185  51 143 147 144  39 153 121  38  40  41 264]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0316
INFO voc_eval.py: 171: [2084  279 2076 ... 2735 2736 3616]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1090
INFO voc_eval.py: 171: [376 108  82  94 410 113 319 412  96 504 375 104  84 257 377 393 402 368
 101 158  95 469 243 289 676 378 409 114 170  20 681 186 291 250 509 267
  49 164 292 391 382 847  21 486 660 283  87 459 367  50 470 321 187 190
  83  75 285 154 318 685 404 798 508 246 460 501 405 765 178 223 219 244
 277 272 278 831 406 260  99 466 171 323 424 222 467 181  37 683 221  34
 141 109 545 687 301  11 329 253 284 479 507 153 242   3 386  65 240 218
  85 476 312 533 433 252 273 472 185  92  55 755 208 829  61 844  47 435
 370 197 387 194 842 505 151 506 570 200  68 462 327 290 265 819  44 156
 226 293 248 746 202 541 833 180 665   2 484 860 499 256 770  81 686 373
 836 305 157 458 664 441 680 502  76  46 300 161 149  63 299  59 155 191
 555 584  80  41 729 168 112 516 152 184 266  48 380 103 175 162 331 678
 159 306 310 820 407 449 758 684 667 835 205 174 581 760 427 397 127 276
  24 160 249  64 503 369  16 177  77 745 332 271 850 206  58  51 567  13
 500 727 485  28 832 445 247 317 821 384  10 812 517 498 520 270 142 217
 558 379 295 400 121 440 286 537 255 126 188 528  69  54 297 429 258 525
  42 262 148  18 853 201 239 229 444 480 268  53  43 763 145 595 209 671
 254 818 236 372 849   7 861 401 759  30 775 662 385 749 799 575 287  40
 457   5 544 294 808 481 701 756 282 800 426 666 203  26 394 245 583  70
 428 261 483  60  27 593 475 810 447  36 192  86 522 259 845 682 661 830
  15 296   1 846 540 726 725  62 269 100 172  35 767 390 389 482 207 672
 403 425  72 571  57 199 560   4 856 169 214 395  12   6 392 210 670 173
 552 408 673  45 328 859 734 527 852 211 220 234 750 463 837 275 107  91
 147 817  17 530 431 135 807 411 288 513 803  74 264  32 315 339 125 761
 124 769 587 579  33 863 383 814 166 120 471  78 768 816 772 840 677 733
 748 841 538 738 762 251 344 822 813 804 582 230 453 298  29 314 526 782
 139 811 851 341 374 423 855 805 773 461 815 809 371 732 326 388  52 130
 134 806 854 731 535 751 281 303 140 381  66 843 204 313 554 399 129  39
 703 465 539 307 167 771 456 536 337  67 263 512 274 360 549 591 436 566
 309  31 774 586 514 302 568 316 137 477 674 531 474 858 578 747 834 398
 131 279 136 143  79 189 596 340 182 754 351  73 523 225 241 565 280 617
 320  38 117 452 396 439 430 150  56 324 468 308 224 766 133 176  71 473
 668   8 848 585 857 862 132 839 144   9 779 713 304 138 330  25 333 615
 700 325 195 737  88 105 146 776 757 311 741 564 198 548 739 705 714 823
 838 213  19 464 728 777  14 730 116 577  23 478  22 231 612 563 781 322
 454 179 753 574 592 128 669 354 529 827 594 432 335 626 347 597 437 215
 590 515 216 752 123 227 613 237 434 740 621 233 238 193 196 232 556 588
 694 620 632 627 778 438 599 663 165  89 492 119 455 679 334 212 780 576
 696 450 338 764 442 446 451 122 655 828 106 559 572 448 183 675 228 551
 707 609 443 235 629 336 342 366 709 348 688 598 826 345 118 357 349 702
 656 589 744 742 824 657 825 358 494 712 352 532 518 361 630 580 618 418
 619 353 611 659 362 689 364 111 343 419 658 350 346 573 557 359 510 363
 610 561 625 710 534 691 622 693 356 546 543 624 524 355 797 690 519 163
 415 365 614 616 736 607 631 628 550 493 553 608 623 542 788 521 640 547
 796 704 866  90 721 792 708  93 794 110 706 743 511 793 789 416 715 635
 639 783 786  98 649 420 787 634 417 865 717 791 711 692 421 422 102 697
 795 569 790 633 636 698 642 784 699 637 785 720 600 651 724 695 496 115
 641 638  97 718 722 490 491 646 647 606 643 601 497 716 735 644 723 719
 602 604 603 487 605 489 488 648 653 652 414 654 645 495 650 562 801 802
   0 413 864]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0713
INFO voc_eval.py: 171: [ 599 1378  190 ... 1535 1530 1532]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2260
INFO voc_eval.py: 171: [245 277 174 258  63 281  60 243 263 289 274 285 219  67 234 169 288 172
  70 253 250 205 171 257 126  64 394  79 223 251 202 461 495 398 395 497
 272 166 256  61 279 265 476 222 460 478 512 236  57 220  84 399 477 221
 380 462 390 186 247  47 502 128 178 295 175 523 466 474  66 500 131 179
 235  88  71  50 237 459 467  49 193  86  83 400 127 397 515 103 284  23
 480 104 464 201 129 471 381 252 241 411 475 379 259 282  65 504 329 465
 479 439 392 180 469  27 472 190 296  73 149 402 147  74 506 393 177 458
 309  34 463 510 414 187 382 468 216 191 173 451 125 452  48 412  81 470
 520 298 332  72 372  56 217 517 519 359 244 267  75 488 410 518 361  80
  44 123 130  51 273 165 132 482 249 528 417 420 487 203  32 358 499 246
 486 493  30 209 157  69  82  53 148 328 483 341 360 213 168 401 305 218
  85 473 143 189 188 280 133 283 409 485 387 278 533 114  77 501 102  28
 240  55 100  26  22 206 214 204 494 122 254 302 299  43 484 268 418 167
 438 351 313  87 269 385 164 532  41  58 421 525 337 311 194 124 389 391
  54 489 434 416 242  29 170 413 432 303 181 535  90 153 531 176 336 374
  17 357  59 140 207 286 415 117 425 330 446 431 255 356 312  76  62 441
 182 349 386  89  78 405 301 338 326 304 342  19 106 384 524  68 238 406
 447 310  52 340 300 297 404 142 333 377 118 442 327 378 334 508 192 101
   5 136 354 239  31 105  33 162  18  40 314 396 262 383 315 152 248 325
 110 427 513 107 109 212 135 344 111 146 440 443 141 139 335 343 163 331
 271 339 200   4 521 308 112 423   3 345  37 275 511 355 116 115 362 509
 522 376 113 134 375 183 403 108 445 368 350 121 276  94 151 119   9 138
 527 150 373  42 144 419 264 347 215 428  10 225 371 156 120 370 516  38
 529 352 430 160 232 448 505  13 348 145  93   1 316  39  99   7 161 184
 514 137  95 498  97 450  16 230 224  21 507 196 530 210 266  98   6 195
 526 208 346 369 408 306 496 534 287 227  20 367 481   2 388  14 231 503
  46 429 270 155 422  96 307 159 154 449  12 211  45  15  11 454 158 424
 226 453 233 426 229 185 353 433 228 457 407 455 290 318 436   8 444 317
 365 456 293 321 291 294 319 292 366 323 363 437 364 199  92 320 198  24
  25 435 260 197 322   0 492  91 324  35  36 261 490 491]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0506
INFO voc_eval.py: 171: [945 110 843 ... 967 355 817]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0627
INFO voc_eval.py: 171: [209  34 128 126 129 220  38 206  31 166 213 207  15  35 119 212  33 221
 127 172 222 218 163  36 208 133 131 138 137 121 115 217  61  95 215   3
  32 177  13 134 136 226  99  62 196  54  60 135 223 130   1 216 150 211
  71 191 139 214 112  19 132 227   7 170  86  14 113 175 224  43  96 205
  49 106  65 185  18 194  87  85 104  52 169   8  94  51 142  67 144   6
  45  20  88 193  68  11  46 228 182 198 171 186 179 178 219 225 201  48
  17 165  42 199 173  16 152  53  69 157 192   4 210  73  66   2 195 105
 181 190 174 117  57 180 123  47  74  40 183 176 202 124  44  78   9 149
 197 118  63  50 200 107  39 230 167 143 154 102 140 168  72 164 103 184
  64  97  98  55 148  76 235 100 111 232 116 231 141 234 109  10  58 108
  30 233 122 151  82  41  37 101  93   5  56 229 159  70 161  59 236 238
 155  80 187  75  12 110  22  77 204  81  84  79 189 120  83 237 125 156
  27 188   0 147 146 114 145 153  91 158  26  29  90  92  25 160  21 162
  24  89  23  28 203]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3842
INFO voc_eval.py: 171: [ 7861  7148 10738 ... 16812 16810 22700]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.2182
INFO voc_eval.py: 171: [ 222  239  242 ... 1805 1809 1808]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2236
INFO voc_eval.py: 171: [ 73 478 204  62 513 527 892 198  47 461 506  49 459 915 503 134  31  44
 498 135 185 908 877  40 515 495 189 452 912 487 898 514  42  71 517 516
 467 449  37 199 131 521 706  33 874 193 896 502 882 450 460 203  76 510
 872 499 833  43 886 183 471 535 481  56 448 576  29 187  57 186  66 455
 955 900 229 180 470 195 132 225 473 891 466 456 913 873 523 819 496 741
 177 964 786 451 468  63 511 176 121 507 332 130 194 525 392 880 465 729
 500 889  60 458 976 836 475 893 914 124 883 530 887 494 902 520 484 165
 233 463  72 490 725 453 884 505 519 526 184 454 536 133 875 486 191 485
 961 843 645 890 501 476 529 568 944 472 522 842 181 734 464 649 838 479
 531 646 173 218 969 482 810 222 123 202  54 910 945 457 899 379 563 834
 474 462 534 179 532 249 524 950 260  35 488  51 509 136 399 844 489 197
 939 508 400 571 235 647 966 227  52 170 396 828  68 492 528 803 217 979
 841  53 829 972 390 477 551 951 250 240 200 469  64 497 491 128 226 575
 182 815  38 518  39 192  59 555 572 398 533 512 493 480 977 190 830 157
 805  41 823  46 888 947 483 938 504 558 837 650 312 728 313 228  34 894
 162 963 794 159 239 904 806 137 648 941 800 310 188  55 559 832 879 965
 651 809  21 812 382 574 981 397 567  69  70 953 247 742 907 839 807  36
 566 798 201 878 246 224 901 573 223  78 845 253 416 906  75 363 196 164
 307 954   4 973 175 166 221 125 258 940 377 264 167 816 219 822  23 311
  67 266 562  25  45 974 570 787 956  65 968  77 958 738 804 359  28  26
 726 970  50 820 814 158 160 169  48 127 542 561 168 220 600 957 236  27
 797 881 770 304 948 975  32 245 155 897 840 403   5 161  61 241 943 730
 735 980 737 808 358 129  24 263 300 740 791 172 251 743  22 207  30  58
 242 174 876   6 315 942 216   7 801 171 163 243 154 230 967 960 971 765
 550  74 303 299 120 959 345 317 178 949 821 401 577 962 905 790 139 802
 138 122 540 257 978 215 885 544 818 296 255 952 314 933 736 946 265 537
 153 598 731 817 560 376 126 789 334 796 316 831 335 274 739 365  98 208
 733 557 393 261 732 269 259 705 788 291 827 813 795 156 234 727 349 148
 545 231 305 151 565 792 835 355 262 824 302 309 348 248 306 543 143 811
 554 799 362 209 378 254 826 117 352 308 301 825 793 633 152 149 297 361
 581 119 298 232 549 548 150 356 237 676 707 340 244 547 425 346  96 692
 252 366 553 350 339 541 406 256 238 105 405 569 268 775 364 147 357 643
 354 205 290 628 782 330 118 695 784 539 292 380 277 424 636 710  15 682
 666 351 270 564 596 418 272 715 360 615 909 631 552 331 384 546 538 693
 640 343 271 556 427 749 704 347 383 353 423 414 103 381  93  95 412 389
 871 142 417 385 386 395 634 212 766 618 338 992 276 700 768 267 145 337
 678 275 673 426 697 701 870 716 273 783 996 584 780 580 388 781 853 325
 869 665  19 582  97 785 420 672 140 211 415 387 696 857  94 404 658 328
  17 210 994   9 723 144 895 214 213 409 428 854 652 419 146 698 410 779
 588 773 344 653 858 852 660 341 408 407 699 848 744   8 422 613 642 679
 611 626 413 711 641 627 579 937 333 336 421 106 717 750 638 394  20 656
 639 993 936 709 614 342 911 991 769 391 578 702 903 862 995 703 935 714
 637 724  88  12 934 718 583  11 655 774 861  14 630 293 674  86 849 601
 411  83 619  18  10 859 294 712 632 751 722 586 659 708 621 687 101 295
 609  16 104  13 620 111 847 112 114 855 928 115 851 685 623 622 597 657
 323 107  81 856 846 675 860 689 109 593 100 745 654   3 713 590 929 850
 767 746 918 599 322 587 324 683  99 686 917 206 326 591 605 612 327 932
 108 661 625 616 691 667  82 102 110 402 867 719 919 610  80 603 920 747
 866 930 369 624 684 373 931 720  92 748 694 585 921  84  85 629 668 644
 289 606 595 663 604 771 662 664 318 607 721 116 592 440 772 670 690 602
 608 617 321 863 320 635 368 594 589 435 367 141 688 916  79 371 669  90
 319 865 868 436 922 864 446 375 372 671 985 437 680 374 778 925  87 776
 370 677 926 681  89 777  91 927 441 284 444 924 439 329 287 281 438 988
 283 285   2 113 433 431 288 434 432 443 429 282 280 923 286   1   0 430
 445 442 279 447 983 764 986 761 754 755 757 278 753 984 987 763 990 756
 758 759 752 982 762 760 989]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1012
INFO voc_eval.py: 171: [ 69   9  66  67 218 217  78  10 304  77  60 338 160  70  64  93 126 288
 294 340 290 322 289 330  61  89 267 292 130 269 323 219 300 122  90 297
 105 306  24 266 106 254 301 331  57  74 204  65  12 291 295  58 287 296
  72 206  62 255 208 162 151 298 132  71 201 203  96 121 286 268 293 134
  25  73 270 299 131 124 305 303 337  68  63 271 285 345 302 133 114  20
 176  94 148 189 253 353 321 108  11 138 129  75 207  87 161 187 128 256
 265 113 185  99 107 252 259 147 139 109 223 144 343 213 123 257 243 137
 209 127 125 140 163  14 179  98 258 261 146 351 350 110 197 180  88  76
 136 111 158  59 198 233 181 156 143 183   3 135  18 222 112 355 196 195
 193 334 192 178 273 225 182 264 168 246 327 238 153 191  33 224 279  19
  39  37 205  32 284 240  91 333 177 262 174  79 226 339  17 344 186  15
 227   6 202 194  36 150 229 184 119 171 190 326  40 120 315 157  34 155
  21 211  16 188   4 263 159 101 249 316  97 199  92  13 200 210 104 283
 342  80 281 170 154 221 354 332 212 245 103 244 118  35   1 282 352  81
  95 280  86 152 169 235  28  42 347  30 149 102 348  29 346  31 220 341
  83 234 349  22 100   2  55 215  50  41 241 237 335   7 236 328 230  52
 248 247  56 145  84  43 325 272 242 228  51  82  45 312 239  85 324  53
 336 329  38   8  44  54  46   5 164 173 172 231 250 166 165 167 175 314
 142 308 260 313 214  26   0 216  27 320 275 274 311 251 307  23 117 310
 317 309  47  49 318 319 115  48 116 232 141 276 277 278]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1169
INFO voc_eval.py: 171: [475 483 481 479 290 488 476 480 295 478 660 349 474 496 661 485 653 477
  51 186  49  45 188 615 292  57 482   5 182  63  73 301  99 294 502 289
 291 624 520 707 504 261  91 484 701  71 696 112  47 321   8  44  46 344
  52 524 664  61 698 352 362  80 492 102 114  10  84 118 499  70 103 514
 487 491  43 151 697  55 266 617 648 550 305   6 486 189 652 654 345 348
  68 618  69 273 360 269 708 152 568 299 117 354 644 191 494 619 167 659
 695  48 515 694  78 315 614 210 190  54 119 208 627 655   7 710 288 263
   4 106 262 705 284 111 699 364 319 640  81 358 357 274 183 121 215 667
 268 554 313 561 556  56 646 711 593  23 704 228 317 629 585 343 522  13
 350  90  58 551  62 555 116 638 663 153 573 304 107 613 138 512 658 347
 493 150 571 702  79 620 126 636 558 490 552 628 563 675  59 100  72 379
 141 104 511 142  95 307 713 129 168  66 318 361 657 527 244 516  64 300
 124 169 503 441 712 576 245 513 166 509 322 553 371 155 703 158 160 616
 505 154 381 525 677 506  50 411 408 709  85 572 574 316 665 510 575 489
 368 202 560 594  28 220 234 264 562 125 671 612 306 634 611 559 353 221
 363 366 123 622 584 122  21 681  98 209  12 557 656  31  87 243 195 164
 185 145 581  77 666 196 293 296 157 242 197 214 427 120 172 171   3 422
 434 265 356 286 706 531 156 248 566 543 410 579 569 621 523 320  35 700
 237 101 521 650  83 130 206 143 532  24 207 328 635 267  18 526 565 226
 649 108 127 115  53 355 346 367 278 246 113 370 495 626  75 139 179 420
 238 235 247 311  65  76 239 414  94 161 596 369 280 181 670  27 163 365
 538 674 717 203 507  11  89 135  17  97  86 180  25 212 133 639 165 417
 564 567 132 402 308 442 233 327 323 383 187 198 110 577 672 372 603 592
 597 586 570 351 528 394 193 131  14  74 109 134 205  32 324 500 272 632
  93 359 192 229 194 184  30 204 375 498 136 633  22 225 497 428 223 588
 645 200 604 271 591 314 464 582 222 426 536 625 298 281 140 679 445 137
 213 598 397 401   2 580  82 466 393 418 668 631 144 230 333 275 421 595
 467 335 178 693 128 419 199 146 201 105 454  67 329  92  60 460 336 623
 630 641 578 642  42 459  37 458 404 431 310 416 430 429 176   9   1 256
 224 461 587 405 678 647 456  40 662 544 518  88 211 259 258 692 643 508
 337 260 325  96 583  39 227 590 403 637 302 240 177 231 287 718 589  29
 374 330 535 432 687 174 472 540 690 605 688 331 533  41 338 312  34 175
 680 277 545 376 651 686 159 326 173 673 341 444 285 424 334 669 339 332
 468 534 340  16 719 440 450 415 439 232 406 170 423 309 435 425 279 609
 303 384 437 599 519 517  33 149 400 453 676 276 382 241 283 691 380 218
 219 548 457 236 162 378 407 377 689 257 270 282  38 392  26 373 539 148
 395 409 436  36 463 297 455 607 602 412 610 413 530 254 451  20  15  19
 606 537 396 446 462 443 549 147   0 546 608 433 447 529 452 542 541 547
 386 600 501 471 253 389 465 438 217 216 470 448 449 399 601 252 473 398
 390 250 251 469 714 342 249 388 255 385 391 387 682 684 683 685 716 715]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0521
INFO voc_eval.py: 171: [514 515 476 447 279   0 627 500 474 611 340 193 239 626 559 506 444  23
 479 258 260 188 560  49 175 280   4  19 277 248 488 354 342 612 256 282
 252 273 264 268 443  13 247  59 263 147 635  11 350 242 270  55 316 174
 451 471 257 134 344 261 490  24 290 343 229 562 445 103 135 351  17 269
 628 352 233 251 384 449 613   9 287 313 108 358 274 275  51 508   3 565
 452 583 454  58 232 286 157   2 498 296 238  37 576 509 507 314 399 567
  18  62 582   7 610 513 187 312 462 346 218 341 395  12 178 139 450 276
 548  86 636  57  50 357 299 620  67 364 288 208 446  32 379 231 469 534
 150 284 501   1  95 240   6 301 142   8 511 267  21  90 523 210 366  73
 512 372 377 475  87 315 146 502 622  10 206 561 470 328  42 110 417 633
 271 338 409 241 415 243 137  52  33 347 466 180 616 359 326 149 531 494
 393  38 614 177 154 143 563 569 281 520 510 564 226 383 221  45 303 216
 245 109 196  63  53 111 423 145  47 289 522  20 574 300 320 319 217 278
 566  60  99 244 132  54  94  15 356 184  56 156 164 123 302 112 538 495
 467  48 483 327 349 585 345 373  27 492 632 246 578  89 140 537  14 547
 375 234 463 460 410  30 516 629  75 101 214  83 335 339 144 634 297 468
 631 272 182 361 100 183  22   5  41 127 456  84 424 227  98 225 407  69
 532 609  61 325 461 371 391 422 189 598 401 568  25 529 540 464 114 550
 497 115 211 186  88 324 237 386 630 543 430 587 198 204 412 254  66 457
 250  79 249 230  91 539 556  16  68 141 212 330 397 368 398 120 365  36
 526 113 118 536 209 165 525 480 215 385 519  71 625 389 148  74 197 624
 413 570 619 259 337 603 433 637 116 185  76 213 362 533 107  85 472 521
 527 223  81 236 179 458 455 601  64 265 428 387 292 481 535 431 207 176
 322 363 530  92 459  34 219 486 262 416  82 392 306 317 106 102 235 104
 283 168 348 518 623 374 593 394 438 496 194  28  46  39 588 220 205  43
 333  78 602 551 378 130 353 151 489 499 105 355 360 592 485 336  80 224
 162 382 487  93 442 557  77  29 310  72  70 600 195 285 591 524 595 597
 369  96  65 370 528 558 482 408 329 158 618 228 604  97 573 473 294 321
 367 191 390 190 405 621 577 172 304 173 599 119 388 596 594 334 553 192
 136 163 441 266 222 617 318 331 437 129  40 323 122 484 181 403 615 376
 434 203 332  26 307 491 380 138 167 128 402 477 293 584 421 575 493 419
 308  44 478 161 465 255 298 171 414 291 381 117  31 589 425 426 406 152
 253 435 580 418 202  35 155 453 579 131 125 121 404 440 126 160 546 420
 607 555 133 153 427 432 124 200 429 201 400 541 581 448 295 309 159 170
 439 411 586 549 554 169 166 396 436 305 545 552 542 311 544 503 572 571
 590 517 505 504 608 199 605 606]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1209
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1445
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.313
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.164
INFO cross_voc_dataset_evaluator.py: 134: 0.270
INFO cross_voc_dataset_evaluator.py: 134: 0.165
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.109
INFO cross_voc_dataset_evaluator.py: 134: 0.071
INFO cross_voc_dataset_evaluator.py: 134: 0.226
INFO cross_voc_dataset_evaluator.py: 134: 0.051
INFO cross_voc_dataset_evaluator.py: 134: 0.063
INFO cross_voc_dataset_evaluator.py: 134: 0.384
INFO cross_voc_dataset_evaluator.py: 134: 0.218
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.101
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.121
INFO cross_voc_dataset_evaluator.py: 135: 0.144
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 8499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.375s + 0.094s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.361s + 0.044s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.370s + 0.042s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.358s + 0.041s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.358s + 0.039s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.358s + 0.039s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.359s + 0.040s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.363s + 0.041s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.362s + 0.041s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.364s + 0.041s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.365s + 0.041s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.367s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.370s + 0.041s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.579s + 0.033s (eta: 0:01:15)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.374s + 0.033s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.378s + 0.037s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.367s + 0.038s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.366s + 0.040s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.371s + 0.040s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.375s + 0.039s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.372s + 0.040s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.370s + 0.040s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.375s + 0.039s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.373s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.374s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.372s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.566s + 0.043s (eta: 0:01:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.383s + 0.041s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.386s + 0.042s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.378s + 0.042s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.373s + 0.041s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.369s + 0.042s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.366s + 0.043s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.366s + 0.043s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.366s + 0.043s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.367s + 0.043s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.367s + 0.043s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.367s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.367s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.446s + 0.028s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.365s + 0.034s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.363s + 0.036s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.365s + 0.038s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.368s + 0.038s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.367s + 0.039s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.368s + 0.039s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.367s + 0.041s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.368s + 0.040s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.365s + 0.040s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.363s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.362s + 0.040s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.362s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.675s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [226 323  84 216 313 602 598 122 158 608 194 633  80  48 603 599 604  49
 215 312 137  55  90 620 171 199 272 317 220 190 639 184  95 606 168 164
  56 634 141 325 228 118 326 229 185 266 315 218 142 154 202 146  31  72
 648 227 324 647 289  32 143 318 221 672 669 600 327 230 617 281 625 651
 284 601 623 628 145 621 307 322 225 657 668 139  46 369 366 172 268 297
  53 293 410 447 243 242 138 271 658 234 655 121 610 309 416 627 619 282
 622 291 237 666 135 208 319 222 287 316 219 183 244 356 302 408 729 314
 217 277 362 303 505 630 306 605  33 401 275 609 198 686 285 167 173 178
 414  40 402  73 748 117 159 662  39 654 501 642 169 240 321 224 656  42
 649  89 650 645 204 670 490 305 170  36  43 498 123  92 635 363 643  34
  35 741 290 109  79 508 646  81 472 125 616 157  50 310 193 540 403  37
 196  44 286  51 409 368 629 197  38 301 370 274 528 156  85 462 611 186
 361 311 493 417 682 203  59 671 486  96 638 588 177 166 267 502 238 492
 499 675 500 587 557  82 443 665 411 296 239 232 329 295 201 188  45 273
 114 707 415 407 412 624 174 418 494 497 148 614 615 233 330 728 328 231
 684 510 182 673 637 367 473 288 189 664 513 357 179 320 223 731  67  65
 405 280 413 372 739 406 108  70 641 511 246 112 444 400 364 404 180 144
 163 298 155 278 126  71 176 451  30 195 235 294 120 149 683  88 161 255
 111 175  86  69 162 659 110 360 495 679 461  78 442 532 626 661 653 545
 257 537 113 489 640 487  94 292 736 165 562 596 476 485 607  47 488 514
 660 359 236 371 181 200   1 708 681 613  41 632 612 300 667 132 631 636
 696 187 477 304 579 543 192 130 475 618 191 693  77  75 456 395 386 697
 434 365  52 124 453 358 463 433 577 677 581 503  83 652 387 390 558 734
 270 449 663 735 160 384 529 546 119 515 507  68 676 538 720 738 147 245
 678 644 509 241 491 254 560 516  57 299 583 471 541 695 733 687 593 512
 474 680 576 426 723 591 548  64 496 380 445 506 534 214 467 740 752 342
 354  76 388 704 209 504 751 331 341 116 258  58 115 452 253 457 213 722
 427 256 283 582 725 350  29 594   0 343 737 391 251 381 151 422 334 333
 378 459 685 694 211 150 595 338 373 308  74 536 465 377 701 210 392 460
 597 276 726 563 131 385 379 420 742 212 730 279 389 580  54 700 432 355
 136 539 375 470 269 703 469 721 531 332 745 140 744 134  99 353 455 590
 530 584 592  91 586 746  27 399 464 727 478 339 578 585 724 425 692 152
 346 458 352 468 732 374  19 747 589 556 153 554 519 553 542 544  22 437
  98  60  28 575 347  97 100 555 698  11 573 559 480 349 688 448  93 250
   4 483 398 547 533 344 419 535 436 479  25 351  87 574 340 252 689 348
 337 527 565 454  63 552 699 526 705 690 450 336 424 482 345 127 517   8
 376 249  10 446 335 129 484 570  12 564 247 439 702 520 706  66 248  61
 466  13  20  62 441 691 523  16  18 435  17 524 518 716 423  23 440 431
 394 133 481 743   5 421 438   6 382 521 259 550 207 260  14 429  21 105
   2   9 525 101 522 428   7 717  24 430 674   3 561  15 128  26 712 263
 104 393 103 549 383 397 718 571 107 710 261 205 262 206 713 265 709 714
 106 396 719 715 551 102 711 264 566 572 749 568 567 569 750]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0320
INFO voc_eval.py: 171: [ 70 273  36 218  49 227  28 248 356 355  73  72 351 216  54 332  80 195
 274 221 353 333  82 229 272  27 279 223 234 354 232 115 224 352 113 324
  29 193 291 119 220 276 233 253 157  78 239 225  77  74 174 219  81 228
  79 336 329 339 345 277  76 323 197  71 326 194 237 231 252 222  30 161
  26  75 247 280 307 250 308 114 198 282 284 156 245   4 318 257 327 286
 163 281 275 328 217   2 259 153 256 235 240 238 131   3 226 342 236 126
 344 338 304 320  10  32 196   7 176 278 249  20 293 319 283 285 243 254
  48 260 294 201 171 300  39 244 120 301 290 341 316 128 251 242 199 116
 287 211   0 105 288  24 289 299 155  46 206 340  50 331 269 246 255  23
 177 241 200 302 207 230 100 309 258 130  19  37 118 335 148 167 179 312
 262 209 314 270 129   6   8 295 139 202  31 297 271 132   5 215 168 182
 103 146  34 150 337 213 325 313 263 159 178 151  21 296  33 173 154 303
 152 343 267 145 311 133  14 102 181  41  40 138 175   9 268 122 121 185
 203 306 124  93 298 266 292  13 261 346  94 265  17 123 125 106 310 264
  69 208 322 189 350  38 180  61 205 334  68 317  98  42 165  22 204  66
 117  91 321 210 141 162  12  60 186 135 190 166 212 305 101  59 315 172
  92 107 188 136  63  58 110  96  18 191 349  62 183  97  64 348  16  57
 127 111 169 140  51 137  99  95 142  67  11  65  53 112 184  44   1  45
 170  55 347  90  15 164 143  52  43 149  25 109 104  35 160 144 134 192
 158 330 108 147  56  88  84 214  89 187  86  85  87  47  83]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3120
INFO voc_eval.py: 171: [ 625 2033 1017 ... 5795 5788 5796]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0542
INFO voc_eval.py: 171: [1675 1674 1799 ... 2276 2279 2278]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0660
INFO voc_eval.py: 171: [574 579 747 557 358 527 361 363 744 687  18 572 292 539 296 681 603 316
 581 591 530 356 367 308 761 634 615 576 531 719  19  29 587 280 558 295
 608 317 301 618 362 613 364 369 685 293 659 937 304 345  26 695 699 735
 357 588 607 620 595 320 300 283 319 622 757 697 693 162 957  20 309 386
 311 548 686 532 804 370  21 583 535 385 767 713 368 566  31 805 312 725
 959 559 562 281 310 724 671 656 727 915 565 734 927 546 569 689 931  35
 895 701  17 315 691 698 313 696 683 637 712 571 751 955 318 623 366 702
 759 728 633 303 526 933 544 302  36 287 812 914 717 601 614 160 708 593
 967 284 733 621 729 964 349 663 137 561 298 594 122 765 567 612 690 956
 598  24 632 666 721 314 920 606 692 803 700 135 568 610 766 639 711 874
 360 738 726 703 741  25 960 611 710 609 600  30 462 911 297 286 762 878
 742 597 941 359 707  34 971 365 547 739 450 665 348 172 748 963 152 868
 871 901 694 435 578 754 709  40 290 534 769 580 753 494 158 821 646 817
 235 918 291 469 969 635 299 882 743 705 825 745 809 877 872 718  27 540
 640 643 970 570 750 584 816 294 146 704 156 466 134 720 831 543 592 433
 673 783 487 171 884 771 457 866 923 604  33 599 755 529 770 279 128  22
 965 434 537 938 794 258 644 706  28 657 149 586 153 470 760 764 563 379
 668 688 732 144 864 897 490 723 758 763 353 545 819 954 589 714 497 148
 810 244 756 813 476 448 289 573 869 808 617 730 335 842 740 285 807 150
 452 898 241 619 832  37 528 495 863 958 602 664 682 867  23 684 722 826
 575 776 873 880 236 254 736 660 961 798 746 577 590 925 768 715 752 616
 793 560 451 474 749 596 841 737 858 564 731 237 716 837 538 876 896 282
 288 822 123 161 605 585 381 782 853 651 789 211 140 836 856 582 477 658
 885 942 488 445 491 136 541 865 645  32 669 777 814 496 949 138 881 206
 184 482 443 661 463 334 815 781 240 650 187 337 648 257 432 791 839 455
 649 249 480  41 870 139 953 483 165 460 246 380 906 186 928 247 164 892
 147 672 481 190 182 208 926 142 542 921 189 185 461 647 662 188 177 169
 827 655 108 124 255 533 790 536 510  74  78 485  16 259 549 913 824 875
 328  72 642 155 471 245 468 946 168   7 667 145 908 131 792  54 818 846
 178 243 444 467 893 352 501 479 406 305 327 830 212 242 486 430  15 475
 326 894 785 166 489 129 472 909  44 133 950 473 857 219 446 151 829 154
 210 512 407   3 924 945 943 395 966 484 465 256 786 478 250 464 442 163
 261 780 449 331 447 962 459 260 951 919 883 922  75 157 441  94 851 341
 806 515 879 521 181 936 141 102 779 239  47 500 910 207 502 209 226 234
 625 833 902 179 170  73 835 394 180 670 340 344 838 940 456 125 374 458
 947 251 248 855 215 183  48 429 636 899 266 347 238 143 514 900   0 126
 820 428 904 410 811 176 498 935 230 844 843 930 499 355 823 552 173 346
 404 840 109 917   2 268 332 523 354 939 828 325 626 351 834 101 454 306
  93 852 106 509 797 253 624 412 641 274 175 132 652 220 416 654 110 511
 218 338 787 378 503 932 854 174 350 555  98 508 517  76  46 127  71 408
 638 333 431  95 330 216   9 507 513  45 516 377 396 120 398  91 974 329
 518 916 393 505 504 107 929 391 231 130  11   1 372  59 629 859 653   4
 850 343 205 217 228 167   6 944 968 556 397 392  12 934 390 427  42  14
  87 972 411 159 912 903 973  13 414   5 905 384 907 630 778 952 520 342
 204 339 336 554   8  10 425 525 796 847  92 627 861  96 628 383 273  99
 453  88  97  89  77 492 775 506 553 860 100 522 399  90 848 524 801 265
 948 437 436 307 267 519 631 269 264 849 799 845 263  62  43  61  60 262
  66 388 373 375 112 772 277 788  68 389 405 784 415 227 862  52 795 213
 270 225 376 252 276 774 409  79 413  69 439 426 387  39 382 271 800  82
 440 438 201 890 802  38 493 103 550  53 420 118 214  50  63 278  70  64
 422  67 403 223  49 272 773  51  65 105 121  85 891 423 889 551 322 400
 401 674 402 224  86 113 104 275 233  83 116 221 222 202 424 421 888  81
 111 232 114 119  57 117  80  56 229 680  84 887 321 417 419 115 418 203
 678  58  55 192 191 675 324 679 323 886 200 677 197 371 193 676 195 199
 198 196 194]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1669
INFO voc_eval.py: 171: [301 114 110 325 105 121 143 136 309 116  33  73  71 317 103 106 122 109
 111 142 315 316 303  36 104  34 108  82 144 112 326 307 102 318 304  37
  44  86 308 113  40 125  30  45 324 150  76 118 320  70   7   6 107  74
 314  41  58   0  38 305 153  60  91 148 289  79  81 232 233  50 257 147
  94 195  55 197 310  85 196  46  77 302 243   1  69 337 245 145  62 234
 207 127 286  67  75 115 285 138 241  72 331 294  32 282  48 251  83 129
 133  31 280  51   8 126 131 254  84 332 151 321 132 334 287 341 152 120
 327 281  80 284 283 154  78  39 193 119 306 157 141 208  28 164 246 247
  59 140 194 342 206  56 149 248 312 139 242  63  64 255 319 221  47 202
 137 238 199 230  68 130   5   3 279  66 159 165 239 198 231  65 146 212
 205 288 293  95 236  29 222 134  97 124 190  15  61 123 290  42  52 295
 311 253 250  11 191 192  35  19 218  92 135   9 329 117  54  57 219  49
 128  43  10 211  24   4 292 174  53 323 256  18 215 203 330 237 188 249
 291 240 339 252 101 235  98 201  12 189 225  20 214 229 200 244 296 313
 298 224 278   2 156 213 269 170 100 336  96  89  21  90 176  99 155 268
 217 338 209 223 220 333  87 226  93 158 272 169  88 322 216 177 162 210
 266 168 335  25  26 228  27 204  16 328 340 277 227 161 173  13 299 271
 300  22  17  23  14 297 270 259 274 262 167 273 276 260 179 183 265 181
 267 263 175 163 166 275 258 160 171 261 172 264 178 182 184 186 187 180
 185]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2695
INFO voc_eval.py: 171: [2212 1122 1570 ...  698  697 2508]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1641
INFO voc_eval.py: 171: [106 114 110 109 253 139  13 198 112  99 105 254 255  34  16 107 140 252
  24  31 257 164  29  74 111 195  19 165 229  30  80 137 108  79  98  73
 136 231 260 163 172 233  32  36  33 220  12  83 168 157  17 166 207  85
 148 256 221 259  96 171 103 154 160 251 104  20 258  43 212 170  75 167
  72  95 189 218  27 206  67 169 228  28  18 138 135 100  21 117  44 235
  35 223  82 156 208 230 115 232  76  56 188 190  70  15  86  71  22  64
  11  62  23  87  60 234 116 216 210  14  78 217  65 215 201 222  25 193
  68  69  26  63 130  66  49 129 242  46 132 191 173 224 214  61 159 133
 243 211  59 219  97  54 240 192   4 161  45 153 131 213 186 134  58 155
 118  55   9 202 101 194  52 128 185 158  47 181  48 102 184 239  90 122
 199  88 120   0  84 152  53   5  57   2 205 209 123 127 126  81 119  50
  89  77  42 162  92  10 203   7   6  93   1 177 175 237 196  94 197 187
 113 176   8 241   3 244 182 124 236 245 178 149 174 150 147  91 226 225
 200 204 227 179 125 238 249 248 250 180 247 246  37 146 144 143 183  51
 141 145 142  39 151  38 121  40  41 261]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0316
INFO voc_eval.py: 171: [2079  589  362 ... 2728 2729 3607]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1103
INFO voc_eval.py: 171: [108 377  82  94 411 113 320 413  96 506 376  84 104 257 394 378 403 101
 369 158  95 470 243 291 679 379 410 114 170  20 684 186 293 250 512 268
  49 164 294 392 383 850  21 487 663 284  87 460 368  50 471 322 187 190
  83  75 287 154 689 319 801 405 511 246 461 503 406 768 178 223 219 244
 273 278 279 834 261  99 407 467 324 425 171 222 468 181  37 221 686  34
 141 109 302 548  11 330 253 480 285 510   3 242 387 153 218 240  65  85
 313 477 536 434 252 473 274 185  92  55 758 208 832 847  61  47 436 371
 197 388 845 194 507 151 509 573 200  68 463 328 292 266 822  44 226 156
 749 248 544 202 836 180 668   2 485 863 501 256 773  81 374 690 839 157
 306 442 667 504 683  76  46 301 161 149 300  63  59 155 191 558  80 688
 587  41 112 732 168 286 519 152 184 267 381 103  48 175 162 332 159 307
 311 823 450 408 761 687 670 838 584 174 205 763 681 428 398 127 277  24
 160 249  64 505  16 370 177 748  77 272 333 853 206  58  51 570  13 730
 502 486  28 835 446 247 318  10 385 824 815 520 500 523 142 271 217 561
 380 296 458 401 508 121 441 288 540 531 188 255 126  69  54 298 430 258
  42 528 201 856 148  18 263 229 239 445 481  43 269 766  53 145 598 209
 674 254 821 236 373 852   7 864 762 402  30 778 752 665 386 578 802 289
  40 459   5 547 295 811 482 704 759 283 427 803 203 669  26 586 395 245
  70 484 262 429  60  27 596 448 813 476 192  36 259  86 848 525 833 685
 664 849  15   1 297 543 729 728  62 270 172 100  35 770 391 390 483 207
 675 404 426  57 574  72 199 563 859   4  12 214 169 396   6 393 210 673
 555 676 173 329  45 409 530 862 737 855 211 220 234 753 464 276 840  91
 107 533  17 820 147 432 135 516 412 290 806 810 265  74  32 316 764 340
 125 124 772 590 582  33 866 384 166 817 120 771 472  78 819 680 775 843
 736 751 741 541 844 765 345 251 825 807 816 585 230 454 299 315  29 785
 529 139 814 854 375 342 424 858 776 808 462 389 372 735 818 812 327  52
 130 134 857 809 734 538 282 754 304 382 140  66 846 204 314 557 400 129
  39 706 167 466 542 308 457 774 338 539  67 264 515 361 275 594 552 437
 569 777 310  31 589 303 571 517 136 317 478 475 677 534 861 750 581 837
 399 131 280 137 143  79 599 189 341 182 352 757  73 526 225 241 281 620
 568  38 440 321 397 453 431 117 150  56 469 325 224 309 769  71 176 133
 474 671   8 851 588 860 842 144 865 132 782   9 716 138 260 305 331  25
 334 326 618 703 195 740 146 105  88 760 779 312 567 551 744 198 742 708
 717 826 841 213  19 465 731  14 780 733 116 580 479  23  22 231 615 566
 179 756 784 323 455 577 595 672 355 830 128 532 597 433 336 600 348 629
 438 215 593 755 518 216 237 616 123 227 435 743 624 233 196 238 193 232
 591 559 697 623 781 635 630 439 165 602 666  89 493 119 335 456 682 212
 783 451 767 699 579 339 447 452 443 122 658 831 562 183 449 106 575 228
 678 554 710 444 612 632 337 235 343 367 712 601 349 691 346 829 118 358
 350 705 659 592 747 745 827 660 359 828 495 353 535 715 521 362 583 621
 633 419 622 354 614 662 363 692 365 344 111 420 661 351 347 576 560 513
 360 364 613 564 628 713 537 694 357 625 696 549 546 627 527 356 800 522
 693 163 416 366 617 619 739 634 610 631 553 494 611 556 626 545 791 524
 643 799 550 707 869  90 724 711  93 795 797 709 110 746 514 796 792 417
 718 638 642 789 786  98 652 421 790 637 868 418 695 720 794 714 422 102
 423 700 798 793 572 639 636 701 645 788 787 702 640 727 723 603 654 498
 115 698 644 641  97 721 725 492 491 650 649 609 604 646 499 719 738 726
 647 722 605 607 608 606 488 490 489 651 415 656 655 657 648 497 653 565
 496 804 805   0 414 867]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0714
INFO voc_eval.py: 171: [1372  594  859 ... 1528 1523 1525]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2248
INFO voc_eval.py: 171: [246 278 175 259  64 282  61 244 264 290 275 286 220  67 235 170 289 173
 254  72 251 206 172 258 127  65  81 394 224 252 203 461 495 398 395 497
 273 167 257  62 280 266 476 223 460 478 512 237  58 399 221  85 477 380
 222 390 462 187 248 502  48 129 296 176 179 466 523 474  66 500 132 180
 236  89  74  51 459 238 467  50 194  87  84 400 397 128 515 285 104 480
  24 105 464 202 130 471 381 253 242 411 475 379 260 283  68 329 504 465
 479 439 181 392 469  28 472 191 297  76 150 148 402  77 506 393 178 458
  35 310 463 510 414 188  73 382 468 217 192 174 451 126  49 452 470 412
  83 299 520 332  75 372  57 218 517 519 359 245  78 268 488 410 518 361
  82  45 131 124 274  52 166 133 482 250 417 528 420 487 204  33 358 499
 302 247 486 493  31 210 158  70  54 149 328 483 360 341 401 168 214 306
 219  86 144 473 190 189 281 134 284 387 409 485 279 533 115 501 103  29
 241  56 101  27  23 207 494 215 205 255 123 300 303  71  44 484 269 418
 169 438 351 314  88 385 270 165 532  42 421 337  59 525 312 195 125 391
 389  55 489 434 416 243  30 171 413 432 304 182 535  91 154 531 177 336
  17 357 374  60 141 208 287 415 425 118 330 446 431 313 256 356  79 183
  63 441 349 338 405 386  80  90 326 305 342  19 107 384 524  69 406  53
 311 447 239 340 301 298 404 143 333 377 119 442 378 327 334 508 193   5
 102 137 354  32 240  34 163 106  18  41 396 315 263 383 153 316 325 249
 111 427 108 513 344 136 110 213 112 147 443 440 335 140 164 142 331 343
 272 339 201   4 521 309 113 423   3 345  38 276 511 355 117 116 362 376
 509 114 522 135 375 184 403 109 445 368 350  95 277 122 152   9 120 139
 527 151 373 145  43 419 347 265 216 428  10 226 371 157 370 516 121  39
 529 352 430 448 233 161  13 505   1 348  94 146 100  40 185 162   7 138
 317 514 498  96  98 225 231 450  14 507 211 197  21 530 267  99   6 346
 209 196 526 369 408 534 496 307 288 228   2 481  20 367  15 388 232 503
  47  97 271 156 429 160 422 308 155 449 212  12  46  16 159 454  11 453
 424 234 227 426 230  22 186 353 433 229 457 407 291 455 318 436   8 444
 365 456 294 292 321 295 293 319 366 323 437 363 364 200  93 320 199  25
  26 435   0 261 322 198  92 492 324  36  37 262 490 491]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0506
INFO voc_eval.py: 171: [945 110 843 ... 967 353 817]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0598
INFO voc_eval.py: 171: [208  34 127 125 128 219  38 205  31 165 212 206  15  35 118 211  33 220
 126 171 221 217 162  36 207 132 130 137 136 120 114 216  60  94 214   3
  32 176  13 133 135 225  98  61 195  54  59 134 222 129   1 215 149  70
 210 190 111 138 213  19 131 226   7 169  85  14 112 174 223  43  95 204
  49 105  64 184  18 193  86  84 103  52 168   8  51  93 141  66 143   6
  45  20  87 192  67  11  46 227 181 197 170 185 178 177 218 224 200  48
  17 164  42 198 172  16 151  53  68 156 191   4 209  72  65   2 194 104
 180 189 173 116  57 179 122  47  73 182  40 175 201 123  44   9  77 148
 196  62 117  50 199 106  39 229 166 142 153 101 139 167  71 163 102 183
  63  96  97  55 147  75 234  99 110 231 230 115 233 140 108  10  58 107
  30 121 150 232  81  41 100  37  92   5  56 228 158  69 160 235 237 154
  79 186  12  74 109  22  76 203  80  83  78 188 119  82 236 124 155  27
 187   0 146 145 113 144 152  90 157  26  29  89  91  25 159  21 161  24
  88  23  28 202]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3842
INFO voc_eval.py: 171: [ 7864  7151     0 ... 16814 16813 22692]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.2173
INFO voc_eval.py: 171: [ 222  225  242 ... 1813 1812 1810]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2242
INFO voc_eval.py: 171: [ 74 477 204  63 512 526 891 198  48 460 505  50 458 914 503 134  32  45
 497 135 185 907 876  41 514 494 189 451 911 486 897 513  72  43 515 516
 466 448  38 131 199 520 705  34 873 193 895 501 881 449 459 203  77 509
 498 871 832  44 885 183 470 480 534  57 575 447  30 187  58 186  67 954
 454 899 229 180 469 195 132 225 473 890 465 455 912 872 522 818 495 740
 963 177 785 450 467  64 510 176 121 506 331 194 130 524 391 879 464 728
  61 499 888 457 975 474 835 913 892 882 124 529 886 493 519 901 483 165
 233  73 462 724 489 504 883 452 518 525 184 453 535 133 485 874 191 484
 960 842 644 500 889 475 528 567 943 521 471 841 181 733 463 648 837 218
 478 530 173 645 968 481 222 809 123  55 202 909 944 456 378 898 562 833
 472 461 533 179 531 523 249 260 400  36 949 487  52 508 398 136 488 843
 507 938 197 570 235 646 965 227 395  53 170 827  69 491 527 802 217 978
 840  54 971 389 828 476 550 950 250 240 200 468  65 496 490 128 226 574
 182  40  39 814 517 192  60 554 397 532 479 571 976 511 492 190 829 157
 804 822  42  47 946 887 482 937 502 649 312 557 836 727 313 228 893  35
 162 962 793 159 903 239 805 137 647 940 310 799  56 188 831 878 558 650
 964 808  22 381 811 980 573 396  70 566 952  71 741 247 906 838  37 806
 565 797 246 201 224 877 900 223 572 415  79 844 253 905  76 362 953 307
 164 196 972 175   4 166 221 258 939 125 264 376 167 815 219 821 311  68
  24 266  46  26 561 973 569 786  66 955 967 737  78 957 358 803 725  29
  51  27 969 158 160 819 127  49 169 813 541 168 220 560 599 236 956  28
 880 796 304 769 947 974  33 155 245 896 839 402   5  62 241 161 942 729
 734 979 736  25 807 129 357 263 300 739 172 251 790 742  31  59  23 207
 242 875 174 216   6   7 800 941 171 315 243 163 230 154 966 959 970 549
  75 764 303 299 120 344 958 316 178 948 399 576 820 961 789 904 139 801
 122 138 539 257 884 977 215 543 817 296 951 255 314 932 735 265 945 536
 153 730 597 816 559 375 788 126 795 333 830 334 274 738 364  99 208 732
 556 392 731 261 269 259 704 291 787 826 794 812 156 234 726 348 148 231
 151 544 305 564 262 354 834 791 823 302 309 248 347 306 542 810 143 553
 798 209 361 254 377 825 351 117 308 301 824 792 632 152 149 360 297 119
 580 232 547 548 298 150 355 244 237 706 675 339 546 424 345  97 691 365
 252 552 349 338 540 405 256 238 106 268 404 774 568 363 147 353 356 642
 205 290 627 329 781 694 118 783 379 292 538 423 277 635 709  15 270 681
 665 350 563 595 417 272 714 908 630 614 359 551 383 545 330 639 537 692
 271 342 555 426 703 748 346 382 413 422 352 104  94 411  96 380 388 142
 870 416 384 385 394 633 991 337 617 765 212 276 145 767 699 267 336 275
 677 672 700 425 696 715 273 869 995 779 782 583 579 387 780 852 419 324
 664 868 581  98 784 140 671 695  95 414 856 211 386 403 657 327  19 993
 210   9 214 408 894 144 722 651 697 146 853 418 427 213  17 409 587 778
 772 343 857 659 851 340 652 847 407 406 698   8 743 612 421 610 641 678
 625 412 710 640 578 626 936 332 335 420 107  21 749 637 655 716 393 992
 638 613 708 990 935 341 577 902 768 701 910 390 994 861 702 934 713 636
 723  89 582 933  12 773 717 860 654  11  14 293 629  87 673 600 848  84
 410  10 618  20 858 711 294 750 631 721 585 658 620 707 102 295 608 686
 621 111 619  13  16 105  18 622 112 114 850 684 927 846 656 115 854 596
 108 322 845 855 674  82 859 592 110 688 101   3 744 653 712 928 849 589
 766 745 917 598 321 586 100 685 323 682 206 916 325 326 590 604 611 660
 109 931 690 624 666 615  83 103 866 401 918 718  81 609 746 602 919 368
 865 929  93 623 372 930 683 920 584 719 747 693 289 628 605 667  86  85
 643 594 661 662 770 663 116 317 720 603 606 771 591 601 689 439 669 320
 616 607 367 593 862 634 319 915 366 434 588 141 370 687  80 318 864  91
 668 867 863 435 445 921 374 371 984 670 373 679 436 777  88 924 676 775
 925 369  90 680 776  92 926 284 440 443 438 923 281 437 287 328 283 987
 285 113   2 288 433 432 430 442 431 922 428 280 282 429   1 444   0 286
 441 446 279 756 763 985 982 760 753 754 758 757 986 278 989 762 755 752
 983 759 981 761 751 988]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1012
INFO voc_eval.py: 171: [ 68   8  65  66 217 216  77   9 303  76  59 338 159  69  63  92 125 287
 293 340 289 322 288 330  60  89 265 291 129 267 323 299 121 296  90 105
 305  23 264 106 252 300 331  56  73  64  11 203 290 294  57 286 295  71
  61 205 253 207 161 297 150 131  70 200 202  96 120 285 292 266 133  24
  72 268 298 130 123 304 302 337  67  62 284 269 346 301 132 113  19 175
 306  94 148 188 251 354 107 321  10 137 128  74  87 206 160 127 186 254
 263 112 184  99 108 250 257 146 138 109 221 143 212 122 344 241 136 255
 208 126 124 139  13  98 178 256 259 145 351 179 196 352 110  88  75 135
 111 162 157  58 197 231 142 155 180 182   3 134  17 220 356 195 194 192
 334 191 177 223 271 181 262 167 244 236 327 190  32 152 222 277  38  18
 278 204  31  36 283 238  91 333 176 260  79 173 224 339  16 345 185  14
 225   5 201 193  35 227 149 183 118 170 189 326 119  39 315 156  33  20
 154 210  15 187   4 261 158 316 101 247  97 198  93 209  12 199 282 104
 343  80 280 169 153 219 332 355 243 211 242 103 117   1  34 281 353  81
  95 168  86 279 151 233  27  41  29 147 348 102  78  28 349  30 218 347
  83 342 232  21 350 100  54   2 214  49 239  40   6 235 341 234 335 328
 228  51 246  55 245 144  84  42 325 226 240  50 270  82 312  44 237  85
  52 324 336  37 329   7  43  45  53 163 172 171 229 248 165 164 166 174
 314 141 308 258  25 313 213   0 215 320  26 273 272 311 249 307  22 116
 309 317 310  46 318  48 319  47 114 115 230 140 274 275 276]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1169
INFO voc_eval.py: 171: [478 486 484 482 291 491 479 483 296 481 663 350 477 499 664 656 488 480
  51 187  49  45 189 618 293  57 485   5 183  63  74 302 100 295 505 290
 292 627 523 710 507 262  92 487 704 699  71 113  47 322   8  44  52  46
 345 527 667  61 701 353 363  81 495 103 115 502  10 119  85  70 104 517
 490 494  43 152 700  55 267 620 553 651 306   6 489 190 655 346 657 349
  68 621  69 274 270 361 711 153 300 571 118 355 647 192 497 622 168 662
 698  48 518 697  79 316 617 211 191  54 120 209 630 658   7 713 289 264
   4 263 107 708 285 112 702 365 320 643  82 359 358 275 184 122 216 670
 269 314 557 564 559  56 649 714 596  23 229 707 632 318 588 344 525  13
 351  91  58 554  62 117 558 641 666 154 305 576 108 616 139 515 661 348
 496 151 574 705  80 623 127 639 493 561 631 555 566 678  59 101  72 380
 142 105 514  96 143 716 130 308 169 319 362  66 660 530 245 519  64 301
 125 506 170 443 715 516 579 246 167 512 323 556 372 706 156 159 161 508
 619 155 382 528 680  50 509 412 409  86 712 575 577 317 668 513 492 578
 369 203 563  28 597 221 235 265 674 565 126 615 307 637 614 562 354 222
 364 367 124 625 587  21 684 123 210  99  12 560 659  31  88 244 196 165
 146 186 584  78 669 197 294 158 297 243 198 215 121 429 173 172   3 423
 266 436 357 287 709 534 157 249 569 546 411 582 624 526 572 321  35 238
 703 102 524 653 131  84 207 144 535  24 208 329  18 268 529 638 226 568
 652 109 128 116  53 356 347 368 279 247 114 371 629  76 498 140 180 421
 239 236 248 312  77  65  95 240 415 370 599 162 281 182 673  27 164 677
 541 366 720 204 510  90  11 136  17  98  87 181  25 213 134 642 418 567
 133 570 403 166 309 444 234 324 328 384 188 111 580 199 675 373 595 606
 589 600 573 352 531 132  14 194 395  75 110 206 135  32 503 227 325 635
  94 273 205 230 360  30 193 195 185 376 137 636 501  22 500 430 225 648
 591 607 201 272 594 315 467 223 585 539 299 428 628 682 141 282 447 138
 214 398 601 634 402 583   2 394 671  83 419 469 145 231 276 334 422 598
 470 336 129 179 200 696 420 202 147 106 456  67 330  60  93 337 626 463
 633 581  42 461 644 645  37 460 405 417 311 433   9 432 177   1 431 257
 224 464 590 406 681 458 650 547 665  40 521  89 212 260 695 259 338 511
 646  97 326 261 228 586 404  39 593 640  73 303 241 178 232 721 288 592
 331  29 375 434 538 690 175 475 543 462 608 693 332 691 536 313  34 683
  41 176 339 689 278 548 654 377 327 160 174 342 446 676 335 286 672 426
 340 333 537 471 341 722  16 442 452 441 233 416 171 407 425 437 310 427
 424 612 280 304 602 439 150 385 522  33 679 401 455 520 277 284 383 242
 381 694 219 551 220 237 459 379 163 378 408 692 283 271 258  38  26 393
 374 542 149 410 438 396  36 457 466 610 298 605 413 613 414 255 533  20
 453  15 397 609  19 540 465 448 148   0 445 552 549 611 449 435 532 454
 545 544 550 387 603 504 474 254 468 440 390 218 217 473 450 451 400 604
 253 476 391 399 717 472 251 252 250 343 389 256 386 392 388 685 687 686
 688 719 718]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0521
INFO voc_eval.py: 171: [515 514 476 447   0 279 501 628 474 612 340 193 239 627 506 560  22 444
 479 260 259 561 188  48 175 280  18   4 248 488 354 277 342 613 257 282
 253 273 264 268  13 443 247 263 636  58 147  11 242 350 270  53 316 451
 471 174 258 134 344 261 490  23 290 343 229 563 445 102 135 351  16 269
 352 629 233 251 384 449   9 614 287 313 108 358 274 275  50 508   3 566
 452 454 584  56 156 232 286   2 498 296 238  37 577 509 507 314 399 568
  17  60 583   7 611 513 187 312 462 346 218 341 395  12 139 450 178 276
 549  86 637  55  49 357 299 621  65 364 288 208 446  31 231 469 379 535
 284 150 500   1  95 240   6 301 142   8 267 511  20  90 210 523 366  70
 512 372 315 377  87 475 146 502 623  10 206 562 470 328 110  41 417 271
 634 338 409 243 241 415  76 137  32 347 466 180 617 359 326 532 149 393
 494  36 615 177 153 143 564 570 281 520 510 226 565 383 221 216 303  44
 245 109 196  51  61 111 423 145  46 289 522  19 575 300 320 319 217 278
  57 567  99  78 244 132  52  94  15 356 184  54 155 123 164 302 112 539
 495 467  47 483 327 349 586 373 345  26 492 633 246 579  88 538 140  14
 548 375 234  75 463 460  29 410 630 516  73 214  84 339 335 144 635 297
 468 632 272 182  21 183 361 100   5  40 127 456  83 227 424  98 225 407
  67 533 610  59 461 325 371 391 422 599 189 401 569 252 541  24 530 464
 114 551 497 211 115 186 324 237 386 631 544 430 588 198 204 412 255  64
  92 457 250 249  79 230 557 540  66 212 141 330 397 368  35 398 120 365
 526 113 118 537 157 209 165 525 215 480 385 626 519 389 148  72 197 625
 104 413 571 620 337 604 638 433 116 185  74 213 534 362 107  85 472 521
 223 527  81 236 179 458 455 265  62 602 428 387 292 481 431 207 536 322
 363 176 531  33 459  91 262 486 219 416 392 306  82 317 106 101 235 283
 103 168 348 518 624 374 594 394  71 496 438 194  38  27 589 220  45 205
  42  77 332 603 552 378 130 353 151 489 499 105 355 593 360  80 485 336
 162 442 381 224 487  93 558 310  28  69  68 601 195 592 285 524 596 598
 369  96  63 370 528 482 559 329 408 158 619 228  89 605  97 574 473 321
 294 367 191 390 190 622 405 172 173 578 304 600 119 388 334 597 595 554
 192 136 163 441 266 222 618 331 318 323 437 129  39 403 122 484 181 616
 376 333 434 203  25 307 380 491 167 138 128 421 585 402 293 477 576 493
 419 529 308 161  43 478 465 256 298 171 291 382 414 117  30 590 425 426
 406 254 435 152 581 202  34 418 154 453 580 131 125 404 121 440 126 547
 160 608 420 133 556 427 432 124 200 201 429 400 582 448 542 309 295 159
 170 439 587 411 550 555 169 166 396 436 305 546 553 543 311 545 503 573
 572 591 517 505 504 609 199 606 607]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1244
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1417
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.312
INFO cross_voc_dataset_evaluator.py: 134: 0.054
INFO cross_voc_dataset_evaluator.py: 134: 0.066
INFO cross_voc_dataset_evaluator.py: 134: 0.167
INFO cross_voc_dataset_evaluator.py: 134: 0.270
INFO cross_voc_dataset_evaluator.py: 134: 0.164
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.110
INFO cross_voc_dataset_evaluator.py: 134: 0.071
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.051
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.384
INFO cross_voc_dataset_evaluator.py: 134: 0.217
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.101
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.124
INFO cross_voc_dataset_evaluator.py: 135: 0.142
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 8999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.399s + 0.029s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.341s + 0.040s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.353s + 0.041s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.357s + 0.044s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.358s + 0.046s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.358s + 0.045s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.353s + 0.043s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.353s + 0.043s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.355s + 0.045s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.357s + 0.044s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.358s + 0.043s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.360s + 0.043s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.361s + 0.043s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.502s + 0.035s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.359s + 0.035s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.368s + 0.039s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.368s + 0.041s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.366s + 0.042s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.367s + 0.043s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.373s + 0.042s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.371s + 0.041s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.368s + 0.041s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.371s + 0.041s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.369s + 0.040s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.371s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.371s + 0.040s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.453s + 0.029s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.361s + 0.048s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.367s + 0.046s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.369s + 0.044s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.370s + 0.043s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.365s + 0.043s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.365s + 0.043s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.367s + 0.043s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.365s + 0.042s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.364s + 0.043s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.365s + 0.042s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.364s + 0.041s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.365s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.390s + 0.030s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.367s + 0.042s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.360s + 0.042s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.357s + 0.043s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.364s + 0.041s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.365s + 0.041s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.369s + 0.043s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.366s + 0.042s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.369s + 0.042s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.368s + 0.043s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.364s + 0.043s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.362s + 0.042s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.362s + 0.042s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-00-39-51_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.308s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [226 323  84 216 313 601 122 597 158 607 194 631  80  48 598 602 603  49
 312 215 137  55  90 619 171 199 272 317 220 190 639 184 636  95 605 168
 164  56 632 141 228 325 118 326 229 185 315 218 266 142 154 202 146  31
 647  72 227 324 646 289  32 143 221 318 671 668 599 230 327 616 281 284
 623 650 600 626 145 620 307 322 225 656 667 139  46 369 366 172 297 268
  53 293 410 447 243 242 138 271 657 234 121 654 609 416 309 625 618 282
 621 291 237 135 665 319 222 208 287 219 316 183 244 356 302 729 408 217
 314 362 277 303 505 628 306 604  33 401 275 608 198 285 685 167 178 173
 414  40 402  73 748 661 117 159  39 653 501 169 641 240 224 321 655  42
  89 648 649 644 204 490 669  36 170 305  43 498 123  92 633 363 642  34
  35 741 290  79 109 508 645  81 472 615 157 125  50 310 193 403  37 540
 196  44 286  51 409 368 627 197  38 301 370 274 528  85 156 461 361 610
 186 493 311 417 681 203  59 670 486  96 637 587 177 166 502 238 267 492
 499 674 500 557 586  82 664 443 411 296 232 329 239 295 188 201  45 114
 273 707 407 415 412 622 174 418 497 494 148 613 330 233 614 728 328 231
 683 182 510 672 635 367 288 473 189 663 513 357 179 320 223 731  67  65
 405 280 406 372 413 739 108 640  70 511 246 400 112 444 364 404 180 163
 144 298 155 278 126  71 176  30 451 294 149 120 195 235 682  88 162 255
 111 175  86  69 161 658 110 360 495 678 462 442  78 532 624 660 652 545
 257 537 113 489 638 487  94 292 165 736 561 606 485 595 476  47 659 488
 359 514 236 181 371 200   1 708 680 612 630 611  41 629 666 132 300 634
 695 187 477 543 578 304 192 130 475 617 692 191  77  75 395 696 386 456
 434 365 124  52 433 453 358 463 676 580 576 503  83 651 387 390 558 734
 662 270 449 160 384 735 546 529 515 119 507  68 675 538 720 147 738 245
 677 643 241 509 491 254 560 516 299 582  57 471 541 733 694 686 592 512
 474 679 575 426 548 723 590  64 496 380 445 534 506 214 467 740 752 342
 388 354  76 504 704 209 751 331 341 116 258  58 115 452 253 213 722 457
 427 256 283 581 593 350 725  29 737   0 343 391 381 251 151 422 334 333
 378 459 684 693 211 594 150 308 337  74 373 536 465 701 377 392 210 596
 276 460 726 385 379 131 562 420 742 279 389 212 730 579 699  54 136 432
 355 269 470 531 375 539 721 469 332 703 744 140 134 745 589  99 353 455
 583 530  91 591 585 746  27 399 464 478 577 339 727 724 691 584 425 458
 152 352 346 374  19 732 468 747 588 554 153 556 519 542 553 544  22 437
  98  60 697  28 574 347  97 100 555  11 572 559 480 349 687 448  93 250
   4 483 547 398 344 533 535 351 479  25 436 419 340 252  87 573 688 527
 348 338  63 454 705 564 552 698 526 689 424 450 336 482 345 517   8 127
 376 249 335 484  10 446 129  12 569 563 702 520 247  66 439 706  61 248
 466  20  62  13 441 690 523  16 700  17 524  18 435 716 440 423  23 518
 481 431 133 394 743   5 421 438 382   6 521 259 550 207 260 429  14 105
  21   2   9 522 525 101   7 428 430 673 717  24   3 128  15 712  26 263
 393 104 103 549 710 397 718 383 107 570 205 261 206 713 262 265 396 714
 106 709 715 719 102 551 711 264 565 571 749 567 566 568 750]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0318
INFO voc_eval.py: 171: [ 70 272  36 217  49 226  28 247 355 354  73  72 350 215  54 331  80 194
 273 220 352 332  82 228  27 271 278 222 233 353 231 115 223 351 113 323
  29 192 119 290 219 275 232 252 157  78 238 224  77  74 174 218  81 227
  79 335 328 338 344 276  76 196 322  71 325 236 193 230 251 221  30 161
  26  75 246 279 306 249 307 114 197 281 283 156 244   4 317 256 326 285
 163 280 274 327 216   2 258 153 255 234 239 237 131   3 225 341 235 126
 343 337 303 319  10  32 195   7 176 277 248  20 292 318 282 284 242 253
  48 259 293 200 171 299  39 243 120 300 289 340 315 128 250 241 198 116
 286 210   0 105 287  24 288 298  46 155 205 339  50 330 268 245 254  23
 240 177 199 301 206 229 100 308 257 130  19  37 118 334 148 167 179 311
 261 208 313 269   6 129   8 294 139 201  31 296 270   5 132 214 168 146
 182 103 150  34 336 212 324 312 262 159 178 151 295  21  33 173 154 152
 302 342 266 145 310  14 133 102 181  41  40 138 267 175   9 122 121 185
 202 305 124  93 297 265 291  13 260 345  94 264  17 123 125 106 309 263
  69 321 207 189 349  38  61 180 204 333  68 316  98  42 165  22 203  66
 117  91 320 209 142 162  12  60 186 135 190 166 304 211 101  59 314 172
 107  92 136  63 188  58 110  96  18 348  62 183  97  64 347  57 127  16
 111 169 140  51  95 141  99 137  67  11  65  53 112 184  44   1 346 170
  55  90  45  15 164  52 149 143  43  25 109 104 160  35 144 134 158 191
 329 108 147  56  88  84 213  89 187  86  87  47  85  83]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3120
INFO voc_eval.py: 171: [ 625 1035 2031 ... 5812 5763 5787]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0502
INFO voc_eval.py: 171: [1676 1675 1800 ... 2279 2875 2278]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0650
INFO voc_eval.py: 171: [575 580 360 357 558 748 528 362 745 688 604  18 592 315 682 531 582 540
 295 573 291 635 616 307  19 532 762 720 577 355 366 609 588 279 559 294
  29 316 618 292 300 686 361 368 363 614 344 660 303  26 696 937 356 700
 620 595 299 736 319 608 589 282 318 385 958 308 694 758  20 623 698 162
 310 687 805 533 369 549  21 536 384 714 768 584 311 567 806  31 726 367
 960 563 560 672 280 725 309 728 916 656 928 735 547 566 690 932  36 570
 896 702  17 692 314 697 752 312 699 624 684 572 317 637 956 713 760 703
 934 729 527 365 302 634 545 301  35 286 915 813 718 602 615 161 709 594
 968 734 283 965 622 348 730 137 562 664 297 596 568 122 766 691  24 599
 613 957 722 667 921 313 633 607 693 611 135 701 767 569 804 704 639 712
 359 727 875 739  25 742 711 612 961 610 461 601  30 285 296 912 763 743
 598 942 879 358  34 708 972 740 548 364 666 449 347 171 749 964 869 152
 902 872 695 434 755 579  40 710 770 289 581 535 754 822 159 494 818 646
 290 234 919 469 970 636 883 298 744 826 706 746 810 719  27 873 878 571
 640 643 541 971 751 817 293 585 705 146 157 465 134 832 721 593 544 432
 674 487 784 170 885 771 456 867 924 605  33 756 600 530 772 278 128  22
 966 433 538 939 257 644 795 707  28 149 657 587 153 470 761 765 564 378
 669 733 689 898 865 144 490 724 352 820 759 546 764 955 590 715 148 497
 811 243 757 814 476 447 574 288 870 809 731 619 334 843 741 284 808 451
 150 899 240  37 833 621 529 864 495 959 665 603 683 868  23 685 723 827
 576 777 874 235 881 661 253 737 799 747 962 578 591 926 769 716 753 617
 561 794 474 450 750 597 842 738 859 236 732 565 717 539 838 897 877 281
 287 823 123 163 606 586 380 783 651 854 790 210 140 837 857 583 477 658
 886 943 488 491 444 136 866 542 645  32 671 815 778 496 950 138 882 205
 183 442 482 662 462 333 816 782 239 186 650 336 648 256 431 840 792 454
 649 248 480  41 871 139 483 953 166 459 245 907 379 185 929 246 893 165
 147 481 673 189 181 207 927 142 543 922 188 184 460 663 647 187 176 168
 828 655 108 124 254 534 791 537 510  74  78 485  16 258 550 914 825 876
  72 327 642 471 155 244 468 947 167   7 668 145 909 131 793  54 819 847
 177 242 443 894 467 351 479 501 405 304 326 831 241 211 486  15 429 325
 475 895 786 489 129 472 910  44 133 951 473 858 218 445 151 830 154 209
 512 406   3 925 946 944 394 967 464 484 255 787 478 249 652 463 441 164
 260 781 330 448 963 446 458 259 952 920 923 884  75 440 158  94 852 340
 807 515 880 522 180 938 141 102 780 238  47 500 911 206 208 502 225 233
 626 834 903 178 169  73 179 836 670 393 339 343 839 941 455 125 457 373
 948 250 247 856 182 214  48 428 900 265 346 237 143 514 901   0 126 427
 821 905 409 812 175 498 936 229 845 466 499 844 931 354 824 172 553 345
 403 841 109 918 331 524   2 267 940 353 324 829 627 835 350 101 453 305
  93 509 106 853 798 252 641 273 625 411 174 132 653 219 415 110 217 788
 503 933 377 511 337 855 173  98 556 508 349 517  76  46 127  71 407 638
 332 430  95 329 215   9 507 513  45 516 395 397 120 376 975 392  91 328
 917 518 505 504 930 390 107   1  11 230 130 371  59 654 630   4 851 342
 659 860 227 216 204   6 969 945 396 557 389  14 391  12 426 935  87  42
 160 410 973 904 974 913  13 413   5 908 906 383 631 156 779 341 954 521
   8 335 203 555 338  10 526 424 797  96 862 628  92 848 629 382 272  99
 520 452  89  97  88 492  77 506 776 861 554 398 523 100  90 849 525 264
 949 802 436 306 266 435 519 268 263 632 850 846 800 262  62  43 261  60
  61 387  66 372 374 112 773 276 789 404 785 388  52  68 226 414 863 212
 796 269 224 375 251 275 408 775  79 438 412  69 425 386  39 381 270 801
  82 439 200 803 891 437 493  38 103 419 213 551 118  53  50  63 277  67
  70 421  64 271 222 402  49 774  51  65 105 892  85 121 422 552 890 321
 675 399 223 401 400 274  86 104 113 232  83 116 201 220 221 889 420 423
  81 111 231 119 114  57  56 228  80 117 888 681  84 320 418 416 115 417
 202  58  55 323 190 680 679 676 191 322 196 370 199 678 887 192 194 677
 195 197 198 193]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1699
INFO voc_eval.py: 171: [301 115 111 325 106 123 145 138 308 117  34  74  72 317 104 107 124 110
 112 144 315 316 303  37 105  35 109  83 146 113 326 307 103 318 304  38
  45  87 309 114  41 127  31  46 324 152  77 119 320  71   8   7 108  75
 314  42  59   0  39 305 155  92  61 150 289  80  82 233 234  51 257  95
 197 149  56 199 310  86 198  47  78 302 244   1 337  70 245 147  63 235
 209 129 286  68  76 285 116 140 242 331  73 294  33 282  49 251  84 131
 135  32   9 280  52 128 254 133  85 332 153 321 134 287 334 154 341 122
 327 281  81 284 283 156  79  40 195 306 120 159 210 143 166  29 246 247
  60 142 196 342 208 151  57 248 312 141 243  64 255  65 319 223 204 139
  48 239 231  69 201 132   5   3 279  67 161 240 200 232  66 148 169 214
 207 288 293  96  30 237 224 136  98 126 192  16  62 125 290  43  53 295
 311 253 250  12 194 193  36  20 220  93 137  10 329 118  55  58 221  50
 130  44  11 213  25   4 292  54 176 121 256 323  19 217 205 190 238 330
 249 291 241 339 252 102 236  99 203  13 191 230 216 202 227  21 296 298
 313 226 278   2 158 215 269   6 173 101 336  90  97  22  91 178 100 157
 268 219 338 212 225 222 333  94 160  88 228 172 272  89 322 179 218 164
 211 266 335  26  27 171 229 206 168  28  17 328 277 340 163 175 299  14
 271 300  18  23  15  24 270 297 259 274 262 170 276 260 273 181 265 185
 183 267 263 177 165 167 275 258 162 174 261 264 180 184 186 188 189 182
 187]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2695
INFO voc_eval.py: 171: [2218 1123 1574 ...  700  699 2515]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1647
INFO voc_eval.py: 171: [106 114 110 109 255 140  13 199 112  99 105 256 257  34  16 107 141 254
  24  31 259 165  74  29 111 196  19 166 231  30  80 138 108  79  98  73
 137 233 262 164 173 235  36  32 222  33  12  83 169 158  17 167 209  85
 149 258 223 261  96 172 103 155 161 253 104  20 260  43 214 171  75 168
  72  95 190 220  27 208  67 170 230  28  18 139 136 100  21 117  44 237
  35 225  82 157 210 232 115 234  76  56 189 191  70  15  86  71  22  64
  11  62  87  60  23 236 116 218 212  14  78 219  65 217 224  25 194  68
  69  26  63 131  66  49 130 244  46 133 192 174 226 216  61 205 160 134
 245 213  59  97 221  54 242 193   4 162  45 154 132 215 187 135  58 156
 118  55   9 101 195  52 129 186 159  47 182 102  48 185 241  90 122  88
 120   0  84 153   5  53  57   2 207 200 211 123 128 127 126  81 119  50
  89  77  42 163  92  10 203   7   6  93   1 178 176 239 197 201  94 188
 198 113 177 204   8 243   3 246 124 183 238 179 247 150 175 151  91 148
 228 227 206 202 229 180 125 240 251 252 181 250 249 248  37 144 145 147
 184  51 142 146 143  39 152 121  38  40  41 263]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0316
INFO voc_eval.py: 171: [2083  361 1565 ... 2733 2729 3613]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1059
INFO voc_eval.py: 171: [377 108  82  94 411 113 320 413  96 506 376  84 104 257 394 378 403 369
 101 158  95 470 290 243 680 379 410 114 170  21 685 186 292 250 512 268
  49 164 293 392 383 851  22 487 664 284  87 460 368  50 471 322 187 190
  83  75 286 154 319 405 690 802 511 246 461 503 406 769 219 223 178 244
 278 273 279 835 261 407  99 467 324 171 425 222 468 181  37 221 687  34
 141 109 548 302  12 330 253 285 480 510   3 153 242 387 240  65 218 313
 477  85 536 252 434 473 274  92 185  55 759 208 833 848  61  47 436 388
 197 371 194 846 507 151 509 574 200  68 463 328 291 266 156 823 226  44
 294 750 248 544 202 837 180 669   2 485 864 501 256 774  81 374 691 840
 157 306 442 668  76 504 684 301  46 161 300  63 149  59 155 191  80 558
 588 689  41 733 112 168 519 152 184 267 381 103  48 175 162 332 159 311
 307 824 450 408 762 671 688 839 205 585 764 174 428 682 398 127  24 277
 160 249  64  17 505 370 177 749  77 272 854 333 206  58  51 570  14 731
 502 486 836  28 446 247  11 385 318 825 816 520 500 142 523 561 271 217
 380 296 401 458 508 441 121 540 287 531 255 188 126  69  54 298 430  42
 258  19 528 263 148 201 857 239 445 229 481 269 767  53  43 599 144 209
   9 675 254 822 236 373 853   7 763 865 402 779  30 666 753 803 579 386
 288  40 459   5 547 295 812 482 705 760 283 427 804 203  26 670 395 587
 245  70 429 484 262  60 597  27 448 476 814  36 192 849 259  86 525 686
 665  16 834 850   1 297 543 730 729  62 270 100 172  35 771 391 483 390
 207 676 404 426  57 575  72 199 563  13 860 214   4 169   6 396 393 674
 329 210 555 173 677  45 409 530 738 863 856 234 211 220 464 754 276 841
  91 107  18 147 821 533 135 432 289 412 516 265 807  74 811  32 316 765
 340 125 773 591 124 583  33 867 384 166 818 120 772 472  78 820 844 681
 776 752 737 742 845 541 766 345 251 817 586 808 826 230 453 299 315  29
 786 529 815 139 855 342 375 809 777 424 859 462 389 372 736 130 813 819
 327  52 134 858 810 735 538 755 282 140 382 304 847 204  66 314 557 400
 129  39 707 167 466 308 775 542 457 338 264 539 515  67 275 361 595 552
 569 437 778 310  31 590 517 303 571 136 475 317 478 678 751 534 862 582
 838 399 131 280 143 137 600  79 189 341 182 352 758  73 526 225 621 241
 281  38 568 440 454 321 397 431 117  56 150 469 325 224 309 770  71 176
 133 474 672   8 589 852 861 145 843 866  10 783 132 717 138 305 260 331
 326  25 334 619 704 195 741 105 146 312 567 780 761  88 551 745 198 718
 743 709 827 842 213 465  20 732 781  15 479 734 581 116  23 231 616 566
 757 785 179 323 455 596 578 673 355 128 532 831 598 336 433 601 348 438
 630 215 594 756 216 518 123 617 237 227 744 435 625 233 193 238 232 196
 592 559 698 624 636 631 782 439 667 603 165  89 493 335 119 683 456 212
 784 451 768 580 700 339 443 447 452 659 832 122 183 562 106 449 576 228
 679 554 711 444 613 633 235 337 343 367 713 349 602 692 346 830 358 118
 350 706 660 593 748 746 828 661 829 359 495 535 716 353 521 362 622 634
 584 419 623 354 615 363 663 693 365 111 344 662 420 351 347 577 560 513
 360 364 614 564 629 714 537 695 697 626 357 549 628 546 527 356 801 522
 694 366 163 416 618 620 740 635 611 632 553 494 612 556 627 545 792 524
 800 550 644 708 870  90 725 712  93 796 798 110 710 514 747 797 793 417
 719 639 643 787 790  98 653 421 791 638 418 869 795 721 715 696 422 423
 102 799 701 794 573 640 637 572 702 646 788 789 703 641 728 655 604 724
 699 645 115 498 642  97 722 491 726 492 650 651 610 647 605 499 720 739
 727 723 606 648 608 609 607 488 490 489 652 415 657 656 658 649 497 654
 565 496 805 806   0 414 868]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0713
INFO voc_eval.py: 171: [ 591 1366  188 ... 1521 1517 1515]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.2243
INFO voc_eval.py: 171: [246 278 175 259  64 282  61 244 264 290 275 286 220  67 235 170 289 173
 254  72 251 206 172 258 127  65  81 394 224 252 203 461 495 398 395 497
 273 167 257  62 280 266 476 223 460 478 512 237  58 399 221  85 477 380
 222 390 462 187 248 502  48 129 179 296 176 466 523 474  66 500 132 180
 236  89  74  51 459 238 467  50 194  87  84 400 397 128 515 285 104 480
  24 105 464 202 130 471 381 253 242 411 475 379 260 283  68 504 329 465
 479 439 181 392  28 469 472 191 297  76 149 147 402  77 506 393 178 458
  35 310 463 510 414 188  73 468 217 382 192 174 451 126 452  49  83 412
 299 470 520 332  75 372 218  57 517 519 359 245 268  78 488 410 361  82
 518  45 131 124 274  52 166 133 250 482 417 420 528 487 204  33 358 499
 302 247 486 493 210 158  31  70  54 148 328 483 341 360 401 168 214 306
 219 190  86 473 189 144 281 284 134 387 485 409 279 533 115 501 103  29
 241  56 101  27  23 207 205 215 494 255 123 300 303  71  44 484 269 418
 438 169 314 351  88 270 385 165 532  42 421 337  59 525 312 195 125 391
 389  55 489 434 416 243  30 171 413 304 432 182 535  91 154 531 177 336
  17 374 357  60 141 287 208 415 118 425 330 446 431 313 256 356  79 183
  63 441 349 338 405 386  80  90 326 305 342  19 107 384 524  69 406  53
 311 239 447 340 301 298 404 143 333 377 119 442 378 327 334 508 193   5
 102 137 354  32 240  34 163 106  18  41 396 315 263 383 153 316 325 249
 111 427 108 513 344 136 110 213 112 146 443 440 335 140 164 142 331 343
 339 201 272   4 521 309 113 423   3 345  38 276 511 355 117 116 362 376
 509 114 522 135 375 184 403 109 445 368 350 122  95 277 152   9 120 139
 527 150 373  43 419 145 265 347 216 428  10 226 371 157 370 151 121 516
  39 529 352 430 233 448 161 348  13 505  94   1  40 100 185 317 138 162
   7 498 514  96  98 225 231 450  14 507 197  21 211 530 267  99   6 346
 209 196 526 369 408 534 496 307 288 228   2 481  20 367  15 388 232 503
  47  97 271 156 429 160 422 308 155 449 212  12  46  16 159 454  11 453
 424 234 227 186 426 230  22 353 433 229 457 407 291 455 318 436   8 444
 456 365 294 292 321 295 293 319 366 323 437 363 364 200  93 320 199  25
  26 435   0 261 322 198  92 492 324  36  37 262 490 491]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0506
INFO voc_eval.py: 171: [944 109 843 ... 353 352 817]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0596
INFO voc_eval.py: 171: [208  34 127 125 128 219  38 205  31 165 212 206  15  35 118 211  33 220
 126 171 221 217 162  36 207 132 130 137 136 120 114 216  94  60 214   3
  32 176  13 133 135 225  98  61 195  54  59 134 222 129   1 215 149  69
 210 190 213 138 111  19 131 226   7 169  85  14 112 174 223  95 204  43
  49 105  63 184  18 193  86  84 103  52 168   8  93  51 141  65 143   6
  45  20  87 192  66  11  46 227 181 197 170 185 178 177 218 224  48 200
  17 164  42 172 198  16 151  53  67 156 191   4 209  73  71  64   2 194
 104 180 189 173 116  57 179 122  47  72 182  40 175 201  44 123  77   9
 148 196 117  50 199 106  39 229 142 166 101 153 139  70 167 163 102 183
  62  96  97  55 147  75 234  99 110 231 230 115 233 140 108  10  58  30
 107 232 121 150  81  41  37 100  92  56   5 228  68 158 160 235 237 154
  79 186  12  74 109  22  76 203  80  78 188  83 119  82 236 124 155  27
 187   0 146 145 113 144 152  90 157  26  29  89  91  25 159  21 161  24
  88  23  28 202]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3842
INFO voc_eval.py: 171: [ 7872  7156     0 ... 16818 16819 22696]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.2145
INFO voc_eval.py: 171: [ 222  242  239 ... 1808 1805 1807]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2248
INFO voc_eval.py: 171: [ 74 477 203  63 512 526 891 197  48 460 505  50 458 914 503 134  32  45
 497 135 876 184 907  41 514 188 494 450 911 486 513 897  43  72 515 516
 466 447  38 131 198 705 520  34 873 192 895 501 881 448 459 202  77 509
 498 871 832  44 885 470 182 534 480  57 575 446 186  30  58 185  67 955
 453 899 228 179 469 194 132 224 473 465 890 455 912 872 522 818 495 740
 176 964 785 449 467  64 510 175 121 506 330 130 193 524 390 879 464 728
  61 499 888 457 835 474 976 913 892 882 124 529 886 901 493 519 483 232
 164  73 462 489 724 451 883 518 504 452 525 183 535 133 874 485 190 484
 961 842 644 500 889 475 528 567 944 471 521 180 841 733 463 648 645 530
 968 217 478 172 837 481 221 809 201  55 123 909 456 945 377 898 833 472
 562 461 533 531 248 178 523 399  36 259 950 487  52 508 397 136 843 488
 196 507 939 234 570 646 966 226 394 169  53 827  69 491 527 216 802 979
 840  54 828 972 388 476 550 239 951 249 199 468  65 490 128 496 225 181
 574  39  40 814 517 191  60 554 396 479 532 511 571 492 977 189 804 156
 829  42 822  47 887 947 502 938 482 311 836 649 557 727 312 227  35 454
 893 161 158 793 963 238 903 137 805 647 309 799 941  56 187 831 878 650
 558 808 965  22 380 811 573 981 395  71  70 953 566 741 246 906 838  37
 806 245 797 565 223 200 877 900 222 414 572  79 844 252 905  76 361 163
 195 306 973 954 220 174   4 165 257 940 125 263 375 166 815 218 821 265
  24  46 310  68 974 561  26 569  66 786 969 956 737 958  78 357 803  29
 725  51  27 159 819 970 157 127 813 168 541  49 167 219 599 957 560 235
  28 880 769 796 303 975 948 244  33 154 896 839 401   5  62 160 240 943
 729 734 736 980 807  25 129 356 262 299 739 171 790  59 742 250  23  31
 206 241 173 875 215   6   7 800 942 170 314 162 242 229 153 971 960 967
 764 302 549  75 120 343 298 959 177 315 398 576 949 962 820 138 139 904
 801 789 122 539 884 256 978 214 543 295 817 952 254 313 933 735 264 946
 536 152 730 597 559 816 374 788 795 126 332 830 333 273 738 363 207  99
 732 391 556 731 260 268 704 258 826 787 290 812 794 155 726 233 148 347
 545 230 150 304 564 261 834 353 791 301 823 308 247 346 305 542 553 143
 810 798 208 360 253 376 825 350 117 307 300 824 792 151 632 149 296 359
 119 580 297 547 548 231 236 354 243 706 674 546 423 338 344  97 691 364
 251 337 552 348 540 404 237 255 106 403 267 568 774 362 147 352 355 642
 289 204 627 781 328 118 694 783 291 538 378 422 276 635  15 709 664 349
 269 680 563 416 595 271 714 630 908 614 358 551 382 692 329 544 537 639
 341 270 555 425 703 748 345 690 381 421 412 351 104  94 379  96 410 387
 415 142 870 384 383 633 393 336 992 617 765 211 275 145 266 699 767 335
 274 671 424 696 700 676 869 272 715 996 782 583 779 579 386 780 852 663
 323 784 868 418  98 581 670 140 856 413  95 695 210 385 657 402 326  19
 994 209   9 407 213 722 144 697 212 894 146 853 417 426  17 651 408 778
 587 772 659 342 857 652 339 851 406   8 743 698 405 847 420 612 641 677
 610 625 411 710 626 640 578 937 419 334 331 107  21 749 655 392 716 637
 638 993 340 991 613 936 389 708 701 910 768 902 577 995 861 702 935 713
 636 723  89 717 934 773 582  12  14 860  11 654 629  87 292 672 600 848
  84 409  10  20 618 858 711 293 750 631 721 585 658 707 620 294 102 608
 685  13 112 111 621  16 105 619  18 622 854 114 850 115 596 927 683 656
 321 846 108 855 845 859  82 673 592 101 110 687 744   3 653 712 589 849
 928 766 745 917 598 586 320 100 322 681 684 205 916 324 325 590 109 603
 932 611 660 615 689 624  83 103 665 400 609  81 866 718 602 918 746 919
 930 367 865 931 371 682 623  93 693 920 747 719  86 584 594 288 643 628
 929 605  85 666 770 661 662 604 116 606 316 720 591 771 601 438 688 668
 607 319 616 318 366 593 634 862 915 433 365 141 588  80 686 369 864 317
  91 667 867 444 434 921 863 373 985 370 435 777 669 372 678 924  88 925
 775 368 675  90 679 776  92 439 926 283 923 437 442 436 327 280 286 988
 284 281   2 287 432 429 113 922 441 431 430 427 279 282 443   1 285 428
   0 445 440 278 760 754 986 983 753 763 756 752 757 758 987 990 277 755
 762 984 982 759 751 761 989]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1011
INFO voc_eval.py: 171: [ 66   8  63  64 215 214  75   9 301  74  57 336 156  67  61  90 122 285
 291 338 287 320 286 328  58  87 263 289 126 265 321 297 118 294  88 103
 303  23 262 104 250 298 329  54  71  62 201  11 288 292  55 284 293  69
  59 203 251 205 159 295 147 128  68 198 200 117  94 283 290 264 130  24
  70 266 296 127 120 302 300 335  65  60 267 282 343 299 129 111  19 173
 304  92 186 145 249 351 105 319 134  10 125  72  85 124 157 184 204 252
 261 110 182  97 248 106 255 135 143 107 219 140 119 341 210 133 239 206
 253 121 123 136  13 176  96 254 257 177 348 194 349 142  86  73 108 132
 109 160 154  56 195 229 139 152 178 180 131   3  17 218 353 193 192 190
 332 189 175 221 269 158 179 260 165 242 325 188 234  32 149 275 220 276
  18  31  38  36 202 281 236  89 331 174  77 258 171 222 337  16 342  14
 183 223   5 199 191 225  35 181 146 116 187 168 324  39 313 153  33 151
  20  15 208 185 259 155   4  99 314  95 245 196  12  91 207 197 102 280
 340  78 278 167 150 217 352 330 209 241 240   1 101 115  34 279 350  79
  93 277  84 148 166  27 231  41  29 144 345 100  76 346  28  30 344 216
  81 339  21 230 347  98   2  52  47 212 237  40 233   6 232 333 326 226
  49 244  53 141 243  82  42 323 224 268 238  48  80 310  43  83 235 322
  50 334  37 327   7  51 161 170 169 227 246 163 162 164 172 312 138 256
 306 311  25 211   0 213  26 318 271 270 309 247 305  22 114 307 315 308
  44  46 316 317  45 228 112 113 137 272 273 274]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1170
INFO voc_eval.py: 171: [476 484 482 480 290 477 489 481 295 479 661 348 475 497 662 654 486 478
  51 186  49  45 188 616 292  57 483   5 182  74  63 301 100 294 503 289
 291 625 521 708 505 261  92 485 702 697  71 113  47 321   8  44  52  46
 343 525 665  61 351 699  81 361 493 103 115  10 500  85 119  70 104 515
 488 492  43 698 152  55 266 618 551 649   6 305 189 487 653 655 344 347
  68 619  69 273 269 359 709 153 299 353 569 118 645 191 495 620 167 660
 696  48 516 695  79 315 615 210  54 190 120 208 628 656   7 711 288 263
   4 107 262 284 706 112 700 363 319 641  82 357 356 274 183 122 215 668
 268 313 555 562 557  56 647 712 594  23 705 228 317 630 586 342 523  13
 349  91  58 552  62 117 556 639 154 664 574 304 108 614 139 513 659 346
 494 151 572 703  80 621 127 637 491 559 553 629 564 676  59 101  72 377
 142 105 512 143  96 714 307 130 168 360 318  66 658 528 244 517  64 300
 125 504 169 441 713 245 577 514 166 510 322 554 370 704 156 158 617 160
 379 506 155 526 678  50 507 411 408 710  86 575 573 666 316 490 576 511
 367 202 561  28 220 595 234 264 563 672 126 613 306 635 612 560 352 221
 362 124 365 623 585  21 682 123 209  99  12 558  31 657  88 243 195 164
 146 185 582  78 667 196 293 157 296 242 214 197 427 172 121 171   3 421
 265 434 286 707 355 532 248 567 544 410 580 524 622 570 320  35 237 701
 102 522 651 131 144  84 206 533  24 328 207 267  18 527 636 566 225 109
 650 128 116  53 354 345 366 246 278 369 114 627 496  76 140 179 238 419
 235  65  77 311 247 413 239  95 368 597 161 280 181 671  27 539 675 163
 364 718 508 203  11  90 136  17  87  98 180  25 212 134 640 133 416 565
 400 165 308 568 442 233 323 332 381 187 198 111 578 673 371 604 593 598
 571 587 529 350 132  14 392  75 193 110 205 135  32 226 501 324 272 633
  94 229 192  30 358 204 194 184 373 499 137 634  22 498 224 428 646 589
 605 200 271 592 222 465 314 537 583 626 298 426 680 281 141 447 213 138
 395 599 326   2 399 632 581 391  83 467 669 417 145 230 275 330 420 596
 468 178 129 694 199 147 201 418 106 454  67  93  60 624 461 631 337 579
 642 643  42 459 458  37 402 415 310 431 430   1   9 176 429 223 256 462
 335 588 405 679 648 456 663 545 519  40  89 211 259 644 258 693 509 334
 325  97 260 584 227 591  39 401  73 638 302 240 231 404 177 333 719 287
 590 329  29 432 372 536 688 473 174 541 460 691 606 689 534 681  34 175
 336 327 312  41 687 277 546 374 652 159 173 674 340 444 670 285 331 424
 339 338 469 535 720  16 440 450 439 232 414 423 406 170 309 435 425 422
 279 610 303 437 600 150 520 382 677  33 398 518 453 276 283 241 380 378
 692 218 549 219 457 236 162 376 375 407 690 270 257  38 282  26 390 540
 149 436  36 393 409 464 455 608 297 603 611 412 403 531 254  20  15 451
  19 607 538 394 463 445 148   0 443 550 547 446 609 433 530 452 543 542
 548 384 601 502 472 253 438 466 387 217 216 471 448 449 602 397 252 474
 388 396 470 715 250 251 341 249 386 255 383 389 385 683 685 684 686 717
 716]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0522
INFO voc_eval.py: 171: [514 513 475 446 278   0 626 500 473 610 339 192 238 625 505 559 443  22
 478 258 259 560  48 187 174 279  18   4 353 247 487 276 341 611 256 281
 252 272 267 263 442 246  13 634 262  58 349 146  11 241 269 315  53 257
 450 470 173 133 260 489  23 343 289 342 228 562 102 444 134 350  16 268
 351 627 232 383 250 448 612   9 286 312 107 357 273 274  50   3 507 565
 451 453  56 583 231 285 155   2 497 295 237  37 576 508 506 313 398 567
  17  60 582   7 609 512 186 311 461 345 217 340 394  12 449 177 138 275
 548  86 635  55  49 356 298 619  65 363 287 207 445  31 230 468 378 534
 283 149 499 239  95   1   6 300 141   8 266 510  20  90 522 209 511  70
 365 371 376 314 474  87 501 145  10 621 205 561 469 327 416 109  41 270
 632 337 408 242 414 240  76  32 136 346 465 615 179 358 325 532 148 493
 392  36 613 176 152 563 142 569 519 280 509 225 564 382 220 302  44 215
 244 108  51 195  61 106 110 422  46 144 288  19 574 521 299 319 318 216
 277 566  57  78 243 131  52  15  94 355 183  54 301 154 163 122 111 538
 494 466  47 482 326 348 585 344 372  26 491 631 245 578  88 537 139  14
 374 547 233  75 462 459 409  29 515 628  73  84 213 338 335 143 633 296
 467 271 630 181 182 360  21  99   5  40 126 454  83 226 423  98 224 406
  67 608 531  59 324 460 421 370 188 390 597 400 568 251 540  24 529 463
 113 496 550 114 210 185 323 236 385 629 429 543 587 411 203 197 254  64
  92 456 249 248  79 229 539 556  66 211 140 329 396 367 397  35 364 119
 112 526 117 536 156 164 208 524 214 479 384 624 518 388 147  72 196 623
 103 412 570 618 336 602 432 636 115  74 184 212 533 361  85 471 520 222
 525 235  81 178 457 600 455 264  62 427 386 291 430 480 535 206 321 362
 175 530  33 458 261  91 485 218 305 415  82 391 316 105 100 234 282 101
 167 347 517 622 373 593 393  71 495  27 437 193  38 219 588  45 204  42
 601  77 331 551 129 377 488 352 150 498 104 354 592 484 359  80 334 441
 223 161 380 557 486  93 309  69  28 599  68 194 591 284 523 595 596  96
 368  63 369 527 558 481 407 328 617 157  89 227 603  97 472 573 293 320
 366 190 389 620 189 404 171 172 577 303 598 387 118 594 333 553 191 162
 440 135 265 221 616 317 322 330 436 128  39 483 121 180 402 614 375 332
 202 433 306  25 379 490 166 137 127 292 401 476 420 584 575 492 528 418
 307 160 477  43 464 255 170 290 297 413 381 116 589  30 424 425 405 253
 434 151 580 201 417  34 153 452 579 130 124 403 120 439 125 546 159 419
 606 555 132 426 431 123 199 200 428 399 541 581 447 308 294 158 169 438
 410 586 549 554 395 165 168 435 304 545 552 542 310 502 544 572 571 590
 516 504 503 607 198 604 605]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1197
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1410
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.312
INFO cross_voc_dataset_evaluator.py: 134: 0.050
INFO cross_voc_dataset_evaluator.py: 134: 0.065
INFO cross_voc_dataset_evaluator.py: 134: 0.170
INFO cross_voc_dataset_evaluator.py: 134: 0.270
INFO cross_voc_dataset_evaluator.py: 134: 0.165
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.106
INFO cross_voc_dataset_evaluator.py: 134: 0.071
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.051
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.384
INFO cross_voc_dataset_evaluator.py: 134: 0.215
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.101
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.120
INFO cross_voc_dataset_evaluator.py: 135: 0.141
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 9499
