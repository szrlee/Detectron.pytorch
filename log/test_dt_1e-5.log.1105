Start testing on iteration 2499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.411s + 0.001s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.340s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.346s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.350s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.354s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.353s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.352s + 0.001s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.356s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.353s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.353s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.354s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.355s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.357s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.541s + 0.012s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.373s + 0.003s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.361s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.359s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.365s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.369s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.377s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.374s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.373s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.374s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.370s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.369s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.367s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.420s + 0.001s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.362s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.366s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.364s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.360s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.356s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.361s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.359s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.362s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.361s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.361s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.362s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.361s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.495s + 0.001s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.408s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.387s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.378s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.371s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.370s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.371s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.368s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.370s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.368s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.366s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.366s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.364s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.292s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 49 183 231  58 124   5  23  42  41 144 293  50  99  86 220  84 176 311
 156 131  28 315 134 141  83 205 301 327  30  47 277 179  48 161 117   2
 238   6  66 128  26  82  16 188 330 275  46  45 106 226 259 155 184 175
  21 276 229 160 163 180 121  20 158 263 258 113  73 139 100  35 246 326
 211 225  59 195 228  43   9 261 287  87 162 307 114 322  36 285 210  11
  54  34 169 241 292 170 147   0 323  89 157 178 109 264 312 140  74 237
  25 191 202 230  56 143 101 262 243 271 185 206  12  24 104 324 318  76
 154  69 111 288 294  40 336 222 300  85 278 242  80 328 172 337  61  29
 223 159 186 316 289 112 319 244 284 207 268 333  92 252  77 150  14 192
 135  18 209  52 303 218 132  39 148 253 320 142 325 236 168 208 290 298
  95 309  71 272  37 201 118 137 133 257 314 149  91 250 166  51 240 233
 177 249  72  68 138 217 265 171 105  44 122   4 280   1 273  79 302  98
 270 129 119 115 266  17 204  27  15  67 123 187 125 283 213 103  70 224
 102 297  75 215  57 151 255   3 110 279 329 182 199 334  78 254 248   8
  31 116 107 193  63  97  53 174 331 260 219  62  10 239 234 153 291 305
  94 167 181 308 130 281 282  32 126 274 190 269  96  93 145 164 216 247
 196 221 165  33 235 189 286 198   7 203  88  90  81 173 194 313 146 108
 251 127 214 317  22 304  38 245 335  55 299 296 321 227 295 256 332 200
  13 267 197 136  60  64 212 152 306 232  65 120 310  19]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3539
INFO voc_eval.py: 171: [ 71  31 234 155 207 256 253 266 269 267 173 208 255 157 154 221  19 175
 116  27 209 257 201  40 184 101 216 270 258 242  13 245  12 218  11 204
  83 135 271  61 104 252 265 226 262  29  58 283 134 138 199  77  70 130
 238 229  74 240 125 194 103 281 161  35 122 215  18 114  89 181  33 244
 247  63 143 100  42 162 212 282 264 172   3 277  16 268 165 220 182 196
  37  62  60 145 174   4 166  99  98 248   8 139 185 113  41 105 222 243
  45  81 179 151 178 225 200   9 272 146 164  51 254  10  50 186 188  20
 205 147 158  65  68  64  25 163 102 168 259 171 187  67  76 150 160 228
  30   7 149  59  53  34  73  85  86 176 210  96 109 167  75 191 120 202
  32  78  52 260 159  79  69 127  46  88 280  48 278  22  93  14  57 227
 156 141  97  72 111 224 148 128 144 189 119 246  84 136  66 233 237 110
 112 251 197  21 275 190  43 183 250 241 192  80  47 106 115 263  87 230
 152  92 239 198 276 219 231 108  95  56 284 118  15 177 223 206  90 236
  24 123 131 235 274  28 249  23  94 133 211 213 140  54 132   1 153 261
 142 137 117  44  82 214  26 170  36 232  38 126 279 180 124  49  17 217
   0 107 169  55 273  39 129 195 203   6   5   2  91 121 193]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6442
INFO voc_eval.py: 171: [ 72  89  28  76   3  55  57  88  56 175 189 191 174 190 107 150 176   9
  42 141   2 216 108  77  30 196  67 154 151  92 145  70  84 206  48  37
 157 220 165  26 111 210 221 119  33 124 198  69 204 180 202 163  73  12
  68 160 159 193  79  87 214  45  13 142  31  49  85 199 143 173 153  78
 146 155  21  51  80  58   4  82 195 209   6 192  61  93  10 136 222 137
 172 170   7 140 127 182  27 103  63  60 183  25 131  62 177 126 200 114
 178  24 101  98 134 100 125 158   5 121 219  54  17 148   1  53 188  86
  94  11  15 203 130  18 122  90 123 106  14 110 102 164  44 187  22  40
 161  59 152 115 156  29 139  97 109 215  38 213  19 184  47 205 211  36
 168  64 181 179  46  43 194 201 120  74 149 167  75  66 147  95  41 218
   0  83 133 171  39 166  99  16 212  50 129 118  23 197  20 128 135 104
 105  91 112  81 223   8  71 217  32  34 144 113 116 162 186  52  96 207
 117 138  35 132 208 185  65 169]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2531
INFO voc_eval.py: 171: [125 453 387 386   2 109  62 321   8 388 171  66 418 552 139 138 225 554
  27 322 412 424  32 540 489 320  30 444 231 314 119 292  39 530 107 637
 305 479 365 389 325 307 202 329  77 169 315 121 470 591 549 328 378 249
 411 130 193 221 143 157 502 491 490 541  35  80 381 371 555 559 481 598
 636 588  43  96 178 458 324 219 344 187 229  41 620 475 310 362 513 141
 501 514  20 372 287 443 363 286  23 439 614 203  54 610 538 254 464 410
  44 304 227 526 213 632  34 393 480 364 595 619 245 634 455 525 520 316
 258 341 562 260 565  38  81 492 252  59 507 127 338 547  24 117 251 189
 466 194  29 246 332 523 405 238 485 556 397 284 108 168 460 391 271 158
  85  64 585 509 546 522 105 417 454 277 617 352 535 482 173 584 235 369
   7  94 395 201  83 374 289 463 521 580 436 624 493 290 426 368 133 416
  87 146 398 177 264 326 312  46 110 425 356 572 437 190 558 250 113 273
 435 226 553 268 532 136 608 355 474 385 198 295 505  89 639 544 467   3
 433 149  76 440 432 342 167 288 640  48 600 211 239 497 102 358  86 159
 334 276 353 423 361 539 551 476 465 204 434 401 380 209 267 607 147 181
 396 469 642 120 222 616 270  92 468 542  49 114 450  47 140 155 399 604
 471 613 421 496 488 233 519 420 560  42 576  56 152 621 278 550 180 206
 499 390 548  84  98 199 451 197 375 317 370 602 529 379 296  52 612 123
  13 112 223  74 115 511 205 627 303 350 583 447 335 359 590 623 606 472
 568 357 557 150  22  10 166 413 618 573 366 575 220  65 333 275  55 456
 592 232 172 330 255 339 301 349 587 212  19 186 237 571  50 578 543 486
 564  58   0 200 512 124 534 438 256 165 605 419 351 367 263 259 428  53
 570 134 253 402  99 383 184 111 427 247 408 272 403 400 441 236 340 106
  93 498 297 462 347 291 641 603 185 495 132 376 244 188 265  60 145 214
 306 494 126 210 176 336 215 354 446  15 626 567  51 415 406 174 622 566
  61 234 151 269 596 459  73 545 483  36 156 574 208 506 531 518 308 104
 500 302 631 384 442 449 536 404  37 503 175 533  31 373 633 394 628 274
 192 170 101   6 638 240  57 528  68  69 243  21 228 409 477 569 298 283
 516 216  78  33 582   5  63 148  45 280 429 163 122 382  12 537  40  72
  71  67 144 524   1 629  11  25 261 484 563 601 160 242 281 597 487  95
  79 116  88 414  18 299 282 515   4 162 224  90 309 191 517 161 360 100
 343 461  14 407  28 577 153 135 266 319 293 611  82 615 345 473 561 422
  97 230 527 279  26 589 630  17 327 452 182 129 128 217 318 445 164 207
 131 348 581 103  91 196 137 248 323 257 392   9 313 431 346 311 430 337
 285 635  70 154 300 586 142 510 478 331 262 594 218 457 448 241 625 599
  16 118 508 195 183 609 579 504  75 593 179 377 294]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3003
INFO voc_eval.py: 171: [382 374 192 212 194 387   3 210 380 361 317 235   7 177 195 213 316 266
 196 178 468 424 193 469 381 414 288 307 318 109 304 372 314 124  73 373
 239  29 375 252 408  18 379 222 167 388 315 447   6 319 259 325 274 150
 376 326 232 215 449 396 273 360 191 139 245  15 249 328 168 400 394 385
 399 386 378 311 406 446 158 342  38 438  43 103 164 118 337 134 377 137
   8  94  61 221  80 416 159 220  28 203 384 407 444  68   5 310 151  39
 216 272 263 440  24 423 323 261  71 398 276  74 244 234 170 457 410 104
 344 187 173 166 322 397 299 415 110 293 217  50 305 257 277   0 459 182
   9 295 126 270 156 227 426 184 100  40 206 391  97  35 355 135  85 413
  13 401 393 160 133 119 201 327 112  60 461 197 451 236 419 390 186 140
 463 430 207 471 231  31  63 465 208 238  25 117 153   4  51  46  92 443
  59 147 349  66 242 223 292 237 179 425 335 336 296 154  44 333 345 320
 352 136 445 421  81 302 404  89 409 454 229 214 114  91 300 251 467  86
 339 107 190 403 470 258 218 281  14 280 330  45 121  75 125 265 149 348
 432 181 262 209 341 395 148 405 458 334  64 306 228 363  76 141  47 241
 392 106 132 291 200 279 456 389 453  12 455 176 370 303 433 230 347 219
  34 284 128  30 439 183 338 267 202 108 175 321  98 240 161 383 343 354
  93 429 428 289 205 427 351 350  10 211  99 418 105  52 324 264 269 271
 285 198 313  16 369  48 282 294 417  90 155  65 331  49  79 290 340 145
 402 356  70 420  62 174 286 102 364 287 144  19 462 172 163 434 431 450
  96 157 260 169 367 278 225  58  55  69 441 226 250 116 301  82 246  37
  41 152 101 130 366 460 359  72 371 199 411 165 297  11 142  36 362 346
  95 113  26 358 146  17 122  27 357 180 256 353 448 442 309 233 435 131
  84 189 254  22 123 247 253 452 365  77  42  67 464 312 412 436 437   1
 171 111 188  53 162 422 283 308 298 248  88  78  57 120  87  54 127 143
 368 329 129 268 466 204 243 115  21  23  33 332 255  20  56  32  83 138
 224 185   2 275]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5340
INFO voc_eval.py: 171: [ 38  74 183  21  27  71 109 118   5 166 126  91  32 185 167  83 168  31
 143  19 152 194  24  11  69  88  90 133 178 113 175  49  29 174  78 124
 192 135 190  81 140  89   9  45  26  82   1 155  80 142 181 127  65  47
  98 105  22 110  33 112 108 161  58 101  55 138 117  17 146 191  66  48
  75 158 102 163 107 125 184  73  95 172  39  86 132  54 100 141 116  50
 157 122  68 162  63 130  93  43 193  51  46  35  10 159  42 187 111 176
 179  25 177 154 149 186 171  52 170 129 189  96 123  77 153 103 139 121
 131  84  97 195  87  61  92 120  34  59 182 156 188 144  53  64 148  36
 165   7  30   8   3  18 150 136  15 151  67 106 169  13  20  56 173 104
  23 164 134  85   4  12 145 137  72  37  14  57  76 114  70 115  62 147
  41   0 128 180  94  44   2  40  60  79 119  16 160  99  28   6]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6024
INFO voc_eval.py: 171: [354 333 156  79 483 437 361  34 522 220 105 180 151  59 239 459 471 484
  21 390 473 267 299  26 243 445 277 531 242   5 498 384 455 388 460 263
 489 293 395 106 302 181 385 350 438 330 205 245 442 272  46 435  73 217
 332  60 397 271 295  53 516 444  72 414 303 468 296 107 182 284 441 370
  61 386 278 140  93 297 247 136 383 137  89 413 433 313 377 102 193 341
 308 538  64 163 426 525 408 507 229 279 162  55 171 534 371 179 270 461
  75 280 434 286 287 407 173 422 281 346  27  58 208 240 223 288 318 469
 327 170 419 232 320  83  45 493 132 416  23  65 412  12 236  62 249 134
  36 474  20  94 448 400  44 200 241  54  70  10 362 169 164 338  25 321
 117 392  47 506 301 481 238  88 213 145 539  28   2 347 135  92 331 345
 201  38 409 246 372 148 470 207 252  41  19 326 131 310 304 380  37 456
 475 389 515 276 192 210 161   8  85   4 142 529 275 482 486 218 325 141
 512 154 309 355 405 457 501  97  99 454 283 527 319  87 112 187 285 446
 406 462 149 447  63 152 495 466 225 344 305 348 244 153  98 255 110 195
  56  40 185 394 138 328 335 523  90 111 186 155 282   1 199 260 300 123
  66 314  76 103 265 423 231 168 505 403  86 410 453 427 451 261 114 178
 215 189 530 514 521 458 450 443 463 508 311 432 449  51 316 221  77 258
 503 467  95 363  33 504 431 274 487 206  91 203 128 360 476 198 373 518
 535 294   9 351 122 387 491 100  57 502 376 147 439 324 237 157  48 101
 485  31 369 404  14  35 257 374  80 197 273 337 425 194  68 216 537  84
 166 421 424 256 382 150 121 536 497 381 436  52 352 130  69 212 517 509
 428 402 269 291 418 113 188   7 356 452  78 159 340 532 104  49 358 465
 306 144 289 323 174 125  74 353 396 143 478 343  39 524 292 430 519 264
 250 172 259 464 364 233 375  22 378 228 477 329 528  16 359 119 108  17
 183 520  96 488 357 254 127 492 224 230 290 234 248  11 177 158 322  32
 393 499 368 379 490 139 165 398 479 312 513 214  67 262 307 190 440 115
  29 367 226 146  71 315 120  30   6 124 366 184 109  81 266 204  24  18
 415 496 480  82 526 511 391 133 349 209  42  13   0 251 298 202 167 339
 365   3 219 129  15 420 472 160 399 211 126 235 417  50 401 317 411 268
 176 253 500 118 222  43 175 533 191 510 334 227 494 429 116 196 336 342]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4652
INFO voc_eval.py: 171: [16  5  4 73 24 17 61 32 15 54  0 69 60 68 13  6 65 47 23 46 10 31 41 19
 35 59  7 58 26 22 12 72 36 40 70 18 43 37  3 64 34  1 53 63 38  8 28 48
 14 20  2 67 33 42 50 51 55 52 74 25 30 62 49 21 45 56 66 57  9 11 29 71
 27 44 39]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0468
INFO voc_eval.py: 171: [ 379 1617  697 ... 1061 1332 1155]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.4914
INFO voc_eval.py: 171: [19 37  3  5 15 55 68 83 39 16 63 72 73 17 77 82 52 47 28 29 80 36 46 11
 62 60 69 76 54 31 59 78 53 56 71 48 35 23 49  9 57  7 33 26 79 20 67 42
 22 40 18 81  8 86  2 50 24 61 13 38  4 70 75  1 44 14 34 45  6 41 58 10
 43 25 64 32 66 21 87 51 30 27 12 84 85 74 65  0]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1820
INFO voc_eval.py: 171: [478 215 230 209 120 414  54 508 279 293 305 302 166 410 186  63 457  52
 424 431 188 185 285  61 318 183 330 352   1 384 537 102  24 306 488 307
 349 340  12 256 387 539 231 517 344 137 210  36 138 441 341 108 316  29
 386 168  35 205 211 235 442 200  19 487  57  84  77 263 233  98  80 474
 331 544 507 358 427 406  23   9 151 357 294 130 303  87 124 308 451 485
  42 169 322  92 481  33  79 248 181  71 350 461   6 542 482 317  48 311
 510 234 323 449 127 501 178  41  47  64  39 201 156 267 509 179 534 195
 216 514 363 144 469 532 326 438  73  16 447 479 452 207 290 296 360 177
 462 189 171 264 140 313 278 339 112 190  81 511 504 245 150 493 432 146
 526 292  22 275 147 512 304 486 310  83 353 375  91  45 122 522 440 521
   0 184 197 141 250 346 369 164 132 496 354 436 134  28 475  51 370 477
 394 142  21 187  75 118 343 225 502 173 301 174 274 199 530  74 500 276
 471 125 460 464  59 395 107  94 492 361 420 540 309 499 399 454 426 265
 329 355 101 439 408  99 389 281 159 155 240 489 543 126 419 271 467 228
 497 337 533 366  46 365 152   3 113 167 418 287 525 338 362 273 223 131
  60 220 282 470 111 129   4 320 214 239 176 498 172 163 531  25 468 212
 476 252 258 422 506  18 277 224 385 192 445 391 238 490 226  65  38  37
 448 466 213 253 356 402 535 157  88 191 437 379 328 459 396 291 520 128
 257  10 300 288 121 450 421 364 473 334  96 345 446 268 249  32 242  72
 541  13 524 315 180 196 182   8  30 143  40  58 336  20 404  11 260 117
 519 373 371 297 319 136 503 505 538 455 154 359 401 463 327 193 372 444
 232 236 376 513  49 494 325 194 382 383 247  34  15 374 425 417 407 335
 295 321 170 162 203 262 148  82  78 483 347 286   2 392 114  31 405 348
 217 218  53  17 161 516 149 135 484 100 103 412 368 244 254 227 105 351
 219 324 398 243  66 342   7 266  55 115 400  85 458 434 202 397 246 380
 411 456 158 269 110 393 429 229 472  56 123 153 377   5 272 222 332 378
 198  14 261 259 109 255 515  27 381 284 480 527  95  93  69  97 298  67
 175  44 491 283 416  76 119 518  50  70 208 415 165 206 314 545 453 104
 333  26 536 465 367 495 116 312  43 403 529 270 280 145 241 221  62 133
 423 204 430 390 388 409 413 428 435 299 106  90 528  86  68 523 289 251
 237 443 433  89 160 139 546]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4128
INFO voc_eval.py: 171: [  1 109  30 121 118  65  23  51  91  57  62  14  90  82  64  29  19   5
   0  66 119  95  28   3  76  52 116  10  33  43 120 104  53 130 129  77
  18  22  83  45  58  37 127 125  67  79  17 110  61  31  48 124 101 105
  24  39  13 107  94  26   4  89 111  81  97  68   2  96  34  59  44  72
  75 131  60  55  56  93 126  21  11   6  87  78   8 123 102  16  25 122
  80  99  86  15 115  71  50  49  32  47 100  74  36  73  27  85  69 103
 113  40  98  38 114  46  84  92  63  41  54 108 117 128   9  35 112 106
  88  12  20  70   7 132  42]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0593
INFO voc_eval.py: 171: [76 12  9 64 90 51 32 63 61 93 60 53 92 52 11 56 15 10 79 37 80 27 85 42
 19 86 83 23 47 31 38 20  2 87 18 69 45 84 66 77  1 62 29 57 25 54  3 34
 81 89 50 24 35  6 43 59 82  5 72 75 46 91 21 39 40  4 30 88 33 71 44 55
 26 48 70 58 41 14  0 67 73 13 22 16  8 78  7 68 36 74 65 28 17 49]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.4227
INFO voc_eval.py: 171: [ 27 163  19 108 173 157 112   9 129  77  14  80 150 123  70  30 113 144
   8  86 146  93 117 156  16 125  47 172 139  90 114 142 167  98  87 136
 130 100  51  75  43 121 104  74  40 109  25  99  79 127  63 138 115 149
  55   1  62 148  15 176  89 143  36 174  66  85  21  96  57  83 166  60
 161  76 175  18  56 124 137 107  45   0  84  71 169 145 105  73 153  46
  39  65  38 158  31  94  97  33 151  34  52  53 147 131 168 122  49  50
 120  17  68  22  44 118 134 106  23 152  13 170  88  59  42 154  32  20
 132  82 103  48  29 171 160   4  67  58 101  24 165  28  81  10 119  37
 162 135  72  41 140  78 155  91   5  95 141   2  12 102  64   6  61 133
 126   7  69 111  26 116 128 159 164  92 110  11  35  54   3]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7384
INFO voc_eval.py: 171: [1597 1473  974 ...  283 1034   50]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5904
INFO voc_eval.py: 171: [516 144 112 229 757 158 255 813 200  76 683 127 243  78   7 319 600  79
 324 647 755 256  77 518 561 511 817 149   8 105  80 542  55 655 721 232
  82 843 280 257 472 464 682 814 478 169  92 294 417 716 177 242 747 107
 244 598 291 823 816 373 497 594 562 754  57 389 875 536 235 756 656 128
 377 725 176 362 739 731 555 553 529 136  96 120 325 418  39 236 540 888
  48 133 290 738 199 374 359 340 295   9 662 693 610 685 556 819 659 821
  10 532  49 657 824 395  38 316 684 421 488 609 885 800 135 626 767 758
  70 537 443 729  54 779 815 233 686  68 876 422 801 496 502  95 646 507
 557  99 326  15 601 571 463 375 498 297 792 782  74  67  56 354 892 279
 283 658 627 429 175  81 517 403 574 334 636  52 649 419 533 223 523 397
  25 371 670 152 827 245 323 575 304 785 512 597 284 737 286 890 790 692
 873 188 761 706 187 388 360 837 343 818 526 181 559   2 276 210 414  88
 492 566 543 513 675 763 660 613 661 292 352 671 530 584 331 667 164  13
 783 699 361 406 572 329 400 126 740 640 202 407 499 771 690 204 219 113
  62 644 201 420 434 741 356 462 413 581 441 677 804 764 307 505 605   5
 850 567 822 342 563 459 728 426  14 191 508 423 633 142 891 853 839 894
  97 887 688 121  42  59 702 247 486 469 825 592 895 308 866 845 259 564
 834 189 676 759 565 833 263 886 582 349 384 252 379 736 705 724  41 350
  63 447 330 698 809  28 554 368 183  46 551 717 298  24 411 865 810 251
 862 162 830 802 430 781 745 391 254 293 249 635  12 477 146 465 595 184
  72 558 238 893 665   6  11 751 664 225 629 730 358 301 129 116 123 369
 239 607 130 165 215 398 545 786  87 713 424 604  53 722 318 832 634 780
 714 587 784 102  23 367 148  34 775 520 812 224 579 509 500 150 734 855
 619 578 190 515 703 387  86 370 616 762  60 100 357 580 793 115  17 217
 811 332 611 645 789 586 484 355 481 493 643 797 192 549 881 482 577 882
 138 336 221 679 689 539 726 366 494 410 622 285 147 568 390 351 452 435
 303 534 347 444 348 271 345  84 206 114 248 641 870 473 694 132 803 674
 246 727 524 431 637 615 861  50 312 846 648 344 153 489 550 104  40 788
  22 474 773 281  61  75 278 108 485 522 639 593 618 651 744  91 678   0
 378 311 231 327 857 858 372 864 433 195 752 134 583 889 364 450 305 596
  19 277 438 718 341 385  51 808 166 768 663 838 111  85 380 310 141 774
 535 258 213 510 617 457 624 272 415 650 826 840  45 154 394  66 807 170
  47 709 849 528 269 218 261 521  71 896 856 720 777  43  16 306 226 606
 220 612 519 353 544  35 841 288 691 265 632 483 795 125 240 704 628 139
 275 309 174 707 794 871  21 446 392 171  37 772 708 621 585  64 161 798
 470 514 471 314 547 860 299 451 227 746 416 652  29 180 211 260  58 346
 835 487 439 408 296 765 428 399  89 396 591 867 234 404 237 569 437 103
 167 137 250 197 769 205 432 207 427 425 253 214 101 863 320 209 872 230
 382 770 267 185 401 603 695 760 753 897 445 266  73 491 106 669 527 159
 381 412 145 883 178 742 203 163 719 859 110 140 228 160  20 711  90 268
 282 287 173 687 701 588 715 750 131 501 208 338 820 321 844 710 151 642
 602 117 630 868  18 300 666 274 548 454 560 182 681 262 172 879 531 805
 620 673  36 124   4 376  44 796  93 851 878 791 212 322 766 480 806 672
 155 363 631 302 538 186 723  26 614 216 402 625 732 289 570 589 877 836
 122   1 552  27 787 365 778 455 168 460 333 623  98  31 479  33 475 525
  69 339 700 118 328 386 546 576 143 735 196 156 453 119 776 383 317 712
 869 315 490 264 749 241  94 157 854 599 654 880 668 884 697 448 337 503
 504 799 476 458 456 393 273 409 440 829  32 405  30 335 653 467 436 874
 313 842 468 179 449 848 852 198 828 495 680 696 541 847 831 573 193 194
 461 466 743  65 608  83 109 590 733 270 506 638 222 748   3 442]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5927
INFO voc_eval.py: 171: [ 1 53 50  3 33 29 48 63 62 46 26 51 38 44 19  9 61 27  4  5 41 66 24 22
 43  6 25 23 30 11 68 45 13 60 28 31 21 34 16 32 57 59 47 20 37 56  2  7
 49 35 39 64 42 12 15 52  8 40 14 55 36 58 17 65 18 67  0 10 54]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3427
INFO voc_eval.py: 171: [ 76  80  28  63   5  99 157 103 128  95  72 123  59 107  83  78 150 113
 116  57  62  65 138  82  89 129 106  39  66  56  61  73 145 124  19  67
  92 132  43 111  11 159 144  25 126  94  97  40  81 146  96  37  38 153
  49 134  29  15 114 122  42  16 140  58 101 155 119  98  18  54 136 141
  30  84  46  85  14  27   3 139  47 131  33  51 127 149 158 130   8  69
 118 115  52 112 133  36 104   9 117 142  31  48 137  79  87  93 151  23
  77 100  45  44  35  86  10 109 105  24  26  91  90  50  20  64  17  55
  41  71   7  34  32   0  68 125 152   6   4  21  70 135   1  74 148 108
 102   2 147  53  12  88 121  13  75 120 154 110  22 156  60 143]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.2926
INFO voc_eval.py: 171: [ 19 147  93  23  48 125  51  77 107  38  80 101  18 185  78 119  16 198
  83 151 109  55  21  22 194 167  89  52  73 187 162 165 205 196 123 116
  54 212 120 197 163 179  62  42  68 199  85 166 124  36 136 127 121  97
 186  17 207  82 200 110 131 111  98  41  58 108 126  32 191 156  99 171
 143  28 188 183 173  15 175  29 114  79 189  39 170 132  47 105  34 139
  64 164   7 213  56 137 145  88   1  71  74  59   8  27  67 149  90  70
 130  87 190  12 157 193   6  94  14 203  96  72  33   0 174  57 102  91
  30 181 128  50 177   3 158 214 204 169 206 133 106   2 201   9 141  65
 192  20 210 152 160  40 168 153 178 100  45  11 103   4  43 144  49  81
  46 209 113 176 172  63 148 180 118 155 140  92  13 146 161 195  66 150
  24  10 138 104 112  76  35 117 135 115  31  69 211  95 215  53  37 142
 182  26  84  75  61   5  60 184  86  44 134 129 122 159  25 154 208 202]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3958
INFO voc_eval.py: 171: [131 188  73 148   2  29 295 247 204  98  19 304 196  41 276 261 158  69
 118  17 106 184 187 223 183 217 212 162  77 190  55 240  63  32 159 313
 219 191 138 279 116 171   6 311 300  13  40 152  27 178 224 242 154 120
 133  97  26  10 185 222  88 144 210 140 193  31  12 287   7 170 146  42
 182 136 283 250 142  56 189 195 236  62  96 207  60 315 198 151  84 301
 205 134  45 294  18 216  43   4 174 128 135 299 229 260  23 234  68   1
  94  61 226 100 267  54 288  22 194   8  52  11 269  16 312 165 125 233
  34  75 202 211 206 281   9 164  24 199 201 282 172  36 280 238  37 160
 220 124 117 177  48 252  78 235 166 241 215 255 112 127 153 264  81 103
 259 270 271 109  74  20 257  66 317 105 110 157   5 209 123 132 141  72
  30 256 214 251 208 292 308  91 245  46 102 168 139 213 155 173 218  51
 265 306 254  95  90 290  93  39 180  15 121 253   0  86 192  33  35 203
 176 293 291 221 266  21 107 268 227 278  14 231  99 263 318  38 181 289
  59 309 248 310  58 186 258 113 298 228 179 137 122 129 145 302  87 200
  76  70 104 130 246 115 147 237  57  47 297  67 111 314  53 156  85  92
 249  79  64  80 272 244  25 305 296  49   3  82 277 316  28 167 143 239
 230 232 114 108 286 285 273 101 163 243 284  89 275 197  65 169 307 274
  44 119 126 175  83 161 225 150  50  71 262 303 149]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4332
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4077
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.354
INFO cross_voc_dataset_evaluator.py: 134: 0.644
INFO cross_voc_dataset_evaluator.py: 134: 0.253
INFO cross_voc_dataset_evaluator.py: 134: 0.300
INFO cross_voc_dataset_evaluator.py: 134: 0.534
INFO cross_voc_dataset_evaluator.py: 134: 0.602
INFO cross_voc_dataset_evaluator.py: 134: 0.465
INFO cross_voc_dataset_evaluator.py: 134: 0.047
INFO cross_voc_dataset_evaluator.py: 134: 0.491
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.413
INFO cross_voc_dataset_evaluator.py: 134: 0.059
INFO cross_voc_dataset_evaluator.py: 134: 0.423
INFO cross_voc_dataset_evaluator.py: 134: 0.738
INFO cross_voc_dataset_evaluator.py: 134: 0.590
INFO cross_voc_dataset_evaluator.py: 134: 0.593
INFO cross_voc_dataset_evaluator.py: 134: 0.343
INFO cross_voc_dataset_evaluator.py: 134: 0.293
INFO cross_voc_dataset_evaluator.py: 134: 0.396
INFO cross_voc_dataset_evaluator.py: 134: 0.433
INFO cross_voc_dataset_evaluator.py: 135: 0.408
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.547s + 0.002s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.384s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.373s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.362s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.369s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.371s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.368s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.369s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.369s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.368s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.368s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.372s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.371s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.529s + 0.001s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.382s + 0.003s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.386s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.371s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.369s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.371s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.383s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.379s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.375s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.374s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.372s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.372s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.370s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.500s + 0.002s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.366s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.368s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.367s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.364s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.360s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.360s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.358s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.359s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.362s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.362s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.363s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.363s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.341s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.365s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.364s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.365s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.363s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.356s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.358s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.357s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.358s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.357s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.355s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.355s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.355s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.547s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [183  54 231  61   7  23 125 290  44 146  99  46  55 219 130  87 175 155
 306 297 312  85 179 323 204  50 144 184 132  51   3 120  29  31 154  68
 135   8 271 256  39 186  47  93 106 160  84 236 326  45  25 158 321  12
 274  74 140 273  27 174  17 157 243  21 227 113 210 162 283  59 180 100
 229 122  26  63  20 259  37  89 115 185 230 168 302 242 291  36 261 258
 176 228 319 254 240 320 169  60 280 198 191 239 235 289  52 309   2 170
 315  75 141 305 314  88 267 209  42 161 171 102 205 206 296  79 295 202
 223 163  24  92 311 328  43  53 226  49  30 104 139  73 275  62 272 234
 138 178 136 246  81 214  83 241 224 324 248 207 266 129 150 318 133 208
  56 201 151 167 287  86 114 253 331 279 262  18  95 281 105 292 332  96
 284 149 325  65  34 316  67 245 147  71 187 237  70 193 264 131  19  72
 111 218 165 215 116 299 156 322   0 101  38 107  97 260  48 109  91  28
 108 123 233 268  15 192 232 222 121 304 103 317  40 307 110 112  14 269
 166   4  78 196  16 203 327 265 159 249 257 127 200   9  90 285 152 197
  35 190 277 211 278 199 216 181 212 194 308 330  69 172 118  94 189  98
 294  64   5 173  57  82 251 238 263 244  80 301 298 195 255 134 119 148
 143 142  77  76  66 270 221  11 164 276 145 293 124  41   1  32  22 213
  58 126   6 128 153 117  33 182 313 247 220 286 300  13 310  10 177 225
 250 217 188 329 288 303 282 137 252]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3481
INFO voc_eval.py: 171: [ 71  35 231 153 201 270 259 257 272 271 156 173 148 274 202 268  31 215
 245  34 196 205 273 106  82 113 265  10  21 175 212 129 260 183  44 210
 261 197  12 256 238 103  11 258 276 269 122 251 125  69  36 105 136 280
 275 181  20 123 207  77 279 223 225  63  32 241 167  45 131 250 244  39
  54 168 283 203 120  59 118 174  89 267 249 190 286 150 214 163  43  52
  94  13 138   1 149  55  67  81  70 227   6  87 194 191   9 112 100  49
 220 206 228 264  47 164 102 216 213   2 139  37 208 166  14 160 246  92
  61 252 278 185  22 204 176  42 199 226 145 132 262  65 141 239 143  62
 217  78   5 184  75  29  79  53 154 180 200 222 133  73 209  64  18 277
 151 253 240 247 115 127 142 152  76  15 172  85  72 233  48 109 266 285
  99  98  46  23  80 218  90 137 101   4 107  66 235 282  50   3 232 146
  97 182 281 134  24 157 158 186 121  27  38  41   8  16 221  40 255 155
 144 187 117 242 170  26 130 124  88  86  83  74 263 179  25 119  33 189
 171  84 178 177 147 234   7  57 111  19 224 287 162  96  60 237  95 140
 161 192 254 193 188 219 110  68 165 230  93 135 243  58 229  28 126 236
  30 128 116 198 104 114  91 211 284 169  51  17   0 195  56 248 108 159]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5464
INFO voc_eval.py: 171: [ 66  81  27  50  70   1  78  49  51 177 159 179 178 161 132 102  29 166
  38   2 141 103   7  61  63  72 137 185 144 197  87  77  52 200  41 193
 140  64 111 123 192 190 147  54  36 205 180  33  62 148 168  71 149 107
 154 163  11  73 186  79 152 204  82 143 181  43   3 133 206 145  67  24
  42 182 129 191  32 135 169 201 187 170  40  20  10  74 124  25 196 114
 117 136   8 128  13 118  80  58  76 125 108  46 142 126  91 121 105  56
   4 175 156 120   0  90 184 158 165  88  48 138  94  12  23 110  65 151
  19 116 203 150  99   6  57 153  22 106  98  97  96  83 194 162 183  45
 174   5  17  75 113  28  55 198  14  95 172  15  34 112  21 127 188  68
  69 115 122  44  60  18 189  89  16  84  31 171 199 155 164 100  92  37
  30 109 157 146  93 176 173 139 119 130  86 134   9 167 202  47  53 104
 160  59 131 195  35  39 101  85 207  26]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2469
INFO voc_eval.py: 171: [440 372 373 119   1 103  57 300 374   7  25 165 301  61 410 135 543 537
 432 134 214 405 294 299 478 221  99 397 304  26  29 527 349  38 516 468
 375 302 480 278 113 193 162 212 292 609 361 286 115 152  71 184 240 533
 312 568 126 458 542 489 502  19 572 219 564 470 345 395  31 195 344  74
 296 317 140 477 453 396 100 366 303 446 202 306 138 488 290 245 614 209
  46 170  59 441 271 124 272 581  23  36 237 352  70 469 169  28 429 216
 544 507 348 512 525 285 463 249 295 591 177 606  78 588 120  91  52 546
 500 423 498 106 185 144 242 594 340 381 554 494  22 602 511 368 378  37
 379   6 161 555 570 451  54 585 313 550 552 563 549 565 531 508 291 191
 605 593 365 454 311 346 509 553  88 166 211 392 146 522 324 578  40 255
 280 452 180  80 404 270 338 213 403 254 263 532 154 350 595  84 383   3
 258  33 187 281 260  45 566 420 288 150 132 505 575 228 243  35 110 318
 444 460 492 142 293 474 412 580 250 371 603 316 123  32 377 128 479 238
 382 276 385 153   9 226 539 205 437 425 343 462 194 159 215 223 483 388
 526  47 598 336 283 229 384  98 426 171 481 246 518 253 448  79 149 582
 244 376  75 274 551 222  92 197 401 409 308 141  76 179 400 597  14 435
 199 201  72  60 163 136  97 203 407 188 567  51 486 520 600 241 427 164
  96 252 108  16 287 341  13 217 145 210 438 471 357 118 347 315 297 364
 612 143  10 358 419 456 472  18  69 367 422 562 182 461  48 524 424 360
 330 314 491 332 455 363 413  41 168 337  77 611 464 334 586  63 431 587
 333  66 577 200 264 275 559 289  27 536 267 319 541 408 473 122 155  53
 398 466  44 104 356 127 331 225 475 485 247 545 399 261 174  94 530 284
 447 147 266 414 443 328 394 540 351 584 109 190 529 117 503 504  12 519
  56 186 389 133 434 390 173  20 172 417  42 579 130 129  21 569 326 499
 528 239 459 442 574   8 523 196 112 607 387 487 583  68 262 114 418 450
 510 321 608 105 457  11  17 259 273 590 339  15 571 476 248 158  43  81
 322 445 610  87 557 230 362   5 355 256 327 116 497 506 157 207 282 558
 556 501 151  67 513 235 101 121 137 265 421  90 107 514 416  86 538 402
 599 490 436 181 548 386 596 465 231 604 535 309 467 353 406 268 257  82
 325 198 380  58 601 592 307 204 178 232 175 160 449 329 218 430 131 167
 277 305 547   2  83 415 482 439  39  93  65 484  64 139 323 183  89 298
 369 561 496 428 234  95 342  49 370  85 534  24 613 589 320  30 560  73
 224 335 515 493 433 411 393 359  55 269 233 220 310 156   4 176 521 111
 189  62 495  34 206  50 517 125 148 576 208 573 236 251 279 354   0 192
 227 391 102]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3054
INFO voc_eval.py: 171: [397 386 387 215 194 218 195   2 396 326 219 241 325 198   5 180 392 373
 271 487 197 489 181 196 110 294 313 327 312 436 124 323 444 399 243 389
 258 395 391  30 167 401 430 324 408 342  76  21 415 366 405 390 328 265
 276   8 224 467 398 151 394 277 238 279 417 371 388  18 168 122 337 251
 321  42 141 255 457 393 469  29  78 357 285 222 343 466  39 423 165 419
 428   6 348  82 226 152 439 229 106 139  71 187 414  53 204 169   4 330
 429 331 322 278  40 223 267 410 117 159 132 443 172 250 304  77 404 207
 412 160  27 233 282 403 158 105 319 201 268 137 411 433 420 193   3 301
 136 355 225 478 446  50  96 456  60 365 472   9 211  16 112 179  74 212
 314 329  86 104 461 161 308 333 154 166 351 356 360 186 283 362  99 205
  64 257 367  33 220 208 116 125   0 450 471 376  83 353  12 214 427 442
 316 131 339 402 237  15 249  31  44 244 176 153 384 345 242 490 284 340
 470 336 248 338   7 200 406 409 286 149 431 432 407 349 413 246  92 305
 234 441 435 422 453  17 302 335 454 299 425 438 228 421 463  54 288  65
 142  62 177 245 199 188 240 315 477 100 295 479  91 230 183  69 274 476
 182 213  51 488 447 334 368 354  89 171 309 320 341 143 127 130 382  98
 281  28 344 352 108 164 481 235 287 318 475 383  63 458 364  32 111 480
 232 483 332  87  61 227 162  35 303 298  95 347 113  68 451  97  67 239
 379 178 145 119  56 350 170 380 418 359 437 191 263 486 174 157 400 445
 306  34  93 185 148 221  37 260 273 462 448  19 155 449 101 173  11  72
 485  55 310 346 372 216 102 206 378 150  45 311  84  90 361  26  13  85
 270 363 128 269 118  25 261  20 358 465 138 280 247 468 455 484 192 374
 272  94 264 381  43 123  81 300 144 289 459 473 369 474  73 440 259 290
 317 109 482 416  22 254 163 262 377 253 426 134 452 231  79 434  49   1
 266  66 375 189 296 140 252 114  75 292 202 121 135 297  36 120 146 147
  10  14 275 217 203  46 370 103 175  23  48  70 236 385  52 184 107 464
  41 307 491 210  57  58 424  80 115 126  59 293 190 133 291 209 256 129
  24  47  38 156  88 460]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5133
INFO voc_eval.py: 171: [ 30  61  18 157  58  21  92   4 102  77  69 122 114 167 151  27 107 131
  16 159  19 117 144 155  73  34 105 145  76 153 113  95 150 143  65  22
   1 118  23   8  25  91  75  67  57  47  26  88 136 121  81  68  59  53
  84  37  66 124 100 119  13  29 166   7 130 156  38  54 146 162 106 112
 120  45  31  85 138 135   2 134 149 158  99 165  20  41 147 140 133  35
  80 126  40 108  71  63  94  44 104  82  93  89  62 154  90 128  56  46
  49 103  32  52  36  79  12  39 111  78   3 110  28  24 109  74   6  48
 152 139  86  33  70 160 115  14 141 164   5 116  96  43 137 101  97 161
   0  50  51  83 129  10 148 163  72  17 132 123 142  60  42   9  64  15
 125 127  55  87  98  11]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6285
INFO voc_eval.py: 171: [341 320  83 156 420 465 348 507 466 151  33 178 107 453  20 230  60 440
 257 375  25 236 214 286 455 233 443 482 258 376 471   5 254 424 381 201
 450 515 371 235 421 370 283 428 337 262 418 289 177 106  62  46 319 316
 438 212 290  68 384 426 261 138 398  53 265  58  77 417 103 369 416 282
 281  92 350 181 110  42 134 397 358 372 273 189 223 300 523 269 171  63
 314 206 234 396 325 159 425 227 238 276  44 493  71  91 277 304 392 218
 467 456 357 267 432 259 172 475 429 284  70  27 176 188  61 287 409  37
  36 499 160 328 270 231 194 461 441 452 132 491  22 205 271 365 524 130
 511 308  97 243 401 127 228 307 116 318  57 226 202 390  41 170  48  80
 402 512 264 288 520 208 294 232 203  86 167 135 359  24 164 463  64  31
   4 385 141 470  96 333 311 413 334 190  35 126 213 305  16 168 393 448
 295 198 435 175   3 199 335  54 149 280  94 513 329 405 195 139 299 394
   8 266 451 204 250 229 351 154 109 336 125 180 216 378 374  73 473 101
  39  10 246 388 508 239 446 275  88 360 447 315 142 488 313 389 412 496
  56 182 111 146  17 367 179 155 108 361 404 272  81 486  32  90  26 415
 439 383 291  50 301 298 209 185 192 391 114 500  79 380 292 268 431 427
 368 502 225 406 293 516  84 322 422 490 483 437 433 469 145   2 342 220
 489 153 338 152 210 514  66  75 509  67  69 503  98 419 137 506 410 122
 263  85 480  23 347 363 105 430 400  89 387 255 147 303 362 221 248 373
  15 472 474 323 454 211 215 364 521 144 343 252 197 449 445 505  18 260
 163 274 464 237 477 136  52  87 129 249  38 317 444 434  95 165 459 161
 458 162 330  12 340 407 113 184 436 495 279 112 481 183 517 115 186 148
 379 476 143 193 349 442 253  51 118  78  47 331  49 327 104 324 133  40
 344 356 166 497 498 240 296 219   1 173 310 191  21 353 460 245 256 244
 306 382 332 386  99 457   0 117 102 241 157   7  55  30 484 468  74 140
 479 121 124 403 395 352 492 217 408 123   6  13  14  45 120 312 478 355
  43 309 321 326 200 187 247 150 131 485 339 345 411 222  72 100  34 525
 354 174 207 377 285 519 346 169 196 501 224  11 504 414 302 518 366  76
 462  19  82   9 494 158 487  65  28 399  59 297 278 128 251  93 242 119
  29 510 522 423]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4611
INFO voc_eval.py: 171: [15  7  4 71 16 30 62 68 51 23 13  0 58  6 25 57 44 38 63 11 45 22  9  8
 31  3 69 64 52 26 40 29 17 53 60 32 28 56  5 12 41 20 70 27 18 61 55 39
 24 72 33 37 50 48 47  2 49 43 59 21 42  1 14 54 65 46 35 10 36 34 19 67
 66]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0286
INFO voc_eval.py: 171: [1683  176 1687 ... 1767  270  970]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5144
INFO voc_eval.py: 171: [18 31  2  3 48 12 65 15 70 32 74 44 72 16 75 56 40 63 67 29 23 14 22 62
 49 47 54 39 41 69  5 76 46 51 55 57 37 24 34  4  6  8 73 64 58 52 60 13
  1 28 42  0 35 25 33 11 36 71 26 45 43  7 59 61 20 66 19 21 10  9 53 38
 17 27 30 50 68]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1753
INFO voc_eval.py: 171: [199 447 110 193 214 386 275  50 264 286 170  57 477 155 384 284 283 427
 392 399 270 194 171 330  56 299 310 240  91 169 475   1  23 296 500 287
 127 327 362  30 448 366  11 503 196  70 471 411 319  22 285 365 412 173
 154  87 217 218 126 185  73  97 459 153 322 396 483 311 191 339  78 201
 320 443 440 481 380  24 460  19 430 485 321 338 509 303 469  53  34  35
 141 120 422  74 219 288 163 292  79 507 432  69  58  68  43 297 226  47
 174 348 229 113 117 467 473 274 200 329 144 499   5 116 232 281 437 304
 245  16  28   8 349 132 451 182  38 457 166 488  33 416 165 277 192  64
 463 258   0 476 139 301 421  27  81 112 318 387  18  17  80 419 248 435
 382 237 357 407 175  66 413 371 508 164 186 307 405 313 294 410 220 241
 409 480 465 128 344 152 149  45 261 263 444  54 504 216 104 124 211  14
 257 479 272 221 496 493 259 369 424 345 402 267 461 363 341 470 249 332
 136 114 494  39 131 178 439 334  65 260 167 273 323 315 374 268 486  26
 415 495  83 280 145 183 105 351 256 342 446 101 309  89 290 347 383 295
 213 324 458 428  96 119 250 376   3 143 336 360 150 262 498 158 391 445
 441  10 122 238 367 187 252 177  59   2  67 429 317 454 453 147 487 401
 246 225 462 247 123 190 343 350 168 395 197 300 111 466  85   9 172 188
  20 202  77 142  49 107 137 406 189 195 279 212 484 160 434 364 130 121
 372 180 209 129 389 375 146 333 464 118 198  93 326 125 346 162 204 393
 230 109 352 390  75 282  52  90 271 436  95 207  94 222 134 266 388 140
 228  55 210  36  32 426 176  76 242 418 305 265 425 400 356  25 276 224
 227 452  71 269 306 397 358 340  37  44 377 302  51 433 293 234 308 368
 455 236 135 325 243 381 133  61 355 231 370 501  88 331 233 403 423 431
 148  72 253  84 205 244 490 312  82 468 106 157 450 353   6 206  48 298
 478  92 108  41 354 208 239  31  60  40 417 361  13 161 492 489 510 184
   7 156 115 385  99 378 359 337 151 251 472 254 278 442 181 502  62 328
 379 100 103 394  15 138  12  63 102  46  29 505 179 255 215 235 203 335
 314 316 482  42 414 497  86 491 398 506 438   4 223 159 420 408 373  98
 404  21 456 291 289 474 449]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4182
INFO voc_eval.py: 171: [105   1  15  62  54  86  23 114  60  79 117 115  30  63   4   7  50  49
   0  20  21 116  85  51  27  19  28  32  73 123  44  40 108 113  53  74
  80 122  59  46  90  76 119   6  64 100 112 124  35  33  37  22  29  34
  24  25  78  18 104  91  14  41  39 101  81  70  58  83  89  99  61   5
  10  56  65  87  71 120  97  52 109  77  88  94  96   8  68  57  11 106
  95  72  75  47  82 111  43  26 107  69  31  17 102  92  12 103 110  67
   2  45  84 118  36 121   3  16  66  93  42  13  48  55  98  38   9]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0564
INFO voc_eval.py: 171: [67  8 10 59 46 80 82 57 58 25 56 49 55 84  7 48  9 70 71 34 29 75 11 40
 52 76  2 14 15 24 17 21 28 72  3 77 13 45 37 66 74 19 16 78 62 38 83 31
 44 65 68 22 18 20 41 60 35 33  6 32 53  5  1 26 69 54 27 43 64  0 51 39
 42 79 36 81 50 73 47 30  4 61 63 12 23]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.4360
INFO voc_eval.py: 171: [ 30 188  19 123 196  88 180 128 143   6  93  12 169  32  82 129 138   5
 179 133 141 162  52  85 150 153  13  58 100 130 106 154  63 192  43 111
  48 124 118  74 202 104 101 145 152 103 112  87 159 164 142  71  76 160
 137 136 113  61  51  90  81 201  37   1  53  65 205 131  56  59  72  28
 191  98 168 158  96 184  86 194   4  24 108   2 140  14  15  16  99 107
  36  62 181 151 193  33 203   9 163 155 122  22 197 119  55  75 204  11
  50 117 170  20 161   0  68  77 183 135  49 110 139 147  95  39 116 186
  31 146  42 173  34  84 134 125  25 176 105 195  70  47 172   7 102 165
  10  60 187 206 174 190 182 156 207 149 177  27  40  78  21 144  46 114
  97  38 166 120   8  44  45 121  73  41 200  79  80 175 115 189   3 127
  91  64  69  83  23 157  94 167  66 126 178  92 198  89 185  29  67 199
  17  57 109 132 148  26 171  54  18  35]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7466
INFO voc_eval.py: 171: [1764 1907 1098 ...  771 1876  606]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5842
INFO voc_eval.py: 171: [119 148 474 214 766 239 710 189 644 156  81 305 133   7 225  78 152 604
  80 708 296   8  83 518 240 768  55  79 463 610 563 476  82 394 499 679
 647 433  87 226 795 242 767 263 165 675 275 272 453 555 438 227 217 709
 174  91 703 173 707 368 559 611 519 115 688 354 777  57 510 357  90 134
 142 507 141 615 425 769 779 306 695 681 220 396 140 775 824 717 342 729
 319 398  66 628  40 771 492 686 487  12 651 613 341  47 693 284 614 277
  67 834 606 188 356 643 446 486 456  15 573 788 169 520 751 645  52 128
 293 154 376 676 830 711 509 219 528 336 523 103 560 752 229 454 694 465
  68 495 797 415 304 570 617  39 770 458 399 564 307 468  95  70  97 228
 321 452 616 546  17 531 612 748 279  59 218 623 265  85 348 231 493 355
 384 488 738 181 475 395 513 378 646 428 653 535 589 407 207 483 712 772
 835 369 489  41 716 259 825 382 232 480 823 727 313 746 521 338  10 536
 600 180 363 500 696 278 139 339 719 262 312 450 632 790 266 120   1  65
 190 170  94 308 656 836 784 801 798 160 587 477 149 744 774 608 634 401
 778 667 343 581 545 132   9 287 429 839 566 196 330 720 714 179 267 387
 236 780 419 455 596 195 391 472 799  72 478 756 397 624 626  49 286 469
 649 544 598 698 435 323 776 244 697 167 145 440 562 171 462 794 388  34
 524 230 591 816 274 753  92 311  42  61 837 637 238 344 731  29  25  18
 268 416 235 349   4 159 764 803 832 191 208 556 585 654 813 831 657 808
 381  62  11 360 685 680 370 205 105 322 603 282 840 789 484 392 765 574
 533 269 441 445 807 276 597 822 506 619 629 527 346 640 182 402 763 692
  96 529 110 385 742 701 609 512 548 622 588 295 687  89 665 534 150 163
 153 541 539 575 206 762 302  38  88 328 683 718 261 508 359 417 161  99
  56 280 739 172 470 705 177 467 838 200 202  30 471 567 532 138 601 633
 590 491 123 515 300 316 329 671 135 621 400 572 490 670 318  93 540 740
 347 504 367 660 324 294 331 327 204 289 466 426 209 408 129 586 192  64
  20  48  51 479 554  63 580 741  24 447  14 340 793 582 351 187 482 631
 641 627 743  23 699 248 558 245  36 553 260 405 162 130 379 137 733 449
  43  53 517 436 421 151 320 551 648 781 365 761   0 434 184 444 374  46
 833 424 201 352 337 403 669 666  19 485 345 735 734 451 255 754 713 234
  31 594 800 285 568 809 427 806 432 309 443 787 144 136 291 547 108 542
 430 252 131 650 164 303 501 682   6 505 583 325 104 124 818  26 386 372
 550  75  50 404 183 579 459  28 827 292 792 101 706  60 442 221  71 726
 724 297 166 722 526 194  77 460 829 578 116 760 155 250 636 223  22 281
 503  45 146 121  54 625 411 332 664 273 126 113 678 326 502 552 334 109
 821 728 658 389 375 715 186  16  32 677  84  74 569 414 747 147 237 723
 557 745 364 270 773 668 203 543 642 412 811  37 114 618  73 178 672 549
 216 111 247 721 143 635 607 251 371 652  58 413 786 211 353 577 288 805
 815 804 537 605 439 638 782 814 215 662 253 122 464 358 107 538 298  21
 457 125 224 437 737 522 193 785 750  69 158 783 725 420 602 290 565 494
 584 497 700 496 410 431 674 689 256 530 828 759 283 461  27   3 271  86
 418 673 819 812 301 749 473 663 299   2 383 802 377 199 571  98 390 117
 639 213 361 732 810 561 423 796  76 817 595 511 758 157 112 241 481 661
 393 448 525 684 222 704 691 362 730  35 690 176 175 593 212 366 106 350
 335 333 243 516 422 118 406 820   5 100 314 659 254 791 127 246 755 620
 185  44 315 249 102 310 630 655 826 599 702 168 576 210 198 233 380 757
 258  13  33 373 409 592 257 197 317 264 736 498 514]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5846
INFO voc_eval.py: 171: [60  2 58  1 34 37 69 55 53 30 71 50 61 68 73  5  9 23 74 33 46  7 13  4
 47 32 49 29 36 35 67 66 27 28 15 63 64 51 39 38 26 24 40 25 62 43 31 14
 41 16 59 54 17  3  6 18 10 11 56 19 22 44 72 12 48  8 42 45 65 20 57  0
 52 21 70]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3061
INFO voc_eval.py: 171: [ 71  72  22  60  99 153   4 103  68 131 104 120  78  95 114  55  80  92
 111  36  63  90  58 155  75  62  53  88 137  91 125   3   8  52 101 146
 112 143  64 108  30  11 151  94  59  47  54  61 142 144  44  93  15  37
  27  28 138 102  76 119  12  69 133 110   6 121  79 117 100  29 154  13
 140  42  31 136  26  10 132  16  70  24  39  48  84   9  41  14  38  32
  83 150  87 115  96  89  35 122  33 127  40  85  65  50   7  23  74 135
 126 116   2  49  20  46 113  34 129 147  45  56  51  17 107   5  67   0
 123  81 118 130 145 134  97  25 148  66 139  77 109  98 105  19  82 124
 141 152 106 149  73  57  21  18   1 128  43  86]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3512
INFO voc_eval.py: 171: [118  14  77  18 100  63  40  42  88  30  67  85 153  13  64 156  89  43
  95  70 171 166  11 135  16 103  98  17 174  74  45  61 120 136 130  80
  90  12  50  96  94  56 158  24 148 101  91 141 172 109 112 168  35  28
  37  58 131  47  82  41 116 138 180  21 145 124 170 176  69  73  97  81
 108  71 157 151  99  62  22 142  87  10  54  26 162  93  65  31 113 160
 128 117  46 179  25 104  36 105  55  44 133 132 146 119   3 126  86 164
   0  15 143 121 137 102  92  78 127 150 169 134  57 181 110   4 177  83
 114  48 129 155  60  53  52 165  20 140  72  84  23 167 175   5 149  51
 178   6 115 159  27 152   2 163 122 161   7 106  75 147  19 111 123   1
  34 125  49 144  32 139  79  76  38   8 107 173  39  66 154  68  59   9
  33  29]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.4449
INFO voc_eval.py: 171: [140  81 205   4 161 221 319  34  23 271 170 200 286  77 328 108  20 298
  45 215 125 228 222 199  37 204 236  71 206 116 175 262  89 337  64 172
 241 184  99   7  48 208 312 213 325 107  27 148 198 335 340 238 166 210
 267 227  36  46 183 123 242 127 152 145 276  31 150 313 167  16 202 247
  52 156  12  62 182  24 211  47 110 135   3  50 307 142  76 300 207  63
 293 341   8 118 239  42  57 106  61  70  72 160  32 318  28  95  51 244
 164 185 250 137  10 173 304 193 117 168  65 212 131 144 237  13 261 214
 326 223 153  39  80  25 113 180 280 342  29 124  67 278 189 336 303  78
 187  87 279 285 301 268 104 119 266 130   9 235 339  92 323  18 259 191
 240 306  69  19  15 282 195 284 141   6 136  91  94 281 294  38 310 209
 309 243 263  86 115  75 299  96  60 151 102 132 234 291 143 165 128 290
 314  82  43 248  30  22 283 329 178 260 174 258 287 264 220  93 330 196
  66  11 138 332  21 253 311 257 216 188  74 333 100 149 230 296 103 292
  85 159 194 277 246 315 274 171 302 126 218  90 231  98 190 114  41 201
 252 163  59 157 154 316 289 133 197  17  44  14 225  26 249 169 112 179
 162 308 322  40  88 288 327  35 334 229 272 270 105   5 251 203 219   0
 269 181 147  97  54   1 297 109  49 320  83 317 232 275 192 139 254 111
 176 121 265 101 256  53 224 338 255 217 122 233 158   2  33 177 324 321
 120 155 129  84 134  68  55 186 331  56 226 305 295  58  79 146 245  73
 273]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4561
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4076
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.348
INFO cross_voc_dataset_evaluator.py: 134: 0.546
INFO cross_voc_dataset_evaluator.py: 134: 0.247
INFO cross_voc_dataset_evaluator.py: 134: 0.305
INFO cross_voc_dataset_evaluator.py: 134: 0.513
INFO cross_voc_dataset_evaluator.py: 134: 0.628
INFO cross_voc_dataset_evaluator.py: 134: 0.461
INFO cross_voc_dataset_evaluator.py: 134: 0.029
INFO cross_voc_dataset_evaluator.py: 134: 0.514
INFO cross_voc_dataset_evaluator.py: 134: 0.175
INFO cross_voc_dataset_evaluator.py: 134: 0.418
INFO cross_voc_dataset_evaluator.py: 134: 0.056
INFO cross_voc_dataset_evaluator.py: 134: 0.436
INFO cross_voc_dataset_evaluator.py: 134: 0.747
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 134: 0.585
INFO cross_voc_dataset_evaluator.py: 134: 0.306
INFO cross_voc_dataset_evaluator.py: 134: 0.351
INFO cross_voc_dataset_evaluator.py: 134: 0.445
INFO cross_voc_dataset_evaluator.py: 134: 0.456
INFO cross_voc_dataset_evaluator.py: 135: 0.408
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 7499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.351s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.372s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.359s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.363s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.372s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.369s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.362s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.362s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.362s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.362s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.361s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.364s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.364s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.563s + 0.002s (eta: 0:01:10)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.385s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.383s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.373s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.375s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.379s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.385s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.384s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.379s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.381s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.380s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.381s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.375s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.393s + 0.002s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.375s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.369s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.380s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.376s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.375s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.374s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.371s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.368s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.366s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.366s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.364s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.364s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.603s + 0.001s (eta: 0:01:14)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.374s + 0.003s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.359s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.352s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.349s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.348s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.351s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.352s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.353s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.353s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.354s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.354s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.353s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.961s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [176  60 225   6  53 290  96  20 122  44 211 138  54  50 147 167 124  87
 298 311 145 308 177  82 170  43  83 115 137   2  49 322 197 178 321  46
  29 128 126 255 270  45 326 271 234  23 103  81  67  26 150  38  12  92
  72 132 109 155 283 258 242 222 318  56  24 117 153 220 156   7 261  18
  28 241 182 291 313 203  98 166 240  34 304 223  59 168 111 239 319  51
   4 173 280 162 266 257 259 152 160  36 279 310 129  41 306 253 272 161
  75 135  32 190 196  42 216 309 127 163 143 200  79  16 289  55 146 148
 172 314 198 245 139  57 194 273 207 296 221 213  90 278 287  84 328  99
 131 202 246 110 201 187  37  21 293 159 292  17 324 252 199  85  80 299
 180 330  58  66 141 102  11 120 112 250 149 233 101 151 121 235 244   1
 181 195 133 277 231  73 262 230  22 192 208 210 118 205  48   0  88 105
 165 142 265  27  65 331 119 114 219 217  25 184  74  62 113 189 332 134
 256 171 237 164 204   9  71 327  93  69 264 179 323 274  33 286 267 251
 263 209 254   5  52  95 302 285  94 106  15  91 315 123 297 107 305 157
 100 312 236   3 307  14 268 316  70  86 191 116 154 108 193 228  40 136
  76 249 158 144  61  13 188  30  77 248  63 317 281 229  19 243 212 276
 247 294  35 175 269 320 303  78  68  97 295 214 186  47 284 218  39 275
 260 282 206 232  31 169 140 185 301  10 130 325 329 227 300  64 215 104
 183 174 288 226  89   8 238 125 224]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3532
INFO voc_eval.py: 171: [ 68 226  32 145 194 261 271 258 148 272 275 142 166  29 274 196  21 260
 210 195  31 242 115 189 120 262 109 268  78 190  11 204 169 267  14 179
  33 257 103  43  66  20 176  99 284 119 241 128 273 248 102  35 143  44
 201 160 222 278  13 220  54 116 199 123 237 162 217 168 280 197  52  92
 111 287  75 244 100 224 288 113 167  30 225  61  41 238   1 270 186  12
   7  60  34  47 174 152 133  87 212  77 252 140 221  65 177  55  98 240
 156 239 249  28 283 216 146 118 246  84 151  90  85 108 125  67 286 158
  22  48 144 265 157 188  18  38 112  46 132  40 269 154 131 192 229 218
  70 253 285 181 211 137  97 138 209 173 247 165 227   2 150 291 207 149
   6 135 193 263 175 266 282 245  64 198   9 228  53 223 187 139  39 289
   8 236  23  73  81  95 105 243  58 107  74 219  71 136  10 251 208 256
 126  59  63   5 230 281  56 213  96 134 259 101  69  83  82  72 124 147
 114 130  88  94  25 290  37 178 129 141 153  76 254 233 191  15 184  49
 205  86 279  93 170  17 232 183 185 231 121 163 200 235  91 122 155  27
 206 172 202 127 292 234  26 164 161 255 250   3  16  19  24  36  50  57
 277 276  80 215 203 182 117 264 104  89 180  42  45  51 106   0 159  79
 214   4  62 110 171]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6386
INFO voc_eval.py: 171: [ 64  27  80  48  47  69   1  77  46 173 158 175 129 174 160  96  24   2
  58  34  97  61 198 161 140   8  49  76  82 143  71 184 136  52  36 116
 190 193 121  50  12 176 153  60  59 147 204 145 201 138 205  30  70 150
  81  79 195 191 185 162   3 166 146  33  72 122  40 178  23 144 165  73
 168 141  37  87 188 102  78 126 182 131  19   6  29 113 177  11  14 133
 171 180 114 186  22 128 139   9  98  53 142  65  42 179 118 130 151 200
 124   4 135  92 115  74  17  35 137 202 196  86 127 149 163 109 172 117
  55  75 167 105  57  67 181  94 119  84  32 112 156 155  88  38  13  16
 101 148  93  28  44  85  99  95  54 111  26 103  25  45 189  15 154  83
 183  66 125 106 132 169  91  56 187 104 108 100 164  63 192 197 199  20
 157   0  41  68  62 152 120  21  89 194   7  43 203 206 110  18 123 159
  39  31  10   5  51 170  90 107 134]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2442
INFO voc_eval.py: 171: [345 409 346 107   0 347  90  50   6 278  22 148 126  54 280 510 381 398
 379 273 277 123 504 279 205 211 443  87 127  24 369 348 147 199 272 434
 281  33 324 256 182 444 101  28 137 334 484 492 264 103  17 224  88  61
 500 290 410 210 569 172 573 533 185 436 367 530 420 320 536 110 505 442
 456  23 340  63 513 425 283 470 432 114 454 292 108 192 158 519 251 284
 228  26  21 288 396 318 275 416 342 322 435 351 514  89 268 368 522 149
 207 159  31   5 313 197 125  67 515 562 353 226  32 132 329 271 221 165
 180 175 476 184 462  52 524 301 547 263 446 200 421 382 479 232 274 529
 566 535 516 267 194 554 466 299 338 506 477 238 543 363 518 352  59   8
 473 422 130 392 475  47 555 480 551 293  20 520 331 315 136 183 177  36
 250 553 328 497  77  38 150 314 321 133 140 468 235 488 451 167 139 464
 258 266 550 537 237   9 249 203 426 213 131  74 494   3 236 354   2 154
 375 294 460 542 242 124 157 109  43 457 142  70 309 415 296 146 168  25
 304 244 412 449 440 571 152  11 187  53 302 141 373 499 395 357 411 240
 298 391 388 563 371 326 286 229  40 212 385  81  69  45 376 557 289  80
 389  84 234 565 364 402  42 532 135 455 222 129  29 248 487 217 311 418
 372 509 333 269  55 121 261 276 427 144 337  94 544 112 253 350  99 406
 356 559 255 319 502 128 523 485  96  66 265 431 378   7  16 344 380 190
 312 407 539 188 161 355  58 517 195 310 469 558  14 490  93 512 106 170
 394 245 339 206  49 428 423 208  97  34  44  65 198 247 343 151 332 252
 282  86  62 450 246 438 531 176 390 552 478  98  85 156 453 511 561 225
 387  51 546 493 335 383 472 214 424  46 429 397 465  18 525 528 336 471
  35   4 361 447  39  68  10 215 548 323  64 327 287 155 377 507 285  15
 568 202  82  83  56  12 105  71 401 360 540 306 366 178 193 498 430 104
 441 138 119 489   1 227 399 171 209 570  13 223 365 120 116 541  79 341
 189 508 496 486 259  57 134 100 556 164 291 118 482 545 160 437 233 300
 297 305 572 527 102 358 162 316 173 374 370 191 181 169 303  75 111 467
 458 483 115 218 393 448 145 534 439 419 262  60 220 400 386 230 349 538
 463 216 325 163  91 179 417 174 526 414 307  78 317  37 408 495 231 243
 113 549  73 122 452  30 295  76 241 405 143 270 560 219  27 461 330  48
 474 481 153 503 362 521 359 254  95 204 564  41 491 459 567 308 445 201
 196 404 413 403  19  72 166 384 433 501  92 117 186 260 257 239]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3270
INFO voc_eval.py: 171: [427 234 428 429 207 209 238   2 432 437 356 206 355 237 260 194 208 527
 294 118 412   5 526 340 195 210 341 137 315 353 452 264 485 479 357 440
 431 354 179 435 433 360 371 464 475 442  37 279 448 434 436 456 358 439
 499 430  25 444 245 299 163 289  81  14  83 351 457 130 300 271 408 508
  20 389 258  35 450  82 301 370 265  48 164 459 277 507 180 308  57 248
 240 374 506  53 252 380 185 290 203 177 368 361 154 446  76 362  31 181
 438 114 217 303 144 352 471 401  90 150  47 396 454 131 342 373 453   3
 460 359 249 152  12 254 443   4 213 272 329 326 219 201 387 476 205 472
 291 166 147 169 393 305 375 349 221 493 239   7  95  54 487 441 509 445
 257 421 247 136 400  15 384 171 397 369 470 232 123 517 483  19 172 306
 186 183 391 529 268  18 312 455 261 229  65 255 105 224 167 165 462 365
 395  49 426  36  59 193  38  69 138 386 451 212 363 528   0 278 128 463
 416 367 161 173 511 480 379 226  91 178 230 267   8 500 324 325 113 270
 307  13 422 474  66 406 243 259 211 110 364  68 468 202 510  74 280 409
  70 344  27 513  58 345 251 388  75 263  39  51 197 330 100 187  42 503
 199  99 497 204 461 244 309 489 311 191 372 515 317 333 141 246 155  41
 447  10 478 282 190 176  16 250 321 107 262   6  60 424 494 120 399 404
  30  55 383  72 423 394 214  17 157 336 482 112 350 196 266  62  79 366
 304 132 411  44 115 192 390 496 198 296 505 151 492 343 520 295 331  98
 101 256 418 235  88 140 175 218  78  73 139 322 142 504 125 146  97 398
 215 285 314 222 522 382 302 106 498  50  32 182 148 465 518 159  77 281
 318 134 231  23 143 168 403 327 316 323  64 223  94 486 407  34 466 103
  24 320  28 288 284 108 158   9 319 488 467  22 310 275 156 242 348 153
 512  43  56 116 220 133 413  86 188 102  92 269 376 458 145 216  61 490
   1 484  11 104 135 184 122 346 530 381 233 253  87 236 402 414 417 477
 519 415 174  45 287 328 335 525 491 521 392 274 283 160 469 119 241 292
 524 385 449  84 227  89 523 276 225  63 337 481  33 286  67  29  96 377
 127 332 313  85 405 162 420 149 273 502 516 339 124 117 473  46 126 109
 298 338  71 200  26 419  21  40 121 293 425 334 347  93 410 514 170 501
 495 378  52 297 189 111 129 228  80]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5452
INFO voc_eval.py: 171: [ 35  21  62 152  59  28  94   5 126  69  78 146 102  40 114 118 107 150
 159 121  26  89 113  19  34  73  22  76 116  65 148 145 106  98  31  29
  96   2 154 130  77  27  10  91 151  68 137  58 136  49 123  67  54   0
  42 105  32 139 124  47 140  16 120 100 119 112  43  55  84 155 153 122
  45  66  95 132  25  61  82  37  24  11 129  99   6 158  15  88 142  30
 117 104   9  85  93 127  14 143 149  41 135  83  70 131 101 109  74  38
  48 108  46  56 103 144  72  17 157  64  60  53 138   7 125 156  80  87
  23  51  44 115  71  79 133 134  20  75  33 147  90  50   4  12  63  81
 128 141 111   3  18  86 110  13   8  52  36  39  97  92   1  57]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6133
INFO voc_eval.py: 171: [340 319 148  82 423 510 467 143 346 104 173  18 255  28 456 466  64 231
  24 286 374 237 210 445 459 447 234 262   4 468 485 378 194 369 251 372
 426 427 421 263 453 368 172 103 337 473 282 431 290 233 517 130 101 291
 348  41  57  72 208 257  91 318  92 264  25 428 196  44 280 398 494 265
 443 226 370 267 299 521 166 397 357 183 381 312 126 105 174 271  42  76
 420 315 220  61 399  54 455 270  50 477 236 182  68 269 287 198 496 152
  32  58 273 213  21 446 281  70 430  33 413 266 356 229 256 419 124 158
 307 522 276 193 458 435 464 302 195 433 238 259 230 392 275 121 189 120
 320 367 317  39 117 505  67 327 303 225 204 112 268  96 241 472   3 209
 258 168 416 171 304 451 405 514 185 339  31 232 483 284  85  45 402 295
 199 486 330 513 309 162 190 170 383 336 454 151 439 140 165 476  23  53
 289 415  59 519 131 133 515 342 240 235  86 351 448 388 363 161 511 390
 248 412 394 461 465 279 245   8 296 350 272 389  51 432 393 156 297 358
  63 429 335  83 298  36 450  27  15 102 212 107  69 176  65   2 192 313
 146 479 177 311 108 187 359  94 408 366 425 228 128 501 205 499 385  60
  56 137 111  84 180 219 294  17 175 106  79 300 216 223  48  10 260 365
 134 492 242 184 512 509 109 409 178  22  30 457 507 437 301 471 377 404
 147 474 460 125  90 316 403 440 488 424 490 293 434 261  62 136 355  34
 401 503 347 129 436  26 153  78 322 144  49 386 345 422  66 441 138 463
 188 411 200  55 493 516 506  99 114 197  14  47 145 135 387 150 164 215
 444 206 373 308 325 328 344 214  77 504 338 123 181  38 167 154  37 391
 305 227 382 186 201 159 249 278 475 333 518 520 110 100 179 277 310  43
 252 410 449 524 115 207 239 341 132 470 247 352   0  98 502 191 211 326
 418  88   5   6 361 414  87 119 116 246 292 254 353 481 113 360  95   1
 500 243 491 495 438 217 283 349 362 274 306  73 169 323  52  19  46 482
 288  16 250 285 487  12   7  81 324 122 127  97 497 142 155  13 314 375
 221 489 321 218 354  35 139 379   9  11 203  93 417 523 253  29 334 224
 331  20 118  74  71  80 498 400 484 407 332  89 163 462 141 478 157 380
 376 406 371 364 442 244 329 222 396  75 469  40 508 149 343 452 202 480
 160 384 395]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4752
INFO voc_eval.py: 171: [ 8  4 14 62 15 28 55  6 59 44  0 23 33 11 51 13 46 50 57 39 25 60 21 36
 10 18 40 32  3  9 20 45 35 27 16 37 34 63 17 29 30 31 43 22 58 12  2 24
 48  5 47 53 42 19 54 38  1 56 61 41 49  7 52 26]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0125
INFO voc_eval.py: 171: [1696 1698  396 ...  148  566 1970]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5255
INFO voc_eval.py: 171: [ 6 31  5 20 61 47 18 24 67 70 22 43 68 32 71 39 30 54 59 19 63 27 48 46
 72 66 40 52  8 55 57 17 38 21 53 34 28 50 69 29 26  7 10 45  9 35 60 25
 14 58 65 33 51  0 13  4 12 16 42 64 49 36 56  2  1 41 62 23 37 44  3 15
 11]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2056
INFO voc_eval.py: 171: [101 427 193 183 366 207 265 255 274  43  50 273 365 166 151 319 164 406
 374 378 186 262  47  19 288 234  81 163 454   1 301 103 428 452 285 476
 270 122 373  61 196 316  18 391 191 480 167 177  79  13 392 210 351 346
  39 211 150 309 362 121 487 350  64 376  30  11 459 108 311 181 326 276
 458 410  70 294 302 149 433  27 448 445  59 422 223 299 418 401  16 484
 127  60 212 241  49 277 267  20 310 258  40 157  58 348  25 474 434  23
  66 335 412  87  28 291 111 314 102 317  36 287 411  44 272 281 220 443
 135 268 142 467  54 396   0  45  14 449 416 370 173 226   5 435  71 159
 331 214 486 363 336 235 161 109 382  12 398 308 123 138 440 441 429 251
   8 457 355 170 146 341 160 393 292  56 338 390 387 327 148  15  73 242
 385 293 118 231 423 414 321 388 451 263 147 402 471 124 432 271 481  55
 107 182  37 466 204 158 354 165 194  22 116 473 322 244 215  94 453 260
 357 176  90 298 323 479 153 254 250 104 400  57 282 330   4 430  32 472
 456 169 300 224  17 261   6  65 248 132 189  72 239 180  95 408  93 364
 126 283 325 419  75 447 106  51 243 436  10 141 313 171  83 208   3 264
 470 446 252 206 253  26 307 462 286 425 178 184  85 145 119 133 343 367
 175  46 112 115 438 136  33 475  69 218 379  86   7 246  67 305 340 185
 168 404 279 358 394 352 110 315 306  88 333  21 384 368 349 249 139  78
 219  98 200 395 329  35 353 359 460 488 190 290 221 197 227 437 403  42
 275 380 461 209  41 174 229 360 259 201 469 442 195 444 240 228 377 143
 232 464 478 222 114 450  80  97 334 356 179 361 409 120 156 297 289 431
 192 320 216 371 296 312 485 324 386 332  82 137 304 345 407  68 468  24
  99 337 202  34 426 236 203 100 278  63 381 233 230 188 328 205 280  77
 318 284 144 369 421 455  29 247  52  76 130  89 105 172 129 199  96 303
 198  48 245 155  62   2 477 213 339   9 375 125 372 465 269 342  31 405
 113 131 217 463 225 140 424 383  92 187  74  53 257 238 439 256 417 347
 397 399 154 152  38 134 389 420  91 415 128 413 266 344 162 117 237 482
 483 295  84]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4209
INFO voc_eval.py: 171: [103  53  59   1  84  57  14  76 113  60 112  24  30   4 114  21  32  85
  50 115  34   0  19  69  49 106   8  58  52  44  31  77  48 119 118  45
  37  89 109  70  18 111  27  25  42 117   7  20  72  38  17  26  74  61
  11  36  41  99 120  64  90  22  82  33 107   6  43  97  68  78  62  39
 102  73  23  87  95   9  56 108 105  54  92  88  65  40  71 104  12  67
  35   5  55  66  93  10  79  94  80  98  13  51  81  86 100  47  15  96
  29  63   3 101 116   2  46  16 110  75  28  83  91]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0542
INFO voc_eval.py: 171: [71 10 13 84 59 61 45 86 88 27 49 57 47 58 48  9 74 64 35 76 82  4 40 60
 19 53 26 30 79 77 18 11  5 32 62 15 20 24 21 17 22 70 38 16 72 33 78 12
 69 46 85 87 42 65  1  7 73 41 23 34 31 36 54 51  2 80 29  8 75 44 25 56
 81 43 67 14  0 52  3 39 28 50 68  6 55 63 83 37 66]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3863
INFO voc_eval.py: 171: [ 32 209  18 139 218  98 143 159 104 186   6  35 199  12  88   5 153 144
 167 146  56 156 178 197 213  91  50 170 112 155  46  80  13  63 134 120
 126 181 124  69 150 140 125 223 114  76 171  87  45 118 157  71  60  66
  96 169 160  57 109   1  64  82  55  33   3 100 145 175 177 111  28 154
  26  77 205 221  89 107  40 215  39 166 185 201 216 121 122  15 117  59
  92  36 184  19  74 113   7 222 219 179 152 119 172  68 133 176 224  14
 132 182 138 168  43  37 187 149  90  38 212 210  16 217  81 135 123 206
 162 183  53 106 192   2 193  54 189 200  86 225 108  23  75 202  61  83
   8 161   0 227  27  34 147 194 203  84 102 198 158 173 115  10 142 204
 137  51  11 211 116 164  85 148   4 151  70 208  58  44 130 110  48 214
  29  30  41  95 128  47  24 188   9  79  17 165 131 220 174  62  65 103
  97 101 141 105  20  73  72 127  94  49 196 163 226  67 207  22 129  25
  31 195 180 190  93  99 191  42  52 136  21  78]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7353
INFO voc_eval.py: 171: [1872 1157 1886 ...   20  738  362]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6183
INFO voc_eval.py: 171: [118 143 470 216 779 245 726 660 152 130 314  80 148 227   7 729  77 188
  79 782 304   6  53 616 458 100  78 243 519 396 498 624  81 229 429 663
  86 471 264 246 696 445 780 808 691 565 276 727 279 228 556 160 167 435
 224  57 165  95 509 725 372 520 507 722 711 625 630 781 138 795 790 359
 563  89  65 129 115 137 787 733 316 398 166 716 440 446  66 642 636 362
 326 341  36 488 627 451 707 700 346 187 785 151 667 232 277 525 783 222
 493 581  45 714 290 836 629 400 765 843 715 135 311 811 510  37 523 447
 419 425  13 345 662 732 617 233 221 486 360 659 766 839  25 454  96 266
 379 626 460 299 628  98 692 547 432 560 463 105  51 101 637 686 401  70
 567  68 649 397 572 436 230 483 281 120 735 321 380 367 708  16 844 175
 728 358 740 261  83 675 267 489   9 718 511 492 210 759 443 596 533 237
 499 361 172 731 796 635  56 756 747 170 575 342 536 411 383 455 789 487
 806  47 126 537 803 472 107  17 761 220 169 136 373 329 521 546 664 475
  74 322   1 672 494 189 648 816 473 320 191 799 810 113 611 405 297 477
  99 573 793 282 280 531 145 835   8 155 846 162 763 534 270 318 427 592
 704 743 646 421 837 805 719 176 849 465 335 178  61 605 545 424 190 433
 841 566  91 249 786 127 272 128  14 769 268 828 737 235 794 457 312   4
 777 399 589 481 633 110 269 524 579 196  90 582 532 239  92 717  85 330
 197 658  93  38 620 212 278 388 332  88 293 697  26 527 468 656 516  64
 814 750 289 669 315 809 158 242 632 820 631 535 389 180 204 149 584 357
 386 231 606 638  59 788  18  67 643 768  24 287 817 840 600 412  19 294
 179 123 252  50 778 671 209 824 453 173 391 561 476 154 133 513 848 144
 595 344  33 408 141 420 241 334 607 374 825 776 350 674 456 437 192 834
 248 683 467 771 395 578 430 687 308 302 538 377 119 371 385  43 508 845
 156 168 406 713 528 439  54 734 621 273  27 428 263 540 462 328 705 652
 491 198  20 775 478 823 736 324  41 792 682 347 548 339 132 338 552 774
 753 307 466 134 490 417 103 562  40 842 655 681 529 253 348 804 703 238
 688 751 415 441 720  62 764 827 283 668 666 295 351 847 353 131 640 569
 333 291 259 807 106 601 661  21 337 262  75 654 381 641  63 586 597 331
  48 402 522 702 818 821 199  30 375 349 505 102 665 363 744 742 585 518
 553 275 387 368 639 464 634 223  12 327 206 721  52 207 219  31 208 812
 448 364 738 240 461 614 684 813 770 833   0 503 550 211 541 257 309 557
 558 501 382 502 758 613 452 354 752 254 236 116 500 423 121 698 184 459
 414 310 369  73  10 784 250 819 185 234 214 403 434  11 730 202 444  72
  39 685  34 754 760 355 426 303 580 410 409 104 485 651 745 183  58 114
 449 205 693 124 416 343 438 336 594 593 482 797 161 157 800  32 111 225
 757 117 559 542 591 830  76 574 755 271 352 159 517 247 201 677 319  69
 506  28 619 746 773 549  82 286 831 653 298 739  29 392 583 112 539   2
 571 177 404 431 407  44 657 181 598 570  84 258 798 679 587 256 801 296
 244 422 709 479 610 609 182 150 554  49 802 340 218 543 317 306  42 650
   5 497 530 125 514 526 122 603  15 260 147 832 576 146 608 673 285 694
 676 568 706  94 612  35 496 748 645 413 140 474 512 288 484 142 680 292
 577 163 194 251 356 384 602 710 450 301 838 300 195 590 313 826 695 365
 370 544 226 701 741 186 109  97  46 469  60 670 618  55 366 284 193 325
 555 213 139 217 376 323 515 393 418 588 255  22   3 274 644 723 829 791
 495 480 678 203 108 564 305 390  87 164 551 153 762 689 200  23 215 623
 724 171 615 699 378 647 822 174 772 815 622  71 599 394 767 604 712 749
 265 504 442 690]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5961
INFO voc_eval.py: 171: [56  2 54  1 32 36 68 52 50 66 47 28  5 58 67 71  9 69 33  6 22 29 15 61
  3 35 24 31 64 26 34 60 65 30 10 45 21 55  8 38 37 23 14 27 46 41 39 63
 62 18  4 12 17 48 51 40 16 57  7 11 44 25 20 59 70  0 43 53 42 19 13 49]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3061
INFO voc_eval.py: 171: [ 74  75  21 103 160 108  71 109   4 125 119  84 135   3  34  53  57 163
  82 115 100  61  93  83  95  30 105  25 116 147  79 153 131  54  26 130
 142  64 113  89 117  94  50  27  90  63 154  29  13 159  52 148  42  78
  97   8 124   7   9  96  14  20 143 137  18  41   1 144  65  87 107  11
 104  28 136  62  17  36 112  72  10 101  49 141  15  91  39 161  45   2
 126  40  80  23 120  32 121 122  46  37  56  92  60  86  68  24  31  51
  44 132 127  16 118 128  81 162 140 138 156  59  47  43 155 123  77  48
  12  55 139  66 146 157   5  35 149  67 134  85 114  69  76 158 129 145
   0  58 133  99  98  88  38 102  22 111 150  70 152 151  73  33 110  19
 106   6]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3292
INFO voc_eval.py: 171: [115  17  69  13  36  57 100  38  82  27  61 149 152  12  83  79 159  58
 164  11  35  63 101  92  68 132 133  16  55  85 154  95  15 166  39  74
  97  22   9  32  44  70  25  34 129 134  91  19  76  52 107 139 108  90
 106  75 105 144  41 113 121  33 118  67  84  62 170  47 167  56 130 155
 153  94  99 111 142  29 163 161 169 131  65 125 171  49  59 102   8 110
 156 147  72  31  77 140 138 157  88  50  37  23 141  96 143  21  98   2
 150 162 123   6 116 117 126  48  89 137  28 172 114  46  26  14  78  71
 168   7 160  51   4  45  30  81  80   3 103 122  10 151 119  42 158 109
 124 145 135  86 128  64  40 120  66 165  43  20 148 136  18  93  87  24
 112   1  60 104  53  54 146   5   0  73 127]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.4013
INFO voc_eval.py: 171: [142  79 212 321 228 164   4 204 274  24 292 174  35 129  20  75 329 110
  45 301 229 233 224  38 315 214 264  64  70 210 178 203 188 176  28 339
 124   9 100 247 239 216 326 108  50  90 202  37 336  48 218 271 150   3
 316 170  49 280 241 146 260 235  52 154 343 112 302 131 220  58 246  71
  76  82  74  33 125  47 137 185  46 310 118  66 248 171 249 127  65 305
 245  23  42 186 173 231  15  16 342 303 209 234 240 155 242 223 109  91
  63 153 158 144  77 297 184 177  62 196 266 205 344 198 215 270  14  26
 139 172  69 253  84 338 122 285 337 282 312 317 167  40 313  10 217 128
 243 134 320   8 345  95 191 307 192 265 283 189 252  30 187  41 121 291
 256 133 138 145 221  18 230  12 225 222 163 308  39 328  97 132  22 287
 136 298 268 120  73 309  86 299 272 311  32  88  29  51 288 183 195  93
 238 107 262 227  80 331 130 254 232 104 269 168  36 293  72 281 102 165
 113 199 340  83  34 152 140 213 101 324  11 267  13 323 295 197 318 123
 334  92 175  21 314 151  87 147  99  44 194 126 156 257  61  89 211  25
 149 179 162 333 306 114 105 207 304 276 190 327 261  81 258 322 143 117
  68 286 250 169 182   5   7 180 330  53 251 111 263 335  57  31  27  67
  98 226 273  78 244 181 119 193 161 319  96 219 289 259  17 325 201   0
  60  54 148 237 290 332 296  19 157   6 208 294   1 300 341  59 166 277
 103  94 200 115 116  85   2 135 279 278  56  43 236 159 255 275 141 106
  55 284 160 206]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4983
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4143
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.353
INFO cross_voc_dataset_evaluator.py: 134: 0.639
INFO cross_voc_dataset_evaluator.py: 134: 0.244
INFO cross_voc_dataset_evaluator.py: 134: 0.327
INFO cross_voc_dataset_evaluator.py: 134: 0.545
INFO cross_voc_dataset_evaluator.py: 134: 0.613
INFO cross_voc_dataset_evaluator.py: 134: 0.475
INFO cross_voc_dataset_evaluator.py: 134: 0.013
INFO cross_voc_dataset_evaluator.py: 134: 0.525
INFO cross_voc_dataset_evaluator.py: 134: 0.206
INFO cross_voc_dataset_evaluator.py: 134: 0.421
INFO cross_voc_dataset_evaluator.py: 134: 0.054
INFO cross_voc_dataset_evaluator.py: 134: 0.386
INFO cross_voc_dataset_evaluator.py: 134: 0.735
INFO cross_voc_dataset_evaluator.py: 134: 0.618
INFO cross_voc_dataset_evaluator.py: 134: 0.596
INFO cross_voc_dataset_evaluator.py: 134: 0.306
INFO cross_voc_dataset_evaluator.py: 134: 0.329
INFO cross_voc_dataset_evaluator.py: 134: 0.401
INFO cross_voc_dataset_evaluator.py: 134: 0.498
INFO cross_voc_dataset_evaluator.py: 135: 0.414
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 9999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.467s + 0.002s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.367s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.361s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.367s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.366s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.363s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.362s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.368s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.370s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.370s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.371s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.374s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.372s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.364s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.345s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.352s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.351s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.358s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.357s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.365s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.362s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.361s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.361s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.362s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.361s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.358s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.461s + 0.001s (eta: 0:00:57)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.374s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.374s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.371s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.366s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.365s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.362s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.364s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.364s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.365s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.365s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.363s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.361s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.457s + 0.001s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.373s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.362s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.361s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.357s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.351s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.353s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.352s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.351s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.351s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.350s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.349s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.350s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.586s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [170 221  54   5 279  89  48  18 209 119  41  50  46 131 141 285 164 113
 167 142 295  40 171 300  77 172 307   1 110  42 165  45 310 195 130 262
  21 248 115  82 121  80  29 230 263 146  34 102 251  69 126 145 274  76
  98  43 305  63  28 233  11 179 166 313 299 219  26 151  33  53  17 280
 237  83 302  55 231 291  19  93 252   3 168 254  47 192 109 157 271 149
 105 259  38 144 203 201 158 220  23  70 156 127 306 137 122 148 235 268
 272 293  39 147 190  74 120   6  31 198  36 108 162 155  24 277 199  52
 311 315 104 270 218 250 296 317 125 239 281  68 132 216  94 154 140 286
 178 266  84 185 196 191 244 206  87 213 163 176 183 245  16 240 112  97
 187 265  51 246 264 207 238 276  58   9 100 278  62 282 303  35 175 267
 205  20 188 226 308 160 283 232  78 290  27 128 314 200  12  71 301  15
 312 161   4 319  79 181 227   2 152  25   0 103 114 111 107  22 229 116
 243 234 257 177 118 117 247   7  72 129  61  30 214  96 202 133 197  88
  56  67  81 236  32  90  91  92  10 208 284  13 255 186 193 294 275 194
 134 180 222 150  85 292  65 204 153 215 124  73 249 136 223 318 138 242
 309 269 288  44 217 159 225 211 139 212 182  99  86  49 106 123  60 258
  95 101 273  14 256  37 135 298 289 253 228 297 143  57 304 169  75 210
 261 260 174 287  64 173 224 316 184 189  59 241  66   8]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3419
INFO voc_eval.py: 171: [190  53 118  23 220 164 228 232 217 124 231 109  24  21 138 219 234 167
  16 175 206  86  33 221  89 158  78 165 160 223  60 225  88 148 171 216
 114 184 113 151  15   8   9 207  50 240  82  34 186 142  63 229 235 134
 204  77 236  75  41 199  40  68  90  95  87 187 180  83 166 212 125 168
 133 122   5   2 146 230 139  84  31 156 214  91 126  36  26  76 242 209
 119 246 123  56 177  35 233 145 245 200  59 211 130 203  73   7 213 100
 227  74 202  45 185 239  99  19 121 115 179 226 131  54  22 117 137 143
 132 181  25  66 195 104  97 237 241 169 106  17 152 243 183  46 116 210
 174 107  37 247  12 110 128 215 111 176  81  64 201  85 188 208  62 162
 141 238 108 224 147  43  44  49   6 157  48 102 197  71  27 159 129 222
   4 196 182  61  57  96  58 194  30  94 153  80 112 149  79 120  29 140
 163 173 103 101  42  65  38 178 193  39 172   3  55  10   1  32   0 205
 135  92 192 105 127  72 198  14  18  69  47 244 150 218  93  70 154  51
  20  11 155 170 191  52  67 161 189 136  13  98  28 144]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6218
INFO voc_eval.py: 171: [ 61  78  26  48  46  67  75  47   1 128 166 151 165 152 167  56  24  95
 193   2  58  36  93 153 136  74  49  53  80   9 185 139 133  52  68 188
 106 168 179  59 110  38 199  13 157 147 143  32  79 186  57 141 119   3
 170 198  77 145  70  69 135  22 122 164  66 138 155 175  71 181  86 126
  76 160  54 140 137   8  29  31 171 156  15  40  39  12 146 197 158 103
  18 142  89 108 162   4 114  96 112 129  10  42 102  21 134  72 144  64
 190  35 191 169 174  45 154 116  50   6 121 176 196 183 130 132 125 148
 180 163  41 107  90  92 194 124  11  84 123  44 105  83  25 113  94  82
  33  37  17  23  51  30 182  87  65  43 187  55 173  27 117 150 161 100
  63  62   7  73  81   5 120  88  98 184  20 195 189  91 101 192  34 109
  60  97 149 115 127 177   0 172 159  14 131  85  28 118 111 104  99  19
  16 178 200]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2318
INFO voc_eval.py: 171: [327 383 328 108   0   8 329  51  87  27 265 146  55 264 471 360 121 356
 259 375 262 464 263 410 120 203 271 197   9 254 145 330 191 123 266 102
 412  29 470  91 241 309  38  88 177 319  32 403 411  21 352 409 104 445
 215 493 169 247 202 384 461  62 526 125 350 109 270  63  25 401 405 179
 185 496 473 157 490 422 374 236 268 426 304 322 419 392 400 431  31 498
 302 333 218 276 468 453 115  28 402 366  34 316 404 438 256 481  89 306
 223 480 523 478  40 324 176 133 178 472 147 249  13 192  59 517 506 188
 484  68 250 428 287 361 164 351 390 260 281 502 189 172   3 441 336  24
 393  14 439 347 279 239 195  30  41 394 280 217 495 294 297 221 509 313
 489 213 520 512 180 242 296  93  95 139 261 132  48 227 257 335  36 196
 423 474  54 425 288 497 298 278 151  75 124 510 442   4 456 246 321 226
 386 235 310 385 130 148 337  74 511 159 369 229 131 365 234 466 508 430
 525 450 339 292 305 150  46 206 435  71 166 483  47 174 283 181 204 110
 380 370 267 225 113  16  97 143 457  45 311 388 165  53 272 460 127 317
 355 415 137 290 346 432 207 219 233 492 491 274 128 248  92 175 518 475
 228 318 373 187 515 387 407 253 289 230 503  50 364 421 378  80 414 251
 341 513 487 243  94 482 399 437   6 183 362 238  35 417 284 334  52  11
 325 273  64 440  84 210 134  17  58  67 326 479 449 214 129 126 119 222
  44 469 452  69 434   2 516 476 349  43 340 465 232 237 368 252 353 122
 433 381  22 135 156  23 427 173 101 314  77 477 418 194 312 501 111 205
  20 488  33 514 358 184 299 372 398 446 395 245 158 467  56  19 396 291
  73 155 359 344 458 107  79 182 231 320 377 141 220 522 286 420 153 106
 331 171 168  82 455 285 161 454  60 447 199  99 499 338  61 436 136  83
 332 200  86 397 216 117 505 448 149 138 186 500 376 406 114 212  78 103
 277 443 504 282 391 293  39  37  72 244 451  85 303 211  12 162 363 105
 345 408 144 193   5 462 140 367 170 208 354 100 343  57  70 142 209 152
   1 444 429 524  98  65 255 494 167  10 224 160  49 507  15 424 463 163
 342 389  90 154 258  96 269 348 416 308  81 301  66 379 190 413  26   7
 300 307 116 315  18 519  76 382 485 521 275 486  42 357 295 118 112 201
 459 323 240 371 198]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3459
INFO voc_eval.py: 171: [425 426 225 197 427 200 230 429   2 432 196 354 229 198 289 255 528 106
 182 527 347 332   5 350 412 333 120 183 199 307 349 490 257 452 436 348
 430 352 437 163 460 481 444 433 377 244  35 431 486 449  79 446 428  78
 435 275 453 351 344 441 268 450 115 399 457 236 148 172 146 292 357  34
 385  16 456 285 293 502  24  13 454 512  57 295 365 253 232  48 138 192
 241 261 372 434 301 511 439 334 510 380  81 134   6  76 376 203 248 128
 356  47 160 166 235 419 451 353 390 287 400  84   3  46 297  30 346 286
 164 382 204 114 443 152 190 440 214 320 483 170 149 504 387 131 195 173
  86 256 317 371 455 374 478 438  17   4 233 246 361 513 136 298 410  53
 267 476  94 529 212 477 343 479 383 459  43 262 358 492   8 463 250 381
 462 231 396 497 252 424 109 205 150  54  25 147 299 294  14 240 175 119
 392  36 362 360 466 487 337 388 264  39 202 121 201 393 415 144 316 174
 367 113 219 237 420  72  65 226 154 336 411 273 260  15   0 206 188 514
 364 300 524 526 471 402 369 155 254 313 516  93 303 266  58  85  67 327
 378 263 315 162 242 102  89  37 304  71  11  73  28 421 465 322 185 156
 494 158 308 186  90 461 238 179 302 270  60 503 405 210 464 523 312 339
  52 482 239 500 227 243 107 116 397 422 159  83 178 335 291 217 272 135
 142 279  66  42 386 448 215 171  74   9 518  21 137 100 499  63 408 366
  41 520 130 495 276 234 306  95 467 496 169  62 191 395 458  91 194 475
 321 416 125 282 530  88 143 124 211 498 223   1  55 209 485 398 508 342
 359  22 391  23  51 470 368  33  82 259 127 389 208  56 355  10 324 251
 501 180   7 531 132 417 105 507 309 103  38 141 296  98  49  77 110 165
 489 126 375 488 184 290 122 379  87 323 345 407 224  45 265 522  80 406
 216 177  31 314 280  50 101 469 129 221 111 318 515 493 394 284 247 277
  75 228  70 153 117 480  99 305 414 341 193 189 218 140 176  68 311  69
 213 187  12 445 108 521 474 532  64 274 403 409 249 442 525 519 473  61
 338 139 373 245  44 157  29 401 423 168 505 118 123 340 281 220 326 468
 404 370 161 325 509  59 328  20 104  40 269  92 384 413 517 167 331 288
  27 418 145 310  96  18 207 283  97 319 506 329 330 151 222 258 133 484
 447 181 271 472  19 491 363  32 112 278  26]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5665
INFO voc_eval.py: 171: [ 29  17  51  48  78  23 131   4  57 126  64  97  84  16 129 105  80 101
  33  94  93  89  74  28  50 138   2  27  62  79 125  53  18  60 108 104
 130 127  96  75  92  88   8  56 102 128 133   9  47 103 115 119  12 120
   0  87  99  36  38  45  22 118  41  14  25  82  66 134  21 100 132  63
  13  44  35 110  81 109  31 116  86  65  54  73  24  67  70  77 123 106
 124  98  15   5  55  58  39  19  90 117   1  59  26  91  20  30  49  32
  34 107  69   6  42  72 112  71 136  46  10 135  68  61 122 111 137  52
  83  40   3 114  37  76  43 113  11   7  85  95 121]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5980
INFO voc_eval.py: 171: [300 324 138 394  78 437 474 132 236  11 160  97 427 329  24 438  57  18
 265 213 348 218 194 419 214 350 418   3 244 439 431 232 391 353 181 453
 345 347 159  96 404 241 263 461 396 317 401 369 122 443 301  37  19 270
  86 423 271  93 330  87 243 212  59 207 258 183 192 280 487 239 180  52
 371  28 246 151 294 400 426  38 483 238 346 266 171 338 414 251  55  40
 182 184 170  54 252  99 162 390  64 463  71 217 118 372 201 374 250 289
 196 242 491 417 447  65  31  51 434 284 148 237 297 356  14 141 245 302
 210 299 253 176 337 248 332 430 442 403 113  46 469 283  36 221 260 106
 402  95 193 188 205 406 178 111 422 123 254   2 384 157 366 156 446 363
  89 153 416 186 405 448 177 216  81 114  41 310  27 362 269 476 211 185
 344 291 247 432 389 202 328 321 287 451 219 460 312 131 473 144 388 480
 222 478 296 174 255 220 425 376 179 314 412  60 276  17 228  58 357  79
  56 125 215 101 164  10 129 454 325 407  23 262 377  33 140 209 154 420
 386 484 117 147 102  15 467 165 104 436 167  74 142   7 331 315  49  62
 397 383  25 359 278 168 105 429 342 409 395 199 100 163 456 365  26  80
 189 410 195 120 336  43 279  42 441 368 275  53 281 145 136 133 475 433
 367 173 341 298 444 128  47 322 110 343 421 477 187 259  94 424   8 381
 308 204   1  98 352 413 161 175   9 277 339 240 191 152  16  73  85 169
 408  44 380 282 415 449  82 293 471  32 481  63 208 273  61 379 458  75
 108 489 319 150 119  39 351  50 375  22  72  88 378 126 143 286 488 470
  30 435 139 335 137 135  83 360 327 230 249 392 197 361 326  45 127 116
   5 457  35 295 200 304 490 198 149 385  68 313 130 158 224 482  91 146
 459 256 103 462 264 307 166 121 466 354  69  70  21 261  12 112 428 455
 285 393  48 464 316 382  34 468 233 398 155  92 373 364  20 272 340 452
 115 399 320 257  29 107 305   4 109  76  77 370 231   0 440 225 334 445
 349 290 223 358 190 267  13 465 268 134 472 411 288   6 306 450 226 303
 323 486 479 124 485 227 206 274 355 229 203  84 309 172 234 235  90 333
  66 311 318 387  67 292]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4621
INFO voc_eval.py: 171: [ 5  2 11 44 12 39 21  3  0 42 32 34 25 41 17 19 23  7 14 43  6  8 10 24
 36 37 26 40 28 45 27 33 22  1 20 18 15 31 16 13  9 38 35 30  4 29 46]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0047
INFO voc_eval.py: 171: [1609 1611  367 ...  434 1281  224]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5244
INFO voc_eval.py: 171: [ 2 31  3 58 23 46 24 18 62 52 67  4 21 42 65 68 30 37 15 57 32 47 27 39
  6 69 63 64 20 55 34 53  5 45 56 51 66 49 50 28 12 35 25 19 60 44 36 22
  8 29 14 10 48 16 26 33 59  7 61 40  0 13 38 17 41 11 54  9 43  1]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2158
INFO voc_eval.py: 171: [376 175  91 167 327 188 245 239 246 227  47  35 147 285 177 335 404 145
 135 339  17 170  16  78 211  46 144 325 377 236 312 259 351   1 334  59
 252 405 398 348 283 173 269 112 428  62 256  81 311 360 248 342 192 193
  39  96 134 148 111 309 276 366 133 364 278 352 410 430 336 264   9  30
 323  60  77 397 355  26 266 164  92 433  38  57 216 291 270 369  25  13
  28 116  58 403 396  69 261  21 157 126 247 419 299 139 136  66 385 372
 233 414 258 277 284  51 426  43 191 121   0 368 132 240  15 436  34  42
  18  11 212  93 302   5 394 304 423 113 141  97 207 365 296 275 130 391
 142 224  10 435 202 292 330 356 316 383 418  98 301 353 347 346  48 408
  55 143 380 390 151 407 156 389 237   7 253 262 343 367 209 287 242 359
 160 222 349 373 108 124 232 362  12 217  95 431 280  84  27  71 131  70
 176 140 254 317  19  14 105 109 338 178  67  53 117 313 315  80 295 425
  31 399  82  86 219  85 214 186  33 268 344 146 221 333  94 150 357 321
 263 288 195 114 424  72  56 226 234 260 303 406 172 341  32 370  63 371
 194  87   6 350 118 294 190 163 422   8 159 119 100 104 395 169 138  37
 324   4 166 272 171 274  73 115 238 281 290  45 125 187 208  65 225 337
 374 206 101 386  41 328 220 231 429 257 189 402 154 411 393 165 434 421
 427  23 218 200 279 230 289 340  99   3  20  76 297 199  68 329  90 322
 273  89 282 198 310 174 129  79 381 184 308   2  22 306 123 120 179 375
 354 420 201 197 180 215 228 382 235 183  49 320 387 223 412 210 103 161
 271 400 392 128 265  24 361 153 102 185 196 213 204  36 415 250 255 416
 417 286 205 110 319 298 345 149 107 409 229 401 314 155 384 331 358 378
 152  61  50 267 326  74 182 251 168 305 244 241 158 293  44 332 122 413
  75  29 432 379 243 318 127  54  52 388 162  40  88 363 181 249  83 137
 106 203  64 300 307]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4037
INFO voc_eval.py: 171: [ 88  71  47  42  12   1  23  45  48  96   2  16  94  97  19  27   0  64
  25  76  40  57  74  15  43  39  66   7  24  92  31  98  36 101 100  35
  21   9  14  99  20  89  70  30  77  63  93  38  69  90  82  53  61   6
  58  49  17  50  51  56  29  33  87  34   4  46  32  73  26   5  86  10
  80  22  83  67  44  11  60  84  28  78  62   8  75  81  59   3  18 102
  65  52  72  91  37  68  55  41  79  54  95  13  85]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0588
INFO voc_eval.py: 171: [67  8 57 78 11 80 59 45 82 26 48 55  9 56 71 47 44 61 76 72 10 34 46 68
  3 73 37 51 29 17 58 40 18 24 75 16 14 79 22 66  4 20 15 27  6 31 19 60
  2 65 32 70 50 52 28 74 81 33 30 42 62 77  7  1 69 13 21 39 41  5 54  0
 53 38 23 63 64 43 35 12 49 36 25]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3998
INFO voc_eval.py: 171: [ 32 229  20 146 103 171 152 203 239  35  13   7 109 179   6 218 153 166
  96 169 234  86  97  52  58 194 157 168  47 217 200 117 147 142  14 182
  98 134  89  46  73  65  94  76 244  62 132 162 133  33 170 119 114  82
 127 123 183 154   1 181   5  59 105 237  70  66 192 172  57  28  85  26
 247  39 240 191 202 140 178 193  25 198 201 223 116   9  19  80 167  36
 220  44  90  75 195   2 112 184 165 129 238 101  15  61 231 141  37  95
 160 128 122  41 111 180  74  93 211 199 124  78 210  64 145 204 242  18
  38 118 107 189 250 224 151 174 235 233 225 164 206 120 102 249  53 251
  55  16  81  87   0  10 135 138 113  11 186 219 216  48 143 245  56  45
 126  42  34  72  31 236 222 212 173  92 161  12  29  54 144 148 241  69
  67  91 121 155 228 207  50 196  51 205 139  99   8 176 230 177 221 158
 108 163 131  60  77 188 232 106  71  49 150  17 137  84  23  24 243 149
 115 197 136   3 159  79 156 214 208  40 209 215 185  30 110 187  43 130
 227  22 104 190  27 246 226 100  83  21  68  63 175 125  88   4 248 213]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7434
INFO voc_eval.py: 171: [1974 1218 1987 ... 1619  246  244]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5998
INFO voc_eval.py: 171: [132 154 471 225 759 253 142 164 705 159 327 655  84 235 316  80   8  83
   6 199 708 764  53 105 460 403  82 497 610 449 237 254 617 520 432  85
 255 795 674  81 276 760 706 569 236 289 678 472 560 648 507 180 172 293
 178 689 381 763 522 619 704  91 702  58 521 151 510 621 774  66 228 175
 762 143 625 783  67  94 371 290 438 712 243 239 162 240 150 353  35 634
 448 532 695 564 329 405 523 616 776 278 444 198 339  88 128 102 359 356
 455 658 372 765 686 794 491 324 694  45 744 693 302 681 707 406 710 258
 618 242  14 494 628 100 116 583 133 374 568 511 450 230 174 711 745 649
 671 620 439 464 260  69 815 424 549 767 624 331 818 773 404 387 408 463
 513 641 473   7 489 630 567 457 148 623 769 682 220 388 529 647 597  19
 675 311 310 377 488 687 357  71 446 787  36 498 296  56 820 373 188 231
  90 697 177  10  47 729 627 657 333  87 612 245  38 709 656 292 487 822
  70 512  95 273 428 793 176 336 788 576 200 653 382 650 578 548 542 493
 139 135 539 420  46 683 538 771 514 383 112 534 536 737 370 437 283 281
  18 817 157 790 392   1  89 113 719 626 800 125 295 201 341 229 742 770
 740 144 668  97 527 412 582  61 204 496  93 430 698 490 603 465 739 824
 727   5 530 784 337 277 233 244  29  96 426 554 203 347 638 478  98 452
 808 748 715 570  64 451 280 607 585  31 746 192 654 343 524 328 325  15
 474 358 828 531 262 160 238 757 477 109 806 189 368 134 282 637 751 459
 304 394 167 264 141 185 679 413 590 149 308 419 622 291 629 778 814 122
 646 294  77 696 586 768 470 422 351 140 287 320 407 263 191 396 714 680
 805  60 575 223 433 816 722 251 813 565 213 791 186 485 663 300  26 110
 350 208 207 286 340 402 758 604 355  68 642 318 476 108 303 614 613 221
 789 156 187 259 248 440 581 799 179  16 458 345  63 323 361 210 155 252
  32  20 741 589  22 703 540  43 305 469 360 397  34 819 480 386 721 631
 249 826 334 338 801 802 596 593 660 315 605 644 307 169 453 588 633 672
 525 528 380 385 153 378  17 716 364 666 821 728 468 146 435 136 754 823
 667 692 342 699 314 366 454 261  48 807 798 717 272 499 733 168 665  54
 611 755 365 651 436 246 804   0 232 720  13 725 434 367 566  30  62 147
 803 673 247 442 195 256  73 274 750 461 362 517 445 632 509 599  23 322
 297 321 409 456 137 163 684 713 766 747 124 562 557  28 553  27 587 785
 811 636 427 550 165 481 484 535 600 561 467 777 158 127 209 503 775 384
 275 688 639 609 661 423 129 786 544 443 344 482 266  75 389 326 418 152
 313 145 269 411 662 218 670  49 504 533 780 194 486 348 495 761 547 735
 643 669 797 515 416 391 173   2  25 219 756 431 700 395 123 483 598 131
 573 664 749 330  24  78 543  37 676 224 685 214  12 301 731 183  92 119
 101 222 738 677 608  41 410 182 734 265 227 635 595 726 475 354 312 363
 415 462  55 555  52 211 652 571 205 369 492 500  21 752 772 138  72 117
  42 126 288 111 335 701  79 399 212 190 352 217 782 466 541 332 417 552
 441 574 501 601 781 107 796  86 825 508 556 401 284 519 257 659 732 421
 226 584 319 181 602 106 723 390 516 577 375 743 120 268  99 429  51 779
 115 317 114  74 645 400 558 537 551 690  44  33 447 526 202  59  57 285
 505 309  11 184 479 398 103 104 270 161  40 580 753 349 279 827 425 591
 592 563 792 572 379 414 393 215 216 724 730 234 166 502 130 298 299 170
 121  65 376 171 606 812 193 546  50 118 736   4 196 241 718 197   3  76
 691 206 615 594 559 346 267 640 579 506   9 810 809 271 250 545  39 306
 518]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5941
INFO voc_eval.py: 171: [55 50  2 29  1 37 64 48 44 47  6 62 69 63 26 52  7 32 11 27  3 57 22 15
 24 68 35 28 33 51 61 58 10 60 12 34 30  4 21 59 25 14 31 23 42 16 53 36
 38 41 18 39 43 67  9 20 66 65 49 17 40 54 45 13 19 56  5  0  8 46]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3068
INFO voc_eval.py: 171: [ 65  66  20  99  95  55 152 100  64 117 112  76  74  32   2   3  24  28
  96  70  52  47  86 137 106 122 143  82 154 132  84  25 107  75  39  48
 108  60 121  27  92  81 104  46 139 124 116  12 144 150  23  88   8 127
  85  69  89  45  13   5  16 134  10  31  97  57  59  18  38  42 140  56
  21 130  83  26 110  53  58  37 153  30  36  98  72  61  93  51   1 113
  78  87   9 126 102   7  94  79  34 151  35 133  43  49 111 135 119   0
  22 141  14  73 149 125  68 114  29 142  71 129 128 101   6  54  77 123
 118  91 109  17  50 146 115 148  67 147  41  33  11   4 103 138  63  40
 105  44  62  90 131  15 120 136 145  80  19]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3727
INFO voc_eval.py: 171: [113  65  15  11  52  32  96  78  23  55  30 140 142  79  74   8 151  33
  53  99  57 155  62 127 145  80  95   5  50  12  88  27  20  14  69 128
  31 130  92  35   9 124 103  40 156 100 105  22 133 106  17  89  70 147
  28 159  67  51 154   7 119 111  37  64 123  43  98  49 104  87 160 126
  68  24  61 135 137  71  47  26  48 143  76 157  90 121  59  56 149  85
 136  34 148 108 125   6 138 109   2   0 141  73  54  21 101  97 116 114
 132  41  46 134 115   4 152 112 144 120 158  75  19 162 161  38  93  83
 153  45  25  66  16  72 102 107  81  44  58  36  94  13 122 139  63  10
  91 150  29   3 131 118  42  18 110 117  60  86 146  84  82 129  77   1
  39]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3828
INFO voc_eval.py: 171: [142  80 213 230 320   5 170 204  24 293 280 174 129  20  35  75 231 314
  48 300 327 236 110  38  27 175  63 226 268 214 183  68 211 193 109  10
 203 337 216 324 125  77 101 252  52  93 242  37 221   4 237  49 202 332
 225 284  60 112  55 146 315 303 222 235 177 250 233 151  70 249  74 168
 205 301 178 244 340  32  65 264  53 157  50 309 245  54 276 155 126 338
  94 251 243  86 137 121  45 132  67 173 220 302 316 269  76 188 116 253
 333  23  78 108  26 189 199  18 154 275 286  66 200 312 298 190  64 246
 335 105 181 311  89 187 139 127 289 256 166 255  36 219 227 136  87  51
 144 258  17 306  19  39  79 194 341  99  69 247 169   9  41 304 123 138
 283 270  43 210 294  30 232 167  16 114 310 135 133  56 161 272 128 287
 234 330 212 148  25  85 336 299 223 292 145   6  72  28 179 308 319  47
 224 274 172 291 339 326  71  92 257 120 215  98 192  73 229 186  42  31
 266 201 107 307 130 295 103  15  22  14 184 273 342 131 198  11  29  33
 317 285 164 115  21 277 182 158 134 195  91 147 313 196 271 140 278 163
 241  84 150 197 102  97 321 207  62 265 153 281 141 124 331 185 176 322
 171  95  96 165 325 248 290 305  81 260  44 100 328 117 143  12   7 261
 329  40 122 228 149 282 262 296 297 259 119 263  61  59 191 113 217 160
 104 323  58 334   0 218   8 279 239  82 208 288 206  57   1   3  46 118
 162 267 318 209 180  90 240 156 152 106  83 238   2  34 254  13 159  88
 111]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4743
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4125
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.342
INFO cross_voc_dataset_evaluator.py: 134: 0.622
INFO cross_voc_dataset_evaluator.py: 134: 0.232
INFO cross_voc_dataset_evaluator.py: 134: 0.346
INFO cross_voc_dataset_evaluator.py: 134: 0.566
INFO cross_voc_dataset_evaluator.py: 134: 0.598
INFO cross_voc_dataset_evaluator.py: 134: 0.462
INFO cross_voc_dataset_evaluator.py: 134: 0.005
INFO cross_voc_dataset_evaluator.py: 134: 0.524
INFO cross_voc_dataset_evaluator.py: 134: 0.216
INFO cross_voc_dataset_evaluator.py: 134: 0.404
INFO cross_voc_dataset_evaluator.py: 134: 0.059
INFO cross_voc_dataset_evaluator.py: 134: 0.400
INFO cross_voc_dataset_evaluator.py: 134: 0.743
INFO cross_voc_dataset_evaluator.py: 134: 0.600
INFO cross_voc_dataset_evaluator.py: 134: 0.594
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.373
INFO cross_voc_dataset_evaluator.py: 134: 0.383
INFO cross_voc_dataset_evaluator.py: 134: 0.474
INFO cross_voc_dataset_evaluator.py: 135: 0.412
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 12499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.547s + 0.002s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.368s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.365s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.360s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.371s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.368s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.364s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.364s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.364s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.366s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.365s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.367s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.368s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.403s + 0.002s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.376s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.371s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.352s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.352s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.355s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.363s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.365s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.365s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.368s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.368s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.367s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.365s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.418s + 0.001s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.368s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.363s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.370s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.360s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.357s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.353s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.355s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.355s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.354s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.355s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.354s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.357s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step12499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.423s + 0.002s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.356s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.348s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.349s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.349s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.346s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.352s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.354s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.353s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.354s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.351s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.351s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.352s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.049s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [237 183   5  60 294  95 224  18  55  48  56 125 149  54 138 301 174 179
 123  47 150 321 184 315 188   1 309 121  49  82 324 117 264 277  51  20
 154  38 246 268 209 107 133  73 137 128 279  31 319  88 289 153  30  83
  81 252  53 234 105 113  68 160 185 306 326 253 176 250  37 151 205  62
 317 295 157  10 276 269 284 314   3 228  16 217 167 271  22 212 134  74
  59 129 100 114 320 168 287  87  39 281 144 132  72 110 292  79  44  26
 204 262   2  35 236 109 208 164  21 251 166 101  58 247 177 126 139 235
   6 255 156 193 260 330 286 296  97 148 215 278  46 158 213 285 220 181
 291  89 104 170 221 165 261 307  27 199 186  99  66 316  12 311 297 325
 191 283 218 202   4 254  43  93   7 256 329 108 173 327  57 119 219 201
 171 146 226  75 135 302 172  24  15 241 267  14  92 280  32 175 249  34
 239 115 169  23  25  19 275 124 293 243 318  65  85 322  33 232 206 118
 263 313 214 194  61 238  71 195 244  94 122 207 216 229 299 290   0  13
 227 332 211 312 102 248 245 258 233  84 120 140 240 192 127 130  45 159
  11  41  40 282 147 200   8  70  64  77 103  17 210 152 300 116 131 136
 273  69   9 298 257 182 222  98 106 190 305 288 196  36 231 331 223 259
 230 323 266 197 242  28 187  42 328 111 145 163  96 142 270 180 189  86
  29  52 198 112 304  78 161  91  80 162 225 308 155 203 143  67  76 274
  63 265 310 141 272  90  50 178 303]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3635
INFO voc_eval.py: 171: [186  53 122  25 216 225 161 233 214 126 228 112  26  21 215  90 140 171
 200  14 184 162  94 217  62 156  82 159 222 213 118 146 220  13 117 168
 166 231  52 150  68  35 185 238  36  10  11 196 199  86  92 114 141  42
  75  99 232 204 138 163 234  81 182 147 144  97 164  23  95 183 230  43
 130  79 129 177 128  87  91   2  38  88 211  28 123 134 223 116 145 227
 173   6 165 202 229  32  61 244 240  37 136 181  80 241 142 210 121 103
 206  78  55 188 237 208 119 104  77  51 125 113  57 212   9 242 131 205
 180 209 176 178 110 101 107  20 198  89  46  49  27  71 197  58 133  47
 224 239 170 235  39  33 151 190 236  83 245 135  29 109  17 195  24 194
 154 179 102 155  41 207  44 149  15 139  84 172 100 203 221  98  50  72
   1  60 187  12 157  31  67 243 143 148 160 153  63 219 124  19   8  96
   7 152   5  30   0  74 218   4 108 191 106 105  93  34  56  54  40   3
  85 120  66  73 169  48 137 201  76  65  22 192 111 115  69  64 189 167
 226  70 132  16  59 193 174 175  45  18 158 127]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6125
INFO voc_eval.py: 171: [ 60  24  76  45  46  47  65  73   1 117 161 145 162 147 163  55 186  92
  21  33  56   0 128  52 146  91  48  72  77 178 171 132  68 179 190 152
 103 124  51 182  75 137  30 164  57 165   8  12 107  20 135 111   3  53
  35 141 113  74 166 175 169 116 174  67  50  66 189 133 160   9 126 130
  84 129 139 156  69  99   7 149  87  16 151  27 131 154 168  14  63 188
 176 173 150  29  11 105  40  98 187 148 172  38  19 108  93  94  70  49
 134 140  18  25 136  44  58  36 119 185 110 158 102 157 101 100  42  10
 181  39  31 120  89 167   5  64 125  88  41 106  85 142 104  78 112 138
  34  15 159 123 122  90  28 153 180   2   4 144  96 184  82  22 155 115
 183  79 127 170  61  62  81  37   6  54  95  83  26  23  13  80  43  86
  71 109 114 118 177 121  97  59  17  32 143]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2329
INFO voc_eval.py: 171: [335 393 336   0 109   8 337  52  90  26 272 148 370  57 477 267 274 386
 367 121 271 264 418 469 207  10 273 144 120 123  88 202 196 338 275 103
 475 414  21 313 178  89 248 420 495 327  29 206  30 283 104 412 221 526
 394 417 279 466 360 169  36 110 276  25 474 254 452 362 384 126  66 402
 324 261  65 257 410 244 157 181 497 312 430 409 499 342 427 231 473 484
 331 280 492  31 309 356 255 357 186 315 263 224 330  15 447 282 374 441
 502 396  27 155 259 435 314 179 131 116 400 176 413 197 437 423 506  34
 480  38 332 284 293  62 200 203  16  91 489 146 448 411 164 345  69 307
  71 223 247 382 450  40 522 398 311 483 509  28 462 292 366 361 404 403
 174 285  24 303 227 519 175 518 512 459  49 346 145 177 397 172  96 190
   3 344 440 124 496 192 301 234 476 112 268 308 269 491 249 498 165 306
 319 212 431 191 150   5 277  44 235 424 434 130  58 237 243  75 242 166
 256 465 461  37 471 233 373 511 524 377 147 159 229  56 355 299 182 363
  93 133  77 125 302 298 348 320  73 111  51 380 141 399 318 456 260  17
 151 291 129  70   2 508 510 494 297 388 250 468 134 439 421 325 359 372
 407 208   7  95 236 252 481 225  81 258 503 114  46 185 187 211  45 472
 455  18 158 326  61  79 485 516 287  99 251 228 238 415  14 101 317  23
 316 395  22 514 142 390 119 305 517 371 349 429 333  55 458 343 102 449
  33 246 339 454 199  60  72 241 383 171 487 323 215 426 443 286 442 210
  53 340 444 153 463 107  13 354  39 294 486   1  82 350 391 405 436 218
  97 240  84 136 310 127 262  63  94 108  59 387 122 152 245 194 163 321
 422 375 521 479 379 132  11 446  42 353 428 470 189 118  87 205 296 220
 438 408 214   6 478 482 493 115 501 204 230 290  92 160 183 334 347 217
  68 433 295 453 216 328 464 358 504 278 198 369 376   4 188 184 515 239
  32 281 137 156  98 505 406  48 490 100  54  83 329  67  76 143 368 128
 378 139 168  35 232 289 135   9  12 389 149  41 170 364 140  80  86  50
 106  74  78 392 167 352 341 460 520 513 226 180 451 416 162 507 488 300
 425  85 113  64 467 161 381 419  20 304 213 253 385 523 365 432 322 288
 265 222 401 457 201 138 173 193  43 351 117  19 525 500 105 270 445 266
 154 209 195  47 219]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3431
INFO voc_eval.py: 171: [445 446 241 207 447 448 210 249   2 372 307 366 113 449 206 554 208 248
 555 351 194 277 369 129 209 352 195   3 327 368 432 278 458 367 518 471
 459 370 483 451  85 512 177 465 453 396 452  36  84 264 466 460 450 488
 456 470 478 363 377 162 185  17  34 124 421 481  63 296 378 406 290 159
 516 252 254 251 486 529 494 313 150 476 304 353 382 315  12 454 202 438
  53 374 473 392 541 373  24 139  82 146 268 273 316 411 282 536 314 174
 238 183 188 538 212 376 383  52 405  11 180   5 255  51  15   6 423 464
 480 399 276 161 105 306  86 294 235 320 375 223 365 380 412 559 530 222
 198  92  31 226 340 126 123 398 400  19 557 253 401 484  25 389 324   4
 250 462 491 508  94 502 336 357 540 408 205  35 384 187 429 270  37 117
 143 283  40 414 482 519 318 495  43   9 514  98 509 160 388 424 479 289
 444 211 228 510 167 186 431 321 362 381 413 224 272 472 234 489 402 418
 523 285 176 148  80 335  90 439 504 284 178 256 358 410 214 260 386 385
 355 434 455  21  59 245 279 156 147 184 199  73 420  67 319 461 544 292
  78  28 426 440 543 422 259 332 334  22 553 295 274 120  88 165 131 132
 323  97 549  42 286 233 394   0 490  46 170 216  38  76 168  79  18  66
 240 225   7 288  23 354 114 492 204 257 527 196 169 485 263 173  74  14
 193 521 556 154 308 155  89 520 197 348 262 419 101 507 297 125  39 258
  47  93 407  77 239 342 330 172 201  81 457 203 522  69  33  72 499 442
 152 142 415 191 379  48  13 463  58 487 217 326 149 416 441 218  70 182
 525 371 417 345 467 387 237 496 403 164 537 108 333 220 517 265 395 551
 506 312 552 243 560  87 547 526 331 106 219 137 227  99 246 531  55 435
 542 328 103 104 118  91 561 291 390 539 524 299 221 190  96  95 111 135
  57  41 427 428 513 269 140 528  54 302   1 329 397 236 409 134  62 545
 436 157 181 500  75 200 301  61 550 261 171 501 511 192 346 338 107 515
 281 562 430 127 144 153 359 115 425 364 498 325 179 232 287 344 213 350
 391 244 136 112 393 503 100 121 361 280 298 300  68 535 475 339 469 468
 266 534 109 347  83 133 215  30 267 189 310 317 443 116  65 128 231  49
 433  10 110 275  26  16 230 474   8 141 102 311 477 305  27  29 242 322
 343 360 158  64  50  44 293 145  60 558 533 151 437 404 130 337 349 309
 271 493 247 497 341 303  32 119 546 505 175  71  45 229 166 163 138 122
 532  56  20 356 548]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5831
INFO voc_eval.py: 171: [ 31  18  52  55  80   4  25  61  66 132 125  96  17  86 130  93  37  82
 104   2  90  29  94  65  81 124  24 138 107 131  64  57  19 103  60  68
  83  92  12  89  77  27 100 118 101 128 127  39  48  76   8  26 108 119
   9  22  88  95  40 114  51  49  54  84 117 102 134  13  69   0  23  14
  98 133  87 109  45  99 135  33  28 122  32  15  75  58   1  20 136  78
  72  62  70 116  63 110 111  21  73  67  97 105  53  43  16 137   6 139
 123  59 121  44  34  74 115 113  41  42  46  50  91  36  71  47   3 129
 126 120  30  56  35   7  85   5 112 106  79  38  10  11]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5861
INFO voc_eval.py: 171: [320 422 343 142  75 502 469 137 247  12 457 468 166  96  58 347  19 283
  26 222 372 448 220 201 490 470   3 221 256 447 217 375 242 369 420 187
 379 461 371  95 165 482 259 432 124 424 281 396 428 335  37  83 189 291
 321 249 474  84 186  60  92 198 515 299 275 349  20 255 284  31 397 219
 251 453  51 370 313 253 156  53 290 193  38 263 188 456 214 442 429 358
 176 264 351 306 254  15  40 204 178 319  55 146 226 261  98 168  70 492
 152 258  32 322 418 302 398 445 464 121 400  63 303 250 182 257 473 518
 431 388 483 209 262 507  50 478 479 267 381 125 105  36 114 357 195 216
 316 227 460 452 112 389 213 430 228 509  54 503 392 199   2 162 225  78
 191 476 180 183  43 315 190 410 268 161  56 488  46  88 511 210 311 149
  16 269 260 289 501 277 218 467 329  29  41 455 344 481 215 334 462 437
  30  94 170 100 425 297 138 504 236 116 331 368  28 305 173 382 103  44
 497  34 171  25 101 133 184 459 155 144 417  27  57 104 174 169  99  59
 434 384 270  71 271 207   7 340 495 223 403 485 122 194 449 338 441  61
 438 274 364 298 339  93 393 300 328 414 512 196 348 185 514 127 318  39
 454 350 472 131  47 408  18 295 390 197  77 395 433 120 412 436 508 475
 140 181 409 402  64  76  82 450 378 157  10 391 367 148 252 211 111  35
 463 296  79 517 505 394 206 151 175 203 359 230 135 158 279 480 399 163
 365 280 118 233 416 317  72  13 145 208 293 307 239  73  65 150 248 143
  48  80 440 167  24  97 421 314  17 361 117 411 292 102 360  62 107 498
 172 224 147 376 465 354 154 282  49 435 113 332 346 212   9 286   1   5
  67 273 229  22 110 419  21 231 200 308  52 489 301 337  66   4  74 493
  11 312 387 272 202 516 401 336 377 352 285 276 179 342  33 128  86 265
 404 192 324 477 243 362 134  91 159 486 119 356 153 287 129 108 130  45
 506 383 245 160 325 407 444  42 234 323 385 141 423 363 205 513 109 491
 246 235 106  23  81 355 406 374 427 487 294 139 386 353 494 484 510 471
 126 415 330 238 136 443  85 164 345  89 244 326  68 500  69 288 309 241
 240 380 405 439 451 132 341 278 499 232 177 123  14 458 327 373   0 304
  87 446  90 366 266 237 426 115   8 496 333 310 466 413   6]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4320
INFO voc_eval.py: 171: [ 5  2 14 38 13 12 44 21  3 33 26  0 11 17 42 15 19 25 31  8 23 43 36  1
  6 45 39  9 40 37 28 35 34 18 27 22 24 32 20 29 41 46 30  7  4 10 16]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0030
INFO voc_eval.py: 171: [1641  388 1648 ... 1694  316  170]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5113
INFO voc_eval.py: 171: [ 1 28  2 55 43 21 19 16 60 65  3 13 20 40 48 63 34 17 66 45 27 54 11 53
 23 29 68  5 35 50 30 52  4 49 61 57 47 12 51 62 64 32 10  6 15 22 42 24
 56 46  8 14 18 33 59 58  9 26 38 36  7 39 37 25 31 41 67  0 44]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2014
INFO voc_eval.py: 171: [375 167  84 160 326 181 245 238 225 244  46  30 333 169 145 142 130 288
 325  75  15 399  14 207 351 401  44 334 337 162 360 376 141 235  59  35
 257 313 312  61  88 164   1 284 254 406 266 108 349 365 241  20 352  78
  25 184 106 279 183 153 186 395 277 309 422  86 363  74 262 136 144 322
 247  23   7  56 335 394 212 427 424  63 260 414 368  57 157  19 328 132
 355 134  22 371 417 405 343 140 242 384  39 265  64  11 122 229 111 255
  51 285 129 400 354 367 393 234 409 293 127 239 270 208  92  66 278  13
   9 166 306 117  16 138 107 413 114  36 223 356 388  94  10  32 390 364
  89 304 104 201 240 294 317 340  54 276 246 139 298 236   5 196 204 348
 415 382 366 421 128 429 379 289 428 344 131  47 302 372  79 362 137 226
 251 147 300 103 267 357 168 305  77 318 314  12  17   3 150  55  29 233
 425 392 404 216 385  58 258 100 146   8 297 221 232 359 163 179 109  80
 151 316 112 210 135 310 250 220   6 345 419 189 332 200 263  53 280 386
 113  68 350 396 324 188 418 198  26  81 115 403 296   0  33  31  99 231
 370 275 170 177 193 273  34 339 407  90 248  60 256 321 116 369 290 416
 156  27 336 292  62  49 323 224 215  93  71 152  18 120  69 281 381 101
 194   4 202 217 237 423 268 230  41 282 249 180 227 185 373 411 123 286
 274 264 311 195  67 159 380 197 307 143  40 269  37 176 121 119 330  96
 110 213 378 420 272 161 172 102   2  38 206 301 329  28 346  50 209 342
 187 171 165 347  43 358 124 192 377  65 402 374  76  98 214 341 191 178
  83 391 308 303  87 154 149 398 299  95 203 222 228  70  52 291 283 175
 353 125 148  82 315 259 261 182 287 412  48 105 133 158 320 155 174 271
 253 410 190 211 295 118 338 243 319 331  72 426  73 383  21  24 219 387
  42 126 361 408  85  97 397  91 205  45 218 389 173 199 252 327]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3924
INFO voc_eval.py: 171: [ 88  13  72  24  47  46  25  48   1  20  27  98  43   3  99  18  65  96
  77   0  58  44  40  35  26  93 102  22  39  67  36   2 101  31 103  91
  10  90   7  15  17  75  78 100  21  92  70  69  83  66  53  82  64  32
  89  56  95  52   8  74  87  84  63  19   5  12  51   6  11  61  71   4
  50  62  34  94  45  54  33  23  29  80  57  30  81  38  49   9  37  28
 104  79  76  73  60  86  59  85  16  41  55  97  42  68  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0687
INFO voc_eval.py: 171: [69 80 61  8 11 84 46 86 62 26 50 63  9 58 73 48 59 45 64 70 75 79 10 35
 53 76 30 47 83 19 20 42 14 38  3 25 13 17 23  6 78 51 54 18 71 68 43  1
 82 72 60 32 33 22 52 67 81 29 31 44  2  0  4 21 49 66  7 34 27 85  5 41
 40 57 74 87 37 65 88 55 16 15 28 36 12 24 39 56 77]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3811
INFO voc_eval.py: 171: [ 29 233  22 146 105 152 206 245  33 180   7  13 173 111 153   6 167  30
  84 220 238 171  52  97 169 197  59  96  46 200 158 147  45 142  72  88
  94 118  75 115 135  99 250 172  14 134 235 183 133 182 219 155 121  98
 125  66 162  60  63  81  21 106   5 203   1 179  70 140 188  64 205 129
 175 242  42  57  43 204 195 117 193 127  15  67  93 174  38  25 165  20
  83 225 252 244  90 222 184   2 214 119 181  26  27 255  35 246 112  76
  53 168 150  77 199  34 191 202 207 213 141 113 164 131  74 159 239  36
 109 124 237 226 122  28 254 130  79 103 177 186 227 138  62 145   8  55
 156 247  37 209  47 210 114 198 228  80  44  73  31  41  18 163 215  68
 221 126  89  92  24  19  10  56  86 201  11 190  61 100 128   0 249 170
 143 178  95 144 176 194 157 232  51 160 148 102  78   9  85 110  91 154
 139 123 224 132 243  54 218 241 137  39 208 231 240  50 223 136  58 251
  12 196  71 187 185 234 116 104 161  32 192  17 108  40 236 216 107 149
 248 189  16  69   3 151 217 229   4 101  48 212 120  87 166 211  23  82
 230  65 253  49]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7560
INFO voc_eval.py: 171: [2064 1287 1195 ... 2001 1105  653]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6171
INFO voc_eval.py: 171: [141 167 246 509 814 153 274 179 173 762 256  85 359 346 697 815  88  52
   8   7  86 765 111 498 440 541 258 484  87 670 727 567 467 661 275  91
 217 276 763  90 849 553 816 301 315 257 510 612 606 196 761 185 731 420
 818 319 568 671 746 192 194 759 164 722 669 817  97 828  68  69  60 556
 672 316 318 254 140 825 154 835 392 261  37 264 107 668 410 667 566 570
 178 771 216 260 573 377 101  94 442 752 161 361 399 483 411 444 137 578
 479 279  89 542 108 851 717 608 396 536 491 798 766 764 473 356 520 751
 162 750 830 109 263 330 768 698 240 673   9 733 337 485 474 569 626  44
 413 557 533 676 367 441 706 740 511 824 681 598 119 502 801 471 163 145
 872  12 445 827 394 879 558 613 528 416 675 282 869 250 425 426 481 501
 679 412  77 460 871 742 151 206 754 576 488 561 492 702 643 609  39 753
 322 193 195 168 789 620  73 682 310 705 251 512 514  10 677 368 529 707
  57  38 769 125 144 850 579 218 339  47 699 266 596 728 581 365 616 820
 252  95 120 219 842 189 833 597 348 175 704 734 128 304 821 736 155 582
 170 299 720 470 599 834 873 262 838  18 210 202 371 430 303 662 591 799
 100 378 627 691 103 532 409 169 856 865  33  28  15 837 535 422 486   5
   1 380 654 321 572 632 126 390 332 822  17 134 306  62 283  66 516 623
 538  76 703 775 102 287 619 482 772 386 755 383 265  45 421 221 826 307
 143 259   0 209 450 788 812 463 743 795 308 629 455 797 451 686 208 590
 876 674 767 317 792 796 285 242 721 586 521 803 113 407 334 858 601 829
 466 272 732 118 507 320 117 465 464 280 366 333 357 878  96 468 142 157
 800 689 397 708 678 526 846 610  20 105 477 574 369 628 419 655  54 281
 862 680 233 497 459 659 404 268 839 253 362  31 229 203 695  61 859 544
 780 197 693 517 547 685 575  14 527 559 324 239 427 415 615 778 666  72
 182 760 400 564 335 423 617 515 636 379 819 694 475  16 424  22  71 432
 868 443 277 867  65 214  26 434 813 402 874 806 756 494 719 784 226 165
 664 723  70 385 376 773 270 355 625 345 881  11 639 810 171 696 519 506
  23 588 462 493 269  29 439 267 548 329 656 338 403 136 231 469  98 791
 146 592 232 877  92 809 518  32 852 353 135 325 571 160 748  56 298 174
  42 857 340 525 646 523 785 350 875 864 428  35 286 774 710 779 863 401
 300 635 611 158  75 326 725 273  30 831 152 870 522 537 183 364 447 504
 607 406 490 745 382 594 454 577  46 249 389 713 212 352 832 278 847 726
 749 448 508 133  82 138 381 614 823 295  48  64 633 114 683 604 360 417
 370 644 235 437  21 709 336 290 565 844 583 642 184 861 549 585 110 391
 480 739 230 631 435 602 600 660 115 476  24 436 211 344 245 648 500 555
 524 234 730 531 181 562 156 311 505 384 177 714 129 811 248 398 147 127
 841 123  53 342 347 546 657 741 860 649 589 106 288  27 349  79 724 647
 595 701  50 802 186 405 453  74   2 836 159 238 227 395 148 225 224 122
 223 121  84 770 449 804 554 408  41 388 621 358 116 487 457 271 294 375
 489 327 593 692 587 213  13 244 684 855 247 305 187 840  25 786 793 513
 545 543 373 651 807 781 729 630 220 495 712 458 757 190 534 843  55  78
 605 783 387 204 641  40 640 737 139 716 393 446 104 188 124 456 201 711
 149  36 323 758 478 665 634 150 735 313 845  80 584 309 652 747  43 297
 618 650 563 207 237  81 715 255 866  83 302 292 293 638 172 241 645  93
  67 243  51 433 199 176 690 854 205 603 637 808  19 663 787 551 658  34
 805 438 580 314 312 166   4 374   6 418 452 499 687  99 744 112 431 539
 880 222 794 552 414 284 236 718  59  58 530 848 496 200 738   3 180 429
 622 132 296 624 503  49 198 853 461 341 328 343 130 228 215 700 472 363
  63 790 191 354 776 777 782 131 560 331 688 372 550 291 540 351 653 289]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5930
INFO voc_eval.py: 171: [51 52 30  1  2 37 45 49  8 48 66 64 72 65 26 54 18 10 28  4 38 13 59 24
 23 53 60 29 33 35  9 15  5 63 62 16 31 34 21 61 20 68 46 55 71 32 69 57
 42 25 14 56 11 17  7 36 50 22 12 41 39 27 58 40 67 70  6  3 44 19 73 47
  0 43]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3148
INFO voc_eval.py: 171: [ 62  63  19  95  96  61 111 148  31   3  91  71 106  22  69  27  52  64
   4  92 136 131  84 119 139  49  46  26  44  80  78  24 150 102 103  21
  13  37 100 124  85  38  12  18  93  43  76 140  15   8 137 145  98  41
  57  66 101  86 142 133 110  42  30  88 134  50  53  70 132  34  56   7
  29 120  90  89 149 135  77  67 129  58  35  94  83   5 144 123  65  60
 104   1 125  11   0  25  81  73   9  82   6  45 147  39  33  47 113  54
  75 126  79 108  72  97  99 128 121  20   2  74  16  87  51  68 127 105
 138 118  14  23  10 146  28 112  32 109 114  40  59  48 117 107 130 143
 116  36  17 115  55 141 122]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3557
INFO voc_eval.py: 171: [115  16  69  12  37  56  97 143  26  82  59  33 145  11  83  78  36 155
 159   6  61  57  84 129  73  13  96  35  21 150   9  66 132  55 106  89
  40  15  94  53  30  18 100 125 109 130  74 160 108  24  90  60  45 164
  32 124 127 151 120  42  71  48  86  65  28 137 113  51  99 122 165  29
 147  38  39 112 138 103 102  77   8  91 135  63 107 162 111 131 142 140
 148   1 153   0  67  80 141  68   5  88 152  27  25  34  92   7  75 149
  46 121 116  17 158 136  98  62  23 118 144  52  43  20  70  22 156 114
 110  79 117  85 161 101 105  93  58  49 126  31 163 134  41  87 154 128
  50 104 167  76 157   2   3  95 166  10  54  72 139  19 119  14   4  44
 133 146  64 123  47  81]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3935
INFO voc_eval.py: 171: [ 88 148 218 235 322 210   3 295  25 177 282 135 175  21 317 236  83  37
  55 302  28 119 241 328 178  70  40 272 229 221 184 216 196  11  75 118
  85 223 243 325 209  51  53 338  54 132 242 211 258 102   1  39 154 225
 251 110 180 120 333 238 286  64  58 249 208 303  57 181 280 255 257  82
 128 162 226 304 158 334 252 259 103  34 339 207  61 312  76 305 273 318
 124 256  41 161 270  84  27 346 143  24 171 227  86 285 315  92  47 250
 278 195 319 117 182 288 254  59  60 137  73  72  18 191 114 160 232 237
 176 142 262 192 336 183 314 313 274 106  26  71 145 134   8 292   4 240
 264  63  69  93 144  91 337  19  98 309 152 153  32  33  38 129 275  50
 193  90 260 214  43 306 139 122 300 203 331  52  45  87  30 172 213 131
 197 189 224 174 299 150 291 276 341  56 136  44 168  79 206  74  17 307
 101 230 123 234 271  22 297 199 311 133 253 215 289 198 296 277 151  14
  78 127 281 141 113  80 179  62 283 100   7 265 194 287 163 228  29 326
 217 320 138 187 116 202 164 205  94  10 121 323 310 156  20 316 140 115
 105   5 167 173  66 248  16 293  42 294 335  31   6 344 169   9 107  23
  13 186 149 321  67 268  68 330 263  77 246 147 329 111   2 290  97 126
 165  15 327 298 247 245 343 269  35 146 125 267 201 155  46 342  36 340
 112 284 222 104 332 204  89 345 200 233 159 166 244 239 190  65 185 220
  49 212   0 231 170 279 301  81  95  96 308  99 108 109 266  12 219  48
 130 188 157 324 261]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4460
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4094
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.363
INFO cross_voc_dataset_evaluator.py: 134: 0.613
INFO cross_voc_dataset_evaluator.py: 134: 0.233
INFO cross_voc_dataset_evaluator.py: 134: 0.343
INFO cross_voc_dataset_evaluator.py: 134: 0.583
INFO cross_voc_dataset_evaluator.py: 134: 0.586
INFO cross_voc_dataset_evaluator.py: 134: 0.432
INFO cross_voc_dataset_evaluator.py: 134: 0.003
INFO cross_voc_dataset_evaluator.py: 134: 0.511
INFO cross_voc_dataset_evaluator.py: 134: 0.201
INFO cross_voc_dataset_evaluator.py: 134: 0.392
INFO cross_voc_dataset_evaluator.py: 134: 0.069
INFO cross_voc_dataset_evaluator.py: 134: 0.381
INFO cross_voc_dataset_evaluator.py: 134: 0.756
INFO cross_voc_dataset_evaluator.py: 134: 0.617
INFO cross_voc_dataset_evaluator.py: 134: 0.593
INFO cross_voc_dataset_evaluator.py: 134: 0.315
INFO cross_voc_dataset_evaluator.py: 134: 0.356
INFO cross_voc_dataset_evaluator.py: 134: 0.393
INFO cross_voc_dataset_evaluator.py: 134: 0.446
INFO cross_voc_dataset_evaluator.py: 135: 0.409
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 14999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.469s + 0.002s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.376s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.364s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.370s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.372s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.369s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.365s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.366s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.366s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.367s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.369s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.368s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.367s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.530s + 0.002s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.386s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.373s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.366s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.365s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.366s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.372s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.373s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.369s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.370s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.370s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.370s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.367s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.390s + 0.001s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.335s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.344s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.345s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.345s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.346s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.347s + 0.001s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.348s + 0.001s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.350s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.351s + 0.001s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.352s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.353s + 0.001s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.352s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step14999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.706s + 0.002s (eta: 0:01:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.386s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.385s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.371s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.369s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.364s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.363s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.361s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.362s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.361s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.358s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.357s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.356s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.097s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [231 177   5 287  58  91 218  18  53  47  54 122 145  52 294 134 168 120
 172  46 143 313 179   1 307 178 302  48 314 259  81 270  38  20 263 114
  50 103 129  69 241 204 150  79 124 133 117 272 148 187 149 282  30  31
  85  78 227 247 248  82  51 110 155 101 300 170 317 311  65 146  96  37
 277 245  60 139 308 207 264 288 306 222 212 266   3 130 125  70 199 162
 274  11 163 128  68  10 200  57 111  39 312  17 285   2  76 175  35 105
 203  93 135  21 159  43 107 242  71 131 228  56 321   6 269  22 186 171
 257 254 230 250 309 289 271 144 152 284 278 165  45 215  27  63 161 123
  26  97 246 316 174 256 208 183 153 276 301 280 216 100 184  87 193 279
 213 249 210   4  42 160 196 303 104 220 318 141  89 251 320 214 195 167
 295  86  32 290  14 233 116  55 169  34  88  24  13  33 164 202 268 236
 225 273  25 232  44  15 112 221 132  72   7 115 121 304 305 238 182  62
 258 244  23  67  59  19 209  83 292 283  80 253 262 185  90 286 310 323
 189 188 166 154 224 211 119  12 255 126  95 275  41 142  61 147 180 102
   0 194  98 297 240 106 217   8 239 237 108 118 299  16 281 136  73 190
 243  40  36 235 206 173 252 267 205 113 127  99 191  28   9  66 226  77
 198 265 223 181 229 192 315 293 151 158 137 261 322 319  75  74 109  64
  29  84 176 197 140 291 201 260 234  92  94 298 296 156  49 138 157 219]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3528
INFO voc_eval.py: 171: [178 118  50  24 205 214 154 221 203 123 217  19 108  25 204  85 135 156
 163  14  23 175 189  89 206  59 152  78 202 114 150 211 140  13 113 160
 209 219  49  65   9 144 188 177  34  32 226 111  81  40  71 159  87 136
  94 220 157  10 194  92 173 133 183 141 222  77 155 218 125 124 176  90
 112 126   2  36  82  41 109  26 119  84 129  83 158 169  86  75 139 200
 191 165 216  35   6  30  20 116 122 143  58 117 174 232  52  74  98  76
 229 180 225 131 199 197 184 110 137 198  99 115 127  73 212 228 121 196
 170 193 172  53  96  11  18 105 230  44  45 187 128 168  79  55 201  56
 162 186 107 224  67 227 145 148 102  97  15 185 223 104  27  16  68 130
 181  46 233  31  48  37 134  80 132 179 120 213  95 171 147 149  64  39
  42  93  22  57   1 164   4 231 210 195  54  88 138  29  28  60   0 146
 192 153   8   5  33  51 103 208  43  91   7  63 142  21  17   3  69  12
 182 101  66 207 100  70  62  47  61  72  38 190 151 167 215 161 106 166]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6084
INFO voc_eval.py: 171: [ 58  24  75  45  46  47  64 115  73   1 143 159 148 160 161  55  56   0
  33 127 185  53  91  28 144  48  90  52  72  76  77 179 131 151  67 189
 178 100  31 123 136  57  54 105  21 182  14 163 106 110 134   3 162 168
 128   8 114  74  51 164 174 140  35 167  65  66 129 173  97 132 158  83
 125 188 138  69 154 146 149   7  85  17  27 175  62  16 130 157 147 170
 186 145  93 187 139  96  40  25  19 152  13 101 112  50  70 104  30  38
 135 176 117   9 141  20 171 155 166 137  36  68  44  10  98  99  43  15
  92  32 181  86 180 124 133  63  87 153 107 184  84  41 108  39 102 103
  11   2   5 165 150 119  34  95 109 156 142  37   4  59  29  78 113 122
  88 172 126  80  81  22 183 116  60  79 121  23   6  12  82  49 169  26
  61  42 120  94 111  89 177 118  18  71]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2107
INFO voc_eval.py: 171: [318 376   0 319 103   8  48 320  84  26 258 143 354 250  53 455 257 351
 370 115 254 248 401  10 139  83 117 195 446 114 256 184 190  97 262 453
 321  21 397 259 232 169  85 403 299 377 310 473  98 194  32 395 207 400
  29 443 505  25 345 104 368 384 162 307 237 452  36 430 267 260 240 347
 244  61  62 119 228 475 393 295 151 392 214 170 477 324 451 410 255 414
 408 471 342 314 173 247 178  16  27 462 359 238 265 379 209  30 242 480
 419 149 171 293 425 313 124 416 396 185 383 111 107 298 268 457 188  59
 278  17 315 426  35  37 231 191 183 499 208 158 465 381 427 439 296 166
 485  66  91 142  24  40 404 420 167  45 266 346 488 277  86 290 350 140
 386  65  38 394 365 269 491 280 180 270 385 466 211 168 330 200 380 328
 286 360 218 474 145 502 118 261 233  74 476 497 123 219 454 356 411 157
  55 164 227   5 413 469 406 239 442 221 217  41 287 181 226 504 449 438
 251 361 252  51  87 326  70  28 105 106 283 303 213 243 174 490  47 146
 363  96 329  68 263 153 292 159 332 160 288  34   2 141  23 109 301 136
 382 126 460 309 340 120 344 371 285 390 445 291 304 274  54 434  73 489
 472 437 127 357 284 341 481 373  90  63  18 234 220 210 177 418  58 308
 487 152 241 378  12 236 212 450 137 495 447 122  19 435 196 423 271 498
 222 199  15 456 302 467 235 433 398 496 306 448 333   7 323 432  57 187
 322  42  33   1  49  52 176 230 148  94 163 493 422  71 459 463 316 339
 113  43 348 421 198 203  22  14 409 300 246 464 101 478 441  89  67  11
 440 367 415  56  95  77  82 202 147 102 387 334 374 355 225 224 279 112
   3 110 201 193 116 294 182 428 215 275 417 331 289  60 405 150 179 108
  92 492 366 205  88  76 229  79 391   6 312  64 130 125 412  31 338 458
  69 388 204 483 431 424 154 186 161 132   9 175 461 282 192  50 364 138
  93 121  46   4 389 100 311 297 362 369 479  78 407 156 189 135 264 494
 343 336 317 223  20 468 500 281 129 402 273 375 133 134  72 501 216 155
 249 482 353 327  75 272 253 503 484 372 165 444 436  44 144 172  99 206
 325 131 305 276  13 352 470 486 399 128 429  81 358 349  80 337 335  39
 197 245]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3388
INFO voc_eval.py: 171: [450 451 242 456 208 452 250 211 308 113 376 561   3 370 453 207 209 249
 562 195 279 355 372 130 210 330 354 196   4 371 463 369 280 528 437 464
 477  84 373 490 455 521 471 177 458 397 457  83  35 267 465 472 454 484
 497 461 476 366 380 213 485  17 162 186  33 489 422  61 127 253 375 405
 252 291 501 159 539 255 301 357 150 494 483 443 479 377 459 314 387 202
 316 378  81 305 139 394 146  50  11 555 271 238 408 284 183 189 171 275
 317 315  24 381 525 403 278 486  49 546 544 257  51 470  16 180 307  10
 161 224   6 400 426 234 379 105 410   7 384 296 198 225 547 566 368 399
 231  85 365 229  25 467 321 513 344 564  41 401 214 254 360 126 500 326
  19 124 507  31 392 251  92  94 409   5 356 548 517 469 188 388 340 433
 273 119  34 206 143 487 529 415 286  36 166 212 160 391 436 427 496  98
 449 226 478 402 358   9 187 444 331 523 386 361 420 414  89 292 511 322
 519 460 518 274 230 466 258 435 339 285 389 216 287 336 148  79  22 131
 294 533 185  87 147  28 412  65  80 262 260 439 319 200 156 246  40 445
 165 281 132  71 553 552  21 288 424  97 498 320 276  57 218  43  77 241
 425 560 429   0  62  23 325 298 363 133 364 510 540 114 302 178 169 395
 557  38 233  88 199 121 421 256 259 530 491 154 197  64 468 492  47 167
  37 265 563 362 423 506  74 309  18 217 323 531 204 239 351 168 290  13
 334  72 172 407 152 475 174 346 462 155  66  12 383 536 417 219 125 532
  75 176 261 223 416  78 194  86  70  45 495  32 205 101 299 244  76 515
  93 313 142 567 374 549 220 404 419 329 149 266 240 502 447 396  67 446
  56 182 192 237 545  96 120  91  53 335 538  99 312 268 390 559 247 432
 535 568 272 526 138 505 430  39 117 222 551 514 338 140 300 554  95 537
  55 440 569 135 235 158 550 245  20 236  90 333 393 191 215 181 106 342
 108 173   2  52 103 111 509 104  73 499 558 504 263 136 332 481 327 541
 293 534 128 385 520 353  60 431 349 282 441 516 428 406  15 398 474 243
 227 522 343 480 193 411 277 269 190 482 367 350 524 144  58 100 473 179
  68 508 306 348 413 270 107 184 232 164 122  63 310 145 289 118 112  48
 303 153  27 295 283  82 175  26 134 228 109  42 110 565 311 488 448  29
 347 512 201 328 163 102 503  59 129   1 438 264 345 141 203 352 418  44
 137 341 493 359 337 543 157 527  30 382 542  69 221 434 297 115 324  14
 442 170 151 123 318   8  46 304 116 248  54 556]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5607
INFO voc_eval.py: 171: [ 25  14  45  48  71   3  22  53  59 114 120  87  13 118  77  84  74  68
   2  95  24  81  57  31  73 113  85  97  20 119 127  49   8  52  56  60
  80  83  94  75  15  69  92  91  32 107  41   6  21 116 108  23  98  18
 115  47   7  33  42  78  76 103  44   9  19 106  86  72 121  61  79  10
  89  93  99   0 124  90  26 111 122  11  27  16  67  58 125 123  38   1
  50  64  70  54 101 102  55  46 105  17  62  36  43  34  65  88  35  12
  96 110   4  51  37 112 126  66  28 117  40  30  63  29  82  39 109 104
   5 100]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5832
INFO voc_eval.py: 171: [321 422 343 141  74 495 137 464 247   9 453 463 164  94  54 347 282  23
  17 444 373 218 465 223 200 485   2 222 257 420 242 370 443 186 376 163
  93 380 456 372 123 477 259 398 424 428 434 189  35 334 280  82 248  18
 185 322 509  83 290 469 300  56 197  91  28 274 256 214 349 250 225 371
  49 313 251 283 155 400 187  48 452 188 220 265  34 289 449 307 351 358
 175 266 203 429 441  13 255 320 177  37  52 323  29  25 487 303 151 145
 167  97 442 401 418  57 433 181 468 508 304 263 474 501 478 389 459 121
 258  61 295 403 124 390  33 261 382 104 209  47 228 229 115 252  50 503
 455 357 448 195 437 317 430 217 497 179 113  77 254  53 182 212 393 190
 159 160 227   1 315 473 198 471 483  87  40 399 260 191 192 311 269 412
 270 210 333 148 219 193 138 262  43 221 288 332 215 345 462 451  27 504
 330 264  26 460 498 149 431  41 236  99 169 425 183 172  31 102 411  22
 134  38 276 154  14 144 103 173  24  98 168 305 116 494 206 454  51 476
 417 385 292 490  55 329 369 510  64 480 301 348   5 491 446  92 367 253
  59 273 224 467 337 438 339 132 299  16 410  65  36  62 196 505 415 319
 502 406 450 475  81  32 440 296 127 350 470 194 180 139 391 211   7 336
  76 383 174 277 395 379  44 392 447 402 499 184 161 231 114 233 308 120
 414 208  11 421 284 293 205 156  66 249 239  79 439  72 359 176  75 368
  71 216  70  78 150 394  21 435 405 314  45 318 281 237  67 170 100 413
 106 362 202 136 230 416 143 297 147 267 112 458  60 232 492 213 285 361
  96 166  15 356 128 365 496 331 152   4  46 432 268  19 279 117 199 419
 146 165  95 457   3 272 346 436 226 352 377 488 157 484  63 271  20 342
  68 153 338  73 109 423 360 118 388 302 309   0 286 335 325  86 479 119
 131 384 234  58 291 324 378 426 107 201 108 486 294 363 407 500 110 243
 506 404  80 158 130 366  42 312 135 246 275  85 178  84 354 472 111 340
 353 386 142 481 507  89  88 133 493 326 387  30 240 245 306 341 364 235
 511 125  69 408 466 381 375 344 316 287 207 298 328   8  39 129 427 126
 310 140  90 204 122 241 445 278 244 105   6 409 355 374 489 162 482 396
  12  10 101 461 327 171 397 238]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4242
INFO voc_eval.py: 171: [ 4  2 11 35 12 19 40  3 30 24  0 14 10 15 17 23 38 28  7 39 31 21 22 36
 33  5 41  1 26  8 37 20 34 16 32 25  6 42 29 13 18  9 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0028
INFO voc_eval.py: 171: [1593  370 1600 ...  758  791  300]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5092
INFO voc_eval.py: 171: [ 1 26  2 52 40 20 15 18 56 61 12  3 47 19 37 16 32 42 10 59 62 51 25 50
  0 64 34 22 46 28 27  4 49 11 45 58 57  9 44 60  5 48 30 14 43 21 36 13
 53 39  8 29  7 23 17 54 63 31 35 55  6 33 38 24 41]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1977
INFO voc_eval.py: 171: [364 162  87 155 315 175 237 231 218 236 140  48 322 164  33 127  76  66
  16  15 200 337 275  46 326 386 157  31 366 321 138 228 301 159 247  90
   1 392 312 274 244 139 256 335 234 338 107 328  21  26 302 345  75 269
 181 179 297 105 383 413  88  61 252  79 351 148  24 133 311 406 178 240
  59 398 205 267 323   7 359 380  20 411 250  60 131 402 408 317 152 341
  22  42 373 384 235  67 329 137 124  54 227 245 126 387 276 355 128 119
 391  13 232 255 109 163  93 394 339 198 283 222 367 346 260 201  14 161
  10 340 112 397 115 268 295 106 136  38 216 287 293 176 129  17 135 378
  91 103  95 376 400 284  69  34 233 125  57 352 229 190 266 385 194 357
 354 344 334 405 369 372   4 219 330 306 350 343 361 102 134 412  47  81
 381 239 294  78 257 303 325 377  32  58 225 374 308 289 360 242  19 143
  12 208  11 248 226   8   3 158  99  64 177  82 409 174 305 132 145 390
 353 110 291 270  83 253 331 348 214   6 320 314 286   0 389 203  84 213
 172 108 165 393 404 184  56 113 187 195  98 310 265 224 375 336  52  35
 192 241 401 285  27 100 246  63 324 146 282 114 111 149 371  37 403 151
 327  50 263  94  92 217 188 191 117  71 183  28 211  43 299  74 407 230
 271 281 258 298 238 399  65 189  45 358 279 280 370 220  44 171 395  62
 342 313 264 142 296 300 180 262 121 290  29 196 362 277 254 223  36 147
  96 156 206  40 186 154 101   5 167 272 209  39  70 251 259 333  30 182
 120 332 202 388 197 116 207 319 396  97 309 166 379 368 160 123 144  72
 347  18 221  73 292 318  86  89 104 118 122  77 141 304  80  85   2 307
 130 150 243  51 249 168 212 173 288 365 261 273 210 363  53 170  68 410
 153 356  55  25 349 185 204 382 414 169 193  49  41  23 215 278 316 199
   9]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3727
INFO voc_eval.py: 171: [ 13  92  77  26  27   4  54  53   1  21  48  51  30 101  82  70  19 102
   0 100  65  49  42  35  45 104  97  28 105  23  52  72  44  11   2  14
  95  83  41 106  94  80   7  96  17  75  71  22  88  74  87 103  91  69
  93  62  34  60  59  89  12  79  68   5  20  66   8  63  57   3  55  10
  99  24  58  25  39 107  32  50  76   6  56  85  98  84  33  67  38  86
   9  43  81  64  46  15  61  37  90  31  78  47  73  36  29  18  40  16]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0510
INFO voc_eval.py: 171: [65 76 57  9 12 81 79 43 58 26 54 59 55 10 46 69 45 44 13 66 72 75 40 11
 60 34 78 50 73 29 20 21 37  4 25 15 18  7 24 67 48 77 64 51 19  2 56 74
 41 32 31 23 42 63 70 28  3 49  1 62  0 30 36  6  8 47 22 38 33 80 27 82
 39 71 35 83 52 17 16 14 68  5 61 53]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3857
INFO voc_eval.py: 171: [ 30 237  23 150 104 213 156 250  34  15   9 185 177 111 157   8 171  84
 242  31 175 226  50 173  96 203  57 206  45  95 151 162 134  70  88  94
  73 118 137 115 255 176 136  98  16 239 225 159 145 189 121  97  58  61
  44 125 106  64 143   6 246  68 166 184 210 212  62  22   1 117  42  41
  17  39 169 194 188 211 179 201 129 199  80 127 178  21  26  65  83 248
 220 227 190 231  90  28  36 186  92 257 251 154 119 112  75   3 197 259
  27 205 172 243 214 219  35  56 144 168 229  29 113  51 218 258 163 232
  72  37 131 122 124 233 141 109  79 224 217  53  10 133  38 181  46 114
 209 102 192 241  60  40  66 167 204 160  71 216  43  32  19 252  86  91
 149  81 221 183 126 180 228  25 202 208  89 196 174  52 236 158 234  13
  54  12  49 138 152 249   0  99 128  74 132 254  14  78 142  20 161 101
 135 140 110  59 223 148 187 164 147 123 245  18 247 103 215  93  69 200
 198 193 116 191 256 222 238 235  11 165 139 207 230  85  48 182  67 155
   2  76 253 153 108 195  33   5  47 100 240  87   7 105  55  77 244 120
 107  63 170   4 130  82 146  24]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7562
INFO voc_eval.py: 171: [2053 2068 1188 ...  574  185 1298]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6136
INFO voc_eval.py: 171: [133 157 230 485 143 787 260 163 169 734   8 241  79 328 341 668 788  47
  82   7  80 737 476 100 421 515 242 464 699  81 639 447 539 735 261  85
 631 526  84 298 825 265 789 183 286 204 243 578 487 584 733 175 791 401
 182 302 540 541 703 640  63 184 718 638 155  62 731 790 696 803  90 529
  55 299 301 642 132 237 262 801 372 808 250 144  33  89 637 636  99 203
 391 547 357 168 743 152 392 531 545  93 670 380 463 344 423 724 798  83
 549 425 826 516 740 244 459 471 641 129 103 771 376 736 101   9 805 511
 315 542 722 741 339 496 607 224 319 669 723 454 580 715 644 705 652 422
 488 347 394 465 648 677 154  40 712 479 249 844 543 571 451 453 110 137
 374 426 774 851 506 811 397  12 544 269 556 530 649  68 585 461 468 841
 509 393 407 195 406 507 141 234 796 151 478 533 593 136 573 159 673 342
 441 185 191 614 749 843 726 550 294  15 552  35 305 694 472 761 239 581
 165 676 827 121 650  65 810 205 348  53 514 266 555 108 288 252 725  34
  10 678 113 813 799  66 489 179 567 508 358 253 186 554 570 122 675 690
 145 588 331 450 320 807  87  44 370 198 596 845 187 235 772 284  92 651
 247 192 557 572  73  95 809 119 624  29 708 161 287 829 564 493  32  77
 268   5 706 738 837 659 466 601 127 350 360  70 795 560 314 503 700 674
 802 646 632 412 290 512  17 207 462  94 390 745  57 246 599 403  16 251
 366 727 363   0 304 739 437  60 402 332 292 431 692 196  41 657 501 748
 245 135 104 785 773  88 643 760 448 267 258 443 313 317 300 291 594 432
 647 592 114 828 767 303 387 197 444  97 502 271 770 768 109 497 206 625
 546 822 850 776 457 849 535  18 484  49 683 400 521 475 834 147 345 216
 134 660 218 598 589 814 446 442 160 653 752 215 831 635 583  56 396 707
 474 504 223 409 381 338 308 645 662 597 792  30 513 359 193 666 263 537
 751 732 519 672 629 455 794 316 495  58 587 227 236 404 664 846 492  14
 445 202 728 744 482 405 783 162 164  86 691 786  22 172 756 384 238 377
   3 321  11 704  52 605 532 835  71 839 561 346 255 595 333 414 693 473
 254 499 126  26 505 150 156 494  20  64 762  21 128 779 710 355 793 852
 312 327 365 259 449  51 667 633 840 420 330 309 582 480 415 824 746  28
 337 323 523 264 285 616 697 351 435 385 754 782 806  43 621 378 410 627
 139 219 717 428 610 709 498 804 565 283 470 800 107  69 685 586  38 538
  27 847 149 569 386 418 579 604 429 680 318 124 220 679 830 797 566 577
 698 832 200 102 349 229 553 130 369 233 362 456 398  42 217 280 819 248
 142 757 335 173 382 125 656 232  59 702  31 352 343 765  67 622 520 842
 120 619 613 436 106 417  54 115 603  19 568 686 486 721 608 148 117 176
 324 460  98 212 209 775 838 711 329 574 167 836 500 548 818  74 361 491
 364 534 371 211 221  23 146 257 833 630 274  72 208 518 558 634 528 784
 816 654 575 817 477 199 615 626 310 618 213  48 720 671 210 416 730 695
 306 111 222 467 713 174 368 562 481 375  76  25  78   1 326 296 600 171
 180 527 434 517 611 379 307  50 753 388 663 214   2 112 469  37 194 430
  24 279 777 228 490 602 483 617 178 701 682 311 389 522 812 373 408 353
 684 440 190 661 177  36 759 510 201 297 766 289 438 118 340 525  96 780
 612 282   6 270 427 231   4 140  61 778 225 781  39 729 272 823 383 716
 188 131 439 688 606 240 687 166  13 277 719 367 815 226  91 820 411 273
 275 590 536 655 658 628 763 278 551 395 848 334 325 623 591 399 158 609
 293 452 138 458 714 356 665 116 620 742 105 276 181 750 563 419  45 524
 755 758 354 123  46 336 681 689 256 322 769 170 433 576 153 189 295 747
 764 559  75 424 281 413 821]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5904
INFO voc_eval.py: 171: [49 29 48  1  3 41  8 46 43 45 62 64 69 63 10 28 26 18 51  4 12 34 23 56
 50 33 22  9 57 30 42  5 15 36 19 61 16 60 59 20 44 32 54 65 35 53 68 66
 39 14 52  7 11 31 24 47 67 13 55 27 17 21 37  2 40 38  6 70 25 58  0]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3140
INFO voc_eval.py: 171: [ 59  60  20  94 142  58  32 109  93   3  68 104  89  24  28  66  62  50
  90 131   4 127  80 115 134   8  47  45  23  44  26 101  77  22 120  53
  75  14  38 145  98  82  39  91 133  19  16  41 100  13  96  73  54   9
  42 144 135 140  63  84 137 130  99 129 108 139  51  37  35  88  31  30
  74 132 128  64  87 121 111  48  92 125  55  79  52   1 116  36  67  57
  81  12   0  27   6 102  86  10  70 122  40  95 119   5  43 141  76   2
  78 105 143  69 103 124  72  21  97 117  65  85 123  61  25 114  17 113
  56  18  34 106 126   7 107 138 136  49  15 112  11  46  29  33  83 118
  71 110]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3497
INFO voc_eval.py: 171: [115  18  71  15  39  58 146 143  28 101  83  61  35  14  84  79  38 156
 159   7 151 129  59  85  97  16  37  13  63 102  23  68  75  57 132 106
  42  17  55  90  20  95 100  32 109  62  76 126  40 166 108 161  26 125
 127 130  34 152 138  53  47  88  91  49 121  30  44  31 137 123  73 112
 113 148  67 103  78  99 147 167  41  65   5 107 142  72  92   6 135 131
  69 163 111  94  10  81   1 154 140   2  29 141  36 153 150  27  48 122
  87   9  19  64 136 118  25  45  70  89  12  43 105  93  98 104 157 117
 110 144 162  80  24 158  51  22 155 114 128   8 164  56   3  86  33 160
   4 168  77  21 139 149   0 169  52 145  60 116 134 133 119 124  50  11
  66  96  82  74  54 120 165  46]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.4081
INFO voc_eval.py: 171: [ 88 147 216 232 206 319 293   3  26 175 279 134 314  19 166 233  83  56
  37  23 299 238 118 176  70  40 269 182 228 214 218 192  85  75  11 326
 117 220 240 322  51  55 205 225 207  54 335 239 130   1  39 178 119 284
 223 234  59 249 257 330 151  64  57 204 109 247 300 179 102 127 161 277
 250  34 331 254  82 301 286 224 251 309 336 157 103 203 123 270  41 256
 316  53 302  84  25 255  76 160  22 282 142  86 312 267 261  60 341 191
 275  47 235 170 181  95  73 113 116 229 253  16  72 141 315 159 271  24
 248 187 242   4 310 259  52  63 237 136   8 333 290 144 152 143 311 133
 173 188 106  71 180  30 334  62 306  32  50  90 153  17 138 305  91 272
 298 121  93 210 209  69 128 107 211  98  87 328  28 258  45  43 186 199
 189 202 169 122 273 129  58 289  79 338 227  20 172 101 177 303 295   7
 313 135 280  74  27 193 268 274 195 297 285 114 149  38  94 163 231 308
  44 100  10  13 221 150  80 126 184 287 167 252 162 317 111 194 190 140
 120 213 323 278 215   6 262   9 320 198 155 137  78 340  18 165 264  42
 226 332  15 294 115 139 132 201 292   5 246 265 307 244  29  66  67 105
 168 339 288 148 171 196 164   2 245  68 110 146 125 327 325 183  12 266
 296  77 185 291 281 219 112  97 230 241  31  21 156 208 124 324 236  65
 318 154   0 343  35 260 197  99  36  48 158 108  14 337  46  49  61  81
  92 283 200 145 276 243 212 174 104  89 329 342 263  33  96 304 131 321
 217 222]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4623
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4046
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.353
INFO cross_voc_dataset_evaluator.py: 134: 0.608
INFO cross_voc_dataset_evaluator.py: 134: 0.211
INFO cross_voc_dataset_evaluator.py: 134: 0.339
INFO cross_voc_dataset_evaluator.py: 134: 0.561
INFO cross_voc_dataset_evaluator.py: 134: 0.583
INFO cross_voc_dataset_evaluator.py: 134: 0.424
INFO cross_voc_dataset_evaluator.py: 134: 0.003
INFO cross_voc_dataset_evaluator.py: 134: 0.509
INFO cross_voc_dataset_evaluator.py: 134: 0.198
INFO cross_voc_dataset_evaluator.py: 134: 0.373
INFO cross_voc_dataset_evaluator.py: 134: 0.051
INFO cross_voc_dataset_evaluator.py: 134: 0.386
INFO cross_voc_dataset_evaluator.py: 134: 0.756
INFO cross_voc_dataset_evaluator.py: 134: 0.614
INFO cross_voc_dataset_evaluator.py: 134: 0.590
INFO cross_voc_dataset_evaluator.py: 134: 0.314
INFO cross_voc_dataset_evaluator.py: 134: 0.350
INFO cross_voc_dataset_evaluator.py: 134: 0.408
INFO cross_voc_dataset_evaluator.py: 134: 0.462
INFO cross_voc_dataset_evaluator.py: 135: 0.405
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 17499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.508s + 0.002s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.376s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.360s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.364s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.368s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.366s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.367s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.371s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.372s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.368s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.369s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.370s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.370s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.417s + 0.002s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.388s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.375s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.364s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.368s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.378s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.380s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.376s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.371s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.370s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.369s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.369s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.368s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.440s + 0.001s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.353s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.368s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.371s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.362s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.360s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.360s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.361s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.362s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.363s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.365s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.363s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.364s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step17499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.558s + 0.001s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.385s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.378s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.363s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.360s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.358s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.359s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.359s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.362s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.362s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.361s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.360s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.362s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.568s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [230 176   5  92 284  59 217  18  52  47  53 122 143  51 290 134 167 120
 171 142  46 310 178   1 303 177 298  48 311 257 267  82 261  38  20 114
 104 129  70 239 150 204 124  80 133 269 117 185 148 149 279  30  86  31
 289  79 226 245 246 110 145  50  83  56 156 296 169 308 314 102 125 146
  66 274  97  37 207 243  61 139 304 262 285 266 221 130 302  71 212   3
 264  54 199 271  11 161 128  69 231 162 200  39  58 111 309   2  10  17
 282  77  93 174 106  35 135  21  43 227 107 240 201  57  72 131   6  22
 158 170 318 184 252 305 268 287 248 255 229 152 281  27 164  64 313 275
 214  45 123 173 254 273  98 244 160 154 101 216 208  26 182 215 196  88
 277 247   4 105  42 219 159 276 299 210 192 315 234 186 140 297 249  90
 213 317 166 194 291  87  32 168  34  14  33  55 116 163  24  89 286 265
 203  13 232  44 233 220 144 132 112  73 270  25 224 300 301  15 181 115
  63 121 236 242   7 256  60  68 307  23 251  19  81  91 280 253 320 183
  96  84 209 188 153 126 119 306 187 260 272 211 223 283  41  62 179  12
 147 141 165 193  99   0   8 293 108 237 235 172  16 238 278 189 295  78
  74  36 118 136 241  40 205 206 127  28 198 113 250 190  67 151 191 180
 312 100 228 263 137   9 259  75  76 157  65 225  94  29 288 109 319 202
 222 316 175 197 292 195 258 138  95 155  49  85 294 218 103]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3525
INFO voc_eval.py: 171: [178 116  49  23 205 214 222 153 203 123  18 217 107  24  84 204 134 163
 156  13 175  22 189 206 151  87  77  58 202 113 149 211 139  12 112 220
 209 160  80  48  64   9 143 177  33  31 110 227 188  70  39 135  86 159
 185  93 221 157  88 194  91  10 173 140 223 132  76 219 124 154 111 176
   2  89  35 125  81 118  25 128  40  83 108  82 169  85 158 138  74 200
 191 165 216  34 218  19  29   6 115 121 142  57 174  51 233  97  73 180
  75 122 230 226 183 109 130 197 199  98 198 136 114 212 126 193 120  72
 229 172  52 196 170  65  95 179  17  11 104 187  43 231 127  44 168  78
 106 225  55 155 162  54 201  66 144 186 147 228  96 101  14  67 103  47
  26 224 184 181 117 234  30  36  45 133 129  15  79  63 171 146 131 119
 148  94  38  92 213  21   1  53  56   3 210   4  27 232 164 195  28  59
   0 137 102   5  32 192 145  50  42 152  68  62  90   8  20  41   7 141
 208 207 182  16  69  60  46  61  99 100  71 190 150  37 167 161 166 215
 105]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6078
INFO voc_eval.py: 171: [ 59  25  75  46  47  48  64 114   1  73 142 158 147 159 160  56  57 183
   0  34 126  54  90  29 143  49  89  53  71  76  77 177 130 150 176 187
  99  32 135 122  66 104  58  55  22 180  15 109   3 105 162   2 133 167
 161 127   9 113  52  74 173 163 139  65  36  67 166 128  96 172  83 124
 137 131 186  69  85 148  18 153 145  28   8  62 174  17 129 157 184 146
 169 144  92 138  95 185  41  20  26 151 100  51 111 103  14  70  10  39
 134  31 116 123 136  68 156  21 140  45 170 175  44  97  11 165 154  37
 178  98  33 179  16  86 152  87 132  91  63  60 106  40  42 107  84 182
 101  35 149 102   6  12 155 164  38  94 108 118  61  80 112 121  30 125
  78  88 141  81   4 115  23 171 181  79  50  24 120  82   7  13  43   5
  27 119 168 110  72 117  19  93]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2107
INFO voc_eval.py: 171: [316 373   0 317 100   8  47 318  82 255  26 140 351 247  52 453 349 254
 367 112 251 245 398   9 137  81 115 193 444 111 253 183  94 259 451 188
 319  21 394 256 167 230 114 400  83 374  35 297 470 308  95 192 205 392
 397 505 441  25 365  29 101 343 381 305 450 235  36 160 264 257 238 226
 345 242  59  60 472 390 293 428 389 149 212 168 474 322 449 409 340 252
 407 413 468 312  27 171 244 177  16 460 355 236 376 262 260 207 240  31
 418 478 291 147 424 169 121 415 393 311 184 380 104 296 108 266 455 187
 275 425  17 401 313 229  37 189  34 499 206 378 164 156 437 483 419 463
 426  88 165  64 294  24  39 139  44 487 263 287 276 138 344 348 486  84
 383 129  63 179 490  38 362 391 278 267 198  57 326 209 166 382 328 464
 268 377 142 356 216 471 258 231 473  73 352 120 155 288 502 497  53 217
 181 452   5 410 225 412 467 215 404 162 219 237 440  40 180 504 284  85
 436 447 359 224 102  50 249 103 143 241 281  69 248  46 211 172 323 327
 301 360  89 489  28  66   2 119  23 307 402 157 106 330 151 379 144 158
 285 342 289 122 300 458  33 443 116 134 387 368 338 283  67 339 302 290
 272 479 469  61 432 353 176  12 488 208  72 370 110 406  87  18 375 435
  56 218 234 239 150 135 232 210 417 306 448 282 445  19 433 299 194 422
  15 431 269 498 454 220 233 495 321   1 465  55 197 357  70 186 395  48
 320 331 146 430  41 126   7 228  32 175 117 337 446 161  92 346 457 201
 439  42 421 492 243  11 408  14  51 196 475 462 420 314  98 148 461 364
 298  22  80  93  54 414 438 145  65 494  76 200 371  99 384 332 199 222
 107   3 274 109 191 292 491 223 113 277 286  86 213 416  58 427 329 363
 178 182 105 127  75 403 310 476  30 227  62   6  78  90  68 304 203 411
 385 481 388 202 159 456 130 131 174 185 152  10 336 366  91 429 423 265
  49 405 459 361 136 190  77 295 154 280  45 261 334  97 500 386 309 271
  20 358 477 496 221 133 480 341 118 372 250 214 153 315 325 493 399   4
 466 132 347 279 324 246 350 270 484  96 125 501 163 485 434  43  71 503
 335 442 204 396 141 170 123  74 303 128 369 482 124  13 195 173  79 273
 354 333]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3313
INFO voc_eval.py: 171: [454 245 455 460 210 456 252 213 311 114 380 565   3 374 457 209 211 251
 566 197 282 359 376 131 212 333 358 198   4 375 467 373 283 533 468 442
 481  84 377 495 459 526 475 178 462 461 401  83  34 268 469 476 458 488
 503 465 480 384 370 215 489  16 163 188  32 493  61 427 128 255 379 254
 409 507 295 544 160 361 262 151 499 305 483 447 487 381 463 203 391 318
  81 382 320 140 308 147 398  49 560 273 241  11 412 184 287 191 172 385
 319 321 407 278  23 281 490 530  48 549 551 218 258 474  50  15 181 310
 162   9 227 404 236 430 383   6 106 414 388 300   7 200 403 552 234 372
 571  24 519 231 471 369 405 568 216  40 348  85 364 256 324 329 293 506
 127  18 396 125 512 253  30 413  95 360 522   5  93 553 473 190 392 344
  33 120 436 276 492 144 439 208  35 534 420 289 214 441 167 502 161 395
 431 228 453 406 362 482  99 189 298 365 448  90 334 425   8 390 517 464
 528 470 296 524 325 259 277 438 233 343 523 288 393  20  21 219  87  27
 340  79 186 148 132 149  65 261 417 538  80  39 322 263 202 444 449 133
 166 249 157 290  71 284 559 557 415 505 244 429  98  42 221 279 323  22
  77  57  62 564 432 367   0 134 328 368 302 516  37  88 115 545 472 306
 399 496 535 201 257 170 561 366 237  64 155 260 199 513 498 426 428 179
  36 122 567 220  75 168 206 313 267 536 242  44 479 326 337 153  17 169
  72  10 411 355 350 156 466 294 422 387 175 173 222  66 541  12  74 419
  86  70 421 225 537  45  78 501 126  31 247 196 177 572 317 224 102 207
 332 143  76 408 243 240 424 400 554 378  91 508 269  94 150  67 121 266
 451 183 550 450  56  52 338 194  97 435 316 394 511 100 542 270 433 569
 303  38 250 139 118 540 274 573 574 558 556 342 141 248 520 226 238 136
 304 159 217  96  19 445 504  54 555 531 182  92 275 239 397 335 346 185
 515 485  73 112 193 174   2 543 105  51 562 107 510 330 104 336 137 389
 434 264 129 285 109 246 521 546 539 525 353 297 357 486  13 271 478 500
 410 230 484 354 280 402 347  68 570 192  60 145 371 527 529 195 101 309
 477 180 416 418 146 514  58 314 165 272 187 113  63 108  26 119 352 292
  47 123 235  41 307 299 286 301 154  25 497 176 491  59 229 205  28  82
  43   1 110 164 363 351 135 103 518 423 331 509 265 152 204 356 111 452
 341 345 223 130 142 315 437 349 443  14 171 116 158  69  29  89 440  46
 138 386 124 327 563 532 494 339 547 312 117  55  53 446 548 232 291]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5597
INFO voc_eval.py: 171: [ 27  14  47  50  73   3  22  55  61 116  89  13 122 120  79  87  76  70
  25  94  98  84  59  33  75 115 100 121  88   8 129   2  54  62  51  58
  83  86  77  97  20  71  16  95 109  43  34  21   6 110 118  24  18 101
 117  49  23   7  35  44  80  90  78 108  19   9 106  46  63 123  74  10
  81 102  92  96  28   0 126 113  93  11 124  29  15 127  69  60   1 125
  40  52  66  72  56 105  57 104  48 107  17  36  45  38  64  67  91   4
 114 112  12  37  99  53 128  39  68 119  30  42  32  65  31 130  26 103
  85  41   5 111  82]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5756
INFO voc_eval.py: 171: [320 341 140 492 418  74 136 246   8 460 450  94 164 461  53 345 281  15
  21 441 217 462 368 199 222 482   1 221 256 241 365 440 185 415 371  93
 163 375 453 367 122 259 393 474 420 424 188 430  33 279 332  82 247 184
  16 321 506 289  83  55 466 196  91  26 273 255 213 299 249 347 366 312
  49 224 250 186 395 155 282 187  48 449 264 219  32 288 306 446 349 202
 174 355 265  11 425 319 438 254 176  36  23 322  27  52 484 302 151 144
 439  98 168 396 414 429  62 465 180 471 262 303 475 498 384 257 120 167
  97 456 294  59 398 123  71 385  31 260 103 229 377 228  47 500 208 251
 114 452 433 354 445 194 426  77 178 494 316 112  51 252 216 505 181 159
 388 211 189 160 225 314 468   0 480 197  39  87 190 258 470 191 394 268
 310 493 407 269 331 147 137 218 209 330 192 220 214 343 261 287 448  42
 459  25 263 457 495  24 328 148 427  40 236  29 421 182 101 171 501  20
 133 406  12  22  37 172 102 154 143  99 169 275 304 491 205 451  35 380
 115 291 487 473 327  54  50 413 477 507 443  69 300   4 346  92 253 364
 362 272 223 464  57 334 488  61 434 131 337 298  14 404 200 417 499 195
 318 447 502  30 410 401  81 295 472 179 193 467 138   7 386 348 276 126
 210 173 390 437 378  76 335 283 387 496 397  44 374 161 444 233 490 231
 307   6 113 207   9 183 119 436 248 156 292  79 238 409  86 150  72  78
 352 204 237 363 356 105 215  45  70  67 170 313 280 175 100  19 408 353
 317 230 389  63 232 400  75 111 266 358 296 284 431 412 135 142 212 201
 166  34 428  96  58 127 152   3 329 489  46 360 146 267 416  95  17 165
  13 454 455 198 116 145 271 419   2 432 485 344 278 350 340 372  73  43
 153  60 481 270 336  18 117 157 118 324 108 285  64 308 383 476 435 379
 234 290 357 301 333 323 293 107 130 422 497 109 227 503 244  41 245 141
  56 361 373  65 242 106 402 158 177 110 399 483  80 134 311 504 338 381
 129 274  89  84  88 342 469 132 351 206  85 478 339 382  28 297 325 411
 235 239 124 305  68 121 359 125 370 463  66 391 403 226 442 376 286 149
 309 315 508  90  38 326 203 139 423 392 240 486   5 128  10 243 369 405
 479 277 104 162 458]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4226
INFO voc_eval.py: 171: [ 4  2 11 35 12 19 40  3 30 24  0 14 15 10 38 28  7 23 31 39 21 17 22 36
 33  5 41  1 26  8 37 20 16 34 32 42  6 25 13 29 18 27  9]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0027
INFO voc_eval.py: 171: [1593  366 1600 ...  219 1290 1162]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.4999
INFO voc_eval.py: 171: [ 1 26  2 51 40 20 18 55 12 59  3 15 46 19 37 16 10 32 57 41 60 50 25 49
  0 62 34 22 45 29  4 27 11 48 44  9 43 56  5 58 47 30 14 28 36 42 21 13
 52  8 39  7 23 17 53 35 61 54  6 31 38 33 24]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2076
INFO voc_eval.py: 171: [364 162  87 154 314 175 236 230 217 235 139  48 321 164  32 126  66  76
  16 336  15  33 200 275  46 325 386 157 366 320 300 137 227 159  90 244
 392 311 274   1 245 256 334 138 337 233 106 327  21  26 301  75 181 344
 269 296 179 104  88 383 253  24 413  61 350  79 147 132 310 178 239 406
  59 398 267 205 360 322 380 411  20   7  60 250 402 315 408  42 151 130
 340  22  67 234 328 373 384 123 136  54 226 125 355 276 387 231 118 391
  13 127  93 163 198 255 108 367 221 394 260 345 338 161 282  14 201  10
 397 111 339 292 294 105 135 286  38 215 268 114  17 128 176 102 134  91
 378 376  95 400 283 232  34 124  69  57 228 351 357 190 194 385 354 266
 343 369 405 333 218 372 342   4 305 329 349 381 101 133 412  47 361  81
 377 293  78 238 324 224  31 257 302  58 307 288 374 404 242  19 225 142
  12  11 248 208   8 158  64   3 152 214  99 174  82 409 352 177 389 131
 304 390 109 270 330 144 290   0  83   6 172  84 165 347 313 107 359 203
 213 393 285 187 309  56 375 319 184 112 223 195  52 401 265  98 192 335
 284  63  35 371 323 247  37 326  27 281 241 113 145 263 150 403 188  45
 191  50  43 148 110 211  28 116 216 258  94 407  71 229 298  92 219  74
 271 183 399 171 237 370 280  44 279 295 358  65 262 189 264  29 297 156
 341 312 120 289  96 141 362  62 395 277 254 196 180 167 252 100  40  36
 222 155 153 299 186 206 209  39 146   5  70 251 388 207 331 182 197  30
 202 115 332 259 272 318 119 379 368 166  18  73 308 396 346 160 143  97
 220 103 122 317  51 291  80 117 240 249  89  72 129  86 149  77  85 121
 168 287 246 243  68  55 261 303 212 348 170 365 306 410   2 173 363 210
  41  25  49  23 273 169 356 204 140 414  53 353 193 382   9 185 278 199
 316]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3708
INFO voc_eval.py: 171: [ 13  90  26  75  27   4  53  20  52   1  47  50  30  80  68  18 100   0
  48  63  99  41  35  44  98 102  28  95 103  22  51  16  11  70  81  43
   2  93 104  92  78   8  40  94  73  17  69  86  21  72  85 101  89  60
  67  34  87  59  91  66  61  54  12  58  64  19   5  77   3  56  97  10
  23  57  24  38 105  32  49  55  74   6  83  82  96   7  33  37   9  84
  65  42  79  62  36  45  14  15  46  76  88  31  29  71  25  39]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0597
INFO voc_eval.py: 171: [64 75 57  8 11 80 78 43 55 25 52 42  9 45 68 54 44 12 65 59 71 39 58 74
 10 60 33 77 49 72 28 19 20 36 40 14 24 17  4  6 23 76 66 15 47 50 63 18
  2 69 56 31 73 30 22 41 62 27  3  1 48 61 35 29  0  5  7 46 37 21 32 26
 81 38 79 70 34 67 82 16 13 51 53]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3898
INFO voc_eval.py: 171: [ 32 238  25 151 106 214 157  36 251  15   9 186 178 112 158   8 172  86
 243  33 176  52 174  98  97 227 205 208  59  47 152 163 135  72  96  75
 119 138 116 256 177  90 137 100  16 240 226 160 146  99 190  60 211  46
 122  63 144 108 126   6 247  66  70 185  64 213 167 180  22  44 118   1
  17  41 170 193 189 212 203 201  82  21 128 130 179  27 249  67  85 221
  43 228 191 232  94  29  38 187 252 258 155 113  77 120 199   3 125 207
  28 260 244 210 173 215 220  37  58  31 145 169  92 230 115  53 114 259
 123 219  39  74 233 164 142 218 234 132 225 110  81  55  40  10  48 182
 134 121 104  68  73  13 206 161 168  62  42  88  34  45 217 242 127  93
 253  19 222 184 181  26  83 150 175 229 209 198 159  54 204  91 237 235
 139  12  51 153 250  56  76 101  14 129   0  30 143 133 255 103 162  80
 224 141 136 111  20 149 246 194  61 105 165 148 216  24 124 248  18 200
  71 195  95 202 192 223 117 257 166 239  50 140 236 156 231 254  87   2
  11  69 183 154 196  49  78  79 102   5   7  35 109 241  89  57  65 197
 188  23 245 107 131 147  84   4 171]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7560
INFO voc_eval.py: 171: [2060 1197 2075 ... 1966  724  321]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6158
INFO voc_eval.py: 171: [134 158 230 148 483 789 261 164 170 242 731   7  78 327 340 790 663  46
  81   6  79 734 474 100 420 514 243 462 694 445 635  80 732 262  84 538
 525  83 627 585 298 267 828 791 183 287 206 244 576 485 730 176 793 182
 401 302 539  62 636 698 184 634 713 156  61 728 792 691 805  90 528 299
  54 301 133 638 238 263 371 803 251  33 810  89 633 144  85 632  98 203
 543 391 356 169 153 740 392 545 584 665 379 461 343 422 800 721  82 737
 548 423 829 515 637 245 469 457 375 771 104 130   8 101 733 540 808 318
 315 718 510 224 603 738 494 664 338 452 720 532 640 578 710 648 421 486
 700 394 345 155 541 644 463 672 250 707 476  40 847 569 449 110 138 424
 373 743 451 855 505 772 397 508 813  11 547 542 270  67 466 645 459 844
 529 393 407 195 582 142 798 406 152 137 235 477 160 191 571 531 341 439
 610 668 185 166 549 294 748 723  13 551 689  35 305 830 846 760 579 240
 470 671 204 122 812 646 266 513 346  64 107 289 554 113  65  52 815 256
 722 801  34 180   9 777 487 673 357 565 186 369 552 506 253 745 684 568
 145 198 331 670 809 448 162 592 849  92  87 555 187 285 248 119 237 192
 647  44  73 570 319 811  94 620  28 269 491 832 597 163 464 128 562  76
 288  69   4 656 314 840 502 558 359 735  31 349 703 669 507 804 701 797
 291 642 511 595 207  16 628 412  93 742 460 695 390   0 247  56 304 435
 252 736 724 362 365 685  59  15 403 293 402 429 103 196 246 654 557 639
 787 136  88 268 446  41 259 831 499 313 300 441 115 643 303 759 316 387
 430 292  96 766 500 272 347 590 197 746 311 109 544 767 770 661   2 442
 589 376 825 621 495 534 455 774 854 520  47 205 838 779 853 677  17 400
 473 216 344 481 752 147 218 658 816 594 649 161 135 834 215 440 631 123
 581  55 223 396 444 380 408 503 472 536 702 593 659 641 337 794 264 750
 358 193  29 667 512 729 518 453  57 493 317 625 796 586 850 404 741 333
 227 165 725 202 490 236 687 785  14 320  86 686 383 405 480 755 715 788
  21  70 601  10 443 239 173 559 839 496 255  51 591 699 795 151 254 129
 842 688 127 471  25 781  50 492 413 482 157 761 705 530 856 419 354  63
  20 309 329 447 751 326  19 364 260 265 826 504 769 580 629 522 478  27
 616 843 377 322 286 612 692 350 384 336 807 744 433 410 140 414 753 583
 108 784 806 426 623 468 330 306 802 219 567 537 712 497  68 674 386  43
 600 563 679 704 851  26 577 575  38 564 102 348 417 550 284 606 150 126
 427 200 835 833 675 229 799 693 217 232 368 220 697 454 334 821  66 131
 398 351 615 361 756 519 233  42 116 121 281 652 106 125 249 381 618 342
  58 323 143 149 434 609  53 566 179 599 174 177  97 764 416 845  18 680
  30 773 484 328 212 258 489 118  74 848 604  72 498 572 630 820 517 168
 208 458 275 363 841 653 706 146 211 717 533 360 836 626 370  23  75 221
 546 786 727 527 475 209 213  24 556 112 573 819 210 622 650 818 310 385
 307 465 690 367  48 611 716 666 479 708  49 175 222 596 614 194 374 526
 388  77 432 560 308 676 837 111 660 607 598  37 325 378 171 199 776 516
 428 696 775 296 312 613 415  22 467   1 214 814  32 172  60 678 389 297
 521 657 353 438 228 190 488 280 372  36 271  99 758 290 178 524 231 436
 283  95   5 608 509 201   3 120 782 765 780 339 188 425 273 783 553 382
 437  39 824 711 726 167 241 714 409 602 225  91 132 817 141 778 278 822
 682 535 719 332 324 276 274 366 651 226  12 619 279 852 587 159 355 117
 709 411 754 762 763 662 523 768 681 395 655 399 418 561 450 181 277 139
 105 757 749 335 624 605  45 588 683 154 456 739 257 747 124 574 823 282
 295  71 321 617 827 501 234 431 352 189 114]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5897
INFO voc_eval.py: 171: [47 28 46  0  2 39  7 41 44 43 59 61 66 60 25 27  9 17 49  3 11 32 22 54
 48 31 55 21  8 29  4 40 14 34 18 15 58 57 56 19 42 30 52 62 51 33 65 63
 38 13 50  6 10 23 45 36 64 12 26 53 16  1 20 35 37  5 24 67]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3072
INFO voc_eval.py: 171: [ 60  61  21  94 143  59  32  93 109   3  69 104  89  25  29  67  63  51
  90 132 128   4  80 115 135   8  75  48  45  24  27  44 101  77 120  23
  54 100  38  15  98 146  82  39  91 134  17  20  41  14  96  55  73 145
  42   9 136 141  84 131 138  64  52 130  99  68 140 108  88  37  35  31
 133  74 129 121  65 111  79  92  87  56 126  49   1  53  58  36  81  28
  12   0   6 102 116  86  71  10 122   2  43  40 103  95 142  76 105 144
   5  70  78 119  72  47 124  22 125 117  97  85  66  26  19  62 113 114
  13  18  57 123  34 106 127   7 107  83 139  50 112 137  46  11  33  16
 118 110  30]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3503
INFO voc_eval.py: 171: [111  17  68  14  38  55 143 140  27  95  80  58  35  13  82  37  76 152
 155   7 147 125  81  56  15  94  12  36  60  22  65  54  72 128 102  41
  25  16  52  19  87  93  98  31 105  59  73 162 122 157 104  39 121 123
 134 126  33 148  51  85  45  88  29  47 109  30 117  43 145 133 108 119
  70  99  75  64 144  97 163  40  62   5 139  69   6 103 127  92 131  89
 107 159   1  78  10   2 151 137  34  28 138  26  46 149 118  84   9  61
  18 132 114  11  24  44  66  91 101 100  86 106 153  67 113 158 112  77
   8  49 141  96  23 124  53  21 154   3 110 160  42  83 156  20  32   4
 146 136 142 150   0 129 115 164 130  79  74  63  57  50 120  48 116  90
  71 161 135]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.4093
INFO voc_eval.py: 171: [ 87 146 216 232 206 319 292   3  26 175 279 133 314  19 166 233  82  56
  37  23 298 238 176 117  70  40 269 182 214 218 228 192  84  75  11 116
 325 220  51 240  55 321 207 225 205  54 239   1 129  39 178 118 283  59
 234 223 249 151 329 257  64  57 334 204 299 247 179 108 126 101 160  44
 250 277 330 270  34 254 300  81 285 251 224 309 335 156 203 122 102 316
 301  53 256  83  25 255 159  76 282  22  85 312 141  60 191 267 340 235
 275  47 181 112 229 170  94  73 253 115  24 271  72  16 140 158 315 242
  52 248   4 310 187 259 150 237  63   8 289 143 332 142 132 311 135 105
 180 173 333 188  30  71  62 306  32  89  17  50 152 137 209 304  90 297
 272 120  92 210 211  69 127 106 327  28  86  45 262 258  97 202  42 186
 189 169 199 121 273  78 177 336 128  20  58 280 302 227 113 288 134   7
 100  27 172 294 163 274 313 268  93 193  38  74 284 296 308 231 148  43
  99  13 161  10 252 221 286 149  79 184 125   6 110 265 119   9 317 194
 190 168 215 139 278 339  18 320 213 261 154 322 136 331 198 195 226 165
  77  41 338  67 138 264   5 244 291 114 293  29 167 104  66 246  15 287
 171 196 201 147 131 307   2 164 245  68 109 124 219 145 185 266 324 326
 295 155 111 183 281  12 230 241 162  96 236 290  31 323 342  21 260  61
  65  98 123   0  48 208 174 153  35  36 318 107 197 212 337 243 200  46
  91  88  80 157  49  14  33 222 303 328  95 103 276 263 144 341 305 130
 217]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4624
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4041
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.353
INFO cross_voc_dataset_evaluator.py: 134: 0.608
INFO cross_voc_dataset_evaluator.py: 134: 0.211
INFO cross_voc_dataset_evaluator.py: 134: 0.331
INFO cross_voc_dataset_evaluator.py: 134: 0.560
INFO cross_voc_dataset_evaluator.py: 134: 0.576
INFO cross_voc_dataset_evaluator.py: 134: 0.423
INFO cross_voc_dataset_evaluator.py: 134: 0.003
INFO cross_voc_dataset_evaluator.py: 134: 0.500
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.371
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.390
INFO cross_voc_dataset_evaluator.py: 134: 0.756
INFO cross_voc_dataset_evaluator.py: 134: 0.616
INFO cross_voc_dataset_evaluator.py: 134: 0.590
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.350
INFO cross_voc_dataset_evaluator.py: 134: 0.409
INFO cross_voc_dataset_evaluator.py: 134: 0.462
INFO cross_voc_dataset_evaluator.py: 135: 0.404
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 19999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.498s + 0.002s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.389s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.382s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.377s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.375s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.374s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.372s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.376s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.375s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.372s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.372s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.372s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.371s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.518s + 0.001s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.365s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.367s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.358s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.352s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.348s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.356s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.356s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.356s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.359s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.360s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.365s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.364s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.455s + 0.001s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.350s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.351s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.357s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.361s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.362s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.361s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.361s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.360s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.364s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.364s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.362s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.363s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step19999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.510s + 0.002s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.358s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.364s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.358s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.360s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.360s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.357s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.359s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.359s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.358s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.355s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.357s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.357s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.408s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [231 177   5  92 285  59 218  18  52  47  53 143 121  51 291 134 168 172
 142  46 119 311 179   1 303 178 298  48 312 258 268 262  38  19  82 103
 113  70 129 240 150 206 124 133 116 270 186  80 148 149 280  30  86  31
 290 227  79 245 247 109 145  50  83  56 309 157 170 296 125 315 102 275
 146  66 208  97  37 139 242  61 263 267 286 222 213 130  71 302 265 272
   3 202  54  11  69 128 162 163 232 200  93  39   2 110 310  77  58 283
  17 175 135 105  10  35 304  21 201 241 228   6  57  43 171 106 306  72
 159 319 131 253  22 185 269 249 288 256 282 230 153  27 165  64 314 214
 215 276  45 274 255 123 174 217  98 101 244 216 104  42 197 161 183 209
 155 248  88 220   4  87 278 316 160 277 299  26 235 211 187 297 151 193
 140  32 250 292  34 167 169 195  14 318  33 164  90 115  24  55 122 234
 266 204  44 221  73 233  89 287 132 300 111 182 301  13  25 144  63 271
 114 225  15 120 237 257 246 243 254 252  60  68  23  96   9 281  81 308
 321  91   7  20 184 188 126 210 180  84  41 154 307 118 189  62 273 141
  12 212 224 284 147  99 294 107 261 194 236   0 173   8 238 190  78 279
  36 166 239  16 295 117 136  74  28  94  40 313 152  75 264 205 199  67
 127 192 112 191 229 251 181 100 137 260 207  65  76 158 203 259 317  29
 198 289 320 293 108  95 176 196 305 138  49 226 156  85 219 223]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3520
INFO voc_eval.py: 171: [177 117  50  23 205 214 219 154 203 124 204  18 216 107  24  86 135 163
  13 156 174  22 189 206 152  89  79 202  59 113 150 140 211  12 112 217
 161  49  65  82 210 176 144   9  33 110  31 225 188  72 220  39 136  88
 159 185  94 218  90  92 157 172 195  10 141 221 133  78 125 111 175   2
  35 126  91 119 129  83  25  85  40  84 108 139  87 168  76 200 165  34
 191 215  19  29   6 116 122 209 173  58 143  98  52 231 179  75 123 158
  77 224 109 228 183  99 198 197 131 199  53 212 127 193 137 115 171 227
 121 169 196  74  66 178  17  96 128  11 187 229  43 104  80  45 167 160
  56 106 155 223 162 148  97  68  14 145  16 201 226  55  67 186 102 105
  48 134  26 184  69  36 118  81 130 222  15 180 232  64 120  30 147  46
  95 170 132  38   4  54  93   1 149  57  28  27 230  21 194 213  51  42
   0 164 103 146 138  60  70   5  32  63 192  71   3  20 182   8   7 142
 153  41 208  61  47 207  62 100 181  44  73 101 190 151 114  37 166]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6005
INFO voc_eval.py: 171: [ 60  26  77  47  48  49  65 116   1  74 144 161 146 162  57 163  58 186
   0  35  55 128  92  29  50 145  91  54  72  76  78 179 132 153 178 190
 101  32 137  56 106 124  68  59  23 182 164   3  15 108 165 107   2 135
 129 169   9  62 115  53  75 175 166  66 141  67  37 130  98  85 174 139
 126 133 189  70  18  87 151 156  63  28 147   8 150 176  17 131 160 148
 187 171 140  20  27  97  42 188 154  52 102 113  71 105  14  69 138 117
 136 125  10 180 142  46 159  45  40  11 157  31  22 181 155 172 100  99
  16 168  38  33 177  88  64 134  61  89 110  43  86 152  41 109  94 170
  36 184  39  93 103 104  12   6 158  96 167  82 111 127 120  90 114   4
 143  80  30 118 123  83  51 183  21  25  24  13  81  84 173   7  44 122
 121  79  73   5 112  19 149 119 185  34  95]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2278
INFO voc_eval.py: 171: [313 372   0 314 101   8  48 315  82 254  27 139 350 246  53 452 348 253
 366 113 250 244 397   9 136  81 115 192 443 112  95 252 259 182 450 316
 187  22 393 255 166 228 399  83 373 468  36 294 305 191  96 116 203 391
 396 501  26 364 440 380 302 102 341  30 233 449  37 159 265 256 236 224
 470  60  61 241 389 343 290 388 210 427 148 472 167 320 448 408 338 406
 412 251  28 176 309  17 243 170 459 466 354 234 238 375 205 263 417 261
 476  32 289 146 423 168 258 414 122 392 183 379 308 105 293 267 109 454
 186 424  18 310 400 227  38 163 495 418 188 436 164  35 204 480 377  88
  25 425 155 462 291  40  65  46 483 138 137 285 275 264 347 342 498 178
 482 128 486  84 196 268 361 277 324  64 165 376 326 381 207  39  58 390
 382 463 351 141 257 355 469 214 229  74 121 471 154  54 286 215 180 411
 493   5 409 223 273 213 451 217 235 403 439 465 161 500  41 248  85 435
 179 142 103  51 445 282 222 240 497 104 279  47  89 209 359 171 247 325
  70 304 298  24 485 322  67   2 156 401 107 340 337 385 378 328 143 120
 123 457 297 133 442 150 287  68 117 157 283 367  29  34 299 477 467  62
 336  12 272 288 352 281 175 431  57 206 369 216 149 134  87 374 484 111
 232  19 239 280 447 405 208 358 303 230 416  73 432 296 420 318 434  16
  56 491  71 356 430 231 193   1 269  20  49 494 221 185 218 195 329 394
 453 429 317 438 145  33 226 335 473 118 345 125 242 174  42  11   7 199
 160 456 419  44 407 194 488 460  80  94 444  15  99 363 461 147  93 311
 437  23  55 274 295 413 144  77  66 100 370  52 490   3 260 383 198 421
 108 330  14 197 220 487 190 110  59  86 415 284 114 211 446  90 126 307
 327 426 177 181  31 362  76 276  69 301   6 402 106  91 225 479 474 387
 200 158 410  63 404 384  10 365  79 360 173 184 130  50 151 458 129  92
 135 334 455  78 201 428 266 271 292 478 153  98 189  21 386 496 332 422
 371 357 492 323 132 278 262 245 306 162 152  97 249 475 339 398 219 321
  43  13 481 489 131 237 172 464 212 346  72  45 312 333 499 353   4 349
 119 331 270 433 441 202 169 127 319 300 124 395 344  75 140 368]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3292
INFO voc_eval.py: 171: [457 458 249 463 213 459 257 216 318 117 382 567   3 377 460 212 214 256
 568 200 287 363 133 215 338 362 201   4 378 470 376 383 288 471 536  85
 446 485 379 499 462 530 478 465 179 464  84 403  34 472 273 479 461 492
 508 468 483 386 374  15 219 493 164 191  63  32 497 431 130 260 381 259
 512 411 300 365 547 161 487 267 152 450 504 491 384 466 311 206 394  82
 385 325 141 327 148 315 399  50 244 278 560 186 194 292 414  11 173 387
 400 286 494 408 326 328 283  22  49 221 550 477 263 552  14 534  51 182
 239 317 163 230   9 434 422   6 390 416 109 305 405 203  24   7 554 237
 523 375 573 474 234 373 218  40 406 368 570 352 261 511 335  86 298 129
 331  17 517 127 258 415  96 364  30 553 526   5 395 476  94  33 122 496
 281 440 289 145 442 445  35 193 537 423 217 507 211 435 168 397 162 231
 366 456 303 407 484 369 451 192 101 521 467 473 393 429   8 391 565 348
 532 443 301 528 264 332 347 293 236 282 427 527  88  20 396 222  19  27
  80 189 149 134 266  66  39 150 135  81 420 541 167 205 452 248 329 448
 268 294 284 290 254 510 158 100 224  43  78  21 559 432 417 346 330 343
  77  64 372 566 371   0  59  37 437 520 136 475 307  90 548 334 118 500
 312 538 401 204 156 518 262 370 265 562  65 240 433 171 154 430 323 223
 209 503 202 569 272 539 482  36 245  75 556 320 341  45 124 169  72 297
 170 333  87 413 389 469 425  10 354 225  46  16 176 180  74 157  67 359
 424  71 251 540 505 228 174 544  79  12 299  31 421 574 128 339 246 199
  92 324 227  76 144 409 178 428 242 402 210 380 105  89 274 123 513 151
  54  68 454  56 185 342 271 551  95 453 516  98 197 501 103 275  38 438
 120 279 344 253 571 545 309 576 575 543 558 140 220 138 255 229 557 142
 509 160 524 308 241  18 247  97 183 280  93 350 188 555 449  57 243 340
 489  99 250 515  73 196   2 519 175 398  52 336 546 392 525 563 115  13
 535 439 108 131 561 490 110 269 107 310 139 361 488 357 481 529  69 542
 549 276 143 195 412 112 358 233 351 506 572 285 302 316  62 321 419 404
 198 104 480 146 533 181 190 486  60 531 166  26 277 418 314 306 495 502
  41 121 356 208 111 116 304  48 238  23 125 337  44   1 367 226 522 177
 313 232  61 514 165 296  25 355  28 291 349  91 426 106 153 155 113 132
 441 159 137 345 114 270 353  83 360 444 498 564 455 119  70 319 207 172
 322 187 102 447 252 295  42 235  55  29 436  58 410 388 147  53 126 184
  47]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5583
INFO voc_eval.py: 171: [ 27  14  47  50  74   3  22  55  61 118  13  92 124 122  80  88  77  71
  25  96 100  85  59  33  76 117   8 102 123   2  63  54  89  51 131  58
  78  84  87  99  20  19  72  97  21 111  34  43   6 112  17 120 103  24
  49  23 119   7  81  35  44  93  79   9 110  18 108 125  64  46  75  82
  10 104  28  94 128  98  11 115   0  95  29 129  15 126  70   1  60 127
  40  52  73  67 107  56  57 106  45  36  38 109  16  48  65  68  91  12
 116 114  37   4 101  39 130  53  30 121  69  42  32  31  90  26  66  62
 105   5 132  86  83  41 113]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5756
INFO voc_eval.py: 171: [322 343 144 499 425  75 140 250 467   8 457 167  98 468  54 347 283  16
  22 225 488 448 469 372 203 231 460   1 260 226 245 369 189 447 421 375
 166  97 379 371 126 398 263 481 427 192 431 436  34 281 334 188  86 251
 323 512 291  87  56 473  27 200  95  17 276 259 218 301 253 349 370  50
 314 254 190 191 159 284 400 456  49  33 268 223 308 290 207 351 453 178
 269 358  12 321 432 445 258  24 180  37 324  28  53 155 491 305 148 446
 103 172 401 435  64 420 478 472 184 266 482 306 504 213 388 296 170 101
 261 124 463  60 127 389  71 403 186  32 233 264 108  91 232 506 381  48
 118 502 459 255 439  79 452 182 357  52 433 198 256 116 511 163 221 318
 392 228 216 193 164 316 475 194 486   0  40 201 262 195 272 312 399 477
 141 333 152 413 222 332 273 196 224 265 345 214 289 466 455  43  26 500
 464 267 434 150 330  25  41  30 240 428 107 176  23  13 137  21 175 106
 412 158 104 173 147 210  38 498  35 307 329 293 384 450 507  55 494 278
 458 119  51 419 303 348 480 257  70   4  96 275  62 366  58 335 135 471
  15 227 441 424 300 339 410 368 199  31 495 320 315 505 484 205 454 285
 185  85 297 197 142   7 183 479 279 390 416 406 474 501 177 402 350 395
 215 237 391 336 382 130 444 165  77 378 451 220  45 497 309 235 212  10
 117   6 508  90  83 242 252 443 160 294 123 354 187  73 241 154  80 415
 209 219 174 105 109 236 513 414 367 282 179 359 356 234  72  46  67  20
 271  63 115 319  36 270 440 286 146 394 217  47 405 418 361  76  99 206
  59  18 496   3 100 298 157 169 168 139 331 426 422 202 461 131 363 149
 352 151 156  14 342 438   2 346 492  74 120 274 437 376 121 462  61 487
 122 338 287  44 310 325 238 280 483 337  19 387 112 292 383  65 145 161
 113 295 326 509 442 111 344 248  66 365 114 249 503 429 246 360 110 409
 230 377  42 304  57 490 510 181 138 132 340  84 277  93 133 162 385 313
 136 407 404  82  88 134 204  92 353 476 299  69 211 470 239 449 408 380
 153 128 243 396 125  68  89 417 386  29 311 341 362 327 129 374   9 317
 364 229 171 328 102 288  94 485  39 397 244   5 302 423 208 493 465 489
  11 514 430  81 247 411  78 143 355 393 373]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4338
INFO voc_eval.py: 171: [ 4  2 10 34 11 18 39  3 29  0 23 13 14  9 30  7 37 27 22 38 20 16 21 35
 32  5 40  1 25 36  8 19 15 31 33 26 41 12  6 24 28 17]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0026
INFO voc_eval.py: 171: [1599  373 1606 ... 1403   42 1543]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5050
INFO voc_eval.py: 171: [ 1 26  2 51 40 20 18 12 55 59  3 46 15 19 37 10 16 32 41 57 60 50 49  0
 25 63 34 22 45 29  4 11 48 27 44  9  5 56 43 58 47 30 14 28 42 21 13  8
 52 39  7 23 17 53 54 36 35 62  6 38 33 31 61 24]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2144
INFO voc_eval.py: 171: [362 163  87 155 312 176 233 229 216 232 140  48 319 165  32 127  67  76
  16 334  15 199  33 274  46 323 384 158 364 318 299 138 160 226  90 391
 243 310 273   1 385 244 335 255 332 139 107 325  21  26  75 300 182 268
 295 342 105  88 180 382  23 412  63 251 348  79 237 148 309 133 179 236
 386 405 397  60 204 266 358 320 410 378  20   7 401  61 249 314  42 407
 152  68 338 131  22 326 124 371 383  54 225 353 126 137 275 367 230 198
 119 164  94  13 365 220 390 128 109 393 254 281 162 259 343 336  14 200
 396  10 112 106 291 337 293  38 214 285 136 115 103 267 129 234  17  91
 376 177 135  96 374 282 399 231 248 125 245  57 190  34 227 355 351 349
  70 265 341 194 217 331 404 147 370 379 340 238 375   4 102 327 347 304
 134  47  81  62 292 359 252 223 411  78 301  58 322  31 256 372 306 287
 403 224 241  19 247   8  12  11 143 207 100 153 175   3 350 213  65 303
  82 388 408 110 269 132 328 173 178 389 166 289   6   0 357  93 392  84
  83 145 202 308  56 345 284 373 212 108 187 317 400 222 185  52 195 113
  99 264  59 333 192 369 321  35  45 280  64 283  37 324 114 246  27 191
 151  43 394 297  72 210 240 402 188  95  28 406 117 257  50 149 228 270
  92 172 111  74 218 215 368  29 235 398 278 263  44 189 294 184  97 279
 262 261 288 356 276 157  66  40 311 339 121 101 360 253 181 168 142 296
 196 250  39 298  36 221 352 206 156 146 154 186 197 205 208 387 116 330
 201 329 316 271 258 377 183   5 395  30  80 366  73 120  18 123 104 307
 219  98 167  51 144 161 344 290 315 260 130 242  85 150 239  89 118 169
 302  77  24   2  86  55 409 413 286 346 211 305 209 170 171 174 363 122
  41  49 203 272 361  71  69  25   9 354 141 193 313  53 159 380 277 381]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3692
INFO voc_eval.py: 171: [ 13  90  26  75  27   4  53  47  20   1  52  50  30  80  68  18 100   0
  48  62  99  41  35  98  44 102  28  95 103  22  51  16  81  11  70  93
   2  43 104  92  78  94  40   8  73  69  86  17  21  72  85  89 101  60
  23  67  61  34  54  87  59  58  12  64  66  91  57  19  77   5   3  56
  10  97  24 105  32  38  49  74  55  82  83  96   6   7  33  36   9  42
  37  84  45  14  63  79  65  15  29  46  76  25  88  71  31  39]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0594
INFO voc_eval.py: 171: [64 75 57  8 11 80 78 43 55 52 25 42  9 45 68 54 44 12 65 71 59 58 74 60
 10 77 33 49 72 28 39 19 20 36 14 24 17  6 40  4 76 23 66 47 50 69 18 63
  2 56 31 22 30 41 73 62 27  3  1 61 48 35  0 29  5  7 37 46 21 81 26 38
 32 79 67 34 70 82 16 13 15 53 51]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3804
INFO voc_eval.py: 171: [ 33 245  26 154 107 218 160  37 257  16   9 188 180 114 161   8 174  87
 249  34 178  53 176  99  98 232 208 211  60  48 155 166  73 138  97  76
 141 118 121 262 179  91 140 101  17 231 163 149 214  47  61 100  64 192
 147 124 109 253 128   6  67 187  71  65 217 182 170  45 120  18  23   1
  42 173 196 191 216 203 205  22  83 131 255 181  28 133  86 225  69 233
 193  95  44 238  30 189  39 258 158 122 115  78 201 264 127 210 250  29
   2 213 175 224 219  38  59  32  93 148 116 236 125 265 117  54 222  40
 223 145 240 230  75 167 112  56  82  41 135  10  49  14 172 105 184 137
 123  68 164 171  43  89 209  74  35  63 130  46 266  94 221 186 244 259
 226 248  27  55  20 162  84 183 212 177 235 200 153 243 206  92 142 156
 129  52 241 256  15 146 102  57 104 111  77 132  31  81   0 136 165 144
 261 113 139 229 252  21 152 106 195  25 254  62 126 220 197 151 168  72
  19 202 194 169  96 227 204 119 263 228 143 246 207  51 237 242 159  70
   3 260  13  50  11   7  12 157 234 198  88 185  79  80 190 103 239  36
  66 247 110 150   5  90  24 108  58 199 134  85   4 251 215]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7561
INFO voc_eval.py: 171: [2067 1206 2082 ...  739  576 2006]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6146
INFO voc_eval.py: 171: [160 136 231 150 484 790 171 262 166 243 734   7  79 328 341 791 667  46
  82   6 751  80 475 100 423 516 244 698 463 447 639  81 735  85 263 527
 541  84 631 299 588 268 792 829 186 289 245 579 208 486 179 733 794 185
 403 542  63 303 638 637 187  62 702 716 158 793 731 696 806 530 300  91
 302 135  51 641 241 264 373 804 252  33 811  90  86 146 636 647 205 546
  99 358 393 155 394 170 741 549 587 669 381 462 801 344 425 739  83 724
 830 640 426 551 517 246 470 377 772 458 105   8 543 102 736 132 320 809
 315 225 454 496 606 721 513 740 668 339 722 535 642 652 713 581 487 424
 704 544 396 157 346 646 676 251 464 477 710 849 572  40 111 451 375 140
 427 743 856 507 399 814 773 453 550  15 545 510 466 467  68 271 828 460
 395 845 648 799 410 197 144 408 139 532 154 585 193 162 574 167 478 342
 534 441 552 614 188 295 290 672 555 726 748 693 831  12 761  35 305 267
 813  66 236 675 471 848 206 582 515 114 124 108 816 649 257 347 557  65
 725  53 802 778 371 183 359  34   9 189 554 488 677 569 745 688 508 332
 571 200 254 595 810 164 450 147 674 850 558  93 121  74 190  88 287 249
 194 812 573  94 651 240 624 314  28 319 270 491 599  44  70 833  76 505
 165   4 565 661 361 841 805 561 737 673 644 350  31 292 514 509 597 705
 209 742 461   0  16 248  95 798 632 437 415  57 689 392 707 738 253 728
 364 304 699 512 367 104 293 404  60 432 198 560  89 247 659  14 405 138
 788 269 832 313 645 448 260 116 301  96 443 316 238  41 502 389 760 159
 311 348 503 433 746 273 767 294 547 444 378 199 110 625 538 665 824 771
 522 768 593 497   2 457 775 855 474 592  47 780 839 681 218 402  17 854
 754 817 835 220 345 411 149 217 600 207 163 382 635 482 663 706 539 137
 442 398 224 653 126 596 265  56 795 506 584 662 643 473 446 360 195 493
 752 671  58  29 749 338 455 732 317 520 797 406 589 786 727 334 629 321
  87 228 692 204 690 385 536 239  55 481 407 718 756 490 658  71 501 708
  13 701  10 499 483 789 562  21 133 840  52 594 255 131 153 604 130 796
 256 782 691 492 762 309  26 176  50  20 843 266 422 125  64 826 753 650
 857 330 445 472 416 703 366 449 327 524  19 583 479 620 261 633 379 351
 764 533 323 770 288 695 808 386  27 851 109 429 586 434 616 744 803 318
 755 337 807 844 785 412 142 357 678 715 331  69 570 500 540 468 553 113
 417 683 349 627 221 430 308 578 219  43 129 852  25 420 603 103 566 152
 836  67  38 233 352 521 580 456 567 230 286  77 679 118 335 370 400 697
 821 122 800 283 609 656 622 151 834 757 324 107 222 787 343 128 363 237
  73 774 383 684  42 498  98 180 568  54  59 182 613 234 485 250 419 259
 602 436  75 519 607 177 214 120 329 202 145  18 634 340 846 765  30 212
 169 575 537 709 657 630 210 365 820 459 837 277 730 148 362 720 842 215
 619 372  23 310 529 112 626 819 211 275 548 306  24 387 476 559 818 369
 196 494 465 576 694 711 223  49 307 528 654  48 598 670 376  78 615 390
 610 618  37 719 664 601 380 435 680 838 777 518 563 192 776 526 272 326
 431 480 298  61 700   1  22 216 469 847 282 489 174 178 229 617 101 312
 213 440 391 815 523 297 682 374 191 354  32 201 418   3 759 123 438 781
 181  36 612   5 511 291 285 232  97 168 414 783 274 428 495 766 203 784
  39 242 384 439  92 714 825 281 325 280 333 822 729 175 623 605 779 356
 556 413 173 686 655 409 134 278 276 531 769 666 143 119 723 717 226 161
 227 591 368 397 827 504  11 388 763 184 712 525 853 660 758 590 750 564
 106 608 336 685 421 279 628  45  72 401 141 127 452 611 355 747 353 621
 823 115 322 296 235 577 156 687 172 117 284 258]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5888
INFO voc_eval.py: 171: [47 28 46  0  2 39  7 41 44 43 59 67 61 60 25 27  9 17  3 49 32 11 22 48
 30 54 55  8  4 21 29 40 14 18 34 15 58 57 56 19 42 31 52 51 62 33 65 63
 38 13 50  6 10 45 23 36 12 64 26 53 16 20  1 37 66 35  5 24 68]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3051
INFO voc_eval.py: 171: [ 59  60  22  93 144  58  32 109  92   3  68 103  88  26  30  66  62  50
 132 128   4  79  89 115   9 136  74  47  44  25  28  43 100 120  76  24
  53  99  37  16  97 147  38  81 135  90  21  40  18  95  15  54  72 146
  41  10 142 137 131  83  63 139  98  51  67 108 141  34  87 130  31 133
 121  73  36  78  64 129  91  86 111   1  48 126  52  35  80  57  29   0
  13   7 101 116  11   2 102 122  70  94  42  39  85 143   5  75  69 145
 105  46   6  71  77 124  23 119  27  96 125  20  84 114 117 113  65  14
  61   8 140  82 123  56  19 127 107  33  45 106 112 138 104  55 118 134
  12  49  17 110]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3440
INFO voc_eval.py: 171: [108  17  68  38  14  56 140 137  27  78  92  59  35  13  80  37  74 149
 152   7 144 122  79  57  15  12  91  36  61  55  65  22  71 125  99  41
  19  53  16  95  90  84 102  31  60  72 159 119 154 101 120 118  39  25
 131  52 145 123  33  82 142 106  85  48  29  46  30 114  43 130 105 116
  73  96 141  64  94   6 160   5  62  40 136  69  89 124 100  70 104 128
  86 156  76   1  34  10   2 148  28 134 135  47  26  81 115   9 146  18
  11 129 111  24  66  44  88  98  97 109 110  83 103  50 150 155  67 138
  54   8  93 121  75   3 107  23  21 151  42 143 157  20 153   4 147 139
  32 133 161 126 112   0  77 162 117  87  63  58  51 127 113 158  49  45
 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.4023
INFO voc_eval.py: 171: [ 87 147 217 233 207 321 294   3  25 176 281 134 316  18 167 234  56  82
  37  21 300 239 177 118  71  40 270 183 215 219  84 229 193  75  11 117
 221 327  51 241  55 323 208 226 206 240  54 109   1  39 130 179 119 285
  59 235 224 250 152 331  64 258  57 336 205 180 301 127 248  44 161 102
 251 332 271 279  34 255 302  81 287 252 337 311  22 225 157 123 103 318
 303  53 257  83  24 256 284 160  76 314  85 142  60 192 236 268 342 276
  47 113 254 230 182  94 116 171  23 272  52 141  73   4 244  15 317 151
 159 312 249 260  70 238   8  63 188 202 291 144 143 181 106 334 133 313
 335  30 136 204  72 174  32 308 189  50  62  89  16 210 153 138 299 305
  90 212 121 273  92 211  27  69 128 329  86 107 263 293 259 203 187 114
  42 190  98  45  19 122 178 200  78  26 282 274  58 129 338 228 164 170
 304   7 290  93 296 135 101 315 173 275 269  38 286 223 194 100 310  74
 232 298  13  43 162 185 149   6 266  10 289   9 150 120 111  79 126 253
 191 319 169 341 216 195 333 340 155 280 322 324  17 196 140 262  41 199
  67 137 214 166 227 265 197   5 139  77 245 288  66 168 247 105 172 115
  28 148   2 295 246 165 309 125 132 220  14 267 186 110  68 112 156 326
 328  61 146 297 231 184 283 242 163 237  99  12 261 325 343  96  31 209
 292 175  65 154  20  48 124  80  91  46  36 108   0 198 222 243  35 320
 158 213  49 201 339  88 306  29  33 104  95 131 307 264 277 330  97 145
 218 278]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4571
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4038
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.600
INFO cross_voc_dataset_evaluator.py: 134: 0.228
INFO cross_voc_dataset_evaluator.py: 134: 0.329
INFO cross_voc_dataset_evaluator.py: 134: 0.558
INFO cross_voc_dataset_evaluator.py: 134: 0.576
INFO cross_voc_dataset_evaluator.py: 134: 0.434
INFO cross_voc_dataset_evaluator.py: 134: 0.003
INFO cross_voc_dataset_evaluator.py: 134: 0.505
INFO cross_voc_dataset_evaluator.py: 134: 0.214
INFO cross_voc_dataset_evaluator.py: 134: 0.369
INFO cross_voc_dataset_evaluator.py: 134: 0.059
INFO cross_voc_dataset_evaluator.py: 134: 0.380
INFO cross_voc_dataset_evaluator.py: 134: 0.756
INFO cross_voc_dataset_evaluator.py: 134: 0.615
INFO cross_voc_dataset_evaluator.py: 134: 0.589
INFO cross_voc_dataset_evaluator.py: 134: 0.305
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.402
INFO cross_voc_dataset_evaluator.py: 134: 0.457
INFO cross_voc_dataset_evaluator.py: 135: 0.404
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 22499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.327s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.344s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.356s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.357s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.360s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.362s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.360s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.362s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.362s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.361s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.361s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.364s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.366s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.609s + 0.002s (eta: 0:01:15)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.397s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.380s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.362s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.367s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.371s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.377s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.373s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.368s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.367s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.366s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.368s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.368s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.365s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.338s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.343s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.360s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.358s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.355s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.354s + 0.001s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.353s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.352s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.351s + 0.001s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.352s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.354s + 0.001s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.356s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step22499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.499s + 0.002s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.392s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.377s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.377s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.372s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.365s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.369s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.368s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.368s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.365s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.360s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.362s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.361s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.321s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [225 176  92   5 280  60 215  18  47  52  53 144 122  51 286 167 135 171
 143  46 306 120 178   1 298 177  48 307 293 252 263  38 149 256  20 103
  83 114  71 130 234 203 125 185 134 117 265 148  81 150  30 275  86 285
  31 222  80 239 241 111 145  50 304  56  84 156 291 168 126 310 270 205
 146 102 140  37  97  67 236  62 257 262 281 131 267 219  72 210 200 297
  11 226 259  54   3 129  70  93 161 162 198  39   2  78 136 278 305 110
  58 105  17 174  35 299  10 199 235 223  21 301  57   6 170  43 158 314
  73 132 247 107  22 243 184 264 283 164 277 250 309  27  65 153 211 212
 224 269 214 271 173 249  45 104 101  98 124 195 206  42 213  88 217 238
 182 155   4 311 160  87 273 229 294 159 151 141 272 180 191  26 292  32
  34 287  33 193 244 208 166 163  14 313  90 228  24 123 116  44 260 218
  55 133 201 295 227  74 112 296  89  25  64 282 242 115 240 231 121 266
 251  13  15 248  96 221 237  69 246  61  91  82 276 154   9  23 169 127
 179  19 186 303 316 183  41 268 147   7 207  85  63 142 187 119 209  12
  99 108 230 302 192 172 289 279 220  36   8  79  94 274 232 255 188   0
 290  16  76  75 152  28 197 233 118 202  59 137 308 258  66  68 190 128
 181 245 165 189  40  29 254 138 261 113 253  77 100 106  95 157 196 204
 312 300 194 109 288 175  49 284 315 139 216]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3510
INFO voc_eval.py: 171: [174 114  49  23 202 211 216 151 200 121 201  18 213 105  24  85 132  13
 160 154 170  22 186 203 149  88  78 199  58 111 147 137  12 208 214 158
  48  64  81 207 172 141   9 108  33  31 185 222  38  71 217 133  87 182
 157  93 215  89  91 155 168 192 138  10 110 218 130  77 122 109  35 171
   2 126 116 123  90  25  82 153  84  83  39 106 136  86  34 197 162 188
 164  76 212  19 156  29   6 112 119  97 169  57 140 206  51 228 176 120
  74 107  75 221  52 195 225  98 180 167 194 209 190 124 113 128  65 196
 134 165 224 118  73 175 193  17  95 125  47 103  44 184  42  11 226 220
 163  55 152  67  14 159 145  16 142 104 223  66  96 101 183 198  54 115
  26 131  68  80 181  36 219  63 144 117 229  53  15  94 127 173   4 177
  45  30 129  37  70  92  41 166 146   1  28  27  79 227  56 210 191  21
  50 102   0 135 161   5 143  59  69  62  20   3  32  60 205 179  46   8
 189 150  40  43   7 139 148 204  61  99 178 187 100  72]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6035
INFO voc_eval.py: 171: [ 61  26  79  48  49  66  50 119   1  76 147 164 149  58 165  59 166 189
   0  36  56 131  30  51  93 148  94  55  74  78  80 182 135 156 193 181
 102  33  57 140 108  60  23  69 127 185   3 167  15 107   2 109 138 168
 132  54 118  63   9 172  77 178  67 169 144  38  99 133  68  87 142 177
 129 136 120 192  71  89  18 154  64 159  28 150   8 153  17 179 190 151
 143  27 134  20 163 174  98  43 157  53 191 116 103  70 141 106  14  73
  46  47 139  11 145  10  41 158 100  16  32 184  90 101  39 160  22  34
 171  91 176  65 137  62 113 128 180 162  88 183  44 155 111  40  42  37
  95 173 104 187  96  12 105 161  84   6  97 114 130   4 170  92 146  31
 123  52 121  82  85 117  19 126 186  21  13  24  25 110  86 175  45  83
   7  75 124  72 152 115 125  81 112   5 122 188  29  35]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2275
INFO voc_eval.py: 171: [308 365   0 309  98   7  47 310  79 248  26 343 135 240  52 446 342 247
 360 110 389 238 244 132   8  78 112 186 436 109  92 176 253 246 443 311
  21 181 386 249 161 391 222 366 461  80  35 289 300 185  93 197 113 384
 388 358 494  25 373 297 433  99 335 227 442  28  36 154 218 230 260 250
 463  59 382 285 205  60 235 381 337 420 465 143 162 315 332 441 400 398
 404  27 245  16 170 237 304 164 452 233 349 459 228 368 199 410 257  61
 469 255 283  30 416 141 163 406 252 118 177 385 372 303 102 261 288 106
 180 447 417  17 393 305 411 221 158 429 159 473 198 488  85 370  37 182
  24 150 418  34  39 286 455 133  63 476 280  45 134 270 341 258 190 172
 124 475 479 336 491 319 263 369 321 272 160 355 374  57 346 375 201 136
 251 383 456 208 350  38 462 223  72 117 149 464 403  84 282 174 209 207
 401 217   4 486 211 229 268 395 432 444 458 493 137  81 100 428  40 234
 156  51 242 101 173  46 216 276 490 440 274 299  86 165 339 353 203  23
 292 320   2  33 151  68  65 317 478 334 378 241 394 449 371 104  22  66
 323 435 119 361  11 128 138 281  62 278 116 145 114 152 345 470 460  53
 169  32  56 294 331 267 200 130 367 284 144 347 424 108 277 275 202 226
 477 232  83 439 210 362 437 291 425   1 313  18  69 224 298  15 397 413
  55 351 484 409  71  48 225  19 423 212 487 187 264 179 324 140 215 387
 189 445 312 466 427 344 422  31 236  10 330 193 220  91  77 399 155 115
 188 450   6  41  96 168 142 431  14 481 412 453  54 430 357 121 269  43
  74 405 306 438 139 204  90 454  97   3 290 483  64 363 254 191 192 376
  13 105 480 414 325 107 184 122 302 408  58 214  82  87 279 111  67 356
  29  73 322 175 171   5 419 271 153 472 467 103 296 194 126 359 402 354
 178 377 380 396 219   9  88  76 167 125 147 471 131 266  89 329  50 448
  49 262 287 293  20 327 451 146 421 489  95 318  75 148 259 364 485 129
 415 183 239 256 379 195 273 231  94 352 243 157 333 213  12 390  42 434
 474 314 206 127 301 316  44 328 340 457 166 348  70 265 196 338 482 468
 392 326 426 307 295 123 120 492 407]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3295
INFO voc_eval.py: 171: [454 455 246 460 210 456 254 213 313 115 378 565 373   3 457 209 211 566
 253 198 283 359 131 334 358 212 199 374   4 467 372 379 284 468 534  83
 483 375 443 459 499 528 475 462 177 461  82 399 501  33 469 270 476 458
 487 465 506 480 382  15 370 216 489 162  61 189 493  31 429 128 257 377
 256 510 406 361 296 545 159 447 264 150 380 500 463 488 306 204  80 381
 390 320 139 322 146 395  49 310 241 184 558 192 275 287 409 171 383 282
 490 396 404  11 321  48 324 280 218  22 474 548 261  50 550 236 161 180
 226 312   9 532 431 418 385 411 106   6  24 401 301 201 233 522 552 471
 215 371   7 231 571 402 364  39 568 369 258 349 330 508  14 127 294 514
  84  17 326 255 125 410 360  94 551  29 392 524   5 120 492 473  32  92
 278 437 442 439 285  34 143 191 504 214 419 535 393 432 228 208 166 362
 160 299 365 448 464 481 403 470 453 190 519 425  99 389 387 563   8 440
 344 260  86 423 297  20 289  44 530 526 343 327 391 234 279 219 525  19
  27 187  78 132 147 263  64  38 133 148  79 245 165 507 203 416 449 539
 290 251 445 323 415 281 221 265 286  76  21 428 156 557 342 412  75 472
 339  62 368 367 325  36 517  88 564   0 134 434 303 496  57 329 116 546
 516 536 366 202 307 259 152 304 154 397  63 479 262 206 430 317 560 567
 220 498 237 426 200 242 169 269 315  45 537 337  35 554  10 248  98 386
  85 421  70  69  73 123  46 293 408 466  65 351  72 174  30 420 168 222
 225 155 502  16 538 542 355  77  13 295 572 335 417 243 172  90 178 126
 319 224 239 405 398 142 424 197  74 376 176  52  87 121 207 104  54 509
  66 271 149 549 338 183 451 515 268 250 118 435 450  93 101 497  37 272
 195 574 505 543 276 157 340 167 573 569 556 217 140  18 521 541 138 252
 555 136 305 181 227 277 244 186 485 346  95  55 247  97  91 553 173 446
 523  71 336 518 388 394 513 240 113 561 331  51 436 544  12 129   2 194
 486 108 478 484 266 137 328 141 559 357  96 311 316  67 527 354 353 407
 105 107 273 193 503 414 570 230 333 347 540 348 477  40 482 158 298 547
 110 533 144 179  60 188 102 400 491 196 164  26  41 531 495 205 274 119
  58 413 529   1 363 223 300 441 122 309  89  59 151 235 175 109 163 520
  43 114 302 494 345 511 308 238 314 352 292 422  25 229 438 332 103  23
  28 341 356 153  42 135  81 350 111 288 185 452 112 291 117 562  68  56
 384 170 130 267 182 145  47 100 232 512 318  53 249 433 427 124 444]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5582
INFO voc_eval.py: 171: [ 29  14  49  52  76   3  23  57  63 119  13  93 124 126  82  89  73  79
  27  97  87 101  61  35  78 118  65   8 125 103   2  80  56  86  60  53
 133  88  90 100  74  20  19  98  22 112  45  36 113   6  17 111  24 122
 104  51  25 121  83   7  46  37  84  94   9  81  18 127 109  66  30  48
 105  10  11  77 130  95 116   0  99  96  15 131  31  72 128   1 129  42
  62  54  75  69  21 108 107  58  59  12  38  47  40  16  50 110 120  92
  70 115 117 123  67 102   4  39 132  32  28  44  41  55  71  34  33  91
  64  68   5 106 134  85  43  26 114]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5705
INFO voc_eval.py: 171: [324 345 144 501 427  76 141 251   8 469 459  99 168  53 470 349 284  16
  22 490 450 222 471 374 203 233   1 462 261 227 247 371 189 449 423 377
 167  98 381 373 127 264 400 192 483 429 432  34 439 188 282  87 336 252
 515 325 292  88  27  55  96 475 200  17 277 260 303 254 218 226  49 316
 372 190 191 255 351 160 285 402 458  33  48 269 224 310 207 353 291 180
 179 270 323  12 455 360  24 433 258 447 326  37  28 307 156  52 149 493
 448  60 403 480 103 172 437  63 474 267 422 184 484 506 308 297 171 390
 102 213 262 125 465 314 128 391  72 235 186 405  32 265 109  92 508 234
 383  47 256 504  80 461 182 119  51 442 359 434 454 257 198 117 164 514
 394 221 229 477 318 193 320 216 194 165 488  39 263 195   0 201 273 401
 142 333 334 152 223 479 415 274 196 468 225 290 266 457 347 214  26  42
 502 435 466 151 268 332  40 242  30  23  25 176  13 430 107 106 175  21
 138 104 173 159 148 210 452 414 500  35 294 331 386 309  54  38 496 460
  50 120 305  97 350 421 259 279  71  61   4 482 337 136 368 276 426  15
 473  57 286 280 413 228  31 443 317 370 301 486 322 507 341 199  85 197
 503 185 456 143 183 298 497 205   7 476 481 392 239 178 404 408 418 509
 397 215 352 166 338 393 384 380  78 237 220 212  91 131 453  44 311  10
 499   6 446  84 118 244 445 511 253  70 356 295 161  74 243 358 238 157
 124 516 105 283 110 174 236 209 219  73 328 416 417 361 369  45 187  36
 436 272 335  62  20  67 116 271 287 217 321 155 428 396 147  46  18   3
 177 170 108 100 101 169 140  58 420 424 158 463 132 354 407 299 363 202
 206 150 365 344  77  75 122  14 348   2 146 494  59 288 498 441 489 153
 440 121 123 275 340 327 240 293  19 485  43 312 302 510 346 385 378 339
 389 431  65 112  64 115 114 464 296 162 505  66 113 281 367 250 512 411
 248 513  41 111 342  86  94 181  56 134 133 472 379 232 249  89 444 278
 306 387 137 409 492 362 139 315 478 231  83 135  93 406 163 300 355  69
 154 398 211 129 126 410 419  68 382 451   9 130 241 245 204 343 376 313
  81 319 491 364 329 388 425  90 330  29 366 289   5 357 145 230 246 467
 395 399  95 495  82 375 304 208  79  11 487 412 438]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4130
INFO voc_eval.py: 171: [ 4  2 10 34 11 17 39  3 28  0 22 13 14  9 30  7 21 26 37 19 16 38 20 35
 32  5 40  1 24 36 18  8 15 31 25 41 33 12  6 23 29 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0025
INFO voc_eval.py: 171: [1592  375 1593 ... 1488  311 1197]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.4970
INFO voc_eval.py: 171: [25  1  2 51 39 20 19 12 55 59  3 46 15 18 36 10 40 31 50 57 60 49  0 63
 33 22 24 45 11 28  4 48 26 44 17  9  5 56 58 42 29 47 14 27 41 13 21  8
 52  7 38 23 53 35 34 16 62 54  6 37 32 30 61 43]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1682
INFO voc_eval.py: 171: [359 161  86 154 310 174 233 227 214 232 138  48 316 163  32 126  66  75
 331  16  15 197  33 272  46 320 379 157 361 315 296 158 137 224 307  89
 242 271   1 380 386 332 253 329 241 230 322 107  21  26  74 180 297 266
 292  87 139 377 339  23 105 178 249  63 345  78 147 306 132 236  60 264
 202 355 381 399 404 317  20 375 395  61   7 247 311  42  68 123 140 401
 231 323 151 335  22 130 223 368 378  54 350 125 136 273 228 196 364  94
 162 362 118 222  13 385 388 127 160 406 391 257 252 333  14 340 109  10
 278 198 111 288 103 106 212 114  38 290 282 334 135  17 128  90 265 134
 371 393 373 279 175  96 229 124 246 188 225 348  57  34 352 263 336 346
  70 192 215 337 328 376 398 372 146 392 367 102 133 344 324   4 301 220
  80  77 289  47 298 356  58  31 319  62 254 250 405 369 293 303 284 221
 235   8 245  19 397  92 239  11  12 142 100 152 176 173 365 205   3 389
  65 383 347 267 300 211 171  81 131 402   0 110 164   6 354 325 387 309
 177  82  83 286 305  93 384 370  52 185 183 144 342  56 200 394 210 219
  45 108 281 112 314 366 318 193  35 186 190  99  59  43 277 244  37 189
 330 321 113  64  27 150 280 255 208 145 116 396 217  28  71 226 238 213
  50  29  73 400 170 261  95 260 291 187 259 148  91 268 275 234  44 285
 156  97  67 390 276 182 120 357 101 251 166 262  40 308 353 248  36 179
 141 349 194 382 203 195 295  39 204 218 184 327 199 155 294 206 153 115
 181 374 326 256 313  79   5  18  30  72 104 243 363 274 269  98 122 165
 119 304 129  51 143 168 299 341 209 117 240 287 216 149 343 207  88 159
 167  84 169 258  49 172  24 302 403 312  41  55 407 360 283 237  69   2
 358   9 201  25  76 121  85 351 270 191 338  53]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3696
INFO voc_eval.py: 171: [ 13  89  26  74  27  52   4  46   1  20  51  49  30  79  67  18  98   0
  47  61  40  97  34  96  43 100  28 101  22  94  50  16  80  11  69  92
   2  42 102  93  91  77  39  72   8  68  85  17  21  84  71  88  59  23
  66  60  53  99  86  58  33  57  12  63  90  65  56   3  19   5  10  55
  76  95  24 103  31  37  48  81  82  73  54   6   7  35  32   9  41  36
  44  14  78  62  15  29  83  45  64  25  75  87  70  38]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0595
INFO voc_eval.py: 171: [64 75 57  8 11 80 78 43 55 52 25 42  9 45 68 54 44 12 65 71 59 58 60 74
 10 77 33 49 72 39 28 19 20 36 14 24 17  6 76 40 23  4 66 47  3 69 50 63
 18 56 62 31 22 41 30 73 27  2 35  1 61 48  0  5  7 29 37 46 81 26 38 67
 21 32 79 34 70 82 53 13 16 51 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3810
INFO voc_eval.py: 171: [ 31 243  23 151 105 216 158  35 255  16   9 186 178 111 159   8 172  85
 248 176  32  51 174  97  96 206 230 209  46  58 152 164  71 136  95  74
 138 115 118 177 260  89 137 229  17 161  45 212 146  98  62  59 144 107
 121 190 251   6 125 185  69  63 101  65 215 180 117  43  18 168  22  40
   1 171 189 194  21 214 201 203 253  81 179 128  26 223  84  67 130  93
 231 191 236  42  28 187  37 256 156 199  76 112 249 119 262 208 124  27
   2 211 217 173 222  30  57  36  91 113 145 220 122 263 234  52 114  38
 142 221 238 228  54  73  39  10 165  80 109  47 132 103  14 170 134 182
  87 162  66 169  41  72 207  33 120 127  61  44  92 160 264  53 184 242
 175 224  25 219 181 210 135 241 198 257 233  82  20 204 247 150  90 139
 254 126 154  50  15 239 143 102  99 123  75  55  29  79 129 110 141  24
   0 163 133 250 104 259 149 227 252  60 218 195 193 200 148 166  70 153
 225 192 116 167  19 157 261  94 226  68 205   3 140 202 244 240 258 235
  11  48 232  13   7 188  78 196 155  77 147 183  64 100 237 108   5  34
 246  12  86  56  49  88  83 197 106 245 131 213   4]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7560
INFO voc_eval.py: 171: [2083 2100 1214 ... 2084 1340  326]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6154
INFO voc_eval.py: 171: [138 161 233 152 491 797 173 263 168 245   7 741 331  80 347 798 676  46
  83   6  81 757 482 101 429 524 246 706 469 453 647  82 742 535  86 264
 550  85 302 639 269 596 189 799 292 837 247 587 211 493 182 801 740 187
 645 551 409 306  63 646 190 724 159 710 800  62 738 704 814 303 538  92
 305 137  51 649 242 265 379 254 172 811  33  91 818  87 148 644 655 207
 555 364 157 400 102 399 748 558 678 387 595 468 809 746 350 431 839  84
 648 432 731 561 525 248 381 476 779 552   8 464 107 323 104 743 816 134
 319 227 460 614 728 503 677 520 747 342 729 543 650 660 721 553 494 589
 430 402 158 712 352 686 654 253 484 857 470 718 580 382 113 433  40 457
 142 514 750 405 864 821 780 554 559  15 472 459 517 473  69 836  35 273
 807 401 466 141 199 656 415 146 854 414 156 685 163 169 540 593 560 582
 348 298 485 622  75 191 447 293 542 564 840 701 733  67 681 755  12 268
 768 116 823 308 208 522 820 238 334 108 684 259 126 590 566 785 477 658
 353 377 856 810 365 732  53 185  66 563 192 495  34   9 576 696 752 203
 603 165 456 567 515 817 149 579 256 123 683  94 858 193 290 819 196  89
 581 251  95 632  76 318  28 607 497 271 659 243 513 322 842   4  78 166
 573 682 670 812 367 570  45 744 605 850 295 652 521 356  32 210 749 516
   0 467 443 250 697  96  16 713 105 745  57 640 735 805 421 255 398 296
 370 307 438 201 410 569 715 374  90 667 519  60 249 139  41 270 795 653
  14 454 261 317 707 841 118 411 304  97 320 449 395 830 354 512 315 240
 753 160 547 275 633 384 767 831 450 112 530 557 439 202 510 674 297 774
 775 463 272 863 481 504   2 778 601 782  47 220 690 408 787 848 760  17
 388 548 844 824 218 643 222 417 672 604 600 226 266 404 164 608 661 151
 714 862 351 489 448 128  56 802 651 671 140 592 366  59 499 501 680 759
 480 756 209 197 321 461 739 452 343 838  29 324 412 793 528  88 700 804
 597 544 488 734 699 230 666 206 391 637 709 490  54 764 509  65 726 507
 413 135 344 498 313 803 133 241 337 132 257 155 716 258 849 500  50 602
  52  21 796 769 612 267 533  13 698 127  26 333 428 835  19 628 852  10
 668  64 789 178 478 657 865 455 486 815 372 422 330 702 771 357 591 326
 111 594 262  20 711 451 291  27 435 859 440 392 687 641 777 624 549 562
 578 418 751 541 762 276 363 813 309 585  70 508 355 336 792 723  68 436
 474 312  71 806 131 853 144 235 426 114 106 120 529 339 124 462 611  73
 692 221 860 223 577 340  25 635 574 423 845 232 828 376 406 705 153 154
  38 664 327 629 289 763 109 688 188  44 286 588 575 642 100 808 130 843
 492 181 781 349 621 794 505 527 425 184 239 260  55 369  74 332 442 122
  42 224 236  58 610  18 214 389 615 617 204 855 252 583 212 213 171 717
 665 147 827  30 179 546 627 737 345 772 150 638 846 371 314 465 216 545
 280  24 393 537 198  23  77 851 310 115 727 471 378 368 217 606 634 375
 311 826 396 536 609 556 703  61 383 784  79 679  48 825 568 584 483  49
 618 274 719 526 502 689 225  37 761 534 623 194 441 662 571 437 626 301
  22 847 195 673 103 386 708 420 316 397   1 329 475 446 783 822   3 219
 496 691 200 300 234 176 231 360 170 625 487 385   5 285  31 531 380 288
 766 125 277 518 183 180 244  36 444 294 788  98 434 722 205 790 620 424
 328 215 362 338 791 773 613  93 419  39 479 834 833 346 829 511 284 162
 631 445 565 121 786 675 175 693 539 390 736 279 177 776 283 506 281 229
  43 136 725 720 335 228 358 373 341 616 237 770 669 663 403 416 532 730
 599 186 758 394 861  11 572 325  72 282 359 110 636 832 119 427 361 765
 145 174 117 458 299 619 129 754 598 695 407 143 287 523 694  99 630 167
 278 586]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5865
INFO voc_eval.py: 171: [48 29 47  0  2 40  8 42 45 44 60 67 62 61 28 26 10 18  3 50 55 33 12 20
 49 31 56  4  9 23 30 41 15 19 35 16 59 57 58 43 21 32 52 53 65 34 64 39
 14 17  7 51 11 46 37 24 13 63 27 54  1 22 38 66 36  5 25 68  6]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3051
INFO voc_eval.py: 171: [ 58  59  21  92 143  57  31  91 108  49   3  67 102  87  26  29  65  61
 131   4 127  78  88 114   9  73 136  43  46  27  24  42  99 119  75  23
  36  98  16  96 146  37 140  80 134  89  20  53  17   0  15  39  94 145
  40  71  10 130 141  82 138  97  50  63  66  62  86  33 120 129 132  77
  30  72  35  90 128 107 110   1  51  56  79  34 125  13  47   7  28 100
 101 135 121   2  11 115  41  52  69  38 142  93   5  68  85  74  84 104
 144  45   6 123  70  76  19  25 118 124  95  22 112  83  60  64  81 116
 113  14  32 122   8 139  55  18 137 126 106 111 133 117 105  44  54  48
  12 103 109]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3520
INFO voc_eval.py: 171: [107  17  66  38  14  54 137 134  27  77  57  91  35  13 108  79  37  73
 146 149   7 141 120  78  55  12  90  15  36  59  53  22  63  70 123  98
  41  19  51  16  94  89 101  83  31 156  71 117 151 118 100 116 105  58
  25 128  50  39  81 142  33 139 121  29  48  30  46  84 112  43 104 127
  72  95 114 138   6  93   5  88  60  62 157  67  40  68  99 122 103  85
  75   1 125 153  34   9 133 145   2  11  28 113 132  47 131  26  18  80
   8 110 143  24 126  64  44  97  87  96 135 152 109  52  49 102  74 119
 147   3  92  82  21 106 140  23  42 148  65  20   4 154 136  32 130 144
 150  10 124 158 111 159  76 115  86  61  56   0  69 155  45 129]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3954
INFO voc_eval.py: 171: [ 87 146 218 233 207 295 323   3  25 176 282 134 318  18 167 234  56  82
  37  22 302 239 177 117 241  71  40 183 271  84 216 219 193 229  75  11
 116 221  51 329  55 208 325 226 206 240  54   1  39 108 130 179 118 286
  59 235 151 224  57 251 333  64 259 101 338 180 205 126 303  44 249 161
 252 334 272 280  34 256 304  81 253 339 313  21 122 156 225 102 320  53
 305 258  83 285  24 257 160 316  60  76  85 142 192 236 288 269 278 344
 112  47 230 255  52 182  23  94 274 115   4 141 170 150  73 314 319  15
 250 261 158  63  70   8 238 188 181 143 337 202 144 292 105 133 336  30
 315 210 204  32  50  72  62 310 136  89 152  16 212 174 189 128 138 307
 301  90 275 120 211  92  27 331 113  69 294 127  86 106 203 264 260 178
  45  19 164 187  26  42 283 121 190  97 276  78   7 200 340  93 306 228
 129  58 100 291 135 297 277 244  38 173 270 317 287  99 223 171 299 162
 232  74 185 194 312  13   9 119   5  43 267 125  10 110 254  79 148 290
 149 191 321 343 342 217 154 335 196  17 169 195 324 326 263 266 137 140
  41 281 197 273 199 215 166 300   6 246 227  66 139 168 289 172 147  77
 104  28 248   2 114 247  61 124 165 296 268 311 220 186  98 330  68 163
 109 155 328  67 243 298 237 284 175 111 132 145 231 184  12 262 345 157
  95  48  31  65  14 222  91 327 209 293 245 107 153 123  36  46   0  80
  20 198  33 213  49  29  35 341 279  88 159 201 308 322 214 242 309 265
 332  96 103 131]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4521
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3997
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.351
INFO cross_voc_dataset_evaluator.py: 134: 0.603
INFO cross_voc_dataset_evaluator.py: 134: 0.228
INFO cross_voc_dataset_evaluator.py: 134: 0.330
INFO cross_voc_dataset_evaluator.py: 134: 0.558
INFO cross_voc_dataset_evaluator.py: 134: 0.571
INFO cross_voc_dataset_evaluator.py: 134: 0.413
INFO cross_voc_dataset_evaluator.py: 134: 0.002
INFO cross_voc_dataset_evaluator.py: 134: 0.497
INFO cross_voc_dataset_evaluator.py: 134: 0.168
INFO cross_voc_dataset_evaluator.py: 134: 0.370
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.381
INFO cross_voc_dataset_evaluator.py: 134: 0.756
INFO cross_voc_dataset_evaluator.py: 134: 0.615
INFO cross_voc_dataset_evaluator.py: 134: 0.586
INFO cross_voc_dataset_evaluator.py: 134: 0.305
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.395
INFO cross_voc_dataset_evaluator.py: 134: 0.452
INFO cross_voc_dataset_evaluator.py: 135: 0.400
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 24999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.388s + 0.002s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.349s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.348s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.353s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.362s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.360s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.359s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.365s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.368s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.367s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.366s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.368s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.370s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.365s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.357s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.367s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.368s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.369s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.372s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.375s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.369s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.367s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.368s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.367s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.368s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.366s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.527s + 0.001s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.387s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.378s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.378s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.373s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.373s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.370s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.369s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.367s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.365s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.363s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.362s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.363s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step24999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.513s + 0.002s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.353s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.345s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.354s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.354s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.354s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.355s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.355s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.357s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.357s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.356s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.355s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.354s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.316s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [231 179  93   5 287  61 220  18  48  53  54 146 124  52 293 173 169 137
 145 314  47 182   1 122 306 180  49 315 301 258 270 262  39 151 105  20
  73 132 116  84 240 127 207 188 136 282 119 272 150 152  30  82 291  87
 228  31  81 247 244 113 147 312  51  57 170 128 298  85 277 158 318 210
 148 142  38 104  98  69 242  63 263 269 274 288 133  74 225  11 204 215
  94 265 305 131   3  72  55 232 164 203 163  40 138   2  79 107 285 313
 112  59  17 177 307  36 198 202 241 229 309   6  58 185  10 172  21 160
 249 322  44  75 271 253 189  22 134 292 166 109 290 317  46  27 284  67
 256 276 216 219 217 155 175 106 230 278 103 255 200  99  89 222 218  43
 187 319 126   4 211 157 245  88 184 153 162 280 143 302 235 161  32  33
 196 279  34 294 299 165 168  14 250 234 213 321  26 223  24 125  45 206
  91 190 303 267 233  56 118 304 117 114 248  25 254  65  35  90  92 257
  97 209 237 246 300 123 289 273 243  71 252  62 183  15 283 227  83 171
 156 191 129  19  13 323 149   9  23  42 275 266  64 236  86 212 311 110
 144  95  80 192 174 214 100 310  12 296 121 226 197 281  77  37   8   7
 286 193 238 297  76  16 135 154 101 264  28   0 201 316 120 130 194 239
 139  60 186 261  68 195  29 251 308  70 259 108 140  96  78 260 205 159
 268  41 115 167 295 102 176 178 224 320 199 111  50  66 208 141 221 181]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3589
INFO voc_eval.py: 171: [176 115  24  49 203 212 218 153 201 122  19 214 106  25  87 202 133  13
 162 172 156  23 188 204 151  90  80 200  58 112 139 149  12 209 216  48
 160  64  83 174 109 143   9  33 208 187  71  38 224 219 134  89  32 184
  95 217 159  93  91 157 170 193 140  10 111 220 110 123 131  79  35 173
   2 117 127 124  92  26  84  86  85  39 138 155  34  88 198 189 164 213
  20 107  78   6 166  30 113 120  99 171  51 142  57 121 158 178 230 207
  77  52 223 196 100 227  75 210 191 182 125 169  65 177 195 114 215 167
 135 129 119 197 108 226  18 194  97 126  42 186  47  74 104  44  11 229
  67 154 222  14  55  76 165 147 161 105 144  98  73 132  66 102 225  17
 116  68 137  82 183  27 185 199  54  63 146 118  53  36  96   4 221 231
 128  37  41  16  81 175  94 130  31 179  45 228   1 168  28  50  29 103
  56 148  15   0  69 192  22 163  59  62 136   3   5 211  70  43 145  21
  46  60 181 141   8 150 206   7 180  61 101 190  72 152  40 205]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6011
INFO voc_eval.py: 171: [ 62  27  80  49  50  67 122  52   1  77 148 166 150  59  60 167 168  51
 191   0  57  37  31 132 149  96  97  56  75  79 184  83 157 136 196  58
 183  35 111 105 155 141  24  61  70 128 110 187  16 170   4 139 112  55
 121 133  64  10 175 169  82  78 180  68 172 145 102  39 134  69  89 143
 123 130 179 137  19  92 195  72  65  29 161 151   9 154 193 144  18  21
 152 165 176 181 135 101 158  54  44  28 142 119 194  71 106 109  74  15
  47  48 140  12 159 146 186 171  17 103  34  93  42 104  11 162 185  36
  40  23  94  66 174 156 164 116  63 178  91 129  41  45 138  38   3  99
  43 114 182 163 160  87 100 107  13 108   5 131 189   7 117  53  95 173
 147  32 124  98  85  22 188 127  26 120  20  14  88  25  90  86  46 113
  76   8 126 177   2  84 153  73 118 125  30 115  33   6 190 192  81]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2074
INFO voc_eval.py: 171: [307 367   0 308 100   7  48 309  81 249  27 343 135 241  53 450 341 248
 360 392 239 112 245 133  80   8 114 186 111 439  94 254 176 446 247 310
  22 181 389 161 250 394 368 223 464  82  35 300 185  95 290 297 196 387
 115 358 391  26 375 495 436 101 334 228 445  61  30 154  36 385 218 231
 466 251 286 383 262 204  62 236 331 468 423 145 336 314 162 444 403 401
  28 246 170 238 304 234 164 349 455 370 229  16 462 413 259 198 284 471
 419  63 256 141  40 163 409 120 253 178 388 374 104 264 180 303 289 108
 420 448 396 305  25 158 221 159 475 432  87 489 197 182 372 421  37 150
 134  39 287  34 147 458  65 282 281  46 478 340 414 136 272 190 172 260
 481 346 318 265 160 407 125 320 335 477 371 274 377 376 492  59 252 355
 137 200 459 207 224  74 350 386 467 119 406 465  38 174 206 208 404   4
 217 230 210  86 435 138 270 487 494 398  83 102 235 447 461 431  47 156
  41 443 103 299  52 257 173 276 216  88 243 491 353 338 277  24 202 165
 319 151   2 293 333 380  29  67  70  68 397 452 242 106  10 373 438  23
 316 322 139 361 121 480 129  64 472 283 345 116 279  54  58 169 118 144
 463 131 369 269 199  33 152 295 347 143 440 330 233  71 285  85 209 442
 201 110 363 227 312 278 427 428 292   1 479  15  57 225 416 298 351 485
  49 179 469 400  18 426 412 226 266 187 488 211 323 140  19  73 344 311
   9 449 192  32 425 390 237 215 189 329  93 434  79 155 453 402 430  98
 188 220 117 271  42 483 456 357 142   6 191 415  56 168  14 433 123 203
  44  76 408 306   3  99 107 482  13 441 365 255  92 484 378 291 457 109
 417 124  66  60 302 184  20 411 324 342  31 213  89 280  69 113 321   5
 171  75 356 359 422  84 128 153 175 296 354 273 405 474 473 399 177 470
 382  17 193 132  11 379  91 126 167 105 268 219  90 263 454 288  21 317
 326  50 294 240 490  12 451 232  78 486  77 328  55 424 146  97 148 261
 244 364 157 149 366  96 381 183 258 275 166 327 418 393 313 130 212  51
 195 348 352 339 194 122 437 476  45 127 384 315 301  72 410 205 267 429
 460 332 337 325 395 362 214 222 493  43]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3314
INFO voc_eval.py: 171: [450 451 244 456 208 452 252 211 114 309 375 561 370 453   3 207 209 562
 251 196 353 280 129 354 329 210 463 371 197   4 369 376 281 464  83 531
 372 479 455 496 440 525 471 458 175 457  82 498 396  31 465 268 454 485
 472 461 503 476 379  15 367 214 160 489  60 491  30 425 126 255 187 374
 507 254 403 357 293 542 443 148 377 157 459 262 497 486 202  80 303 378
 387 137 315 144 317 392  48 239 182 306 190 554 279 272 487 284 380 169
 404 393 401 316  47  11 319 216 470 277 545 235 260  22  49 159 547 178
 225 308 382 408 427 415   9 106  23 397 529   6 519 232 199 298   5 549
 467 213 399 230 361  38 567 368 564 256 366   7 344 325 505  14 512 125
 291  17 253 123  84 321 356 407  94 548 490 389 521 119  28 439 469 275
 436  40 282 501 434 142  32 189 212  92 414 532 227 428 296 390 362 460
 466 481 444 164 158 400 477 206 188 422 386  99 384  86 437 559  20 258
 419   8  43 286 338 339 294 523 388 527 217 185 322 130 261 276 233  26
  78 145 131  19  37 522  33  63 243 163 504  79 201 287 445 146 406 411
 219 536 283  21 278  75 249 441 449 263 511 424 468 553 154 337  61 358
 515  88 364  74  35 334 410 560 320   0 495 132 514 533 543 363 115 430
  56 150 300 304 200 257 324 475 152 204 301 394 426 313 259 563  62 246
 218 417 198  85 141 240 167 267 534 332 223  44 556  10  45  34  98  68
 420 462 311  29 499  71  64 405 220 346 416  72 551 290  69 172 568 224
 166 241 350 153 122 383  90 535 330  76 539 222  16  13 314 402 413 237
 360 292 395 124 170  87  51 421 140 120  73 373 480  53 195 117 506 176
 205 104 248  65 513 174 181 269 333 431 502 546 570 147 447  36 409 266
 155 494 446 101 215 273  93 569 165 335 138 193 565 552 134 540 518  18
 136 179 250 302 245 483  77 341 226 538 274 318 184 242  97  95 331 385
 520 550  54  12 510 516 442  91  70 355 484 171 432 557 238   2 127 112
 391 326 474 541 482 192  50 365 264 108 307 139 191 312 352  66  96 555
 343 328 412 349 135 270 323 156 488  39 524 566 348 473 342 478 229 500
 493 105 537 107 102  25 194 435 203 186 110 544 295 162 299 177 438 398
 271 149 221 528 297  89 492 118  59   1 359 234 517  57  42  58 161 433
 340 508 121 418 310 347  24  41 236 526 173 327 530 109 228 113 336 288
  55 305 116 509 345 558 103  27 289 265 247 351 168 128 133 381 180 111
  67 100  81 143 183 231 423 429  46  52 448 151 285]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5815
INFO voc_eval.py: 171: [ 29  14  48  51  74   3  56  23  62 117  13  91 122 124  80  87  71  77
  27  85  60  95  99  35  76 116  64   8 123  78 102   2  55  84  52  86
  59 131  98  88  72  19  20  96  22  44  36 111 112   6  17  24 110  50
 103 120  81 119  25   7  45  37   9  92  82  79 125  18  30  65 108  47
  11 104 128  10 114  93   0  75  94 129  97  15  31  70   1 126 127  61
  42  53  67  73 113 107  58  46  40  38  12  57  49  21  16 106 109  90
  68 121   4 118 115 100  39  28  43 130  32 101  41  54  33  34  69  89
   5  66 105  63  83  26]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5873
INFO voc_eval.py: 171: [320 423 341 497 144  76 141 248   9 464 454 168 101  52 465 345 281  16
  22 486 224 444 466 370 203 230   2 457 259 225 244 367 189 443 419 254
 372 100 167 376 129 369 395 192 478 425 428  34 435 188 279  88 249 332
 453 511 321 289  89  27  54  97 200 470  57  17 275 256 299 251 218 312
 190 191 368 252 160 347 282 397  33  48 266 306 207 222 349 447 180 288
 319  12 179 267  24 356 450 257 429 442 322  28  37 156 303  51 489 149
  62 475 433 398 105 264 172 469  65 502 184 480 418 304 294 385 104 171
 260 213 127 460 310 386 232 130  73 186  32 400 111 262 504  93 231 182
  81  50 253 500 121 456  47 438 449 378 430 255 355 164 198 119 510 389
 221 472 314 227 195 165 194 216 484 316  39 261 271 196   0 142 201 330
 329 152 463 287 411 223 272 452 263 474  26 151 498 214 431 265  42  23
 461  30  40 328 239  13 426 110 177  25 108  21 175 138 446 173 106 159
 148 209 327  35 496 291  53 381 410 305 492 301  49  38 455 283 346 258
  99 122  72 417  63   5 422  15 276 136 274 468 364 277  31 333  58 477
 409 439  61 482 313 199 226 503  86 298 499 366 197 185 318 337   8 295
 143 451 399 183 236 476 178 471 387 493 205 414 215 392 166 348  92 403
 234 375 335 388 307 379 220 211 505  10  44 448 495  79 133  85   7 441
 241 250 120 354 235 292  71  74 352 507 512 161 233 112 280  36 174 324
 107 157 412 126 208 219 357 432  98 365 413 118 217  45  64 284 187  20
 424 268 269 331  68 240 317 169 102 146 155 176 109 147   4 202 170  18
 158 350  46 343 103 420 134 458 391 340 150 342  59  75 124 416   3 296
 140 490 206 135 407 125 344 285 359 238  60 361  55  78 290 402  14 485
 273 117 336 481 494 323 436 123  19 380 334 308 116 153 396 373  66 114
 293 427 247 508 437 384  43 115  67  95 501 363 113   1 338 509 459 162
 506  41  87 245 193 181 467 382 297 278  90 374 406 246 393 488 128  56
 137 139 440 154 302 210  94 404 228 229 351 132 311 358 163  69 445 401
 473 131 405  84 270  82 377  70 487 242 415 315 204 339 421 145 309 362
 326 371 286 383 360 237 353 394 462  83 408 300  91 212 243  29 434 479
   6 325 390  96  77  11 483 491  80]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4314
INFO voc_eval.py: 171: [ 4  2 10 34 11 17 39 28  0  3 23 13 14 30  9  7 22 27 20 16 37 21 38 35
 32  5 40  1 24 31 36 18 15  8 25 41 12  6 33 26 29 19]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0023
INFO voc_eval.py: 171: [1587  381 1588 ...  605   34  112]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.4945
INFO voc_eval.py: 171: [26  1  2 52 40 21 19 47 13 61 56  3 15 18 37 10 41 32 51 58 60 50  0 64
 44 34 12 23 29  4 25 49 46 17 27  9 57  5 20 59 43 30 48 14 28 42 11 22
  8 53  7 39 24 54 35 16 55  6 63 38 33 31 36 62 45]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1628
INFO voc_eval.py: 171: [360 163  87 156 312 176 235 229 216 234 140  49 318 165  33 128  68  76
 333  16  15 200 275  34  47 322  75 380 159 362 317 299 160 300 139 226
 310 245  90 274   1 381 334 331 256 387 244 232 324 108  27  22 182 269
 295  88 378  24 106 252 341 180 141  64 347  79 309 149 134 238 267  61
 325 356 205  20 406 396 376 319 382  62 400 313 250   7  43 125  69 402
 233 142  23 153 224  55 352 369 132 199 127 379 230 276 138 363 365 164
  95 223 120 337 392  13 389 162 408 260 129 386 201  10  14 281 342 255
 291 113 335 104 110 116  39 107 214 137 293 285 336  91  17 268 130 282
  97 394 126 231 375 249 136 372 177 191 350 227  58 353  35 266 338 348
 373 339 217 195 377  71 393 330 148 399 103 222 368 135 346 326   4  78
 292  32  48 304  82 301  59 357 370 257 321 242 296 306 253 287   8  11
 248 237  19  93 398 407 154  12 101 175 178 207 144 349 366 384  67 270
 166  66 303 173   3 390  83   0 213 133 355 388 311 327  85   6 403 188
 308 179  84 289  46  53 371 385 367 109 395  94 221 284 114 212 185 203
 320  57 344 146 316 100 189 192 196  36 280  60  38 115 323  44 193 404
 332 247  65 152  28 210 258  30 219 283 228  29 264  74 118  96 401 147
 294 278 263 397 288 172  51 262  72 271  92 236 241 190 391 158  45 150
  98 215 102 358 265 239 279 168  41 354 254 184 122 251 206 351  63  37
  80 197 198 298  40 181 383 143 374 220 329 155 187 202 117 315 208 328
 157 105 186  18  73 259 364  31 183 297 277 124  81   5 246  99  25 261
  21 272 170 167 307  52 145 131 121 169 161 171  89 218 343 345 290 209
  86 211 111 119 361 243  50 151  56 174 286  42 405 305 204 409 314   2
   9  26  54  70  77 340 123 194 240 273 302 359 112 225]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3867
INFO voc_eval.py: 171: [ 13  87  26  27  73  51  52   4  46   1  20  30  49  78  18   0  66  47
  97  61  40  96  95  43  99  28 100  22  93  50  16  10  33  79  68  90
   2  42  92 101  76  71  39  89  67   7  83  21  82  17  70  86  59  53
  60  23  65  11  58  98  12  34  88  63  57  64  84  56   3  55  19   5
  24  75  94 102   8  31  37  48  80  72  54  35   6  15  44   9  32  29
  41  14  36  77  45  62  81  25  69  74  91  85  38]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0597
INFO voc_eval.py: 171: [65 76 56 58  8 11 81 79 44 53 43 26  9 69 55 46 45 12 66 72 60 59 78 75
 61 10 34 50 73 40 20 29 21 37 14 77  6 18 25 41 67 24  4 48  3 51 64 19
 57 70 63 42 32 23 31 36 28  2  1 62 74  5 49  0  7 38 30 35 47 68 82 27
 39 33 22 80 71 83 54 13 17 52 15 16]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3619
INFO voc_eval.py: 171: [ 31 245  23 154 218 107 161  35 257  15   8 190 182 114 162   7 175  87
 250 180  32  52 179  99 208  98 211 232  47  58 155  72 167 139  97  76
 141 118 121 181 262  91 140 231  20 164  46 214 147  62 149  59 100 109
 253 124 198   5 128 189  70 103 217  66 120  16  44 184  40 195 174 171
  22   1 193  53 197  21 203 205 255 216  83 183 225 131  26  95  86  68
  64 233 133 194 238  28 191  42  37 159 201  78 251 115 122 210 264 127
  27 213   2 219 224  30 176  57  36  93  74 222 116 258 125 148 265 145
  38 117 230 223 240 236  39   9  82 105 168 111  89 165  13  67 137 173
 172 186 135  41  33  73 209  48  61 163  45  94  54 123 188 244 177 266
  25 185 226 166 243 221 200 212 259  19 235 138  84 206 153  92 142  14
 249  51 129 256 157 130 146 241 104 126 178 101 113  55  77 144  81  29
  75  24 252   0 132 106 136 229 152 261 254 220 202 227  60 151  71  63
 196 119 156 169 170 160  69  17 228   3  96 263  50 143 246 242 237 260
 207   6 150  10 234 204 192  49  65 199  80  12 158 102 239 248 110   4
 187  79  90  34 134 247  11 215  56 108 112  18  85  88  43]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7427
INFO voc_eval.py: 171: [2096 1221 2113 ...  713 1544  579]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6010
INFO voc_eval.py: 171: [160 139 151 232 490 797 173 261 168 244   7 741 330  81 346 798 676  49
  84   6  82 744 481 101 427 522 245 705 468 452 647 742  83 534  87 262
 549  86 301 290 267 189 596 639 799 837 246 587 801 182 740 209 492 187
 645 552  66 407 305 646 190 158 550 723 800  65 709 738 302 814 537 138
 304  93  54 649 241 263 554 377 253 172 811 691  35 820  92  88 654 644
 147 206 558 156 362 398 397 103 749 385 678 595 467 809 747 648 349 839
 429  85 523 379 247 431 731 475   8 779 322 463 107 104 499 743 816 226
 459 319 157 614 728 677 135 748 519 341 659 542 650 729 556 720 493 428
 400 711 589 351 686 252 655 858 483 717 570 380 469 580 553 113 430 513
 456 403 143 865 751  42 821 781 471  14 836 472 562  72  37 807 567 516
 142 458 399 412 271 465 145 198 657 413 855 155 571 169 685 564 539  77
 593 163 582 297 622 446 841  70 191 484 565 291 266 702 541 733 117  28
 823 681 756 108 521 769 258 819 375 785 207 568 307 333 127 684 237 810
 658 352 590 732 476 185 555 857  69  56 192 697 840 752 576 347  36 164
 494 202   9 455 603 123 817 148  95 859 683  78 579 514 255 818 193 250
 496  96 288 632  29 317 581 269 607  90 512 557 162 660 844 812 682 242
   4 165 364 605 668 573 321 294 745 651 520 208 442   0 750 356  47 249
 696  34 466  97 219 746 102 515  16 295 254 735  91 435  60 200 306 851
 408 368 712 195 396 666 419 248 372 640 663 653 140 268  63 805 453 259
 714 518 795 118 316 842  98 303  13  43 409 318 706 314 830 754 546 393
 448 511 354 382 449 633 529 560 159 831 273 112 239 436 673 768 402 201
 296 774 270 863 462 509 775 480 503  51 218   2 778 780 783 763 547 849
 386 846  17 406 601 690 637 264 824 216 415 643 225 604 221 670 661 608
 802 500 530  59 447 350 150 713 363 488 652 141 672 131 680 862 757  62
 498 592 600 760 479 323 460 320 793 197 739 569  89 543 489 838  31 734
 410 527 342 700 665 451 508 389 804 487 597 699 766 708 229 136 205 506
 312  15 725 411  57 803  68 265 336 133 134 501 154  53  55 497 343 532
 835 256 629  21 257 850 602 796 332 128 609 240 770  12 426 698 715 815
 656  19  67 788  10  26 594 703 866 454 485 210 477 591 370 853 329 293
 111 771 718 325 289 687 178 420  27 860 390 260 433 762 438 308 120  71
 528  20 548 355 624 578 585 777 234 335  73  75 813 105 722 416 641 507
 461 424 792 125 434 132 806 361 473 220 274 311 450 152 693 338 231 114
 144 710 339 188 540 326 611 847 829 222 577 642 404 574 374  25 782 861
 854 181 704 635 284 109 630  40 153 130 287 765 588 525 421 491 688 423
 348 575 808  46 621  58 184 353 441 845 504 122 794 238 612 331  76 628
 367 615 212  61 196 737 203  44 171 235  18 551 856 211 545 583 251 223
 387 313 664 638 827 716 726 391 146 369 179  30 149 617 116 214 215 470
 394 464  79 344  23 309 278  64 610 365 310 727 533 373 826 772  24 535
 272 536 634 376  50  52 606 544 852 618 482  80 679 194 825 439 381 418
 764 669 524 559 224 106 170  39 502 689 437 584 384 701 300 366 623 674
   5 758 662 315 773  22 626 495 822 848 440 784   3 445 217 707 395 787
 474 378 275 692 233 279  32   1 561 443 767 328 625 230  99 360 243 383
 126 286  38 843 486 299 337 526 417 292 721  94 620 176 183 517 357 864
 121 791 613 833 631 204 790 776 199 510 675 276 327 161 432 505 828  41
 538 719 478 834 695 388 180 277 177 786 283 444 422 736 282 566 334 724
 236 401 563 280 175 599 531 392 186 227 616 228  74 137 340 345 671 358
 730 119 667 359 761  33 115 414 371 832  45 759 213 572 174 110 166  11
 405 425 598 281 753 167 457 324 298 636 627 100 619  48 789 129 586 755
 285 694 124]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5853
INFO voc_eval.py: 171: [46  5 27 45  0 38  7 40 43 42 58 65 60 26 59 24  8 16  2 48 53 31 11 18
 47 29  3 54 10 28 21 13 39 17 14 33 57 55 41 56 19 50 51 30 63 32 62 37
 12  6 15 44 49  9 35 22 61 25 52  1 36 20 64 23 34 66  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3051
INFO voc_eval.py: 171: [ 58  59  21  89  57 142  32  88 105  49   3  67  99  25  84  30  65  61
 129 125   4  75  85 112   9 135  71  47  44  23  43  96 117  27  24  72
  37  87  95  93  16  38 145 139 132  77  86  20  53  15   0  40  91  17
 128  69  41  10 140  79  50 137  94  63  66  34  62  74 118  83 127 130
  31  70 126 104 107  56  36   1  51 123  35  76  13   7  29 144  48  98
   2  97 119  52 134  42  90 141  11 113  39   5  68  82 101 143  46 121
  81 110  26  78   6  19  22 102  92 114 111  80  73  60  64 122 138  28
 116   8  33  14  55  18 120 124 115  45 103 109 131 136 133  54  12 100
 106 108]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3513
INFO voc_eval.py: 171: [108  17  67  39  14  56 138  28  78  58  92  36  13 109 135  80 147  38
  74 150   6 142 122  55  79  57  12  91  15  37  60  23  64  71 124  99
  20  53  42  16  95 102  90  32  84 156  72 120 118 119 152 101 106 130
  26  59  52  82 140 143  40  34 123  30  31  49 105  73 114  85  47 129
  44  96 116 139   5  89  94  61  69 157  68  63   4  41 104 100   0  86
  76  35 127 125 134 154  11   8 146   1  48 132 133 115  29  27  18  81
   7 144 111 128  65  25  45  88  98  97 136 110  54 103 121 153  51 148
  75   2  43  93 141  83 107 137  22  24  21  10   3 155 151  66 149 145
  33 158 112 126  77  87 117  46  62   9 159  19 131 113  50  70]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3642
INFO voc_eval.py: 171: [ 90 149 222 236 210 299 326   3  25 179 286 137 322  18 170 237  57  84
  37  22 306 180 242 119 244  72  41  86 186 275 220 223 196 232  76  53
 118  11 224 211  56 332 328 243 229 209  55   1  40 110 133 182 120  60
 290 238  58 154 227 254 336  65 262 104 164 341 129 183 307 208  45 255
 252 337 276 308 284 259  34  83 256 342 317  21 125 159 228 105 324  54
 309  85 289  24 261 260 163  61 320  88  77 195 145 239 292 282 114  52
  48 233 273 346 258  23 278   4 117 185  97 144 153  74 318 173  64 264
 323 253 161   7 241  15  71 184 340 296 146 107 191 147 136 205  30 213
 207 339 319  92 215  32  51 314  16  73  63 155 139 141 305 177  93 279
 192 311 131 122 115  19  27 214  95 298 334 167  89 181 130 206  26 287
  70   9 108 190 263  46  96 231 310 123 343  80 280  38  43 301 193  59
 203 100 103 268 132 281 247 138 321 295 176  62 102   5 174   8 165 291
 316 226 188 121  13 303 235  75 197 274 345  10  44 344 112 325 151 128
  81 257 152 294 194 157 338 327 221  17 199  42 200 198 172 271 329 202
 285 266 249 140 143 169 171 277   6 293 230 304 267 270 348  28 150 142
 106 175 219  67 127 251   2 272 250 158  79 189 166 168 101 178 116 302
 113 300 331  69 288 333 246 240  68 111 315 148 265 234 248  12 225  49
 187 135  94  31  66 347  98  78 160 330 156 212  29 109  47  82   0 216
 126 297  36  20  14 283  50  33 201  91 312 313  99 162 245 218 269 124
  35  39  87 204 134 217 335]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4503
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3983
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.359
INFO cross_voc_dataset_evaluator.py: 134: 0.601
INFO cross_voc_dataset_evaluator.py: 134: 0.207
INFO cross_voc_dataset_evaluator.py: 134: 0.331
INFO cross_voc_dataset_evaluator.py: 134: 0.582
INFO cross_voc_dataset_evaluator.py: 134: 0.587
INFO cross_voc_dataset_evaluator.py: 134: 0.431
INFO cross_voc_dataset_evaluator.py: 134: 0.002
INFO cross_voc_dataset_evaluator.py: 134: 0.494
INFO cross_voc_dataset_evaluator.py: 134: 0.163
INFO cross_voc_dataset_evaluator.py: 134: 0.387
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.362
INFO cross_voc_dataset_evaluator.py: 134: 0.743
INFO cross_voc_dataset_evaluator.py: 134: 0.601
INFO cross_voc_dataset_evaluator.py: 134: 0.585
INFO cross_voc_dataset_evaluator.py: 134: 0.305
INFO cross_voc_dataset_evaluator.py: 134: 0.351
INFO cross_voc_dataset_evaluator.py: 134: 0.364
INFO cross_voc_dataset_evaluator.py: 134: 0.450
INFO cross_voc_dataset_evaluator.py: 135: 0.398
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 27499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.474s + 0.002s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.372s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.359s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.357s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.359s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.362s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.362s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.365s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.363s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.361s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.361s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.364s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.364s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.423s + 0.002s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.345s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.350s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.350s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.351s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.356s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.365s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.360s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.357s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.360s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.362s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.363s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.362s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.427s + 0.002s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.348s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.352s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.357s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.358s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.356s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.352s + 0.001s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.355s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.358s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.357s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.359s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.360s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.360s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step27499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.450s + 0.001s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.338s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.345s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.349s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.350s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.349s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.354s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.359s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.359s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.358s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.356s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.355s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.355s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.434s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [229 177  90   5 285  57 218  17  45  49  50 142 121  59 292 171 167 134
 141 313  44   1 180 119 305 178  46 314 256 300 268  36 260  18 102  70
 129 148 113 238  82 186 124 206 280 133 147 116 270 149  28 263  79 290
 226  84  29  78 110 245 242 311 144  48  53 168 275 125 297 208 156 317
  81 138 145  35 101  95 240  66  60 261 267  91 272 286  11  71 130 223
 203 213 128  69 304   3  51 230 162 202  37 135 161   2  76 104 283 312
 109 175  16 306  55  33 197 201 308 239 227   6 183  54 170 158 321  10
 247 269 251  21  41 187  20 164  72 288 131 291 316  25 106 282  43  64
 274 217 214 254 215 103 152 100 253 173 199  86  96 276 228 220  40 318
 216 185 209   4 123 155 243 150 182  30  85 160 233 139  31 278  32 301
 195 159 163 298 277 293 232 248 166  14 221 302 211 320  22 122  42 265
 205 188  24 303  52 115 231  94  88 111 114 246 252  23  62 143  89 207
 244 235  73 255 132 181 120  87 299 271  68  58 250 169 281 189 153  80
 241 287 225 126 146  19  15 322  61  39  92 234 107   9 140 273  13  77
 264  83 172 310  97  74 295 210 196 118 190  12 212 224  34 284 191 279
 309   8   7 236 262  26 151 200 296 315   0 127  65 192 307  98  93 193
  56 184 117 289 237 105 249 257 136 137  67 259 174  75 204  27 258 112
 266  38 294  99 157 222 176  63 319 108 179 165  47 198 219 154 194]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3600
INFO voc_eval.py: 171: [178 116  25  50 205 214 220 154 203 123  20 216 107  26  88 204 134  14
 174 164 157  24 206 190 152  91 202  81  60 113 140 150  13 218  49 162
  65 207 176 110  84 144  10  34 211 189  72  39 226 221 135  33  90 186
 219  96  94  92 160 172 158 208 141 195 112  11 111 222 124 132  36  80
 175   2 118 128 125  93  27  85  86  87  40 139  35 191 166  89 200 156
  21 215  31 108 114  79   7 168 121 100 173  52 143  58 122 180 159  53
 232  69  78 225 198 101 193 229 212  76 126 171  66 184 179 217 169 197
 115 120 109 228 136 130  19 199 196  98  43  75 188 127 105  48  64  45
 231 155  12  56  15 224  77 167 148  29 163 106 145  99  74 133 103 117
  18 227 138  67 187  83 147 185  54 119  42   5  97 201  55  37 129  38
 223 233  82   1 131  16  17  32 104  28  95 181  51 170 177  46  57  30
 230 149  70 194  23  22   4  44  61   0 137 165  71   6 146 213  47  62
 183 210  63 151 142   8   9 182 102 192 153  41  73 161  68 209   3  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6009
INFO voc_eval.py: 171: [ 61  26  80  48  49 122  67  51   1  77 148 167 150  58  59 168 169  50
 192   0  56  36  30 130 149  96  97  55  75 185  79  83 197 158 136  57
  34 184 111  70 105 156 141  23  60 110 128  15 188   4  54 171 139 112
 121 133  63   9 176  82  78 170  68 181 173 102 145  69 134  38 143  89
 123 131 180 137  64  92  18  72 196  28 151 162 155   8 144 194  20  17
 152 177 166 135 101 182 159  53  27  43 142  71 119 195 106  46 109  74
  47 160  14  11 186 140 146  16 187 103 172  93  33  41  35 104 163 157
  10  39  66  94  40  22 115  91 129  37  44   3  62 175  99 165 138 179
  42 114 161  87 183 164 108 107 100  52   5 190 132  12 117  95 124  21
  31   6 174 147  85 120  88  13  25 189  19  98  90 127  24  86 113  45
  76   7 154 126  73 118   2 125  84 191 193 116 178 153  29  81  65  32]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2077
INFO voc_eval.py: 171: [308 368   0 309 101   8  48 310  82 251 134  28 344 243  53 451 342 250
 361 394 241 113 247 133  81   9 115  24 188 112  95 439 256 178 446 311
 249 183 163 252 395 369 465 225  83  36 187 301  96 290 298 198 388 116
 359 376 289  27 392 496 436 102 335 230 445 390 332  61  31 386 156  37
 220 467 234 287 206 384 253 264  62 469 238 146 315 423 337 444 164 403
  29 401 172 240 248 305 236 350 166  17 371 455 231 413 463 261 200 285
 472 419  63 142 165 409  41 258 387 121 180 255 389 375 105 182 266 420
 304 109 288 448 396 306 476 161 160  26 432 223  88 489 199 184 373 135
 421 151  38  40 284 479 458  65 148  35  46 283 192 341 414   7 136 174
 274 347 262 482 319 162 407 372 267 321 126 378 336  59 276 254 377 478
 138 492 209 356 202 226 406 120 460  75 351 466 468 208 219 176  39 404
 212 210   4 435 137  87 103 398 272 487 495 237  84 233  47 462 431 194
 300 447  52 259 104  89 277 218  42 443 175  25 158 204 491 354 339   2
 167 245 320 334 279 381 152 293  71  10  67  68 397 107 452 374  23  30
 438 244 459  64 323 130 362 481 317 122 139 346 473 171  58 117  54 464
 132 281 370  72 145 119 201 271 153 440 144  34 296 313 211 331 111   1
 203 348 286  86 229  16 235 442 364 292 428 352 427 480 485  57 227 299
 470  49 416 280 228 181 141 412  19 213 400 488 324 189 269 426  20 345
 312  33 425  74 434 391 239  94 217 330 191 449  69  80  99 190 222 402
 453 456 157 273 484 430 193  43 143  56  15 118 205  77 433 415 154 170
 358 408  44   3   6 124 100 307 232 108 366  14 483 379 303 441 457  60
  12 186 110 257 125 291  93  21  66 417 215  32 411 325  90   5 282  76
 343 114 129 173 322  85 155 177 422  70 357 360 297 355 474 405 475 399
  11  18 275 196 471 179 270 380 383 169 106 127 295  92 318 221 327 265
  22 454  50 424 149 147 490  98 242 140 294 365 486  78  55  97 150 159
  79 168 314 450 328  91 367 382 246 329 263 131 393 214 260 477 278  45
 185 197 349 340 418 326 128 123 385 363  73 353 338 333 410 268 224 216
 207 195 461 316 437 302 429  51  13 493 494]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3228
INFO voc_eval.py: 171: [452 453 245 458 208 454 253 211 115 311 562 376 371 455   3 563 209 207
 252 196 354 281 130 355 330 210 466 372 197   4 370 377 282 465  83 532
 373 481 457 499 442 526 473 460 459  82 176 501 397  33 467 456 487 269
 474 463 506 478 380  15 368 214 161  61 492 494  30 426 256 127 188 375
 510 255 404 358 294 543 445 378 461 149 500 158 263 489  80 202 379 305
 388 138 145 316  49 394 318 240 183 190 555 280 491 308 381 285 273 170
 405 393 401  48 317 215 472 320  11 546 236 278 261  50 160  22 226 548
 179 310 383 408 428 416 107  23   9 398 488 520 233 199   6 550 299   5
 469 530 213 400 361  38 231 565 257 568 369 326 367 508 345   7  14 515
 126  17 292 254 124 407 357  84 549 322  94 493 441 390 120 522  28 438
 471 505 276 436 283  32 143 212 415  40  92  31 533 468 429 227 189 364
 462 297 391 446 159 483 165 402 479 422 206 387  86 385  99  20 560 439
 420  43 259 287 339   8 340 295 389 132 186 524  26 131 262 217 528 146
  37 234 323  19 277  78  64 244 523 507 164  79 288 201 409 447 412  21
 219 147 537 250 284  75 470 279 451 443 514 264 425 554 359 366  88 155
 338  62  35 335  74 498 321 561 411 365 517 534   0 151 544 133 116 477
 302 432 258 200 306 153  57 325 247 204 314  63 395 303 564  59 427  85
 192 418 260 218  45  10  46 241  68 535 333 198 464 142  98 312 225 557
 167 268  29  34 421 417  65 503 242 173  71 347  69 569 406 220 224 168
 384 536  72  90 552 291 331 540 223  76 154 122 351 403 396 238  16  13
 362 414 125 293  87  52 141 171 423 482 249 121 374 118  73 509 195  66
 205 516 431 105 571 334 270 182 148 177  36 547  56 216 175 410 497 101
 449 156 267 274 553 448 139 222 570 135 566 336 193  18 541  93 246 166
 181 342 275 137 251 484 304 185 539  97 521 551 319 243  77 386  95 332
  54  12 486  91 518 513 315  70 172 502 444 128 558 433 356 327 485 239
 476 542 392  51 113   2 191 309 313  67 271 353 265 490 413 140 344 556
 480  39 110 350 157 329 324 525  96 567 136 349 475 531 496 301 440 203
 230  25 504 343 187  41 106 538 437 102 108 178 399 495  89 194 150 163
 221 103 298 296 119 272  60 360 545 529 341   1 235 519 229 109  44 162
 435 123 419  42 511  58  24 346 289 174 237 328 512  55 111 114 527 337
 144 100 559 104 184  27 348 290 307 228 300 129 112 169  81 424 180 352
 248 266 117 382  47 232 430 134  53 152 363 450 434 286]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5793
INFO voc_eval.py: 171: [ 30  14  49  52  75   3  57  23  63  13  93 119 126  82 128  89  72  78
  27  87  61  97 101  36  77 120  65   8 127  80  56   2  86  88  53   1
  60 135 100  90  73  19  98  20  22  45  37 114 115   6  17  24  83  51
 113 105 123 122  46   7  38 104  25 106   9  84  94  31  18 129  81  66
 111  11 107 117  48 132  10  95   0  76 118  96 133  99  15  32  71 130
 131  54  68  74 116 110  43  62  47  41  39  50  58  12  59  16 112  21
 125  28 109  92  29  40  69   4 121 102  33 134  44 103  34  42  55  70
  35   5 108  91  64  67  85  26  79 124]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5847
INFO voc_eval.py: 171: [321 424 341 142 497  74 139 246   9 465 455  99 167  50 466 345 282  15
  21 487 222 445 467 370 202   2 228 458 258 242 223 367 188 420 444 253
 372 166  98 127 376 369 395 191 479 427 430  32 187 437 280  86 248 332
 454 511 290 322  87  26  52  95 199 471  55  16 275 255 300 250 216 189
 190 313 368 251 159 347 398 283  31 266  46 307 206 349 220 448 320 179
  11 325  23 289 178 267 356 451 256 431 443  27  35 304 154 489 148  49
 476  60 435 399 264  63 470 502 183 481 103 305 171 419 295 385 170 102
 259 211 125 311 386 461 230 128 185  71  30 109 504 261 401  91 229 181
  79  48 252 500 457 119  45 440 450 378 254 163 432 355 412 117 197 510
 389 473 315 219 225 194 486 164 214 193 260  37 317 271 140 195   0 330
 200 329 151 464 288 221 453 272 150 263 498  25  22 475 212 433 265 462
  28  38  40  12 328 237 428 176 108 106 174 447  20  24 136 172 104 208
 158 147  33 292 284 496  51 381 306 492 302  97 456  36 257  47 346  61
 418 423  14  70 134   5 120 274 278 469  29 333 364 277 426  59 441 410
 314 478 483  57 198 503 499  84 224 299 196   8 319 184 141 234 337 400
 296 182 366 452 177 472 477 387  90 392 415 204 213 165 493 232 404 210
 348 308 375  10 388 218 411 379 335 233 354   7 339  77  42 131 449 495
  83 239 249 442 512 293 118 505 352  69  72 160  34 231 507 110 281 105
 173 413 323 155 397 434 357  96 116 124 217 207 215 425  62  43 285 414
 268 168 269 365 100 238 331  19 144  66 107 156 175 153  44 186 318  17
 421 145 342 350 201   4 343 132 459 340 169 101  73 408 123 391   3 149
 122 297 291 344 417 115 133 490  56 286  53 236 138 273 205 359  58  13
 485 324 361 380 482 494 403 114 121 112  76 429 334 438 336 294  64 384
 373 396 309 508 363  41  93 146 245  18 501 509   1 113 180 111  65  39
 439 298 243 192 161  88  85 374 382 393 126 460 152 135 407 506 137 488
 279 276  54  67  92 226 244 209 405 351 129 130 474 227  82 162 446 303
 406 270 316  68 468 422 416 143 377 240 312 310 358 402  80 338 353 360
 362 371 390 235 287 463 383 480 203  78 394 301 241 326 409   6  81  94
 484  89 436 327  75 157 247 262 491]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4295
INFO voc_eval.py: 171: [ 4  2 11 36 12 18 41 30  0  3 24 14 15 17 32  7 23 29 21 22 39 40 37 34
  5 10 42  1 25 33  9 38 19 16 27  8 13  6 26 35 43 31 20 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0023
INFO voc_eval.py: 171: [1593  381 1594 ... 1245  357  673]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.4933
INFO voc_eval.py: 171: [27  2  0 52 40 22 20 47 14 61 56 16 19 37 11 41 33 51 58  4  1 50 60 44
 64 13 34 24 30  3 26 49 46 18 28 10 21  5 57 59 43 31 48 29 12 42 23 53
 15  7 39 25  8 54 35 55 17  6 63 38 62 36 32 45  9]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1804
INFO voc_eval.py: 171: [357 162  87 155 309 175 234 228 215 233 139  49 315 164  33 127  68  76
 330  16  15 273 199  34  47 319  75 377 158 359 314 297 159 298 138 225
 308 244 272  90   1 331 378 328 255 383 243 231 321 108  27  22 181 268
 293  88  24 133 375 106 251 338 179 344  64 140 307 237  79 148 266  61
 322 204 353  20 402 392 316 373  62 379 310 269  43 124 249   7 396  69
 398 232 141 152  23 223 198  55 349 126 366 229 274 376 131 137 360 163
 222  95 362 334 120 388 161  13 385 404  14 259 113 200 104  10 128 289
 332 279 339 382 254 213 110 107  39 116 136 291 283  91 333  17 129 280
 267 230 125 390  97 369 177 372 135 190 176 347 226  58 350  35 265 335
 370 299 374 336 216 345 194 323  71 103 327 389 221 147 343 365 395  78
 134 290  32 302   4  82  48  59 354 256 318 304 367 294 241 252 285   8
 247  11  19  93 236 153  12 394 174 403 346 101 206 172 380 143  67 301
 165  83  66   3   0 384 212 352 132  46 386 324   6 306  85 287 187 399
  53  84 368 178 391 220 109 364  57 184 211  94 317 191 381 282 202  44
 188 114 192 115 341 400 145 320  36 313 195  38  60 100  65 278 246 151
 329 209  28 257 263  29 218 227  30 286 171 292 281 262 363 397 118  51
  45  72 146  74 393  96 276 189 261  98 157 387 235 149 214 102 240  92
 355 238 168  41 264 277 348 122 253 250  37 351 183  63  80 205 197 371
 296 196 201  40 219 180 326 142 105 117 312 154 325  18 186 207 156  31
 258  73 182 361  81  99  52 245 185 275   5 123 260  25 305  21 170 130
 295 144 167 166 121 217 169  89 270 288 208  50 340 342 119  42 242 358
  56   9 150 111 160 401  26  86 210   2 405 284 203 337 311 173 303  54
  70 224 300 112  77 193 248 356 271 239]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3869
INFO voc_eval.py: 171: [ 13  86  26  29  73  51  52  46   4   1  20  30  49  78  18   0  66  47
  61  40  95  94  96  43  98  27  92  50  99  10  14  33  79  22  68  89
   2  39  42  91 100  71  76  67  88  83   7  82  21  70  17  85  53  60
  59  23  65  12  11  57  63  87  56  97  34  64  84   3  55  19   5  58
  24  93  75 101  31   8  37  48  80  54  72  35  16  28  32   9   6  44
  41  15  77  36  45  25  62  81  90  38  74  69]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0599
INFO voc_eval.py: 171: [64 75 55 57  8 11 80 78 43 52 42 25  9 45 54 68 44 65 73 71 59 12 77 58
 60 10 33 49 72 39 19 30 20 36 76 14  6 24 17 40 66 23  4 47  3 50 18 63
 56 69 62 41 31 22 29  2 35 27  1 61 74  5 48  7 37 34 28 67 46 81 26 38
 79 21 32 53 70 82 13 16  0 51 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3619
INFO voc_eval.py: 171: [ 30 242  22 151 216 104 158  34 254  15 187   8 179 111 159 172   7  85
 247 177  51  31 176  96 206 209  95 230  46  57 152  70 164 136  61  94
  74 138 115 118 178  88 258 137 229  19 161  45 212 144  97  58 146 106
 250   5 121 186 196 125  68 100 215  64  16 117  43 181  39 171 192 168
  21 190   1  52  20 194 201 252 203 214  81 223 180  92 128  25  84  63
 231  66 191 188 236 130  27 156 199  41  36 248  90  76 112 119 208 235
 124 259 211  26   2 217 222  29 173 220  35  72  56 255 113 122 261 145
 207 142  37 114 228 237  38 221 233   9  80  86 102 165 162 108 169  65
  13 170 134 183  40  71  32 132  47 160  53  91  60  44 174 120 185 240
 241  24 182 262 163 210 198 224 219 256 232 135  18  82 204  89 139 126
  14 150  50 253 154 143 127 101 246 175 123 238  98  28  73  23 110 141
  75  79 103  54 249   0 129 227 133 149 251 218 225  62  59  69 153 200
 193 167 116 148 157 166 140  67   3 226 260 257   6 147  17 205 243 239
  49  93 189  48  78 202  10 234  12 155 197  99   4  87 245  77 109  33
 244 107 184  42  11 213  55 131  83 105 195]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7461
INFO voc_eval.py: 171: [2107 2125 1227 ... 2159  165  348]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5974
INFO voc_eval.py: 171: [160 139 232 151 490 804 173 168 262 244   7 748 331  79 689 347 805  47
  82   6  80 481 751  98 427 523 245 711 468 452 652 749  81 536  85 263
 551  84 301 290 268 190 600 806 644 843 246 591 808 182 193 747 187 492
 209 650  64 554 407 305 158 552 807 730  63 715 745 302 651 822 539 138
 304  91 820  52 654 264 496 241 556 378 253 172 818  33 695  90  86 659
 649 147 206 559 156 398 363 397 103 756 726 385 681 816 599 467 754 653
 845 350 429  83 380 524 247 431 475   8 738 786 323 101 463 106 824 750
 459 226 320 100 157 735 617 663 755 135 655 343 519 544 736 558 828 493
 399 428  11 717 353 593 690 252 660 483 188 864 382 555 723 585 469 513
 114 572 430 708 403 871 456 142 829 758  40 788 348 381 842 473 564  70
 814  35 567 727 412 400 516 272 145 465 198 458 169 662 413 155 574 861
 688 566 163  68  75 587 297 847 541 597 625 446 191 267 291 484 118 706
 739 830 543 352  13 376 259 793 521 105 763 207 827 776 684 569 334 127
 307 237 817 687 664 354 740 557 185 594 476 192  54 701 863  67 759 108
 164 580 123  76 202 606  34 846 494 455  10 825 148 865 826 686 584 255
 514 497 472 317 270 512  27 637 610 250 288 586 560  88 162 392 685 665
   4 850 819 608 242 365 673 165 577 656 294 322 520 752 208 442   0 700
 757 709 249 257  99 466  93 753  32  45  16  89 296 515 254 435 200 857
 218  58 742 408 369 658 668 248 671 306 195 269 140 419  61 373 396 718
 453 802 645 260 316  94 848 812 518 303 720 835 314 548 761  15 319 409
 356 530  41 511 638 448 712 836 562 274 159 113 678 775 239 271 402 781
 480 462 870 436 295 201 549 216 787 505 852 782  49 770 386 856 790   2
 265 855 406 509 785 415 215 694 225 309 607 642 604 221 648 501 809  97
 675 611 764 666 657 677 447  57 683 531 150 351 364  59 141 131 869 488
 324 719 499 767 596 545 470 800 321 460 479 197 426 746  87 489 570 410
 603 741 704 670 844 508  31 389 487 601 344 703 714 529 451 811 500 312
 773 136 205 229 810  55  66 732 266 134 133 411 337  51 502 634 449 154
 333 256 605 792 534 841 803 128 777 258  53  20 612 823 498 702 240 795
  14  12 661  65 598 477 629 111 721 707 210  18 872  25 485 371 778 454
 691 595 326 571 859 330 289  73 724 866 293 433 769  26 420 120 357 528
 391  69 234 550 261 178 102 438 590 416 821 813 583  71 646 507 125 461
 336 424 311 362 434 110 342 358 132 839 219 152 784 275  19 340 116 339
 729 189 799 327 231 696 614 647 143 834 853 526 582 375 404 789 181 716
 285 867 635 119 107 491 578 542  24 450 153  38 592 860 772 222 579 130
 710 640 184 349 332  56 423 287 421 692 626 506 122 633 801 815  44 355
  74 618 441 212 368 238 196 615 851  60 862 171 203 553 744 211  17  42
 235 588 547 313 669 722 387 832 214 251 223 643 370 630 390 308 117 394
  77 112 149 213 471  28 535  22 733  62 146 310  50 418 279 831 179 273
 622 621 464 613 537 538  23 779 345 609 674 374 639  36 194 366  48 771
  78 104 734 858 377 525 561 383 693 632 682 546 300 170 439 482 437 367
 705 440 680 144 589  37 224 573 627  21 503 667   3 780 794 233 791 713
   5 510 217 276 563 286 243 315 495 765 474 280   1 395 361 854 445 338
 697 527 384 126 417 230 774 849 443  95  29 379  92 783 121 728 628 328
 624 837 532 329 299 576 277 183 797 517 679 161 636 616 292 176 177 432
 486 204 725 504 283 833 284  39 840 699 568 278 798 743 540 236 444 199
 180 175 281 731 335 360 401 388 422 478 533 565 220 768 620 166 359 186
 115 672 737 575 393 602 227 228  72 137 676 414 109 174 838  43 631 581
 325  30 372 124 796 167 282 868   9 766 425 762 760 298  96 623 457 641
 346  46 341 619 318 129 405 698 522]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5788
INFO voc_eval.py: 171: [47  5 28 46  0 39  7 41 44 43 59 66 61 27 60 24  8 16  2 49 54 32 18 11
 48 30  3 55 10 29 17 21 13 40 14 34 58 56 42 19 57 51 52 31 64 33 63 38
 12 15  6 50 45  9 36 22 62 25  1 53 37 20 65 23 35 67  4 26]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2909
INFO voc_eval.py: 171: [ 58  59  21  89  57  32 141 104  88  49   3  67  98  25  84  30  65  61
 128 124   4  75  85 111   9  94 134  47  44  23 116  43  96  27  24  72
  37  87  93  95  38  16 131 138  77 144  86  71  20  53  15  40  17  91
   0 127  41  10  69 139  50 136  79  63 126 117  66  34  74  62  83 129
  70  31 125  76 103   7 122  51  56  35  13 143 106  29   1  36  48   2
  42 112 118  97  52 133  90  11 140  68   5  39  82 100  46 142  78  26
 120  81  19 109  92   6 110  64  60  22  28  80 101 137  99  73 115 113
 121 123  33  14  55   8 114 119 130 132 108 135 102  45  18 107  54  12
 105]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3531
INFO voc_eval.py: 171: [107  17  67  38  14  56 137  27  77  58  91  35  13 108 134  79 146  37
  73 149   6 141 121  55  78  12  57  90  15  36  60  64  22  70 123  98
  19  53  41  16  94 101  89  31 155  83  71 119 117 151 105 118 100 129
  25  59 139  52  81 142  33  29  30  39  72  48 122 104 113  95  43 128
  84  46 127 115 138   5  97  61  88  69   4  68  93 156  40 103  63  99
   0  75  85  34  11 124 126 133 153   8   1 114  47 145 132  28 131  26
  80  18   7 110  24 143  65  44  42  96  51  87 135  50  54 109 102 152
   2 120 140 147  74  92  82 106  21 136  10   3  23 154  20 150 144 148
  66  32 125 157 111  86  76   9  45 116 112  62 130  49 158]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3640
INFO voc_eval.py: 171: [ 90 150 223 238 211 301 329   3  26 180 138 288 325  19 171 239  57  84
  38  23 309 181 244 120 246  72  86  42 187 221 277 224 197 234 336  53
  76 119  11 226 212  56 331 245 231 210  55   1  41 183 134 111 121  60
 292 240  58 155 229 256  65 339 264 105 165 344 130 184 310 209  45 257
 254 340 278 311  35 261 286  83 258 345 320  22  54 126 160 230 327  85
 106 291 312  25 262 263  61 164 323  88 196 241 146 294  77  52 115 284
 260  24  48 349 235 275   4 280 186 118 145 154  98 321  74  64 326 266
 255 243 174   7 185 162  16  71 343 147 108 298 148 214  31 137 192 206
 208 216 322 342  92  51  33  96 317  17 273  63  73 156 307 142 140 116
 281  20 123 314 178 132 193  28 215 168  95 337 182 300  27  89 207 289
 131  46 293 313   9  97 109  70 265 191 233 124  39 282 303  80 346  59
  43 204 270 139  62 283 297 194 324 104 101 249   5 177 133 103   8 175
 122 195 166 189 319  14 347 228 305 237 276 348  75  10 113  44 198 328
 259 153 296 129  81 341 330 152 158 222  18 200 201 173 199 332 251 141
   6 287 268 203 295 172 279 144 170 272 107 151 232  67 306 269 351 169
 128   2 143 176 190  29 220 159 167 179 253 274 252  68 102  79 117 242
 304 248 334 267 335 114 290  69 149 250 227 188 236 318 112  49 225 302
  13  66 350  32  30  93  78 110 213 136 333  12  47  82   0 299  99 161
 285 157 217 127  34  37  50  21  40 100 315 202  91 316 219 163 125 247
  15 218 205 135  87 338 308 271  36  94]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4461
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3973
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.360
INFO cross_voc_dataset_evaluator.py: 134: 0.601
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.323
INFO cross_voc_dataset_evaluator.py: 134: 0.579
INFO cross_voc_dataset_evaluator.py: 134: 0.585
INFO cross_voc_dataset_evaluator.py: 134: 0.429
INFO cross_voc_dataset_evaluator.py: 134: 0.002
INFO cross_voc_dataset_evaluator.py: 134: 0.493
INFO cross_voc_dataset_evaluator.py: 134: 0.180
INFO cross_voc_dataset_evaluator.py: 134: 0.387
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.362
INFO cross_voc_dataset_evaluator.py: 134: 0.746
INFO cross_voc_dataset_evaluator.py: 134: 0.597
INFO cross_voc_dataset_evaluator.py: 134: 0.579
INFO cross_voc_dataset_evaluator.py: 134: 0.291
INFO cross_voc_dataset_evaluator.py: 134: 0.353
INFO cross_voc_dataset_evaluator.py: 134: 0.364
INFO cross_voc_dataset_evaluator.py: 134: 0.446
INFO cross_voc_dataset_evaluator.py: 135: 0.397
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 29999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.368s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.349s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.359s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.357s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.365s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.363s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.362s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.363s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.363s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.362s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.361s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.362s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.365s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.452s + 0.001s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.415s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.395s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.383s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.379s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.380s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.384s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.380s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.377s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.377s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.376s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.378s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.374s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.504s + 0.002s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.419s + 0.001s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.384s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.380s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.376s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.375s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.375s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.373s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.372s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.368s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.368s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.369s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.369s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step29999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.491s + 0.002s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.352s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.352s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.352s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.354s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.349s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.353s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.354s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.356s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.357s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.356s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.356s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.356s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.867s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [230 178  90   5 286  57 219  17  45  50  49 143 121  59 293 172 168 135
 142 314  44 181   1 119 306 179  46 315 257 301 261 269  36 102  18  70
 130 149 113 239  82 187 125 207 281 134 148 116 271 150  28 264  79 291
 227  84  29  78 246 110 243 312 145  48  53 276 169 298 126 209 157 318
  81 139 146  35 101  95 241  66  60 262  91 268 273  11 287  71 131 224
 204 214 129  69 305  51   3 231 163 203 136  37 162 104  76 284 313 109
 176  16  55 307  33 198 202 309 240 228   6 184  54 171 159 322 248 270
  10 252  21  20 188  41 132 317 289 165  72 292  25 106 283  43   2  64
 215 218 275 255 216 103 153 100 174 254  96 200  86 221 277 229 319 217
  40 186 210   4 123 156 151 244 183 234  31  30  85 140  32 302 279 161
 294 160 196 278 164 299 249 167 233  14 222 212 303 321  42  22 122 189
 206 266  24 115 304 232  94  52  88 111 114  23 247 253  62 144 208  89
 236 245  73 133 256  87 182 120  68 272  58 251 300 127 154 170 190 282
  80 242 288 147  19  15 323  39  61 226  92 274  13   9 141 235 265  77
  83 173  74  97 296 311 211 191 213  34 118 197  12 225 192 107 280 310
 237   8   7 285 263 152  26 316 201 297 128  65 308  93  98   0 193 117
 175 185 194 238 105 290  56 250 137 138  27 258 260  67 205 259 112 267
  75  63 223  38 158 295  99 177 320 220 180 108 166  47 199 124 195 155]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3600
INFO voc_eval.py: 171: [178 116  25  50 205 214 220 154 203 123  20 216 107  26  88 204 134  14
 174 164 157  24 206 190 152  91 202  81  60 113 150 140  13 218  49 162
  65 207 110 176  84 144  34  10 211 189  39  72 226 221 135  33  90 186
  96 219  92  94 160 172 158 208 141 195 112  11 111 222 124  36  80 132
 175   2 118 128 125  85  93  27  86  87  40  35 139 191  89 166  21 200
 156 215 108  31 114  79   7 168 121 100 173  52 143  58 122 180  53 232
  78  69 159 225 198 101 171 212 193 229 126  76 179  66 184 169 217 197
 115 136 130 120 109  19 228 199 188 196  43  98 127  75  48 105  64  45
 231 155  12 224  56  15 148 167  77 163  29  99 145 106  74 133  18 227
 103 117 138  67  54  83 147 187 185 119  97   5  55  37  42 201  82  38
 223 233  17   1  16 129 131 177 170  28  51 104  95 181  32  57 149 230
  30  46  70  44   0  23  22   4 194 137   6  61  62 165  71 146 213  47
 210 183  63 151   9 142 182 192 102  73  41   8 153 161  68 209   3  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6009
INFO voc_eval.py: 171: [ 61  26  80  48  49 122  67  51   1  77 148 167 150  58  59 168 169  50
 192   0  56  36  30 130 149  96  97  55  75 185  79  83 197 158 136  57
 111  34 184  70 105 156 141  23  60 110 128  15   4 188  54 171 139 121
 112 133  63   9 176  82  78 170  68 181 173 102 145  69 134  38 143  89
 123 131 180  64 137  18  92 196  72  28 151 162 155   8 144 194  20  17
 152 177 166 135 101 159 182  53 142  27  43  71 119 195 106  46 109  74
  47 160  14  11 186 140  16 187 146 103 172  93  33  41  35 104 163 157
  10  39  66  94  40  22 115  44  91 129  37   3  62 175  99 179 165 138
  42 161 114  87 183 108  52 100 164 107 117  12   5 190 132 124  95  21
   6  31  85 174 147 120 189  13  88  25  19  90  98 127  24  45  86 113
  76   7  73 126 154 118  29 125  84   2 153 193  65 178  81 116  32 191]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2077
INFO voc_eval.py: 171: [311 371   0 312 102   9  48 313  82 254 135  28 347 245  53 455 345 253
 364 397 243 114 250 134  81 116   8  24 190 113  96 443 259 180 450 314
 252 185 165 255 398 372 469 227  83  36 189 304 301  97 293 200 391 117
 362 379  27 292 395 499 440 103 338 232 335 393 449  61 389  31 158  37
 222 236 471 290 208 387 256 267  62 473 240 148 318 340 427 166 448 407
  29 405 174 251 242 238 308 168 353  17 374 417 459 467 233 264 202 288
 476 423 144  63 167 413  41 261 390 122 182 258 392 378 106 184 269 307
 424 291 110 452 163 309 480 400  26 436 162  88 225 493 201 376 186 137
 425 153  38  40 287 483 150 462  35  46  65 418 194 344 286   7 176 277
 138 265 350 486 164 322 270 375 127 411 324 381 257 339 380 279 482  59
 140 496 204 359 211 228 410  75 121 464 354 472 154 470 210 408 221  39
 178 214 212   4 139 439 498  87 491 402 104 275 239  84  47 235 466 196
 303 435 451 220  89 280  52  42 105 262 447 177 160  25 495 342 357 248
 169   2 206 323 282 384 337  10 296  71  68  67 456 108 377 401  30  23
 442 247 365 485 131 326 320  64 463 141 123 349 477 468  58 173 284  54
 373 133 118 147 203  72 120 444 274 146 205 213 155 351 334 316  34 112
   1 237 299 367 432 295 446  86  16 231 484 355  57 289 431 302 474 229
 420 283 489  19  49 183 230  20 143 416 492 404 215 273 327 430 348 438
 191 315  74  95 429 241 394 219  69 193 333  33 453  80 100 224 192 276
 406 488 159 460 457 434 207 119 195  56 145  43 156 361 437  77 419  44
   6 172   3 412 310 101 369 234 125  14 487 109 382 306  21 445  60 126
 294 188 111  12 260 421  94 415 461  66  32 217 285 346  90  91   5  76
 328 130 175 170 325 157 363  85  70 115 426 358 179 300 360 478 403 409
 479  11 198 181  18 272 278 128 171 136 321 475 386 107 383  93 298 223
 268 330  50  22 458 149 142 368 151  98 494  78 428 490 297  99  55 244
  13  79 152  92 249 161 331 317 454 266 385 332 370  15 132 396 352 216
 187 481 329 199 281 422 343 129  45 263 388 341 497 336 366  73 356 218
 414 271 124 465 226 441 209 319  51 433 246 197 399 305]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3231
INFO voc_eval.py: 171: [451 452 245 457 208 453 253 211 115 310 559 375 370 454   3 560 209 207
 252 196 353 281 130 354 329 210 465 371   4 197 369 376 282 464  83 529
 372 479 456 499 441 523 471 459 458  82 176 500 396  33 466 455 269 485
 462 472 504 476 379  15 367 161 214  61 490 492  30 425 256 127 188 374
 508 255 403 356 293 540 444 377 460 149 158 263 487  80 202 378 304 138
 387 145 315  49 393 317 494 240 183 190 489 280 552 307 380 285 273 170
 392 404 400  48 316 215 470 319 236 543  11 261 278 160  50  22 226 545
 179 309 382 407 415 427 107  23   9 397 486 517 355 233   6 199 547 298
   5 495 213 359 399 527  38 232 562 257 565 368 325 344 506 366   7 513
  14 126  17 291 254 124 406 546 321  94 491  84 440 389 120 519  28 437
 503 469 276 435  32 283 143 212 414 530  40  31  92 467 461 428 362 227
 189 296 390 481 445 165 159 477 401 420 206 386  86 384  99  20 438 557
 259 419  43 286 338   8 388 339 294 132 521 186  26 217 146 525 262 131
  19 234 322  37 277  64  78 520 244 505 164 287  79 201 411 408 220 446
  21 147 468 534 250 279 284  75 512 450 442 424 357 365 264 155 551  88
 364 337  35  62 334  74 498 558 515 320 410 531 363   0 151 475 116 133
 541 431 301 258 200  57 324 247 153 204 305 426 561  63 312 394  59 417
 219 192  85 260 302  10 241  45  68 332 532 463  46 198 225  29 268 167
 502 142  98 554 416 314 421  34 566  65 221 172 242 346 405 224 383 533
  69 290  71  90 549 168 223 330  72  76 537 154 122 350 402 395 238  16
  13 218 360 413 125 292  52  87 121 480 173 249 141 422 373  73 507 118
 514  66 568 205 195 182 333 430 270 105  36 544 216 177 175  56 148 497
 267 409 274 156 101 550 447 139 135 335 567 563 193 448 482 538 166  93
  18 246 181 275 137  97 251 341 518 303 536  95 318 228 385  77 548 185
 243  12  54 331  91 511 484  70 313 516 171 432 128 443 501 555 326 239
 483 113 474 539  51 391   2 308 140  67 191 311 265 352 271 412 488 343
 328 478 553 349 110  39 157 136  96 300 348 564 323 496 522 473 528 231
 342 203 439  25 106 108 535 436 222  41 187 102 194 103 178 398  89 493
 150 163 119  60 297 272 358 295 526  44 542 162 235 230 109   1 340 509
  58 418 434  42 510  24 288 174 114 345 111  55 237 327 123 306 180 556
 289 347  27 184 100 229 104 144 299 524 336 112 117 169  81 266 423 351
 248 129 381 134 152 429  53  47 433 449 361]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5794
INFO voc_eval.py: 171: [ 31  14  50  53  76   3  58  23  64  13  94 120 127  83 129  90  73  79
  28  88  62  98 102  37  78 121  66   8  81 128   2  57  87  89  54  61
   1 136 101  74  91  19  99  22  20  38  46 115 116   6  24  17  52 114
  84 106 124   7  47  39 123 105  26 107   9  95  85  32 130  18  82  67
 112  11 108  49 118 133  10  96   0  77 119 134  97  15 100  25  33  72
 131  69 132  55  44 117 111  75  63  42  40  48  60  51  12  59  16  21
 113  29  93 126 110  41  70  30 122   4 103 135  45  34  35 104  56  43
  36   5  71  65 109  92  68  86  27 125  80]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5847
INFO voc_eval.py: 171: [323 344 142 502 428  74 139 248 470   9 460  98 167  50 471 348 285  15
  21 492 224 450 472 373 203   2 230 463 260 225 244 370 189 424 448 255
 375 166  97 127 380 372 399 192 484 431 434  32 188 441 283  85 250 335
 459 516 293 324  86  26  52  94 200 476  55  16 278 257 303 252 218 190
 315 191 253 371 159 350 402 286  31 268  46 207 309 352 222 453 322 180
  11  23 179 269 292 359 258 456 325 435  27 447  35 306 154 148 494  49
 481  60 439 266 403 475  63 486 171 102 184 307 423 298 389 170 101 261
 213 125 313 466 390 232 186  71 128  30 109 509 263 405  90 231  48  78
 182 254 462 506 119  45 444 382 455 256 163 436 358 416 117 515 198 478
 317 393 195 227 221 491 194 164 216  37 262 274 319 140 196 430   0 332
 201 333 151 469 291 223 458 275 265 504  25 150  22 480 437 214  38 267
  28 467  40  12 331 239 432 177 108 452  24 136  20 173 104 330 210 158
 295 147  33 287 501  51 385 308 497 304 461  96 259 349  47  36  61 427
 422 281 134   5  14  70 277 120  29 474 336 367 280 505  59 414  57 316
 488 445 483 199 508 226  83 197 302   8 321 185 236 340 141 183 299 404
 178 457 369 503 477 482 391 165 396 205  89 419 215 498 234 408  10 310
 212 379 351 392 338 220 383 357 415 342  77   7 235  82 131 241 500 251
 454  42 446 510 517 118 296 160  72 355  69  34 512 233 110 174 284 105
 401 155 417 326 438 429 209 360 116 217 219 124  95 270  43  62 288 144
 271 368 100  19 240 334 418 169 107 175 106  66 153 176 156 346 345 353
 320  44 187 145  73 202 425   4  17 132 168  99 343 395 115 449 122 412
   3 300 464 149 123 289 421 495 294 133 238  56 347  53  13 276 327  58
 364 490 138 206 362 384 487 433 114 112 407 121 499 247 442 513 311 339
 376  76 388 337  64 400 297 377 366  18  41  92 172 103 507 514 146 443
  39   1 111 113  65 181 245 152  87 301 161 193  84 378 411 465 397 126
 386 135 511 279 493 410 409  67  54 211  91 228 282 246 137 354 479 129
  81 451 229 130 162 305 426 242 473 420 406 318  79 143 272 314 312 381
  68 356 341 361   6 204 413 237 374 290 485 387 365 363 468 394 440  75
  93 398 328 329 243  88 489  80 249 208 264 496 157 273]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4292
INFO voc_eval.py: 171: [ 4  2 11 36 12 18 41 30  0  3 24 14 15 17 32  7 23 21 29 22 40 39 37 34
  5 10 42  1 25 33 38  9 19 16 27  8 13  6 26 35 43 31 20 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0023
INFO voc_eval.py: 171: [1598  385 1599 ... 1296 1645 1248]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.4933
INFO voc_eval.py: 171: [27  2  0 52 40 22 20 47 14 61 56 16 19 37 11 41 33 51 58  4  1 50 60 44
 64 13 34 24 30  3 26 49 46 18 28 10 21  5 57 59 43 31 48 29 12 42 23 53
 15  7 39 25  8 54 35 55 17 38  6 63 36 62 32 45  9]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1804
INFO voc_eval.py: 171: [358 162  87 155 309 175 234 228 215 233 139  49 315 164  33 127  68  76
 331  16  15 273 199  34  47 319  75 378 158 360 314 297 159 298 138 225
 308 244 272  90   1 332 379 328 255 384 243 231 321  27 108  22 181 268
 293  88  24 133 376 251 106 179 339  64 345 307 236  79 148 266  61 204
 354  20 393 403 374 140 316  62 269 380 310  43 329 249 124   7 397  69
 399 232 322 152 223  23 198 141  55 350 126 229 367 377 274 361 137 131
 163 222  95 363 335 120 389 161  13 386 405 104 259 113  14  10 289 279
 340 200 128 333 383 254 110  39 107 116 136 213 291 283  91 334 280 129
  17 125 230 267 391  97 370 135 177 373 301 190 245 176 348 226  58 351
 336 265  35 299 371 375 337 346 216 194 323 103 390 221 327 147  71 344
 396 366  78 134 290  32   4 302  82  59  48 256 318 355 304 294 368 241
 252 285   8  19 247  11  93 235 153 404 101  12 395 174 347 381 172 143
  67 206 165   3  83   0 385 353 387  66  46 132 212  85   6 324 187  53
 287 400 392  84 369 109 365 220 178 202  57  94 184 317 211 191 382  44
 114 282 188 320 100 313 115 145 306 342  36  38 192  60 195 401 257 246
 278  65 151 263 330  30 218 209  28 227 262  29 292 118 364 286 398 171
 281 394 388 276  96 261 189  45  74 146  72 157  51  98  92 214 102 240
  41 264 237 149 168 349 356 253 277 122  63 250  80 183 352 205  37 296
 372 238 197 219 326 180 142  40 196 201 312 154 325 117 105 207 362  18
  31 258 182  73 275  99  52 156 185 186  81 260   5 217  21  25 295 144
 166 130 123 121 305 169 167 170 343 288  89 341  50 208 270 160 111 210
  26 242 119 150  42   9  86  56 359 402 203 338   2  54 173 406 303 311
 284 248 193 300 112  77 224 271 239 357  70]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3863
INFO voc_eval.py: 171: [ 13  86  26  29  73  51  52  46   4   1  20  30  49  78  18   0  66  47
  61  40  95  94  96  43  98  27  92  50  99  10  14  33  79  22  89  68
  39   2  42  91 100  71  76  67  88  83   7  82  21  17  70  85  53  60
  59  23  65  11  57  12  87  63  56  34  64  84  97   3  55   5  19  58
  24  93  75 101  31   8  37  48  80  54  72  35  16  28  32   9   6  44
  41  15  77  36  62  45  25  81  90  38  74  69]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0599
INFO voc_eval.py: 171: [64 75 55 57  8 11 80 78 43 52 42 25  9 45 54 68 44 65 73 71 59 12 77 58
 60 10 33 49 72 39 19 30 20 36 76  6 14 24 17 40 66 23  4 47  3 50 18 63
 56 69 62 41 31 22 29  2 35 27 61  1 74  5  0 37  7 48 34 67 28 46 81 38
 26 21 79 32 53 70 82 13 16 51 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3619
INFO voc_eval.py: 171: [ 30 242  22 151 104 216 158  34 254  15 187   8 179 111 159 172   7  85
 247 177  51  31 176  96 209 206  95 230  46  57 152  70 164 136  61  94
  74 138 115 118 178  88 258 137 229  19  45 161 212 144  97  58 146 106
 250   5 121 186 196 125  68 100 215  64 117  16  43 181  39 171 192 168
 190  21   1  20  52 194 252 201 203 214  81 223  92 180 128  25  84  63
 231  66 191 188  27 130 236 199 156  41  36 248  76  90 112 119 208 235
 259 124 211  26   2 217 222  29 173 220  72  56 255  35 113 122 145 261
 207 142 228 114  37 237  38 233 221   9  86  80 102 165 162 108 169  65
  13 170 134  40 183  32  71 132 160  47  53  91  44  60 174 241  24 185
 240 120 182 262 163 224 210 198 219 256 135 232 204  82  18 126 139  89
 150  14 253  50 154 143 202 127 246 101 123 175  98 238  73  23 110  75
 141  79  28  54 103 249   0 129 227 133 149 251 225 218  62 200 193  69
 153 167  59 116 148 166 157 140  67  17   3 226 260 257   6 147 243 239
 205  93 189  49  78  10  48 234  12 197  99 155   4  87 245  77 109  42
  33 244 107 184  11 213 195  55 131  83 105]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7592
INFO voc_eval.py: 171: [2111 2129 1227 ... 2164 1472  729]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5975
INFO voc_eval.py: 171: [161 140 152 233 493 806 174 169 263 245   7 750 333  79 349 691 807  47
  82   6 484 753  80  98 430 526 246 265 713 471 455 654 751  81 539  85
 554  84 303 292 267 191 601 808 646 845 247 592 810 183 194 749 188 495
 210 652  64 557 307 409 159 555 809 732  63 717 747 304 653 824 139 542
 306  91 822  52 656 499 559 242 380 254 173 820  33 697  90  86 661 651
 148 207 563 157 400 365 268 399 103 757 728 387 683 818 470 755 600 655
 847 352 432  83 382 527 248 434 478   8 740 788 325 100 466 106 826 462
 752 227 322 102 158 737 619 665 136 345 657 756 522 738 561 547 830 496
 401 431  11 719 355 594 186 692 253 662 486 866 189 384 558 725 586 114
 472 405 575 433 516 710 459 873 143 831 759  40 790 350 383 844 476 416
 567  70 816  35 571 729 402 414 146 272 199 468 519 170 461 664 156 576
 863 690 569  68  75 164 299 588 849 544 570 192 627 598 449 118 708 293
 832 487 741 546 378  13 354 260 105 524 795 778 764 208 829 573 686 336
 127 819 309 689 238 666 356 560 742 595 479  54 193 703 865  67 165 108
 760  76 203 581 123  34 608 848 458 497 827  10 149 867 688 828 585 517
 256 500 475 639  27 319 270 251 515 275 612 587 290 562  88 394 163 687
 821   4 852 610 667 675 166 578 243 367 658 296 324 523 209 445 702   0
 758 711 250 258  99 754 469  93  16  32  89  45 298 255 518 438 744 673
  58 670 196 201 859 308 249 219 410 660 371 141 269 398 422  61 456 375
 720 804 850 318 647 261  94 814 521 305 722 321 551 837 762 316  15 533
 358 411 514  41 451 640 839 565 274 714 160 113 680 404 271 483 777 240
 465 783 439 202 297 872 552 789 217  49 508 771 854 784 388 792 264 857
 858 408 226 787 418   2 696 216 512 311 609 504 222 644 605 650 677  97
 811 613 765 668 450 534 366 685  57 659 679 151 353 142 133  59 473 502
 871 721 491 326 548 802 597 768 748  87 482 198 323 463 429 574 492 412
 511 705 604 672 846 706 743 391 716 346  31 602 490 503 813 532 454 314
 206 137 775 266 135  55 734 812 230 413  66 420 134  51 339 155 505 452
 636 607 257 843 794 537 805 335  20  53 779 129 259 704 825 501 663 599
  65 211 241 614  12 631 709 797  14  18  25 480 111 723 780 596 693 874
 457 488 726 868 861 291 373 295 328 332  73 119 553 770 235  26  69 423
 531 359 101 591 393 415 464 179 823 124 436 262 441 338 360 648 584 510
  71 815 220 419  19 110 427 344 437 786 132 153 313 342 801 232 116 364
 329 841 190 731 341 649 698 836 616 144 855 583 377 182 529 406 637 791
 579 453 120 494 545  24 107 718 286 869  38 773 154 862 712 593 580 223
 131 628 426 351  56 642 185 288 334 424 509 817 357 803 635  44  74 694
 122 239 213 620 617 212 197 444 853 746 864 172 204 370 556  60  17 236
  42 671 589 550 315 252 215 389 834 724 224 645 150 392 214 632  77 117
 310 112 396 372  62  28  22 277 735 538 273 615 474 467 833 624 623  50
 312 180 421 147 774 280 195 540 376 676 541 347 611 368  23  48  36 641
 104 634 528  78 440 736 442 781 379 695 385 684 860 772 171 302 485 549
  37 225 682 564 707 145 443 369 793   3 796   5 782  21 506 513 590 715
 234 629 669 218 477 281 287 566 498 530 363 606 856 397   1 766 276 317
 448 244 776 699 340 838  95 851 126 278 231 381 446  29 331 535 785 184
 121 330  92 630 626 386 730 681 435 638 618 205 301 799 727 294 178 489
 177 162 520 842 701 745 284 285 568 572 835 279  39 800 543 237 507 181
 362 447 176 390 403 200 769 221 282 337 733 481 536 622 167 603 138 674
 187 175 361 678 115 228 577 229 395 739 425  72 128  30 417 582 327 840
  43 633 109 374 870 798 761 300 348  96 283 428 763 643 125 767 460 625
 168  46 289 320 525   9 130 407 343 700 621]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5798
INFO voc_eval.py: 171: [47 28  5 46  0 39  7 41 44 43 59 66 61 27 60 24  8 16  2 49 54 32 18 11
 48 30  3 55 10 17 29 21 13 40 14 34 58 56 42 19 57 51 52 31 64 63 33 38
 12  6 15 50 45  9 36 22 62 25  1 53 37 20 65  4 35 23 67 26]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2909
INFO voc_eval.py: 171: [ 58  59  21  89  57  32 143  88 105  49   3  67  98  25  84  30  65  61
 130   4 126  75  85 113   9  94 136  47  44  23 118  96  43  27  24  72
  37  87  93  95  38  16 133 140 146  86  71  20  53  15  40  91   0  17
 129  41  10  69 141  50  79 138  77  63 119 128  74  66  83  62  34 131
  70  31 127  76  51 104   7 124  56  35  13 107  29   1 145  36  48   2
 120  52 114  97  42 135 142  90  11  68  39   5  82 100  46 144 122 102
  78  26 111  19  81 112  92   6  64  60  22  28  80 139 123  99  73 115
  55  33  14 117 125   8 132 121 116 101 134 103 137 110  18  45  54 109
 108 106  12]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3534
INFO voc_eval.py: 171: [108  17  68  38  14  57 139  27  78  59  92  35  13 109 136  80 148  37
  74 151   6 143 123  56  79  12  58  15  91  36  61  65  22  71 125  99
  19  54  41  16  95 102  90  31 157  84  72 121 119 106 120 153 101  52
 131  25  60 141  82 144  33  39  29  30  73  48 124 105 115  96  43 130
  85 129  45 117 140   5  98  89  62  70   4  69  94 158  64 104  40 100
   0  34  76  86  11 135 126 128 155   8   1  46 116 147 134  28  18 133
  26  81 111   7  24 145  42  66  44  97  88 137  53  51  55 110  50 103
   2 154 122 142 149  75  93 107  83  21 138  20  23 156   3  10 146 150
 152  32  67  47 159 127 112  87  77   9 118 114  63 132  49 113 160]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3641
INFO voc_eval.py: 171: [ 91 151 224 239 212 302 330   3  26 181 289 139 326  19 172 240  58  85
  38  23 310 182 245 121 247  73  87  42 188 222 278 225 198 235 337  54
  77 227 120  11 213  57 332 246 232 211  56   1  41 184 135 112 122  61
 293  59 156 230 257 340  66 265 106 166 345 185 131 311 210  46 258 255
 341 279 312  35 287 242 262  84 259 346 321  22  55 127 161 231 328 107
  86 292 313  25 263 264  62 165 324  89 241 197 147  78 295  53 116 285
  24 236 261 350  49   4 276 281 146 119 155 187  99 322  75  65 267 327
 244   7 256 175 186 163  16  72 148 344 299 109 149 215  31 193 138 209
 207 217 343 323  33  52  93 318  97 274  17 157  74  64 143 117 308 141
 282 124  20 315  28 133 194 179 216 169  96 183 338  27 301 208  90 132
 290 314 294   9 266  98 110  47 234 192  71 125  39 283 347 304  81  60
  44 298 105 284  63 271 205 140 195 250 325 102   5 134 178   8 104 176
 123 196 190 167  14 320 348 229 306 238 349  76 277  10 260 199 329 114
  45  82 342 130 154 297 153 331 159 223 201 202  43  18 174 200 333 252
   6 269 204 142 296 173 145 171 280 288 273 108 307 152 233  68 270 352
   2 144 177 191  29 254 129 160 168 180 221 170 253 275  69 103  80 243
 115 118 268 335 291 305 249  70 251 150 228 336 189 113 226 319 303  50
 237  32  30  94  13 351 137 214  67 111  79 334  48  83 100   0 286 218
 158 162  12 128  34  37 300  51  21 220  40 101 126 203 164 316  92 317
 136 219  88  15 248 206 339  36 309 272  95]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4476
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3981
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.360
INFO cross_voc_dataset_evaluator.py: 134: 0.601
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.323
INFO cross_voc_dataset_evaluator.py: 134: 0.579
INFO cross_voc_dataset_evaluator.py: 134: 0.585
INFO cross_voc_dataset_evaluator.py: 134: 0.429
INFO cross_voc_dataset_evaluator.py: 134: 0.002
INFO cross_voc_dataset_evaluator.py: 134: 0.493
INFO cross_voc_dataset_evaluator.py: 134: 0.180
INFO cross_voc_dataset_evaluator.py: 134: 0.386
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.362
INFO cross_voc_dataset_evaluator.py: 134: 0.759
INFO cross_voc_dataset_evaluator.py: 134: 0.598
INFO cross_voc_dataset_evaluator.py: 134: 0.580
INFO cross_voc_dataset_evaluator.py: 134: 0.291
INFO cross_voc_dataset_evaluator.py: 134: 0.353
INFO cross_voc_dataset_evaluator.py: 134: 0.364
INFO cross_voc_dataset_evaluator.py: 134: 0.448
INFO cross_voc_dataset_evaluator.py: 135: 0.398
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 32499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.475s + 0.001s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.364s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.378s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.374s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.379s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.377s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.373s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.375s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.373s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.373s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.370s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.372s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.374s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.433s + 0.001s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.370s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.369s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.360s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.358s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.360s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.371s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.367s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.366s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.366s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.366s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.368s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.367s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.368s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.366s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.367s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.372s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.364s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.366s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.362s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.365s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.364s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.364s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.364s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.363s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.363s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step32499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.439s + 0.002s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.361s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.349s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.348s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.352s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.352s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.353s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.355s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.357s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.356s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.355s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.356s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.354s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.885s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [230 178  90   5 287  56 219  17  44  49  48 143 121  58 294 172 168 135
 142 315  43 181   1 119 307 179  45 316 257 302 261  36 270  18 102 289
  70 130 149 113 239  82 187 125 207 282 134 148 116 272 150  27 264  79
 227  84  28  78 110 246 243 313 145  47  52 169 277 299 126 209 157 319
  81 139 146  34 101  95 241  59  66 269 262  91 274 288  11  71 131 224
 214 204  69 129 306  50   3 231 163 203 136  35 162 104  76 314 285 176
  16 109 308  54  32 198 202 310 240 228   6 184  53 280 171 159 248 323
  10 252 271  21 188  40  20 165 291  72 318 132 293  24 284   2  42 106
 218  63 215 276 255 216 103 100 153 254  86 174 200  96 320 229  39 278
 221 217 186 123 210   4 156 151 140 183  29 244 234  30  85  31 161 303
 160 295 196 164 300 279 233 167  14 249 222 212 304 322  41 122 189 267
 206  94 305  23 115 232  88  51 111 114  22 253 247  61 144  73 236 208
 133  89 245 256 120 182  87  68 273 301 251  57 283 242 190 127 154  80
 290 170  38  92 147  15  19 324  60 226 235 141  13   9 275  77 265 266
  74  83  97 173  33 312 211 191 297 118 197  12 213 311 107 281 225 192
   8 286 152  25 298 317   7 237 201 263 128 309   0  64  93  98 105 185
 193 194 117 238 292  55 175  26 250  67 258 138 260 137 205 112 259  75
 268 223  99  37 296  62 158 177 321 180 220 166 108  46 199 124 155  65
 195]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3589
INFO voc_eval.py: 171: [177 115  25  50 204 213 219 153 202 122  20 215 106  26  87 203 133  14
 173 163 156  24 205 189 151  90 201  80  60 112 139 149 217  13  49 161
  65 206 109 175  83 143  34  10 210 188  72  39 225 220 134  33  89 185
 218  95  91  93 159 171 157 207 140 194 111 110  12 123 221  79  36 131
 174   2 117 127 124  84  92  27  85  86  40  35 138 190  88 165  21 199
 214 155  31 107  78   7 113 167 120  99 172  52 142  58 121 179  53  77
 231  69 224 158 197 100  76 192 170 211 125 228  66 178 183 216 168 114
 108 196 129 119 135 227  19 198 126  43  97  75 187 104 195  48  45  64
 230  11 154 223  15  42  56 147 166 162  29  98 105  74 132 144 116  18
 102 226 137  54  67  82 146 186 184 118  37   5 200  96  55  38  81  16
 222 232  17 128 130   1 103  51  32  28  94 169 176 180 229  30 148  57
  70  46  44   0 193  22   4 136   6  23  61 164  62 145  71 212  47 209
 150 182  63 141   9 181  68  73 208 191   8 152 160  41 101   3  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6006
INFO voc_eval.py: 171: [ 61  26  80  48  49 122  67  51   1  77 148 167 150  58  59 168 169  50
 192   0  56  36  30 130 149  96  97  55  75 185  79  83 158 136 197  57
 111 184  34 156  70 105 141  23  60 110 127  15   4 188  54 139 171 121
 112 133  63   9  82 176  78 170  68 181 173 102 145  69 134  38 143  89
 123 131  64  18 137 180  92  72 196  28 151 162 155   8 144  20 194  17
 152 177 166 159 101 182  53 135   5  27 142  43  71 119 195 106  74  46
 109 160  66  14  47  11 186 140 187  16 103  93 146 172  33 163  35 104
  41  39  40 157  94  22  10  44 115  91  37 175  62   3  99 129 138 165
 179  87  42 161 114 183 164  52 108 100 117  95 132 190  12 107  21 124
 147  31   6 174  85 120  25  19  88 189  13  90  98  45 128  24 113  76
   7  86   2  73 154 126 118 125  29  84  81  65 178 193  32 153 116 191]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2077
INFO voc_eval.py: 171: [310 370   0 311 101   9  48 312  81 253 135  28 346 245  52 453 344 252
 363 396 243 113 250 134  80 115   8  24 190 112  95 441 258 180 448 313
 185 165 254 397 371 467 227  82  36 189 300 303  96 292 257 200 390 361
 116 378 291  27 394 496 438 102 232 337 334 392 447  60 273 388  31 158
 222  37 469 236 289 208 386 263 471  61 240 317 148 426 339 406 446 166
  29 404 174 242 251 238 307 352 168  17 373 416 457 465 233 202 262 286
 474 422 144 167  62 412  41 389 121 182 256 391 377 105 184 264 423 306
 290 109 450 163 162 434 399 308 478  26 225  87 490 201 375 186 137 424
 153  38  40 283 460 481 150  35  64  46 194 417 282 343   7 176 138 272
 349 261 484 321 164 323 374 410 265 127 380 379 338 274 255 285 480  58
 140 493 204 211 358 409 228  74 120 353 462 470 468 154 210 407 221 178
 212 139 214   4  39 437 401  86 103 495 488 270 239  83  47 464 235 302
 196  88 433 276 220 259  51 104 449 160  42  25 445 177 341 356 492 169
 206   2 383 336 278 248 322 295  70  10  67  66 454 107 400 376 440  23
  30 364 247 325 483 131 319 461 141  63 123 348 475 466 173  57 117 280
 133  53 372  71 147 203 146 442 350 269 205 333 111 119   1 213  16 315
 155 431 354 298 444 294 231  85 237 366  34 430  56 482 288 472 419  19
 301 229  49 183 230 279 143 415 326 268 429 215 489  20 403 347 191 314
 436  94 428 241 393  79 193 332  73  33 219  68 451 287 271  99 224 192
 405 458 455 486 159 145  55 195  43 207 156 360 118 432 435 418  76   6
 100 172   3 411  44 309 125  14 485 108 368 234 305  21 126  59 188 110
 381 443  12 459  65  32  93 414 293 420 345 327  75   5 281 217  89  90
 324  84 170 130 362 157 175 425 179 114 299  69 359 122 477 476 357 408
 284 402 198 267  18  11 181  92 171 297 136 382 385 275 128 106 473 320
 223 329 456 491  50  22  13 296 487 244  77  97 367  54  98 149 142 427
 161 151 152  78 330 331 249 452 369 316  91 132 351 384 216  15 395 260
 479 129 199  45 340 328 342 187 277 494 421 413 218 387 355 439 463 209
 124 266  72 226 335 365 304 197 318 246 398]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3212
INFO voc_eval.py: 171: [452 453 246 458 209 454 254 212 116 311 376 560 371 455   3 561 210 208
 253 197 354 282 131 355 330 211 466 372   4 198 370 377 283 465  84 530
 373 480 457 500 442 524 472 460 459  83 177 501 397  33 467 456 486 270
 463 473 505 477 380  15 368 162 215  62 491 493  30 426 257 189 128 375
 509 256 404 357 294 541 445 378 461 150 159 264 488  81 203 379 388 139
 305 146 316  49 318 394 495 241 184 191 490 553 281 308 381 286 274 171
 405 393 401  48 317 216 471 320 237 544  11 262 279 161  50  22 227 546
 180 310 383 408 416 428 108  23   9 398 487 518 234 356 200   6 299 496
   5 548 214 400 360  38 233 563 528 258 566 326 369 345 507 367   7 514
  14 127 255  17 292 125 407 547 492 322  95  85 121 390 441 520  28 504
 438 470 277 436 284 213 144  32 415 468 531  40  31  93 429 462 363 190
 228 391 297 446 482 160 166 478 402 421 387 385 207  87 100  20 439 558
 260 420 287  43 339 389 340   8 133 522 187 295 263 147 526 218 132  26
  37 235  19 323 278  65 245 521  80 506 165 288  79 221 409 412 202  21
 447 251 469 148 535 285 451 280 513  76 425 443 265 358 366 552  89 156
 338  35  63 335  75 499 559 321 411 516 365 152 532   0 117 476 542 134
 201 432 259 302 205 325  58 248 154 427 306 395 562 313  60  64 303 418
  86 242 193 220 261  10  69 464  46  45 533 226 269 168 199 503 555  29
 333 417 567 243 422 315  66  99 143 173 347 222  34 406  91 225  70 384
  72  73 331 291 534 550 224 169 123 155  77 538 351 239 396 403  16  13
 219 361 126 414 481 293  52  88 250 142 122 174 423 119 508 515 374  74
 206  67 569 183 196 431 271 106 334 217 545  36 149 178  57 176 157 268
 140 410 498 275 102 551 448 136 564 449 568 194 336 483  94 247 539 167
  18  98 182 252 276 138 342 304 519 537 386 549 229  12 319 186 244  78
  96 332 517 529  54 314 512 172  92 485  71 433 444 475 556 129 502 114
 240 484 327  51 392 540   2 312  68 192 141 364 309 350 489 272 344 266
 413 479 353 497 111 158  39 329 554 324 301 523 565 137  97 474 343 440
 232 204  25 349 107  41 103 494 437 188 536 223 109 195  90 151 104 179
 399 164 273 298 296 120 359 543  44 527   1 163 510 110 236 341  61 231
 435  59  42 511 419 346 124 175 238 557  24  56 328 115 289 337 300 181
 145 230 290 101 348 105  27 525 118 185 112 424 307 430 249  82 170 130
 382 352 113 153 450  47  53 267 135 434 362  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5795
INFO voc_eval.py: 171: [ 31  14  50  53  76   3  58  23  64  13  94 120 127  83 129  90  73  79
  28  88  62  98 102  37  78 121  66   8 128  81   2  87  57  89  54   1
 101 136  74  91  19  99  22  61  20  46  38 115 116  24   6  17 114  52
  84 106 124   7 123  39  47  26 105 107   9  32  95  85  18 130  82  67
 112  11 133 108 118  49  96   0  10 119  77 134  97 100  15  25  33  72
 131 111 132  69  75  63  40  55  44  48  42 117  12  59  60  51  16  21
  29 126 113  93  30 110  70  41 122   4 103  45 135  34 104  35  43  56
   5  71  36  65  68 109  92  86  27 125  80]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5847
INFO voc_eval.py: 171: [324 345 143 505 431  74 140 249   9 473 463 168  99  50 474 349 286  15
  21 495 225 453 475 374 204   2 231 466 261 226 245 371 190 427 451 256
 377  98 167 128 382 373 401 193 487 434 437  32 189 284 444  85 251 336
 462 519 294 325  86  26  52  95 201 479  55  16 279 258 304 253 219 191
 316 192 254 372 160 351 404 287  31 269  46 208 310 353 223 456  23 323
 181  11 180 270 293 360 259 459 326 438  27 450  35 307 155 149 497  49
 484  60 442 267 405 478  63 489 185 172 103 308 426 299 391 102 171 262
 314 214 126 233 392 469 129  71 187  30 110 512 264 407 232  90  48  78
 183 255 509 465 120  45 447 458 384 257 439 164 359 418 118 199 518 318
 395 481 196 228 222 494 195 165 217 263  37 275 141 320 197 433 202 333
   0 334 152 472 224 292 461 276 266 507  22  25 483 151 215 440 268  12
  38  40  28 470 240 332 435 178 109 455  24  20 137 174 105 331 211 159
  33 147 296 288 504  51 387 309 500 305 350 464 260  97  47  36  61 430
  14  70 282 425   5 135 121 278  29 337 477 368 491  59 281 317 416 508
  57 200 486 448  83 511 227 303 198 186   8 237 322 341 406 184 460 300
 142 506 179 480 370 393 485  89 206 216 166 398 421 235  10 410 213 394
 381 311 501 352 385 221 339 417 358 124   7 343  77 242 236 457 132  42
  82 252 119 503 297 449 520  69 356 161  72 513  34 515 285 234 403 111
 175 106 156 327 419 383 441 210 218 432 117  96 125 271 361 220  62 145
 289  43 369 272  19 335 101 170 241 108  66 420 107 157 154 176 177  44
 321  17 354 346 347 203 428 146 188   4  73 169 344 123 100 133 290 397
 467   3 239 301 116 414 424 498 150  53  56 295 134 452 348 277 139 328
 365  58 490  13 493 363 409 436 207 122 386 113 115 502  76 298 445  64
 340 338 312 516 248 378 390 402 379 517  92  18 367 148  41 104 510 173
   1 114  65  39 112 182 246 153  87 446 302  84 194 162 413 127 380 399
 388 136 280 468 496 514 482 212 247  54 355 229 422 138 412  67 411 283
 131  91 163 130 454 230 408 315 243 313 144 319  79 306 476 429 273  81
 423  68 357 362 376 342 205 291  94 396 366 389 364 471 488 415 238   6
 330 244 250 443 400  93  75  80 492  88 329 499 209 375 158 274 265]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4297
INFO voc_eval.py: 171: [ 4  2 11 36 12 18 41 30  0  3 24 14 15 17 32  7 23 21 29 22 40 39 37 34
  5 10 42  1 25 33  9 38 19 16 13  8 27 43  6 26 35 31 20 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0022
INFO voc_eval.py: 171: [1602  384 1603 ... 1183 1645  360]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.4931
INFO voc_eval.py: 171: [27  2  0 52 40 22 47 20 14 61 56 16 19 37 11 41 33 51 58  4  1 50 60 44
 13 64 34 24  3 30 49 26 46 18 28 10 21  5 57 59 43 31 48 29 42 12 23 15
 53  7 39 25 54 35  8 17  6 63 38 55 36 62 32  9 45]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1807
INFO voc_eval.py: 171: [356 162  87 155 307 175 232 228 215 231 139  49 313 164  33 127  68  76
 329  16  15 271 199  34  47 317  75 158 376 358 312 295 159 296 138 225
 306 243 270  90   1 330 377 326 253 382 242 319  27 108  22 181 266 291
  88  24 133 374 250 106 179 337 343  64 236 256 305 235  79 148 264  61
 204 352  20 401 391 372 140 314  62 308 267  43 378 327 124 248   7  69
 395 320 397 223 152  23 198 141  55 348 126 229 375 272 365 359 131 137
 163  95 222 361 333 120 387 161 384 403  13 257 104  10  14 287 128 113
 200 331 277 338 381 136 110 107 116 213  39 289 281  91 332 129 125 278
  17 389 230 265 368 371 135  97 177 190 299 234 176 244 346 226 349  58
  35 334 263 297 233 216 369 335 373 344 194 221 103 388 321 147  71 325
  78 394 364 134 288 342  32 300   4  48  82  59 255 353 316 292 302 366
 251 240 283   8 246  11  93  19 153 393 174  12 402 101 345 172  67 165
 379 143 206   3   0 351 385 383  46 212 132  66  83 322   6  85  53 285
 398 187  84 390 363 178 109 220 367 380 315 202 191  94 184 211 280 188
  57 114  44  36 192 304 100 311 318 195  38 145 399  60 340 115  65 276
 245  28 328  30 209 218 151 261 260 227 254  29 171 290 118 279 284 396
 362 146  45 392 386 259 189 157  74 274  96  51  98  72 214 239 102  92
 262 149  41 168 122 347 354 275 205 252 350 249  80  37 183  63 237 197
 370 294 142 180 324  40 201 219 196 117 105  73 310 323 154 182 360  18
 207  31  52 185  81  99 273 186 156 217 258   5  21  25 123 166 130 121
 303 170 169 167 144 339 341 286 293  50  89 208   9 400  42 241  56 210
  86 357 268 111 119 150 160 173   2 203 282  26 301 309 336  54 404 298
 193 112  77 224 247 269 355 238  70]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3866
INFO voc_eval.py: 171: [ 13  86  26  29  73  51  52  46   4   1  20  30  49  78  18   0  66  47
  61  40  95  94  96  43  98  27  92  50  99  10  14  33  79  22  68  89
   2  42  39  91 100  71  67  76  88  83   7  82  21  17  70  85  53  60
  59  23  87  65  11  12  63  57  56  34  84  97  64   3  55   5  19  24
  58  93 101  75  31   8  37  80  48  72  35  54  16  44   6  28   9  41
  15  32  77  36  62  45  25  90  81  38  74  69]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0599
INFO voc_eval.py: 171: [64 75 55 57  8 11 80 78 43 52 42 25  9 45 54 68 44 65 73 71 59 12 77 58
 60 10 33 49 72 39 19 30 20 36 76  6 14 24 18 40 66 23  4 47  3 50 17 63
 56 69 62 41 31 22 29  2 35 61 27  1  7  5 74  0 37 48 34 67 28 46 81 38
 26 79 21 32 53 82 70 13 16 51 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3611
INFO voc_eval.py: 171: [ 30 242  22 151 104 216 158  34 255  15   8 187 179 111 159 172   7  85
 247 177  51  31 176  96 206 209  95 230  46  57 152  70 164 136  61  94
  74 138 115 118 178  88 259 137 229  19  45 161 212 144  97  58 146 106
 251   5 121 196 186 125  68 100 215  64  16 117  43 181  39 171 192 190
 168  21  52   1  20 194 253 201 203 214  81 223 180  92 128  25  84  63
 231  66 191  27 188 236 130 199 156  36  41 248  76  90 112 119 208 235
 124 260 211  26   2 217 222  29 173 220  72  56 256  35 113 122  42 262
 145 142 207 228  37 114  38 237 221  86   9 233  80 102 162  65 165 169
  13 170 108 134  40 183  32  71  47 160 132  53  91  60 174  44 185 241
  24 240 120 182 210 263 224 163 198 219 257 232 135 204  18  82 126  89
 139  14 150 254 143  50 154 127 202 246 101 175 123  73  98 238  23  75
 110  28 141  79 103  54 250 129 227   0 149 133 252  62 225 218 200 167
  69 153  59 193 148 116 166 157  67   6 140  17 147 258 261   3 226 243
 205  49  93 189  78 239  48 234  10  12 197  99 155   4  87 245 244  77
 109  33 107 184  11 213 195  55 131 249  83 105]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7639
INFO voc_eval.py: 171: [2109 1226 2127 ...  530 2483 2106]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5975
INFO voc_eval.py: 171: [161 140 152 233 493 806 174 263 169 245   7 750 333  79 691 349 807  47
  82   6  80 484 753  98 526 430 246 265 713 455 471 654 751  81 539  85
 554  84 303 292 267 191 602 808 646 845 247 593 810 183 194 749 188 495
 210 652 557  64 409 307 159 555 809 732  63 747 717 304 653 824 139 542
 306  91 822  52 656 242 559 499 254 380 173 820  33 698  90  86 661 651
 148 207 563 157 400 365 268 399 103 758 728 387 683 470 818 755 601 655
 847 352 432  83 382 527 248 434   8 478 740 788 325 101 106 466 826 752
 462 227 322 102 158 737 619 665 757 657 345 136 522 561 547 738 830 496
 401 431  11 719 355 595 186 253 692 663 486 866 189 384 725 558 587 114
 472 516 405 575 433 710 873 459 143 831 760  40 790 350 383 844 476 416
 567 816  70  35 571 729 402 414 146 272 199 170 468 519 461 664 156 576
 863 690 569  68  75 164 299 589 849 570 627 544 449 192 118 293 708 599
 832 487 741 378  13 546 354 260 105 524 795 829 208 573 777 764 336 686
 127 819 689 309 666 238 356 560 742 479 596  54 703 193 865  67  76 108
 165 761 123 203 582  34 609 458 827 848 497  10 149 867 828 688 586 517
 256 500 475 275 319 251 270 613  27 515 588 639 562 290 163  88 394 687
   4 611 821 852 667 676 166 579 243 367 296 658 324 523 445   0 702 209
 759 258 711 250  99 469 754  93  16 298  89  32  45 255 744 438 674 671
  58 518 410 201 660 859 249 219 141 371 308 196 269  61 456 375 261 422
 850 398 720 804 318  94 647 722 814 521 305 321 551 837 316 358 411 533
 514  15 451 640  41 839 565 274 714 113 160 578 680 271 483 404 778 240
 465 202 297 217 439 789 872 783 854 552  49 508 771 388 784 858 792 264
 857 408 418   2 696 226 311 610 216 222 512 787 504 644 650 606 811  97
 678 659 614 668 765 151 685 679 534  57 450 366  59 142 133 353 502 473
 491 721 326 768 548 871 323 598 802  87 463 748 429 574 482 198 492 511
 412 706 705 490 673 743 716 605 346  31 846 503 454 603 813 532 391 137
 775 314  55 206 734 266 812 230 135 413  51 134  66 420 339 155 505 636
 452 841 335 794 608 537 805 257  20 259 779  53 129 825 704 615  12  65
 600 501 211 662  18  25 797  14 241 480 111 709 780 723 597 631 693 874
 457 488 120 726 373  73 332 328 291 295 861 359 553  69 770 531  26 868
 423 235 823 592 464 100 393 124 436 415 441 179 262 585 815  71 510 360
 338 419 437 132 153 313 220 648 427 110  19 116 731 190 364 341 342 329
 232 344 649 801 842 786 697 617 855 791 836 637 406 529 377 182 584 144
  24 286 869 718 121 107 580 494 223 545 712 453 862 131 773 594 581 154
 635 351 334 288 185  56  38 424 426 642 628 357 694 509 122  44  74 817
 803 213 212 853 197 444 239 556 172 746 620  60 204 864 370 672 550  42
 590  17 236 645 215 834 252 315 150 396  62 310 389 117 214  77 474 632
 112 224 724 372 735 273 616  22 392 538 421 277 312 147 624 623 833  28
 540 180  50 612 467  23 376  36 195 774 280 641  48 347 541 677 368 528
 442 440 772 695 684 171 302 385 379 781  78 104 634 736  37 145 860   3
 225 513 549 485 369 682 564 707 443 506  21 591 782 793   5 629 566 234
 715 796 448 477 856 317 363 766 498 607 530 756 669 397 218 244 276 287
   1 281 278 838 119 446  29 126 851 340  95 231 699 381  92 184 626 630
 730 162 785 535 331 681 386 776 520 435 301 178 330 638 177 205 618 727
 489 294 284 568 572 237 843 507 285 362 543 799  39 279 835 701 745 800
 221 390 403 181 343 337 481 200 769 282 536 176 187 733 447 425 577 229
 622  72 675 739 361 228 115 167 138 175 395 604 109 670 128 417 633  30
 625 374 327 840 300 767 763 168 762 798 348 643 125  43 428  96  46 583
 460 283 525 700 870 407   9 320 621 289 130]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5795
INFO voc_eval.py: 171: [47 28  5 46  0 39  7 41 44 43 59 66 61 27 60 24  8 16  2 49 54 32 18 11
 48 30  3 55 10 17 29 13 21 40 14 34 58 56 42 19 57 51 52 31 64 63 33 38
 12  6 15 50 45  9 36 22 62 25  1 53 37 20 65  4 35 23 67 26]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2909
INFO voc_eval.py: 171: [ 58  59  21  90  32  57 144 106  89  49   3  67  99  25  85  30  65  61
 131 127   4  76  47  86   9 114  95 137  23  44 119  43  97  27  52  73
  24  37  88  94  96  38  16 134 141 147  87  72  20  53  15  17  92  40
   0 130  41  10  69  50 142 139  80 105  63  78 129  75 120  34  66  84
  62 132  71  31 128  77  51 125  56  35   7  13   1 146 108  29  36   2
  48 121  98  42 115 136  91 143  68  11  39   5  83 101 145  46 123 103
  79  26  19 112   6  82  93 113  22  81  64 100  60  28 140 124 116  74
  33 126  55  14   8 118 122 133 117 104 111 102 138  45  18 135  54  70
 110 109 107  12]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3518
INFO voc_eval.py: 171: [109  18  69  39  15  58 139  28  79  60  93  36  14 110 136  81 148  38
  75 151   7 143 123  57  80  13  59  92  16  37  62  66  23  72 125 100
  20  55  42  17  96 103 130  91 157  32  73  85 121 153 107 120 102  53
  26  61 131 141  83 144 119  34  30  31  40 124  74  49 106 116  97  44
  86  46 129 118 140   6  99  90  71  63   4  70  95 158 105  65 101  41
   0  77  12  35  87 135 126 128 155   9   1  47 147 117  19 134  29 112
  27 133  82   8  25  67 145  43  45  98 137  89  54  52  56 111  51 154
   2 122 142  76 149 104 138  94 108  21  84  22  24  11 156   3 146 150
 152  68  33 113  48 159 127  88  10  78 115  50  64 132 114   5 160]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3631
INFO voc_eval.py: 171: [ 91 151 224 239 212 302 330   3  26 181 289 139 326  19 172 240  58  85
  38  23 310 182 245 121 247  73  87  42 188 222 278 225 235 198 337  54
  77 120 227  11 213  57 332 246 232 211  56   1  41 184 112 135 122  61
 293  59 156 230 257 340  66 265 106 166 345 185 131 311  46 210 258 255
 341 279 312 242  35 287 262  84 346 259  55 321  22 127 161 231 328 292
  86 107 313  25 263 264  62 165 324  89 197 241 147 295  78  53 116 285
  24 261  49 350 236   4 276 281 155 146 119  99 187 322  75 267  65 327
 244   7 256 175 186  16 163  72 344 109 299 148 215 149 194  31 138 217
 207 209 323  93 343  52  33  97  17 274 318  64  74 157 143 117 308 282
 141  20 315 124  28 133 216 179 169  96 183 338 301  27 208  90 290  98
 132 314 294   9  47 192 234 266 110 125 283  71 304 347  39  60  81  44
 140 105 205 284  63 271 325 195 298 250 102 193   5 178   8 134 104 123
 176 190 167 348 196 320 349  14 229 306 238 277  76  10 260 114 329 130
  45 297 342 199  82 154 331 223 153 159 201 202  18  43 200 174 333 142
   6 204 173 252 269 296 145 280 288 171 233 273 152 307 270 352   2  29
  68 191 144 177 108 221 180 168 160 129 254 170 275 253 103  80  69 243
 305 335 115 268 118 291 249 228  70 150 336 251  50 303 237 189 319 113
  13  32 226 351  30  94 111 137  67  79 214 218 162 286 158  48 334  83
   0 100  51 128  12 300  34  37 164 220  21 316  88  92 203 101 126 219
 317  40 248  15 136 206 339 272  36 309  95]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4469
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3980
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.359
INFO cross_voc_dataset_evaluator.py: 134: 0.601
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.321
INFO cross_voc_dataset_evaluator.py: 134: 0.579
INFO cross_voc_dataset_evaluator.py: 134: 0.585
INFO cross_voc_dataset_evaluator.py: 134: 0.430
INFO cross_voc_dataset_evaluator.py: 134: 0.002
INFO cross_voc_dataset_evaluator.py: 134: 0.493
INFO cross_voc_dataset_evaluator.py: 134: 0.181
INFO cross_voc_dataset_evaluator.py: 134: 0.387
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.361
INFO cross_voc_dataset_evaluator.py: 134: 0.764
INFO cross_voc_dataset_evaluator.py: 134: 0.597
INFO cross_voc_dataset_evaluator.py: 134: 0.580
INFO cross_voc_dataset_evaluator.py: 134: 0.291
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.363
INFO cross_voc_dataset_evaluator.py: 134: 0.447
INFO cross_voc_dataset_evaluator.py: 135: 0.398
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 34999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.412s + 0.002s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.358s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.347s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.343s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.347s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.343s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.348s + 0.001s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.351s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.353s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.356s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.356s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.358s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.362s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.486s + 0.002s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.380s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.370s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.361s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.361s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.364s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.367s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.365s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.366s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.368s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.370s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.370s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.370s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.405s + 0.002s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.356s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.361s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.376s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.370s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.368s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.368s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.367s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.367s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.364s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.368s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.367s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.367s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step34999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.504s + 0.001s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.368s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.385s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.375s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.372s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.368s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.370s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.371s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.371s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.369s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.367s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.366s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.367s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.447s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [230 178  90   5 287  56 219  17  44  49  48 143 121  58 294 172 168 135
 142 315  43 181   1 119 307 179  45 316 257 302  36 261 270 102  18 289
  70 130 149 113 239  82 187 125 207 282 134 148 116 150 272  27 264  79
 227  84  28  78 246 110 243 313 145  47  52 169 277 126 209 299 157 319
  81 139 241 146  34 101  95  59  66  91 269 262 274  11 288  71 131 224
 204 214  69 129  50 306   3 231 163 203 136 162  35 104  76 314 285 109
 176 308  54  16  32 198 202 310 240 228 184   6  53 280 171 159 248 252
  94 271  10 323  20 291 188 165  40  21 318 132  72 293  24 284 106   2
  42 218  63 276 215 216 255 103 100 153 200 174  86 254 221  96  39 278
 320 217 229 186 123 210   4 156 151  29  30 183 244 234 140  85  31 161
 303 196 295 160 164 300 233 279 167  14 222 249 304  41 322 212 189 122
 232 267  23 115 206  93  88 305  51 111 114 253  22 247 144  61 236 245
  73  89 133 256 208 120 182 251  57  87 301 273  68 127 154 190  80 242
 283 170  19  15 147  38 290 324  60 226  13   9 235 141 275  83  74 266
 265  77 173  97  33 312 297 191 118  12 211 213 192 197 107 281 225 311
   8 286 152  25 298 317   7 237 201 263 128  92 309   0  64  98 105 185
 193 194 117 238 292  55 175  26 250  67 258 138  75 137 205 112 259 260
 268 296  37 223  62 158 177  99 180 321  46 108 124 199 220 195  65 155
 166]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3602
INFO voc_eval.py: 171: [177 115  25  51 205 214 220 153 202 122  20 216 106  26  87 204 133  14
 173 163 156  24 206 189 151  90 201  80  60 112 149 139  13 218  49 161
 207  65 109 175  83 143 157  34  10 211 188  39  72 226 134 221  33  89
  95 185 219  91  93 171 158 208 140 194 110  12 111 123 222  79  36 131
 174   2 127 117 124  27  92  84  85  86  40 138  35 190  21 165  88 215
 199  31 107 113  78   7 167 120  99 172  52  58 142 121 179  53  69 232
  77 225 197 100 170  76 192 229 212 125 178  66 183 217 155 168 108 114
 196 135 119  19 129 228  43 198  97 104  75 195 187 126  48  64  45 231
  42 154 224  15  56  11 147 166  29 162 105  98  74 132 116 144 102  18
 137 227  54 146  82  67 184 186 118   5  96  55  37  16 200  81  38 223
 233   1 128 130  17  28  50  94 103 180 176 148  32 169  30 159 230  57
  44  70   4 193  46  22 136   0   6  23  62 164  71 145  61 213  47 210
 182  63 150 141   9 181  73 191  41   8 152 160  68 101 209 203   3  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5970
INFO voc_eval.py: 171: [ 61  26  81  48  49  67 123  51   1  78 149 168 151  58  59 169 170  50
 193   0  56  36  30 131 150  97  98  55  76 186  80  84 159 137 198  57
 112 185  34 157  70 106 142  23  60 111  15 128   4 189  54 140 172 122
 113  63 134   9  83 177  79  68 171 182 173 103 146  69 135  38 144  90
 124 132  64 181  18 138  93  73 197  28 152 163 156   8 145  20 195  17
 153 178 167 160 136  53 143  43   5 183  27  71 120 196 107  46 161  66
 110  75  14  11  47 188 187 141 102  16 104  94 147 105  95 158 164  33
  35  41  40  39  10  22 116  37  92  44  62 130   3 100  72 176 166 139
 180 175  88 162  42 115  52 109 101 184 165 191  12 133 118 108  96 125
  21  31 148   6 174  86  19  25  89  13 190 121  91  45 129  24  99 114
   7  77  87 155  74 127   2 194 126  85  29 119 117  32  82 154 179  65
 192]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2077
INFO voc_eval.py: 171: [310 371   0 311 101   9  48 312  81 253 135  28 346 245  52 454 344 252
 363 397 243 113 250 134  80 115   8  24 190 112  95 442 258 180 449 313
 185 165 254 372 398 468 227  82  36 300 292 303 189  96 257 200 391 361
 379 116  27 291 395 497 439 102 232 337 334 393 448  60 273 389 158 222
  31  37 470 236 289 208 387 263 472  61 317 148 240 427 407 339 447 166
  29 405 174 242 251 238 307 352  17 168 374 417 458 233 466 202 262 286
 423 475 144 167 413  62  41 390 121 182 256 392 378 105 184 264 424 109
 306 290 163 451 435 479 400 162  26 308 225  87 491 201 376 137 186 425
 153  40  38 283 482 150 194  46  64  35 461 418   7 343 282 176 272 138
 349 261 321 485 164 323 375 381 265 380 411 285 255 127 338  58 140 274
 481 494 211 228 204 410 358 120  74 353 469 463 154 471 210 408 139   4
 214 212 178  39 438 402 239 489 496 103  83  86 270 465  47 196 302 235
 434  88  51 220 450 104 259 276 446  42  25 177 493 341 356 160 169 206
   2 384 336 248 278 322  10  70 295  67 455  66 401 107 377  23 441 365
 221 484  30 319 325 247 131 462  63 123 141 348 476  57 467 173  71 373
 133  53 203 147 117 280 146 350   1 269 119  16 205 155 213 111 315 443
 237 298 333 432  85 354 367 231  34 294 445  56 431 483 288 473 183 229
  19 301 420  49 230 279 143 215 430 437  20 416 490 347 268 326 191 404
 314 241 429 394  94 193 219 332  68  73 452  79  33 287  99 224 192 406
 271 159 459 145 207 487 456  55 195 433  76 156   3 360 118 419  43 436
 412 100   6 486 309  44 172  14 108 369 234 125  21 305 126  12  59 188
 382 110 415 421 444  93  32  65 293  75  90  89 345 327 460   5 217 170
 324 362 175 130 157  84 281 359 114 477 426 478 299 179 357  69 284 409
 403 122 267  11 198  18 181 297  92 320 128 136 364 275 474 386 171 106
 329 457 492 244  22  13  50 383 223 488 161 296  77 151  98 368 142 152
 428  97  54  78 149 132 330 453 331 316  91 370 249 216 396 351 342 385
  15  45 199 129 260 340 187 328 277 495 218 335 124 440 266 209 366 226
 480 464 414 355  72 422 318 304 399 246 388 197]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3212
INFO voc_eval.py: 171: [453 454 246 459 209 455 254 212 116 311 562 377 372 456   3 563 210 208
 253 197 354 282 131 355 330 211 467 373   4 198 371 378 283 466  84 532
 374 481 458 502 443 526 473 461 460  83 177 503 398  33 468 457 488 270
 474 464 507 478 381  15 369 162 215  62 493 495  30 427 257 189 128 376
 512 256 405 357 294 446 543 379 462 150 159 264 490 305  81 203 380 139
 389 146 316  49 497 318 395 241 184 191 492 281 382 555 308 286 171 274
 394 402 406  48 317 216 472 320 237 546  11 262 161 279  50 227  22 180
 548 310 384 409 417 429 108  23 399   9 520 489 234 356 200 498   6 299
 550   5 401 214 361  39 565 233 258 568 326 370 509 530 345 368 516   7
  14 127  17 255 291 408 125 549 494  95 322 442 391  85 121 522  28 471
 439 506 277 437 284 213  32 144 416 469 533  40 463  93 430  31 364 228
 392 190 447 297 484 166 160 479 403 422 388 386  87 207 100  20 440 260
 421 560 287  43 339 390   8 340 133 187 524 263 295 147 132 528 218  26
  37 323  19  65 235 278 245  80 508 523 360 165 288 413 410  79 221 202
  21 448 470 251 148 537 515 285 452 280  76 426 444 367 265 358 156  89
  35 554 338  63 335 501  75 561 518 152 366 412 321 477 534   0 544 134
 117 201 433 302 259 154 248 325 205  58 564 428  64 313  60  86 396 242
 220 193 261 419  69  10 535 465  46  45 226 333 199 557 418 168 269 505
  29 243 222 143 423  66  99 315 569 407 347 173  34  91 225 385 331  70
 224 536  72  73 169 552 292  77 155 123 239 351 540 397 404 362  13 219
 415  16 126 293  52 483 174 122 424 119  88 250 142 510  74 375 517  67
 183 571 206 196 271 432 217 334  36 547 106 149 178 176  57 140 157 268
 102 275 500 553 411 449 570 136 450 247 194 566 485  94 336 182 541  18
 252 342  98 138 304 276 167 303 539 387 521 244  12 319 186 551 487  78
 519  54 332 229  96  92 172 531 514 129  71 504 314 434 476 558 445 327
 486 114 240 312   2 393 542  51 309 365 192 141  68 491 480 350 344 266
 414 353 272 329 111 499 158  38 324 556 525 301 567 475  97 441 137 343
 204 232 349 538 103  41 107 223 438 496 188 306 195 179 151 164 104  90
 109 400 298 359 273 296 120 511  61 341  44 236 163 545 110   1 436 231
  42 420  59 513 529 115 328  24 124 346 238 559  56 289 175 527 181  27
 337 101 105 185 145 300 112 230 348 118 383 170 290  47  82 113 352 425
 431 249 130 307  25 135 363 435  53 153 267 451  55 482]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5790
INFO voc_eval.py: 171: [ 31  14  50  53  76   3  58  23  64  13  94 120 127  83 129  90  73  79
  28  88  62  98 102  37  78 121  66   8 128  81   2  87  57  89  54   1
 136 101  74  91  19  99  61  22  20  46  38 115 116   6  24  17  84  52
 114 106 124   7  39  47 123  26 105 107   9  95  32 130  18  85  82  67
 112  11 133 118 108  49  96   0  10 119  77 134  97 100  15  25  33  72
 131 132 111  69  48  40  55  44  75 117  42  63  12  60  59  51  16 126
  21  29 113  30 110  93  41  70   4 122 103  34  45 135  35 104  56  43
  71  36   5  65  92 109  68  86  27 125  80]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5847
INFO voc_eval.py: 171: [325 346 143 506 432  74 140 250   9 474 464  99 168  49 475 350 287  15
  21 496 225 454 476 375 204   2 231 467 262 245 226 372 190 428 452 257
 378  98 167 128 383 374 402 193 488 435 438 189  32 445 285  85 252 337
 463 520 295 326  86  26  52  95 201 480  55  16 280 259 305 254 219 191
 192 317 255 373 160 288 352 405  31 270  46 208 311 354 223  23 457 181
  11 324 180 271 294 361 260 460 327 439  27 451 308  35 155 149 498 485
  50  60 443 268 406  63 479 490 185 309 172 103 427 300 392 171 102 263
 214 315 126 233 393 470  71 187 129  30 110 513 265 408  90 232  48 183
  78 256 510 466 120 448  45 385 258 459 164 440 360 419 118 519 319 199
 396 482 196 228 222 495 195 217 165 264  37 276 141 197 321 434 334 202
   0 335 152 473 293 224 462 277 267  22 508  25 484 151 441 215  12 269
  38  40  28 471 333 240 436 178 109 456  20  24 137 105 174 211 332 159
 289 147  33 297 505  51 388 310 306 501 465  97 261  47 351  61 431  36
 283  14 135   5  70 279 426  29 478 121 338 509 369  59 492  57 282 417
 318 200 449 487  83 227 512 304 198 237 301 186 323   8 142 342 184 461
 407 507 179 481 371 486 394  89 206 399 166 216 422 235 411  10 213 312
 353 502 340 395 359 382 386 221 418 344 236   7 124  82  77 242 458 450
  42 253 132 119 298 521  72 357 504  69 514 161  34 286 516 234 111 404
 175 106 328 156 420 442 384 218 125 145  96 210 433 117 220 362 272 290
  62  43 101 170 241 157 107 336  19 176 273 370 177  66 347 108 154 421
  44 355 348  17  73 322 188 146 429   4 203 133 116 123 169 100 345 239
 291 302 296   3 415 453 150 468 349  53 398 425  56 499 494 329 366  13
 134 364  58 491 278 410 139 437 207 113 387 503 115 122 391 339  64 313
 403 299 379 249 341  76 446 517 518 368  92 380 173  18 104 182  41 511
 148 153  39 112 194  65 162 447  84 114   1 246 400 303  87 136 127 414
 381 389 469 281 497 413 229 138 412 248 515 356 131  67 284  54 212  91
 483 130 144 455 163 423  79 230 243 274 307 314 316 320  68  81 430 409
 424 477 343 358 363 489 377  94 367 365 205 292   6 390 238 472 397 416
 444 401 330 331 251  80 493  75 244 266 247 209 275  93  88 500 158 376]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4293
INFO voc_eval.py: 171: [ 4  2 11 36 12 18 41  0 30  3 24 14 15 17 32  7 23 21 29 22 40 39 37 34
  5 10 42  1 25 33  9 38 19 16 13  8 27 43  6 26 35 31 20 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0022
INFO voc_eval.py: 171: [1605  385 1606 ...  102 1800  288]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.4935
INFO voc_eval.py: 171: [27  2  0 52 22 40 47 20 14 61 56 16 19 37 11 41 33 51  4 58  1 50 60 44
 13 64 34 24  3 30 49 26 46 18 28 10 21  5 57 59 43 31 48 29 12 42 23 25
 15 53  7 39 35 54  8 17  6 63 38 55 36 62 32  9 45]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1803
INFO voc_eval.py: 171: [357 163  88 156 308 176 233 229 216 232 140  49 314 165  33 128  68  77
 330  16  15 272 200  34  47 318  76 359 159 377 313 296 160 297 139 226
 307 244 271  91   1 331 378 327 254 383 243 320  27 109  22 182 267 292
  89  24 134 375 251 107 338 180 344  64 237 257 306 149 236  80 265  61
 205 353  20 392 402 373 141 315  62 309 268  43 328 125 379 249   7  69
 396 321 398 224  23 153 199 142 349  55 127 230 273 376 366 360 138 164
  96 132 223 362 334 121 388 162  13 385 105 258 404  10  14 114 288 201
 129 278 332 339 382 137 347  39 214 117 108 111 290  92 282 333 279 126
 130  17 231 390 266 369 372 136  98 178 191 300 235 177 245 350  58 227
 335  35 298 264 374 234 336 370 217  71 345 195 104 389 222 322 326 148
  79 395 289 343 365 135  59  32 301   4  48  83 256 317 354 293 241 252
 303 367 284   8  11 247  94  19 154 102 403  12 175 394 346 173  67 166
 380 207 144   0   3  46 352 213 133  84  66 384  86 323   6 386 286  53
 188 399  85 391 368 179 221 364 110  95 185 381 316 192 203 193 212 281
  44  57 189 341 196  60 319 115 116 305  38  36 400 146 101 246  65 312
 277 262  28 329 219  30 210 285  29 152 255 228 261 280 397 291 119 363
 172 275 190  45 147 260  75 387 393 158  97  99  73  51 103  93 169 215
 263  41 150 240 123 253 206 276 355 348 250  81  37  63 238 351 184 371
 295 198 202 311 197 325 220  40 143 181 155  74 106 118 324 361 208 183
  31  18 100  82 186  52 157 187 124   5 171 168  21 122  25 145 170 131
 167 259 274 218 304  50  42 287  90 294 340 342  56 209 161 174 401 358
 211 242  87 112   9 120 151 269  26 283 310 302 405 337 204  54  78 113
 299 248 225 194   2  70  72 270 239 356]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3864
INFO voc_eval.py: 171: [ 13  86  26  29  73  51  52  46   4   1  20  30  49  78  18   0  66  47
  61  40  95  94  96  43  98  27  92  50  99  10  14  79  33  22  68  89
   2  39  91  42 100  71  67  76  88  83   7  82  21  17  85  70  53  60
  59  23  87  65  11  12  63  57  56  34  84  64  97   3   5  55  19  24
  58  93 101  75  31   8  37  80  48  72  35  54  16   9  44   6  28  15
  41  32  77  36  62  45  25  90  81  38  69  74]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0599
INFO voc_eval.py: 171: [65 76 56 58  8 11 43 81 79 53 25  9 55 46 69 45 66 74 72 60 12 78 44 59
 61 10 33 50 73 39 19 30 20 36 77  6 14 18 24 40 67 23  4 48  3 51 17 64
 57 70 41 31 63 22 29 35  2 42 62 27  1  7  5 75  0 37 49 34 68 28 47 82
 38 26 80 21 32 54 83 71 13 16 52 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3703
INFO voc_eval.py: 171: [ 30 242  22 151 216 104 158  34 255  15   8 187 179 111 159 172   7  85
 247 177  51  31 176  96 209 206  95 230  46  57 152  70 164 136  61  94
  74 138 115 118 178  88 137 259 229  19  45 161 212 144  97  58 106 146
 251   5 121 196 186 125  68 100 215  64  16 117  43 181  39 171 192 168
 190  21  20  52   1 194 253 201 203 214  81 223  92 180  25 128  84  63
 231  66 191 188  27 236 199 130 156  41  36 248  76  90 112 119 208 235
 124 260 211  26   2 217 222  29 173 220  72 256  35  56 113 122  42 145
 263 142 207 228  37 114  38 237  86   9 233 221  80 102 162  65 165 170
 169  13 108 134  40  32 183 160  71  47 132  53  91  44 174  60 241  24
 240 185 224 210 182 264 163 120 198 219 232 135 257  82  18 204  14  89
 126 139 150 254  50 154 143 127 202 101 123 246 175 238  73  98  23 141
 110  79 103  28  75  54 250 129 227   0 149 133 252  62 200 218 225 193
 167 153  69 148  59 116 166 157  67   6 226 258   3 261  17  49 147 140
  78 189  93 205 243 262  10  48 239 234 197  12 155  99 109  87 245 244
  77 107 184  33   4 213  55 131 195  11 249 105  83]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7509
INFO voc_eval.py: 171: [2112 1228 2130 ... 1434 1277 1795]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5974
INFO voc_eval.py: 171: [162 141 233 153 492 804 174 262 170 244   7 748 332  80 689 348 805  48
  83   6 483  81 751  99 524 429 245 264 711 454 470 749 652  82 537  86
 552  85 302 291 191 266 806 600 644 843 246 591 808 183 194 747 188 494
 210 650  65 555 408 306 160 553 807 730  64 745 715 303 651 822 140 540
 305  92 820  53 654 557 253 241 498 379 173 818  34  91 696  87 659 649
 149 207 561 158 399 364 267 398 104 756 726 386 681 469 753 816 599 653
 845 351 381 431  84 525 247 433 477   8 738 786 324 102 107  67 465 461
 824 750 321 227 103 617 735 159 663 655 344 755 137 559 521 736 545 828
 495 400 430  12 717 354 593 186 252 690 661 485 865 189 383 556 723 585
 471 404 115 515 432 573 708 458 872 829 144 758  41 788 349 382 842 475
 415 565 814  36 569 727 401 413 147 271 171 199 467 662 518 157 460 574
 861 567 688  70  76 165 298 847 587 542 625 568 119 192 448 292 706 597
 830 486 739 238  14 353 377 106 544 259 793 523 827 208 762 775 571 335
 684 128 817 687 308 664 239 355 558 740 478 594 701  55 193 864  69 109
 166  77 759 124 203 580  35 607 457 825  10 846 150 496 866 826 686 584
 499 516  29 255 474 514 250 611 318 269 274 637 560 586 289 164 393  89
 685 609 819   4 665 850 674 167 366 577 242 295 656 323 522 445   0 209
 700 257 757 709 249 100 468 752  94 297  18  90 254  59 658 672 669  46
 437 742 248 409 857 517 201 455 307 196 142 219 268 374  62 370 397 421
 260 848 802  95 317 718 645 520 720 304 812 320 549 835 513 531 315 357
 638  17 450 410 837  42 563 161 114 576 273 678 712 270 482 403 776 550
 202 787 464 217 781 852 871 296 438 769 508  49  11 387 407 263 856 782
 790 608 417 310   2 855 216 503 642 226 694 785 648 511 222 809  98 604
 676 612 666 683 657 763 449  58 677 365 152  60 532 143 352 134 490 472
 546 322 719 501 800 325 596 765 870 462 746  88 428 572 491 481 198 509
 411 704 703 489 671  33 844 741 714 530 345 811 601 502 390 603  56 313
 453 265 773 138 810 206 732 136 230 135 412  52 419 338 451 156 634 839
  68 504 334 792 535 803 606 256 130 777  54 258  22 823 660  66 500 598
  13 702 211 691  16 479 240  20 613 778  27 112 707 795 595 721 629 873
 294 456 487 290 372 331 529  74 327 724 121 859  28 768 551 235 358  71
 422 867 125 414 440  51 101 392 463 590 821 435 426 359 813  72 337 261
 510 179 583 418 154 111 312  21 220 646 133 799 647 190 343 341 340 784
 363 436 328 833 232 729 840 117 615 789 527 853 695 582 405 376 182 145
 108 635 285 578  26 122 493 543 771 132 223 710 452 868 155 579 716 185
 592 626  39 633 333  57 425 350 640 860 423 692  75 815 356 123 507 801
 287  45 443 197 212 213 851  61 204 618 554 369 172 744 670 588 236  19
  63 314 215 251 548  78  43 832 643 151 309 722 214 224 862 395 388 630
 118 113 371 473 614 391 276 733 272 420 536  24 311 621 831 148 538 180
  50  30 622 610 466  37  25 279 539 346 195 639 772 367 375 675 734 526
 779 632 441 301 770 105 439  79 682 384 378 693  38 146 863   3 858 512
 484 442 225 562 680 368 705 564  23 589   5 234 780 505 547 713 794 754
 447 286 791 316 476 764 497 854 667 275   1 396 627 528 362 605 218 280
 243 380 836 120 339  96 127 849 444  93  31 697 231 277  15 533 628 783
 330 184 728 624 774 163 679 385 616 434 205 636 293 177 178 725 329 488
 300 519 834  40 570 566 699 506 237 797 798 541 278 743 283 284 361 200
 389 480 534 221 176 336 731 181 342 187 620 841 281 767 446 424 575 228
 229 139 402 394  73 602 737 673 175 168 360 116 129  32 631 623 326  97
 838 416 110 668 169 760 761 766 299 282 581 459  47  44 373 427 796 347
 126 641 619 319 288 406 869 698 131   9]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5789
INFO voc_eval.py: 171: [47 28  5 46  0 39  7 41 44 43 59 66 61 27 60 24  8 16  2 49 54 32 18 11
 48 30  3 55 10 17 29 13 21 40 14 34 58 56 42 19 57 51 52 31 64 63 33 38
 12  6 15 50 45  9 36 22 62 25  1 53 37 20 65  4 35 23 67 26]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2909
INFO voc_eval.py: 171: [ 58  59  21  90  57  32 144  89 106  49   3  67  99  25  85  30  65  61
 131 127   4  76  47  86   9 114  95  23  44 137 119  43  97  27  52  24
  37  73  88  96  94  38 134  16 141 147  87  72  20  53  15  17  92  40
   0 130  41  10 142  69  50 139  80 105  75  63  78 129 120  66  34  84
  62 132  31  71 128  77  56 125  35  51   7   1  13 146 108  29  36  48
   2 121  98  42 115 136 143  91  68  11   5  39  83 101  46 103 145 123
  26  79 112  19 113  93   6  82 140  81  28  64  22  60 100 116 124  74
  33 126  55  14 118   8 122 133 135 117 102 111 104  45  18 138  54 110
  70 109 107  12]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3516
INFO voc_eval.py: 171: [110  18  69  39  15  58 141  28  80  60  94  36  14 111 138  82 150  38
  76 153   7 145 124  57  81  13  59  16  93  37  62  66  23  73 126 101
  20  55  42  17  97 104 131  92 159  32  74  86 122 108 121 155 103  53
 132  26 143  61  84 120  34 146  30  31  49 125  75  40 107 117  98  44
 130  46  87 119 142   6 100  71  91  63  70   4  96 160 102 106   0  65
  41  78  35  88  12 127 129 137 157   9  47   1 118 149  83  19  29 136
 134 113  27  25  67   8  45 147  43  72  99  90 139  52  54  56 112 156
  51 144   2 123 105  77 151  95 109 140  22  85  21  11  24   3 158 148
 152 154  33  48 114  10 161 128  68  89  79 116 115  50  64 135 133   5
 162]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3631
INFO voc_eval.py: 171: [ 91 152 225 240 213 303 331   3  26 182 290 140 327  19 173 241  58  85
  38  23 311 183 246 121 248  87  73  42 223 189 279 226 236 199 338  54
  77 228 120  11 214  57 333 247 233 212  56   1  41 185 112 136 122  61
 294  59 157 231 258 341  66 266 106 167 346 186 131 312  46 211 259 342
 256 280 313 243  35 288 263  84 347 260  22  55 322 127 162 232 329  86
 293 107 314  25 264 265  62 166 325  89 198 242 296 148  78  53 116 286
  24 351 262  49 237   4 277 282 147 156 119  98 188 323  75  65 268 328
 245 257   7 187 177 164  72  16 345 109 216 149 300 150  31 195 218 139
 208  93 324 344  52  33  96 275  17 319  64 117 158  74 144 142 283 309
  20 316 124  28 134 217 180 170  95 184 339 302  27 209  90 291 315 132
  97   9 295  47 267 235 110 193 125 284 348  71 305  39  60  81 105 272
 285 206  44  63 141 326 196 251 299   5 102   8 194 135 179 104 176 200
 191 123 349 168 321 197 350 230  14 307 278 239 261 114  76 330 343  82
  10  45 130 155 298 160 224 203 202 101 332 154  43  18 201 175 334 270
 253 205 143   6 174 297 281 153 172 146 289 234 308 210 274 271 353 192
 108   2  29 161  68 178 169 222 171 276 129 254 181 145  69 255 103 244
 115 118 250  80 306 336 292 269  70 252 151 229 337 304 113 190  50 238
 227 352  30  32  94 320 111  79 215  13  67  99  48 335   0 219 138 159
 163 287  12  83 128  34  37 301  51 100 221 220  40 204 165  88  92 317
 318 126 133  21  15 249 207 340 137  36 310 273]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4468
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3976
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.360
INFO cross_voc_dataset_evaluator.py: 134: 0.597
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.321
INFO cross_voc_dataset_evaluator.py: 134: 0.579
INFO cross_voc_dataset_evaluator.py: 134: 0.585
INFO cross_voc_dataset_evaluator.py: 134: 0.429
INFO cross_voc_dataset_evaluator.py: 134: 0.002
INFO cross_voc_dataset_evaluator.py: 134: 0.493
INFO cross_voc_dataset_evaluator.py: 134: 0.180
INFO cross_voc_dataset_evaluator.py: 134: 0.386
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.370
INFO cross_voc_dataset_evaluator.py: 134: 0.751
INFO cross_voc_dataset_evaluator.py: 134: 0.597
INFO cross_voc_dataset_evaluator.py: 134: 0.579
INFO cross_voc_dataset_evaluator.py: 134: 0.291
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.363
INFO cross_voc_dataset_evaluator.py: 134: 0.447
INFO cross_voc_dataset_evaluator.py: 135: 0.398
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 37499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.509s + 0.002s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.353s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.357s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.360s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.365s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.358s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.359s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.362s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.358s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.362s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.360s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.364s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.365s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.493s + 0.002s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.377s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.369s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.369s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.370s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.374s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.382s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.382s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.377s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.376s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.376s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.375s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.372s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.440s + 0.001s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.378s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.361s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.366s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.367s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.365s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.363s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.361s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.361s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.361s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.363s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.362s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.363s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step37499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.422s + 0.001s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.359s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.349s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.346s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.348s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.350s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.352s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.352s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.352s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.353s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.352s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.354s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.354s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.647s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [230 178  90   5 288  56 219  17  44  49  48 143 121  58 295 172 168 135
 316 142  43 181   1 119 308 179  45 317 257 303  36 261 271 102  18 290
 130  70 149 113 239  82 187 125 207 283 134 148 116 273  27 150  79 227
  84  28  78 110 246 243 314 145  47  52 169 278 209 126 300 157 320  81
 139 241 146  34 101  95  91  59  66 270 262 275  11 289  71 131 224 204
 214 129 265  69 307  50   3 231 163 203 136  35 162 104  76 286 315 109
 176 309  16  54  32 198 311 202 240 228   6 184  53 281 159  94 171 252
 324 248  21 165 272  20  10 292 188  72 319  24 132  40 294 285 106   2
  42  63 218 277 215 255 216 103 100 153 254  86 174 200 221  39  96 321
 279 217 229 186 210   4 123 151 156 234 183 140  30  29  85  31 244 161
 296 196 164 304 160 280 301 233 167 249 222  14  41 323 305 189 122 212
 268 232  23 115  93  88 306 206  51 111 114 253  22 247 144  61 256 133
 182 208  73 245 236  89 251 120  68  87 190 302  57 242 284 127 170 274
 154  80 147  19  38 291 326  60  15 235 226 141   9 276  74 267 266  13
  83  77 313  97  33 173 213 191 118 197 298 211 225 312 107  12 282 192
 152 287   8 264  25 318 201   7 299 237  92 310 128  64  98 105   0 185
 193 175  55 293 117 238 194 250 138  26  67 258 158 297 137 269 259 112
 260 223 205  37 322  62  75 180 177  99 195 199 155 220  46 124 263  65
 166 325 108]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3605
INFO voc_eval.py: 171: [177 115  25  51 205 214 220 153 202 122  20 216 106  26  87 204 133  14
 173 163 156  24 206 189 151  90 201  80  60 112 149 218 139  13  49 161
 207  65 109 175  83 143 157  34  10 211 188  39  72 226 134 221  33  89
 219  95 185  91  93 171 158 208 140 194 110  12 111 222 123  36  79 174
 131   2 127 117 124  27  85  92  84  86  40  35 138 190  21 165 215  88
 199  31 113   7 107  78 167 120  99 172  52  58 142 121 179  53  69 232
  77 225 197 100 170 192  76 229 212 178 125  66 217 183 168 196 155 108
 114 135 119  19 129 228 198  97 104  43  75 195 187 126  48  64  45 231
  42 154 224  15  56  11 147 166  29 162 105  98  74 132 116 144 102  18
 137 227  54 146  82  67 118 184 186   5  96  55  37  16 200  81  38 223
 233   1  17 130 180 176  50  94 103 128  28  32  30 148 230 169 159  44
  70  57  22 136   4   0  61   6  46  23  62 193 164  71 145 210 213  47
 150 182  63 160   9 181  68 141 101 191  41  73   8 152 209 203   3  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5969
INFO voc_eval.py: 171: [ 62  27  82  49  50 124  68  52   1  79 150 169 152  59  60 171 170  51
 194   0  57  37  31 132 151  98  99  56  77 187  81  85 160 138 199  58
 113 186  35 158 107  71 143  24 112  16 129   4  61  55 190 141 173 123
 114  64 135  10  84 178  80 172  69 183 104 174 147 136  70  39 145  91
 125 133  65 182  19 139  94  74 198  29 153 164 157   8  21 146 196  18
 154 179 168 161  54 137 144  44  28 184   5  72 121 108 197  47  76  67
 111 162  15  48  12 188 189 103 142  17 148 105  95  36  41 165 159  34
  96 106  40  42  11  23  45  38  93 117 101 131  73  63   3 177 140 167
 181 163  43 176  89 116  53 102 110 185 166  13 134  97 109 192 119 126
  22  87  32   6 149 175 191  90 122  20  14  26  25  92   9 130  46 100
 115  78  88   7 120  75 128 156 127   2  30  86 195  66 155  33  83 180
 118 193]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2250
INFO voc_eval.py: 171: [308 369   0 309 100   9  48 310  80 251 134  28 344 243  52 453 342 250
 395 361 241 112 248 133  79 114   8  24 189 111  94 440 256 179 447 311
 391 184 164 252 370 396 466 225  81 298 188  36 301  95 290 255 198 389
 359 377 115  27 289 495 393 437 101 335 230 332 446  60 271 387 220 157
  31  37 234 468 287 206 385 470 261  61 315 238 147 405 425 445 337 165
  29 403 173 240 236 249 305 350  17 167 372 415 456 464 231 200 260 284
 421 473 143 166 411  62  41 388 120 181 254 390 376 104 183 262 422 304
 288 108 162 433 449 161 398 477  86  26 306 223 489 199 374 136 185 423
 152  40 281  38 480 193  46 459 149  64 416  35   7 341 280 175 270 137
 347 259 319 483 163 321 373 263 379 126 283 253 378 409 479  58 336 272
 139 492 408 202  73 226 209 356 119 351 461 467 153 469 208 406 177 212
 210   4 138  39 436 400 102 487 237 494 268  82  85 463 300 195  47 233
  87 432 274 103 218  51 257  25  42 448 354 491 159 444 176   2 168 339
 204 382 334 246 276 320  10 293  67 452  70  66 399 106  23 375 439 219
 363 245 482  30  63 130 323 122 317 140 460 346 474  71  57 465 172  53
 132 371 278 201 116 313 267 348 145 146   1  16 110 211 441 203 292 331
 296 430 352 365 235 229  84 118 154 429 443  56  34 227 286 481 471  49
  19 182 418 299 142 435 277 228 428 414 324 213 488 266  20 345 239 190
 402 427  93 312 392  78 450 192  72  68 330 217  33 285 222 404  98 158
 191 269 144 457  55 454 485 205 194   3 434  75 417 155 358 117  43 410
  99 431   6 307 171 484  44  14 107 367  12 232  21 124 125 303  59 109
 187 380 343  65 413  32 442 291 419  74   5 458  92  88  89 174 156 169
 129 215 360 325 279  83 475  69 322 297 178 113 424 355 357 401 476 282
 121 135  18 407 180 264  11 362 105 127 273  91 472 384 295 170 318 196
 242  50  13 221 490 327 381  22 455 426 150 314 160 366  97 151  54 486
  76 141 294  96  90 328 329 247  15 451 368 148 131 214 383 340 394 349
  77 338 275  45 197 186 258 128 326 207 420 493 412 333 438 216 265 224
 462 364 353 478 386 123 302 397 316 244]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3210
INFO voc_eval.py: 171: [452 453 245 458 208 454 253 211 114 310 561 376 371 455   3 209 252 207
 562 195 353 281 129 354 329 210 466 372   4 196 370 377 282 465  82 531
 373 480 457 501 442 525 472 460 459  81 175 502 397  33 467 456 487 269
 463 473 506 477 380  15 368 160 214  62 492 494  30 426 256 187 126 511
 375 255 404 356 293 378 461 542 445 148 263 157 489  79 304 201 379 137
 388 144 315  49 496 394 317 182 491 189 554 381 280 307 285 273 169 233
 393 401 405  48 215 316 471 319 236 545  11 261 159 278  50 226 547 178
  22 309 383 408 416 428  23 106 398   9 519 488 234 355 198 497 549   5
   6 298 360 400 213  39 564 232 257 567 325 369 508 344 367 529   7 515
  14 254 125 290  17 407 123 321  93 548 493 441 390 119  83 521  28 505
 470 438 276 436 283 212  32 142 415 468 462 532 363  40 429 227  91  31
 188 391 483 296 446 478 158 164 402 421 385 387 206  85  98  20 439 259
 420 559 286  43 338 389 131   8 339 294 262 523 145 217 185 130 527  26
  37  19  65 322 277 235 244 507 359 522 163 287 220 200 412  78 447  21
 409 469 146 250 284 451 536 514 279  76 366 425 443 264 357  87 337 553
 154  35 334  63 500  74 517 560 533 150 365 320 411 476   0  75 543 115
 132 199 432 247 301 258 152  58 324 203 563  60 427  84 312 418 219 395
 241  64 260  69 191  10 268  29 464  45 534  46 568 417 332 197 225 504
 166 556  97 221 422 346 242  66 314 384 141 171 406  34  89 224 291 330
 551  72 535 223 153 403 396 350 239  70 539  73  77 121 361  13 218 414
  52 167 124  16 482 249 292 140  86 120 423 117 509 172 516 570 374 431
 181  67 194 270 216 204 546  36 104 333 147 176 138 100 155 174 499  57
 267 552 274 448 410 192 449 134 565 246 569 180 335  92 484 540  18 165
 136 275 341 251 520 303  96 386 538 205 302 318 184 243  12 550 486 513
 331  54  94 518 228 433  90 170  71 127 503 313 557 475 444 530 112 240
 326 485 311 541  51   2 490 190 392 271 364 308  68 139 352 413 265 479
 343 349  38 300 498 328 156 109  95 566 474 524 555 135 440 202 342 323
 231 348  41 149 437 186 101 495 222 105 537 162 193 305 107 177 358 102
  88 118 297 399 272 108  44 295 435 237 544   1  61 510 340 161 528 512
 288  42 230 419  59 238 327  56  24 113 345 122 173 558 351 143  27 103
  99 183 116 347 299 229 424 179 526 110 336  53 168 248 128 306 289  80
 111  47 430 382 133 151 434  25 450 362 266 481  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5793
INFO voc_eval.py: 171: [ 31  14  50  53  76   3  58  24  64  13  94 120 127  83  90 129  73  79
  28  88  62  98 102  37  78 121  66   8  81 128   2  87  57  89  54   1
   9 101 136  74  91  99  23  61  19  20 115  18  46  38  25 116   6  84
 114  52 124 106  47   7  39 123  26 105  21 107  32  95 130  85  17  82
  67 112  11 133 118 108  49  96   0  10 119 134  77  15  97 100  33  72
 131 132  69 111  48  55  40  75  63 117  44  42  12  60  59  51  16  29
 126 113  30  93  22 110  41 122  70 109 103   4  45  34 135  35  43 104
   5  56  71  36  65  92  86  68  27  80 125]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5837
INFO voc_eval.py: 171: [327 348 508 144 434  74 141 252   9 476 466  99 169  49 477 352 289  15
  21 498 227 456 478 377 205   2 233 469 264 247 228 374 191 430 454 259
 380 168  98 129 376 385 404 194 490 437 440 190  32 447 287  85 254 465
 522 297 328 336  86  26  52  95 202 482  55  16 282 261 256 307 221 192
 319 193 257 375 161 290 407 354  31 272  46 209 313 356 225  23 459  11
 326 182 181 273 296 363 262 462 329  27 441 453 156 310  35 150  50 487
 500  60 445 270 408  63 481 492 186 311 173 103 429 302 394 102 172 127
 265 216 317 235 395 472  71 188 130  30 110 267 515 410 234  90 184  78
  48 512 258 120 468  45 450 387 260 461 442 165 362 421 118 521 321 200
 484 398 197 230 224 497 196 166 266 219  37 278 142 198 436 323 337 203
   0 338 153 475 226 295 464 269 279 510  22  25 486 152 217  12 443 271
  38  28  40 473 335 242 438 458 109 179  20  24 138 175 105 324 212 334
 291 160  33 148 299 507 390  51 312 308 503 467 353  97 263 433  61  47
  36 285  14  70 281 136 428   5 340 121  29 480 371 511 494 419 320  57
  59 284 201 451 229 489 514  83   8 199 239 409 185 325 303 187 306 463
 344 143 180 509 483 396 488 373  89 401 167 424 218  10 237 207 413 397
 314 342 214 355 504 223 361 384 346 388 124 238   7  82  77 420 244  42
 452 460 523 255 300 133  72 359 119  69 162 506 516  34 236 406 288 518
 176 330 106 157 111 444 422 220 386 211 117 146  96 435 125 292 274 364
  43 222 372 107  62  19 171 158 275 349 243 101 177 339 155 108 178 423
  17 357  44 350  73  66   4 189 204 431 147 100 123 347 417 116 134 170
 304 293 241 298   3 470  56 151 455 400 126 427  53 501 351 493  13 215
  58 280 331 135 496 368 412 366 115 389 505 113 439 208 140 519 301 251
 343 341 122  64 405 370 393 381 448 104 315 520 382  18  92  76 174 513
 149 183  41 112  39 154  65 305 195  84 402 128 163  87 114   1 248 449
 416 383 137 391 499 471 414 283 132 485 131  91 358 415 231 517 139  67
 250  54 213  79 276 164 425 286 145 232 457  81 316 318 322 245 309  68
 411 426 432 479 360 369 365 491  94 379 345 399   6 206 474 446 240 294
 418 367 246 253 249 333 332 392  80 403 495  75 268 210 277 502  93 159
 378  88]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4291
INFO voc_eval.py: 171: [ 4  2 11 36 12 18 41  0 14 30  3 24 15 32 17  7 23 21 29 22 40 39 37 34
  5 10 42  1 25 33 38  9 19 27 16 13  8 43  6 26 35 31 20 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0022
INFO voc_eval.py: 171: [1606  387 1607 ... 1351  609  290]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.4931
INFO voc_eval.py: 171: [27  2  0 52 22 40 47 20 14 61 56 16 19 37 11 41 33 51  4 58  1 50 13 44
 60 64 34 24  3 30 49 26 46 18 28 10 21  5 57 59 43 31 48 29 12 42 23 25
 53 15  7 39 35 54 17  8  6 63 38 55 36  9 32 62 45]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1807
INFO voc_eval.py: 171: [358 164  89 157 309 177 235 231 218 234 141  49 315 166  33 129  68  77
 331  16 273  15 202  34  47 319  76 360 160 378 297 314 161 298 140 228
 308 245 272  92   1 332 379 255 328 384 244 321  27 110  22 183 268  90
 293  24 135 376 252 108 339 181 345  64 258 239 307 238 150  80 266  61
 207 354  20 393 403 374  62 316 142 310  43 269 126 329 250 380   7  69
 289 397 322 399 226 154 201  23  55 350 143 128 232 274 367 377 361 165
 139 225  97 133 363 335 122 389 163 386  13 106 405  10  14 259 203 115
 279 340 130 383 138 333 348 216 109 112 118  39 291 283  93 280  17 334
 131 127 391 233 370  99 267 137 373 193 179 301 237 178 246 351 229  58
  35 336 299 337 236  71 371 346 375 265 219 197 224 390 105 323 327 149
  79  32 344 290 136 396 366  59   4  48 302 318  84 355 257 294 304 242
 368 253 285   8 248  11  95  19 155 347  12 176 404 103 395 381 209 167
  67 174   0 145   3  46 353 215  66  85 385 134 324  87 387 190   6 287
  53 392 365  86 400 222 369 111 180 317  96 205 186 194 282  44 214 191
  57 382 195 117  60 102 198 306 147 116  36 320  38 278 342 313  65 401
 247 330  28 221  30 212 263 153 286 230  29 256 262 120 292 173 398 364
 394 261 148 281 388  98 276 159  45  75  73 104  51 100 192  94 217 170
 241  41 264 151 349 356  81 208 124 277 254  63  37 251 200 240 372 352
 296 185  40 312 326 199 182 204 223 144 156 119 107  74 325 362 210  31
  18 187  52 101  82 188 158 184 220 189 169 275 171 172 168 260  91 146
  25   5 132 125 305 123 288  42 211  50 343  21  56 295 341 243   9 213
 402 162 270 113  88 359 152 175 311 121 338 303 206 406  26 284  54 300
 196 227 249 114   2  78  83 357 271  70  72]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3860
INFO voc_eval.py: 171: [ 13  87  27  30  74  52  53  47   4   1  21  31  50  79  19   0  67  48
  62  41  96  95  97  44  99  28  51  93 100  15  10  34  80  23  90  69
   2  92  43  40 101  72  68  77  89  84   7  83  22  18  86  61  54  71
  60  24  88  66  11  12  64  58  57  35  65  85   3  98   5  56  20  25
  59 102  94  76  32   8  36  38  81  49  73  55  17   9  45  16  29   6
  42  33  78  37  63  26  46  91  82  39  75  70  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0603
INFO voc_eval.py: 171: [66 78 57 59  8 11 44 83 81 54 25  9 56 47 70 46 67 76 73 61 12 45 80 60
 62 10 34 51 74 40 19 31 20 24 37 79  6 14 18 41 68 23 49  4  3 17 52 65
 58 71 42 32 27 64 22 36 30  2 43 75 63 28  1  7  5  0 38 50 77 69 29 48
 84 39 82 55 21 33 35 85 72 16 13 26 53 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3692
INFO voc_eval.py: 171: [ 31 245  22 153 106 219 160  35 258  15   8 190 182 113 161 175   7  87
 250 180  52  32 179  98 212 209  97  47 233  58 154  72 167 138  62  96
  76 140 117 120 181  90 139 262 232  19  46 163 215 146  99  59 108 148
 254   5 123 199 189 127  70 102 218  16  65 119  44 184  40 174 195 193
 171  21  20  53   1 197 256 204 206 217  83 226  94 183  26 130  64  86
 234  67 194  28 191 239 132 202 158  37 251  42  78  92 114 121 211 238
 126 263 214  27 220   2 225  30 223 176  74  36 259  57 115  43 124 147
 266 210 144 231  38 116  39 240  88   9 224  82 236 104 165 172  66 168
 173  13 110 136  41  33 186  73 162  48  54 134  93  25  45  61 177 244
 243 188 166 122 227 185 213 267 201 222 137 235 260 207  23  84 141  14
 128  18  91 152 257 145 205  51 156 129 103 178 249 125  75 241 100  24
 143 112 105  29  77  81 253  55 131 230   0 151 135 255  63 203 228 221
 155  71 170 196 150 159 118 169  69   6   3 264 261 142 149  50 229 246
  17  80 192 208 265  49 242  10 237  95  12 200 164 157 101  79 247 111
  89 248  60 216  34 187 109   4 133  56 198  11 252  68 107  85]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7639
INFO voc_eval.py: 171: [2111 1228 2129 ... 2174 1816 2198]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5962
INFO voc_eval.py: 171: [162 141 233 153 492 804 174 170 262 244   7 748 332  80 689 348 805  47
  83   6 483  81 751 100 524 429 245 264 711 454 470 749 652  82 537  86
 552  85 302 291 191 266 806 600 644 246 845 591 808 183 194 747 188 494
 210 650  65 555 306 408 160 553 807 730  64 745 715 303 651 140 821 540
 305  92  52 654 254 557 379 499 241 173 818  33 828  91 696  87 659 649
 149 207 561 158 399 364 267 104 398 756 726 681 386 469 816 753 599 653
 847 351 381  84 525 431 247   8 433 477 738 786 324 102 461  67 107 465
 750 823 227 321 103 735 617 663 159 655 344 755 137 559 521 736 545 495
 400 430  11 717 354 593 186 252 661 690 485 189 867 556 383 723 471 585
 115 404 515 432 573 708 458 873 827 144 758 788  40 349 382 843 475 415
 814 565  35 569 727 401 413 147 171 467 199 271 662 518 157 574 460 863
 567 688  70 165  76 298 849 587 625 568 119 192 542 292 830 448 706 597
 486 739 353 377 238  13 106 793 259 544 523 208 826 775 571 762 335 128
 817 684 687 308 664 239 355 558 740 478 594 701  54 193  77 109 866 166
  69 124 759 203 580 607  34 457 824 496 868 848   9 150 825 686 498 584
 516 514 474  28 611 274 269 318 256 250 637 560 586 164 289 393  89 685
   4 609 819 665 852 167 577 366 674 242 656 295 323 522 445 700 209   0
 258 249 757 709  99 468 752  94 297  90  17 253 255 669 742 658 672 437
  58  45 517 859 248 409 201 196 455 142 268 307 374 219 370 850  95 802
 260 317 397 718 645 520 812 304 720 548 320 838 315 531 357 513 410 638
 450  16 563 839  41 421 273 114 576 161 678 270  59 712 403 482 781 217
 202 787 549 776 438 296 872 854 464  48 769 508  10 387 216 263 407 858
 782 857 608   2 790 310 226 503 694 417 831 222 642 511 648 604 785  98
 809 676 763 612 666 683 657  57 365 449 677 532 352 143 152 134 490 472
 596 322 719 800 325 546 501 765 462 746  88 871 198 572 481 428 491 509
 411 671 489 704 703 502 714 741 846  32 345 390 601 811 136 230 453 313
 138 773 603 265  55 530 206 810 732 135  51 634 412 338  68 504 840 156
 451 334 419 792 803 257  53 130 777 606  21 822 598  66 211 702 500  12
 778 112 613 660  15 795 707 479  26 691  19 595 535 294 487 240 551 629
 721 529 874  74 456 372 121 327 331 861 724 290 422  71 550 235 768 869
  27 358 440 590 820 101  50 435 463 125 414 583 813 337 220 179 392 261
 359 426  72 510 154 312 190  20 646 133 111 341 343 647 363 729 784 232
 418 799 835 328 436 340 855 789 582 527 615 695 117 635 405 145 108 182
 376 285 122  25 493 578 543 223 710 716 870 132 425 771 579 185 592 155
 333 424 633 452 862 123  56 350 626  38 801 640 507 692 356  75 287 853
 212 443 197 815 213  44  61 172 369 554 204 618 744  60 588  18 670 215
 236  42  78 834  63 251 643 314 224 473 113 309 395 118 630 388 214 864
 151 722 622 420 536 371 276 614 311 733 272 391 148 610 180 832  49 844
 466 538  30  23 301 639 105 367 346  24 772 675 279 375 621  36 539 195
 770 632  79 682 734 441 693 779 378 439 526 384 865   3 368   5 860 833
  37 680 512 484 562 225 146 705 442 564 589 794 547 505 780 234 713 829
 605 280 764 667 754 316  22 286 791 362 627 275   1 497 476 396 447 528
 218 856 243 277  14 444  29 339 837 851  93 231 127  96 728 120 380 697
 184 385 679 533 624 783 163 330 628 774 300 519 329 434 636 616 177 178
 205 293 725 488  39 699 566 506 836 541 237 570 743 283 798 278 284 361
 797 116 842 342 620 336 200 281 221 187 446 767 575 731 228 176 480 181
 534 394  73 360 423 737 229 139 389 168 175 602 402 673 631 326 841  62
 169 623 110 416 129  97 668 766 299 373 126 282 641 347  43  31 319 796
 459 760 427 761 581 406 698 131 288  46 619]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5786
INFO voc_eval.py: 171: [47 28  5 46  0 39  7 41 44 43 59 66 61 27 24 60  8 16  2 49 54 18 32 11
 48 30  3 55 10 17 29 13 40 21 14 34 58 56 42 19 57 51 52 31 64 63 33 38
 12  6 15 50 45  9 36 22 62 25  1 53 37 20 65  4 35 23 67 26]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2909
INFO voc_eval.py: 171: [ 58  59  21  90  32  57 143  89 106  49   3  67  99  25  85  30  65  61
 130   4 126  76  47  86   9 113  95  23  44 136 118  43  97  27  52  24
  37  73  88  96  94  38 133  16 140 146  87  72  20  53  15  17  40 129
  92   0  10  41  50  69 141 138  80 105  78  75 119 128  63  66  34  84
  62 131  31  71 127  77 124  51   7  56  35 107  13  54   1 145  29  36
  48   2 120  42  98 135 114  91 142  68  11  39   5  83 101  46  79 144
  26 122 103 111  19 112  82  93  60  64 139  81   6  28  22 100 123 115
  55  33  74 132  14 125   8 117 116 121  45 134 102 137 110  18 104 108
 109  70  12]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3515
INFO voc_eval.py: 171: [110  18  69  39  15  58 141  28  80  60  94  36  14 111 138  82 150  38
  76 153   7 145 124  57  81  13  59  16  93  37  66  62  23  73 126 101
  20  55  42  17  97 104 131  92 159  32  74 122  86 108 155 121 132  53
 103 143  26  61  84 120  34 146  31  30  75  40 125  49 107 117  98  44
  46  87 119 142   6 100  71  91   4  63  70  96 160 102 106   0  41  65
  78  88  35  12 127 129 137 157  47   9 149   1 118  19  29  83 136 134
 113  27  67   8  25  45  43 130 147  99 139  72  90  52  54  56 112  51
 156 123 144   2 151 105  77 109 140  95  85  22  11  21   3  24 158 148
 154 152  33 114  48  68  10 128 161  79  64  89 116 115  50 135   5 133
 162]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3634
INFO voc_eval.py: 171: [ 90 150 223 238 211 301 329   3  26 180 138 288 325  19 171 239  57  84
  37  23 309 181 244 119 246  86  72  41 221 187 277 224 234 197 336  53
  76 226 118 212  11  56 331 245 210 231   1  55  40 183 134 110 120  60
 292  58 155 229 256 338  65 264 104 165 343 129 184 310  45 209 257 339
 254 278 311 241  35 286 261  83 344  22 258  54 320 125 160 230 327  85
 291 312 105  25 262 263  61 164 323  88 240 196 294 146  77  52 114 284
  24 260  48 348 235 275   4 280 154 145 117  97 186 321  74 326  64 266
 243   7 185 255 175 162  71  16 342 298 214 107 147 148  31 216 137 193
 206 341 322  92  33  95  51 273 115  17  63 317 156 142  73 307 281 140
  20 314 122  28 132 168  94 215 337 182 178  27 300 207 289  89 313  96
 293   9  46 130 108 265 233 191 123 345 303 282  38  70  59  80  43 270
 103 204  62 283 139 194 297   5 249 100 324 133   8 192 102 177 189 174
 121 346 198 195 166  14 347 319 228 237 276 305 259  75 112 296  10 153
 340 328  81 128  44 200 158 222 152 330 201  18  42 199 173 332 295 203
   6 251 141 144 170 279 172 151 268   2 287 272 208  29 306 269 350 106
 159  67 232 176 127 143 274 167 179 190 101  68 169 334 253 252 242 220
 267  79 304 248 290 116 113 250 227 335  69 111 236  49 302 149 188  93
 318  30 349  32 225  13 213 109  78  66 333  98  47   0 217 136 157 285
 299  36  99 126  12  82  34 161  50  39 218 315 202 163  87  91 124 316
 219  21 131 247 135  15 271 308 205]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4459
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3989
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.360
INFO cross_voc_dataset_evaluator.py: 134: 0.597
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.321
INFO cross_voc_dataset_evaluator.py: 134: 0.579
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 134: 0.429
INFO cross_voc_dataset_evaluator.py: 134: 0.002
INFO cross_voc_dataset_evaluator.py: 134: 0.493
INFO cross_voc_dataset_evaluator.py: 134: 0.181
INFO cross_voc_dataset_evaluator.py: 134: 0.386
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.369
INFO cross_voc_dataset_evaluator.py: 134: 0.764
INFO cross_voc_dataset_evaluator.py: 134: 0.596
INFO cross_voc_dataset_evaluator.py: 134: 0.579
INFO cross_voc_dataset_evaluator.py: 134: 0.291
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.363
INFO cross_voc_dataset_evaluator.py: 134: 0.446
INFO cross_voc_dataset_evaluator.py: 135: 0.399
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 39999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.385s + 0.001s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.329s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.350s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.357s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.362s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.364s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.364s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.365s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.363s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.364s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.363s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.367s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.369s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.570s + 0.001s (eta: 0:01:10)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.367s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.368s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.363s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.362s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.363s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.372s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.371s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.370s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.370s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.371s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.371s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.369s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.466s + 0.002s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.387s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.365s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.367s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.365s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.364s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.362s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.361s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.358s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.358s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.359s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.361s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.359s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 1e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 30000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 10000, 20000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.1,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 10000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/ckpt/model_step39999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.547s + 0.002s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.401s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.386s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.379s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.377s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.375s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.374s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.377s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.379s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.377s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.373s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.371s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.370s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_dt-clipart-VOC/Nov04-20-33-03_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.741s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [229 177  90   5 286 218  56  17  44  49  48 143 121  58 293 171 167 135
 314 142  43 180   1 119 306 178  45 315 256 301  36 260 269 102  18 288
 130  70 149 113 238  82 186 125 206 281 134 148 116 150  27 271  79 226
  84  28  78 110 245 242 312 145  47  52 276 168 208 126 298 157 318  81
 139 240 146  34 101  95  91  59  66 261 268 273 104  11 287 131  71 223
 203 213 264  69 129 305  50   3 230 163 136 202 162  35  76 284 313 175
 109 307  54  16  32 309 197 201 239 227 183   6  53 159 279  94 170 322
 251 247 270 165  10 290 187  21 317  20  24  40 292  72 283 132   2 106
  42  63 275 217 214 254 215 103 153 100  86 173 199 253 319 220  96  39
 216 277 228 185 209 123   4 151 156 182  29 140 233  30  31 243  85 161
 294 195 164 302 160 278 299 232 166 221 248  41  14 321 188 303 211 122
 266 231  23  93 115 304  88  51 205 111 252 114  22 246 255 144  61 181
  73  89 235 133 207 244 300 120 250  87  68  57 189  80 147 272 154 169
 282  38 289  19 241 324 141 234  15  60 225   9  74  77  13 274  83 265
  97 172  33 311 190 296 118 210 212 191 196 280  12 224 107 152 285   8
 316 310 297 128  25 308 236 200 263   7  92 184 192  64  98 105 174   0
 117  55 291 127 193 237  26 257 138  67 222 249 158 258 259 137 295 112
 267  62  75  37 320 204  99  46 179 155 219 194 124  65 198 176 262 108
 323]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3606
INFO voc_eval.py: 171: [176 114  25  51 204 213 219 152 201 121  20 215 105  26  86 203 132  14
 172 162 155  24 205 188 150  89 200  79  60 111 217 148 138  13  49 160
 206  65 108 174  82 142 156  34   9 210 187  71  39 225 133 220  33  88
 218 184  94  90  92 170 207 157 139 193 109 110  11 221 122  36  78 173
 130 126   2 116 123  27  91  84  85  83 137  35  40 189 164  21  87 214
 198  31 112   7 106  77 166 119  98 171  52  58 178 141  53 120 231  68
  76 224 196  99 169  75 191 177 211 124  66 228 216 167 182 113 195 107
 154 134 128 118 227  19  96 197  74 186 103 194  43 125  48  64  45 230
  15 223 101  42 153  56  10 146 165 161  97  29 104  73 131 115 143 136
 226  18  54  81 145 185 183 117   5  16  95  37  80  38 199  55 222  17
   1 129 232 102 179 175  28  50 127  93 168 229  30  32 147  69 158  57
  44   0 135  22   4 144  23 192  70  46   6  62  61  47  63 209 163 149
 212 181 140 159 180  67  41  72 151   8 208 202 100  12 190   3  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5966
INFO voc_eval.py: 171: [ 62  27  82  49  50 124  68  52   1  79 150 169 152  59  60 171 170  51
 193   0  57  37  31 132 151  98  99  56  77 186  81  85 160 138 198  58
 113  35 185 158 107  71 143  24 112  16 129   4  61  55 141 189 173 123
  64 114 135  10  84 178  69  80 172 182 175 104 147 136  70  39 145 125
  91 133  19  65 181 139  74  94 197  29 153 164 157   8 146  21 195  18
 154 179  54 168 161 144 137   5  28  72  44 183 121 196 108  47 111 162
  67  76  12  48  15 187 188  17 142 103  95 148 105  36 159  41  96  34
 106 165  42  40  23  11  45  38  93 117 131   3 101 177  63  73 167 140
 174  43 163  89 176 116  53 110 102 119 166 134 191  97  13 109 184 126
  22 149  32  87   6  14 190  20  90 122  26   9  25  92 130  46  78 115
 100   7  88  75 127   2 120 156 128 194  30  86  83  33  66 180 155 118
 192]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2251
INFO voc_eval.py: 171: [309 370   0 310 100   9  48 311  80 252 135 345  28 244  52 454 343 251
 396 362 242 112 249 134  79 114   8  24 190 111  94 441 257 180 448 312
 392 185 165 253 371 397 467 226  81 299 302 189 291  36  95 199 256 390
 360 378  27 290 115 394 496 438 101 231 336 333 447  60 272 388 221 158
  31  37 235 469 288 207 386 471 262  61 316 148 239 406 446 338 426 166
  29 404 174 237 241 250 306 351  17 373 168 416 457 232 465 201 261 285
 422 474 144 167 412  62  41 389 121 182 255 391 377 104 184 263 423 289
 108 305 434 163 478 399 450 162  26 307  86 224 490 200 375 137 186 153
 424 282  40  38 194 481 150  64  35 460  46 417   7 281 342 176 271 348
 138 260 320 484 164 374 322 380 264 254 284 379 140 337  58 127 273 480
 410 493 203 357 227  73 210 409 120 352 154 462 468 470 209 407 139 213
 211   4 178  39 437 238 495 488 102 401 269  82  85 301 196  47 234 464
 258 433  87 275 219  51 103  25  42 449 160 445 355 492 177   2 205 169
 340  10 335 277 383 247 321  67 294  70  66 453 106 400  23 440 376 220
 364 483 123 131  63 347 318 324 246  30 141 475  57 461  71 173 466 133
 372  53 117 279 202 147 146 349 268   1  16 314 204 110 442 293 212 297
 332 236  56 366 119 353  84 230 431 155 118 430  49  19 287 482  34 472
 183 444 419 300 228 436 143 278 229 267 489 214 429 346 325 415  20 240
  92 195 191 403 313 428 331 393 193 451  78  72  68  98  33 159 192 223
 270 405 286 145 458 206 486 156  55 455 418 435   3 116 359  75  43  99
 411 432   6 308 485  44  14 368 172 107 126  12  21 233 304 125 109  59
 381 188 344 414 443 420  32 292  88  74 459  65 361  93  89   5 476 157
 326 170 175 280  83 216 130 323 358 402 425 113  69 356 179 477 122 283
 298  18 181 265 408  11 136 319 197 385 105  91 328 363 128 473 296 171
 274  50 243  13 142 456 382 222 218  22 491 315  76  97  96 151 152 427
 295 161 487  54 367 248 452 330  15 329 369 132 149 395 341 215  77  90
 384 350 198 259  45 339 276 129 327 187 494 354 266 365 208 479 217 225
 421 439 463 413 245 124 398 317 334 387 303]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3204
INFO voc_eval.py: 171: [452 453 244 458 207 454 252 210 113 310 561 376 371 455   3 208 562 251
 206 194 353 280 128 354 329 209 466 372   4 195 370 377 281 465  82 531
 373 480 457 501 525 442 472 460 459 174  81 502 397  33 467 456 487 268
 463 473 506 477 380  15 368 159 213  62 492 494  30 426 255 186 125 511
 375 254 404 356 293 378 461 445 542 147 262 156 489  79 304 200 379 136
 388 143  49 315 496 317 394 491 181 188 381 279 554 307 285 232 168 272
 401 393 405  48 316 214 471 319 235 545  11 260 158  50 277 224 177 547
  22 309 383 408 416 428  23 106 398 519   9 488 233 355 544 197 497   6
 212 549 298   5 360 400  39 564 256 231 567 508 344 325 369 367 529 515
   7  14 253 124 290  17 122 407 321  93 548 493 441 118 390  83 521 505
 438 470  28 275 436 282 283 211  32 141 468 415 462 532  40 363  91 226
 429  31 391 187 446 296 483 157 478 163 402 421 385 387  85 205  98  20
 439 258 420 559  43 286 338 130 389 339 294 261 144   8 523 184 129 216
  26  37 527  65 234  19 322 243 276 507 359 522 162 287 199 412 219  78
 447 409 469  21 249 145 451 514 536 278 284 366 357 425  76 263 443  87
  35 553 337 153  63 334 500 517 149 365  74 533 560 320 476   0 114  75
 131 411 198 432 151 246 257 301 202 324  60 427  58 563 312 418  84  64
 395 240 259  69 218  10 190  29 534  45  46 267 464 568 504 196 332 417
 241  97 220 225 346  66 314 556 165 422 406 170 384  34 222 223  89 140
 330 535 238 551 403 350 152 539  72 291  70 396  73  77 120 361  13 217
 414  52 123  16 166 482 292 119 248 116 423 139  86 509 171 570 374 516
 431  67 180 193 215 269 203 333  36 546 104 154 146 173 552 265 100 499
 273 137 175  57 410 448 133 245 191 565 484 449 569 335 341  18 179  92
 520 540  96 135 274 250 164 538 242 303 302 386  12 204 318 550 331 183
 486 227  54 518 513  94  71 433 503 169 126 313 475  90 485 557 111 444
 239 326 308 364   2  51 311 541 352 490 343  68 138 270 189 392 264 349
 498  38 479 413 328 155 300 109 566 440 555 134 524  95 474 230 323 201
 342 348 101 221 437 495 148 185  41 537 105 161 305 358 192  88 107 176
 102 297 117 271 530 399   1 108 160 435  44  61  42 236 340 295 512 510
 543 528 288 419 229  56  59  24 121 558 345 237 172 327 112 115 424 299
 142 382 182 178  27 103  99 228 351 336 110 306  47 289 167 430 247 127
  80  53 526 132 481 434 362 450  25 150 266 347  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5790
INFO voc_eval.py: 171: [ 31  14  50  53  75   3  58  24  64  13  93 119 126  82  89 128  72  78
  28  87  63  97 101  37  77 120  65   8  80 127   2  86  57  88  54   1
   9 100 135  73  98  90  61  19  23  20 114  46  38  18 115  25   6 113
  52  83 123 105   7  39  47 122 104  26 106  21  32  94  17  84 129  81
  66 111  11  62 132 117 107  49  95   0 118  10 133  76  15  96  99  33
  71 130  68 131 110  55  40  74  48 116  44  42  12  60  59  51  16  29
 112 125  30  22  92 109  41 121   4   5 108  69 102  45 134  34  35 103
  43  56  36  70  91  85  67  27  79 124]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5837
INFO voc_eval.py: 171: [326 347 143 510 435  74 140 251 478   9 468 167  98  49 479 351 288  15
  21 500 226 458 480 376 204 232   2 471 263 246 227 373 189 430 456 258
 379  97 166 128 375 384 403 192 492 442 438 188  32 449  85 253 286 467
 524 296 327 335  26  86  52  94 200 484  54  16 281 260 255 306 190 318
 191 256 374 160 289 353 407  31 271  46 208 312 355 224  23  11 461 220
 325 180 179 272 295 362 261 328 464 443  27 455 155 309  35 149 489 502
  50  61 447 269 408  63 483 494 184 310 171 102 429 301 393 170 101 126
 264 234 316 215 394 474  71 186 129  30 109 517 266 410 233  78  89  48
 182 514 257 119 470 452  45 259 386 164 463 444 421  12 361 117 320 523
 486 198 397 195 229 223 499 194 218 265 165 277 141  37 196 437 322 336
 201   0 337 152 477 225 294 466 278  22 268 512  25 488 445 151 216 270
  38  40  28 475 334 241 439 460 108 177 104  20 137  24 173 221 290 323
 211 333 159 298 147  33 509  51 389 311 307 505 352 469 262  60  96 433
  47 284  36  14  70   5 428  29 135 280 120 339 482 513 419 370 496 319
  57  59  83 453 199 238 228 516 491 283 197   8 343 183 409 324 302 305
 185 142 465 178 511 485 490 395 372  88 400 424  10 206 236 217 413 341
 213 396 313 360 354 222 506   7 383 237 123 387 243 345  77  82 420 462
 454  42 525 254 358  72 132 299 118 161  69 235 508 518  34 174 520 406
 287 105 329 446 110 156 385 145 219 422 436 124 210 116  95 273 363 291
  43 100 106 169 274 348 338 371  19 157  62 175 154 242 176 107 356 423
  17  66 349  44  73 202 187   4 431 146 168 346 292 122 417   3 115 133
  99 297 240 303 472 503 457 125  56 399 150  53 427 350 440 214  58 134
 388 330  13 279 412 367 495 139 507 365 498 112 207 114 300 380 369 450
 392 250 340  64 342 405 404 121 521  18 172 522 103 314  91  76 381 153
 515  41 148 181 304 111  39 193  65 162 127  84 401   1 113  87 247 390
 451 416 382 136 414 282 501 130 519 357 230 212 473  67 487  55  90 138
 415 131 231 459 249  79 275 285 163 144 425  68 321 315 411 481 432 308
  81 244 426 359 317 493 344 378 368  93 239 448 398   6 418 205 293 366
 364 476 245 248  75 252 209 267  80 402 332 497 391 276 441 158 203 434
 504  92 331 377]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4280
INFO voc_eval.py: 171: [ 4  2 11 37 12 18 42  0 31 14  3 24 15 17 33  7 23 21 29 22 41 40 38 35
  5 10 43  1 25 34  9 39 19 27 16 13  8 44  6 26 36 20 32 28 30]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0022
INFO voc_eval.py: 171: [1608  385 1609 ...  829  102 1068]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.4928
INFO voc_eval.py: 171: [27  2  0 52 22 40 47 14 20 61 56 16 19 37 11 41 33 51  4 58 49  1 13 44
 60 64 34 24  3 30 50 46 26 18 28 10 21  5 57 59 43 31 48 29 12 42 23 25
 53 15  7 39  8 54 35 17 38  6 63 55  9 36 32 62 45]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1807
INFO voc_eval.py: 171: [359 164  89 157 310 177 235 231 218 234 141  49 316 166  33 129  68 332
  77  16 273 202  15  34  47 320 361  76 160 379 315 298 161 299 140 228
 309 245 272  92   1 333 380 255 329 385 244 322  27 110  22 183 268  90
 294  24 135 377 252 108 181 340  64 346 258 239 308 238 150  80 266  61
 207 355 394  20 404 375  62 142 317 311 269  43 126 250 330   7 381  69
 289 323 398 400 226 154 201  23  55 351 128 143 232 274 362 368 378 165
 225 139  97 133 364 336 122 390 163 387  13 106 406  14 259  10 115 203
 279 349 130 138 341 334 216 109 384 118  39 112  93 283 291 335  17 280
 127 131 392 233 267 371  99 137 179 374 193 302 246 237 178  58 352 229
 337  35 338 376 300 236 347 372 265 197  71 219 224 391 105 324 328 149
  79 290 367 345 136  32 397  59   4 303  84 356  48 319 295 257 305 253
 242 285 369   8  11 248  19  95 155 405  12 176 348 103 396 382  67 174
 209 167   0 145  46 354 386 134  85 215   3  87 325  66 388 190   6 287
  53  86 393 366 401 111 180 370 222 194 205  96 186 318  57 214 383 195
 191 282  44 116 117 307 321 102  60  38 198  36 147 278 343  65 402 247
 314 212  28 331 221 263  30 256 286  29 153 262 230 293 399 173 120 365
  75 148 261 276  45 389 159 281 104 192  51  98  73 100 395 217 170  94
 264  41 241 357 124  81 277 151 254 350 208  63  37 251 373 240 297 200
 185 304 353  40 313 327 182 199 204 223 144 107 119 363  74 156 210  18
  31 187 326 101  82 184 158 171 188 220  52 189 168 275 172 260  91 146
  25 125 306 123   5 211  50 169 342 344 132  56  21  42 288   9 213 243
 403 162 270 113  88 360 152 175 296 312 121 339  26 206 284  54 407 196
 301 358 227 249 114   2  70  72  78  83 271 292]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3861
INFO voc_eval.py: 171: [ 13  87  27  30  74  52  53  47   4   1  21  31  50  79  19   0  67  48
  62  41  96  95  97  44  99  28  51  93 100  15  10  80  34  23  90  69
   2  43  92  40 101  72  68  77  89  60  84   7  83  22  86  18  54  61
  71  24  88  66  11  12  64  58  57  85  35  65   3  98   5  56  20  25
  59 102  94  76  32  36   8  38  81  49  73  55  17   9  45  16  29   6
  42  33  78  37  63  26  91  46  39  82  75  70  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0599
INFO voc_eval.py: 171: [66 78 57 59  8 11 44 83 81 54 25  9 56 47 70 46 67 76 73 61 12 45 80 60
 62 10 34 51 74 40 19 31 20 24 37 79  6 14 18 41 68 23 49  4  3 17 52 65
 58 71 42 32 27 64 22 36  2 43 75 63 30 28  1  7  5  0 38 50 69 77 48 29
 84 39 82 55 21 33 35 85 72 13 16 26 53 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3688
INFO voc_eval.py: 171: [ 31 247  22 155 221 107 162  35 260  15   8 192 184 115 163 177   7  88
 252 182  52  32 181  99 214 211  98  47 235  58 156  73 169 140  62  97
  77 142 119 122 183  91 141 264 234  19  46 165 217  16 148 100  59 129
 109 150 256   5 201 125 191 103 220  65 121  44 186  40 176 197 173 195
  21  20  53 199   1 258 208 206 219  84 228  95 185 132  26  64  87 236
  67 196  28 193 204 241 134 160 253  42  37  93  79 116 123 213 240 128
 265 216 222   2  27 227  30  70 225 178 261  75  36  57 117 126  43 149
 268 212 146 233  38 118  39  89 242   9 226  83 238 105 167  66 174 170
 175  13  33 138 111 188  41  74 164  48  54 136  94 179  45  25 245 246
  61 190 215 229 168 187 269 124 203 224 237 262 139  23 209 143  85  92
  14 130 259 154  51 158 207 147 131 127 104  17 251 180 101 243  76  24
  78 114 145  29  82 106 255  55 133 232 153   0 137 257  63 205 230  71
 223 198  72 157 172 161 152 120 171  18  69   6 266   3 263 231 151 144
 248  50 194 267  81 239  49  96  10 210 244 202  12 166 249 102 159 250
  90  80 112 218  60 110   4  34  56  11 200 135 189 113 108  86  68 254]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7509
INFO voc_eval.py: 171: [2118 1233 2136 ...  909  268 2083]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5970
INFO voc_eval.py: 171: [162 141 233 153 492 804 174 170 262 244   7 748 332  80 348 689 805  47
  83   6 483  81 751 100 524 429 245 264 711 454 470 749 652  82 537  86
 552  85 302 291 191 266 806 600 644 246 845 591 808 183 194 747 188 494
 210 650  65 555 408 306 160 553 807  64 730 745 715 303 651 140 821 540
 305  92  52 654 254 379 557 241 499 173 818  33 828  91 696 659  87 649
 149 207 561 399 158 364 267 104 398 756 726 681 386 816 469 753 599 653
 847 381 351  84 431 525 247   8 433 477 786 738 324 102 461 107  67 465
 823 750 321 227 103 735 617 663 159 655 344 755 137 559 736 521 545 495
 400 430 717  11 354 593 186 252 661 690 485 189 867 383 556 723 471 404
 585 115 515 432 573 708 458 873 827 144 758 788  40 843 349 382 475 415
 814 565  35 727 569 401 413 171 147 199 271 467 662 574 157 518 460 863
 567 688  70 165  76 298 849 587 118 568 625 292 542 830 192 706 448 597
 739 486 377 238 259  13 353 106 793 543 523 208 571 826 775 762 335 128
 817 684 687 308 664 355 558 239 478 740  54 701 594 193  77 109 866  69
 166 759 124 203 607 580 457  34 824 496   9 848 150 868 825 686 498 584
  28 474 514 274 516 611 256 319 269 250 637 560 164 586 289 393  89 685
   4 609 819 665 852 167 577 674 242 366 295 656 323 445 700 209 522   0
 258 249 757 709  99 468 752  94 297  90  17 253 255 669 741 658 672 437
 248  58 142 517 859 268 409 455 196 201  45 374 370 307  95 802 219 317
 850 260 397 645 718 812 304 520 720 548 320 838 638 513 357 531 450 315
 410 563 839  16 114 421 576  41 161 678 273  59 270 712 403 482 781 217
 787 776 549 464 296 202 438 872 854 769  48 508 387 407 216  10 858 263
 790 857 608 782 503 226 310 417   2 694 831 222 642  98 676 648 763 809
 604 683 657 666 511 785 449 612  57 677 365 152 490 532 472 134 352 143
 546 596  88 800 719 501 765 322 325 462 746 481 871 428 198 491 572 509
 411 489 671 703 502 714 704 390 811 345  32 846 601 773 138 810 230 136
 313  55 530 453 265 206 135 603 732 504 338  51 634 840  68 156 334 451
 412 419 792 257 606 130 822  21 803  53 777 211  66 598 702 112 778  12
  26 500 691 479 707 660 613 487 595 294 121  19 240 795  15 535 529 629
 551 874 861  74 372 724 331 456 290 721  71 235 358  27 550 768 422 869
 327 440 820 590 435 813 414 125  50 463 101 392 179 220  72 154 359 510
 337 261 426 133 647 646 190 341 312 583 111 418 343 363 729 799 340 835
 232 328  20 436 855 117 527 695 789 784 582 182 635 405 108 376 615 145
 578 285 493 122  25 131 155 870 771 592 185 425 579 710 333 350 626 123
  38  56 633 544 862 223 716 640 507 213 692 287 801 424 452  75 356 815
 197  44 212 853 204  61 554 172 618 443 369  63 588  60  18 215 670 630
 314  42  78 834 473 236 643 251 224 395 151 309 119 214 113 722 276 388
 311 536 371 864 272 733 622 420 614 391 180 466 610  49 832  23 538 675
 621 195 539 639 105 772 844 346  24  31 301 375  36 148  79 279 439 441
 384 734 378 146 779 632 693 770 367 526 682   3 833 860 368 865   5 484
 562 225 680 512 705 442 280 829 234 564 528 547 794  37 713 589 780 754
 447 667 362  22 627 277 791 605 316 764 218 275 497 243 505 476 396 286
   1 120 697 380 444  29 837 728 127  96 231  14 856  93 851 339 624 783
 184 385 628 679 774 330 533 163 237 519 177 742 725 178 300 616 329 205
 434 636 570 836 488 541 566 506 221 797 283 278 798 293 361 699 744 534
 342 200 842 446 336 187 620 284 743 575 731 228 176  39 480 116 767 737
 673 402  73 394 229 841 389 281 181 139 168 423 360 175 602 110 623  62
 169  97 326 129 631  30 347 427 581 796  43 299 766 761 282 416 760 619
 126 641 373 459 288 318 668 698  46 132 406]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5799
INFO voc_eval.py: 171: [48 29  5 47  0 40  7 42 45 44 60 67 62 28 25 61  8 17  2 50 55 33 19 11
  3 49 31 56 10 18 30 14 41 22 15 35 57 59 43 20 58 52 53 32 65 64 34 39
 13  6 16 51 46  9 37 23 63 26  1 54 12 38 21 66  4 36 24 68 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2909
INFO voc_eval.py: 171: [ 58  59  21  90  32  57 143  89 106  49   3  67  99  25  85  30  65  61
  86 130   4 126  76  47   9 113  95  23 136  44 118  43  97  27  52  37
  73  24  88  94  38  96 133  16 140 146  87  72  20  53  15  17  40 129
  92   0  50  10  41  69 141 138  80 105  78  75 119 128  63  66  34  84
  62 131  71  31 127  77 124  51   7  56  35 107  13  54   1 145  29   2
  36  48 120  42  98 135 114 142  68  91  11  39   5  83 101  46  79 144
  26 122 103 111  19 112  82  93  60  64 139   6  28  22 100 123 115  81
 125  55   8  33  74  14 132 121 134 117  45 116 102 137 110 104  18  70
 108  12 109]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3605
INFO voc_eval.py: 171: [110  18  69  39  15  58 141  28  80  60  94  36  14 111 138  82 150  38
  76 153   7 145 124  57  81  13  59  16  93  37  66  62  23  73 126 101
  20  55  42  17  97 104 131  92 159  32  74 122  86 108 155 121 132 103
  26  53  61 143  84 120 146  34  40  75  30  31  49 125 107  98 117  44
  46 119  87 142   6 100  71  91  63   4  70  96 160 102 106   0  41  78
  12  88  35  65 129 137 157 127  47   9 149   1 118  19  29  83 136 134
 113  27  67  25   8 147  45  43 130  99 139  72  90  52 112  54  56 156
  51 123 144   2 105  77 151  95 140 109  85  22  21  11  24   3 158 148
 152 154  33 114  48  89  68  79  10 128 161  64 135 116 115  50   5 133
 162]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3634
INFO voc_eval.py: 171: [149  89 223 238 211 301 329   3  25 179 288 137 325  18 170 239  56  83
  36  22 309 180 244 118 246  85  71 187  40 221 277 224 197 234  52 336
  75 212 226 117  11  55 331 245 210 231   1  54  39 182 109 133 119  59
 292  57 154 229 256 338  64 264 103 164 343 183 128 310  44 209 257 339
 254 278 311 241  34 286 261  82 344  21  53 320 258 124 159 230 327 291
  84 312  24 104 262 263  60 163 323  87 240 196 294 145  51  76 284 113
  23 260  47 348   4 235 275 280 153 144 116  96 186 321  73  63 266 243
 326   7 185 255 174 161  70  15 342 106 214 298 147  30 146 136 216 193
 206 322  91 341  94  32 273  50 114 317 155  16  62 141  72 307 281 139
  19 314 121  27 215  93 167 181 131 177 337  26 300 207  88 289 313  95
  45 293 129 233   9 107 265 122 191  37 303 345 282  61  58  79 138 283
  69 270 204  42 194 249 102   5 297 324  99 192   8 132 101 176 346 189
 120 198 173 347 165 228 195  14 319 305 276 237 111 259 152  74 127 296
  80 328 340  10  43 157 200 151 330 222 201  41  17 199 332 172   6 295
 140 203 251 279 150 184 171 272 143 268 208   2  28 169 287  66 158 232
 105 306 350 274 175 269 126 142 166 190 178 168 242 253 220 334 252 248
  67 100 267  78 304 115 112 290 250 227  68 110 236 335 148 188  48 302
  29 318  31 349  92 108  13 225 213  77  97  65 333  35  46   0 285 135
 156 217 299  12  98 125  33  81 160  49 219 218  38 316 123 162  86  90
 315 202  20 247 205 308 134 271 130]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4387
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3983
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.361
INFO cross_voc_dataset_evaluator.py: 134: 0.597
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.320
INFO cross_voc_dataset_evaluator.py: 134: 0.579
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 134: 0.428
INFO cross_voc_dataset_evaluator.py: 134: 0.002
INFO cross_voc_dataset_evaluator.py: 134: 0.493
INFO cross_voc_dataset_evaluator.py: 134: 0.181
INFO cross_voc_dataset_evaluator.py: 134: 0.386
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.369
INFO cross_voc_dataset_evaluator.py: 134: 0.751
INFO cross_voc_dataset_evaluator.py: 134: 0.597
INFO cross_voc_dataset_evaluator.py: 134: 0.580
INFO cross_voc_dataset_evaluator.py: 134: 0.291
INFO cross_voc_dataset_evaluator.py: 134: 0.361
INFO cross_voc_dataset_evaluator.py: 134: 0.363
INFO cross_voc_dataset_evaluator.py: 134: 0.439
INFO cross_voc_dataset_evaluator.py: 135: 0.398
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
