Start testing on iteration 499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.399s + 0.001s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.345s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.351s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.352s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.362s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.367s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.370s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.369s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.369s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.369s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.371s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.370s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.371s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.449s + 0.001s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.378s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.394s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.384s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.383s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.384s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.389s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.387s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.381s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.381s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.381s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.381s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.378s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.454s + 0.001s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.381s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.370s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.375s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.371s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.369s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.365s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.366s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.367s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.365s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.365s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.366s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.368s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.521s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.381s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.376s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.372s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.371s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.363s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.363s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.363s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.362s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.365s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.363s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.362s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.362s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.167s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 62 229 135  54 279  55   3  53  63 153  28 304   6  88 138 286  29 110
 187 221 102  86 103 140  60 208 237 283  38  34 145 188  21 130   9 297
 176 293  40 226 246 129 184 253  11  30   4 105  80 147 198   7 133  75
 136  90 256  89  87 317  83 152 156 162  56 259 314 302 275 118 100 278
  94 217 247 150 178  16  85 264 159 160   0 169 121  26 182 141 271 104
  13 231  37 101 262 196 205 245  59 215 183 288 292 158  67 308 251 318
  32 273   1 132 228  66 157  46 167 250 173  18 305  42 126  49 321 309
  73  48 211  93 111 120  58  57 195 171 148  81 310 202  69 258 307 306
 214 209 284 175  96 236  52 186 144 164 166 260  84 255 127  97  15 263
 193 180  10 197 199 298 123  91 163 300 155  25 285  47  27 232  98 223
 181  19 224 225 216  79  44  51 206  64  12 243 244 280 165  35  39 291
 134 240 233 235  95  23 142 222 238  61 274 227 270 272 192 170 143 234
  78 113 296  50 151  36 219  70 146   8 194 282  99  72  82 149 277 139
 311 218 290 204 248 241 289 210 131 114 281 242 200 303 174   5 128 207
 267  77 212 179 312 213 266 316 268 315 125 168   2 185 265 177 294  74
 122  68  31 323 137 172 287 230  22 295 319 109 106 320  92 249 119 257
 154 276  20 254 269 220 108  65 116 115 322 189  33 301  17 107  45 261
 190  24 117 112 313 191 299  43  76  71 203 252  14  41 124 201 239 161]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3284
INFO voc_eval.py: 171: [ 21  41  93 154 132 164  96 166 170 133 177 173 172 107  92 174 108  26
 109 147 136  58  11 121 135 141  60 151  62  48  17 134 171   6 162 163
 180 129  80 110  18 167  28  73  65  72 125  36 158 152  13  78 143  83
 102  34  23 175  75  61  10 131   2 183   7   0 111 116  71 161 179  77
  64  25  37 157  12 160 178  27 122  88 153 168  49  24 176   1  94  70
  45  67  19  42  14 137  44  84 146  95  16 139 123  79  30  90  46  99
  32 104 169 128 103  50  29  89  55 114  98  47 144 117  82  69 155  87
   3 100 140  66 106 105 182 138  85  63  20  97  91 184 124  40 112  57
   4 181  31 115  33  56   9  54  51  43 150 159 165 126   5 156 118 149
 130 145  39 119  86  81  35  15  22  52 113 127  74  76 142  59  38 120
   8  53 101 148  68]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.7018
INFO voc_eval.py: 171: [208  82 231   6 384 211 234 458 162 163 456 248 161 490 432  27 120 537
 489 488  84 357 348 224 405 386 106 459 194   7 246 545 292  37 368 539
  36  65 290 373 114  94 223 213 269  39 517  88 536 512 193 392  70 150
 109 122 134 453 179 225 455 508 272  68 282 413 431  19 547 151 496 415
 113  67 265 328 136 436  35 471   5  48 390 130 260 366 397 497 295 212
 254  25 195  46 516 540 274 324 356 441 222 555 358 261 167 318 280 247
 105 343 334 168 181  91 472 527 483  15 172 115 268 538 221 209 119 544
 549 111 482 229  76 259 414  62 251 451 187 284 533 135 380 285 363  97
 257 108 126 190 464 506 133 147 230 503 467 396 184 367 153 158 141  55
 484 236 427 241  57 435 332 501 500 479 226 264 372 245 395 138 127 197
 214 140 258  30 306 522 116 145 369 298 511 485 400 148 215   8 192 382
 270 242 294 325 401  92 499 437  71 364  86 403 142  89 439 252  74 476
 430 315 398 169 450  61 387 323 310 374 107 291 449 243 329 183 322  42
 171 228 417 220  43 365 411   3 342 469 521 526 319 530 477 552 389 331
 178  49 273  83 253  14 553  20 393 494 421 335 117 267 309  24  41 338
  53 219  31 518 296  16 510  34 379 157 262 470 125  47  98   2 299 281
 177  40 186 360 548 406 201 287 452 353 495  66 255 227 316 394 491 424
 121 232 336 399 418 445 498 419 429 303 330  50  69 426 104 333 203  87
 100 341 543 481 286 460 457 191 440 355  90  93 124 466  28 383 529 275
 311 519 433 509  44 256 110 391  22 313 362  56  45 244 170 474 486 408
 128 278 263 173  51 381 132 402 425 520 438  80  95 182 416 493 528 504
  52  17 551 279 185 218  58 349 266 154 535 361 377 301 327 410 297 523
   4 237 346 307 155 176 554 103  77 513 385 143 276 534 317 164 149  63
  26 320 217   1 412 478  60 289 407 454 216 139 505 146 238 102 314 370
 434  29  23  85  33  12 447 475 305 351 210 531 166 304 507 345 541  78
 118 532 160 359 308 340 101 131 409 420  81 404 198 375 300 174 200 376
 468 249 350 240 288 354 542 277 180 422  18 444  96 347  11 293 302 443
 344 487 442 283 428 326  72 156 189 175 448 492  38  32 312 463 112 165
 461 205 371 152 546 502 423 204 206 515 235 250 271 123 339  59 465   0
 202 388 378  10 462 550  21 446 352 337  99 239 188 137 199 524 207 233
 473 321   9 196  73  54 480  13 129 525 514  79 159 144  64  75]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.3247
INFO voc_eval.py: 171: [ 86 287 338 286  41   0 243  77   5 102 240 118 288 101 405 305 411 245
  44 315 171 366  26  21 331 242 270 165  16 232  79 289 160  82 105 277
 370 183  88 344 311 460 398 394 361 188  22 239 246 133 403 218 447 151
 263 273 351 267 322 321  85 412 457 241  67  17 142 126  54 271 120 235
 191 194 387 196 132 302  53 230  90 199 227 106 443 406 416 363 453 410
 210 378 292 290  42 353 329 170  60  36 349 310 244 343  27 222 454 459
 418 332 341 275 119  95 205 213 358 110 430 371  71 434  30   4 348 265
 113  59 432 428 381 367  43 284 197 320 247 229 420 117 176 417 248 386
  13 174 445 415 390 234 392 401  51 442  15 145 304 180  32 177  83 385
 342 306 276 437 423 369 455 254 238 268 256 125 309 337 262 264 446  18
 313 426  39  81  33 360 408 152  28 173 228 162  87 136  24 226 326 424
  64  40 175 279 103 274 404  69 167 131 123 409 323 121 169 182 414  14
 134 150   9 333  35 372 115 164 259 255 252 269 308  12 127 187 202 297
 156   6 439 376 359 224 163 384 451  23 407 295 458  78 350 330  31 303
  89 396  97 208 427 190 291 217 114 395  76 373  48 128 109 324 154  55
 278 107 380 250 347 231  92  25 362  75 377 201  74 357 307  52 346 124
 456  29  11 352 236 293  63 336 431 143 365 200 379 312  50 429  37 139
 168 211 421 327 155 285 140 144 161  45 318   1  62 179 122  57 364 283
 100 249 158 138 104 258 129 172 280 281 317  10 294 356 425  47 299 400
 462 397  68  56 209 296  72 300 225 221  34 178  99 438 450 157 237 261
 399 146 389 130 452 383 108 319 354 314 419 374 147 203 216 272  19 214
 375   8  61 192 137 382 334 298  66 207 204 461  91 223 444 141 166 206
 402 116 153 448 449 436  73 189 135 184 282  80   3 391 355 219   2  94
 388 253 316  70 435  46 328 186 148 340 422 220  93 111  49  65 345  98
 301 193 393   7 212 233 257 215 112 413 181  38  58  84 433 339 441 149
 368 440  96 198 325 251 335  20 260 195 266 185 159 463]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3272
INFO voc_eval.py: 171: [322 184 181 332 320   4 314 211 414 242   8 164 166 185 277 152 278 319
 153 266 168 372 362 413 167 165  92 327 267 279 321 255 308 121   7 323
 213 325 104 329  18 129 138 232 284 191  60 203 336 328 290 250  25  85
 161  98  23 236 147  28 149 281 112 357 394 280  31 227 324 338 344 248
 292 381 282 346 361 354 345 348 148 383 188 342 404 395 340 398  29  65
 359 390  13 111 330  68 254  93  37 201 365 130 144   6 353  58 311 246
  30 243 262 363 249 219 102 326 399  57 264  51 355 283 233 268 106  38
 373  21  53 299 298 415 410 251  88 158 315  11 258 137 371 204 295 120
  10 391 366   0 123  61 134 189 389 180  83 393 337 110  89 218 333 209
  20 117  44 334 347 114  70 265 341  50 291 289 206 306 235 368  96 351
 294  59 195 154 118  42 200 131 212 247 245  43 288  78 190  34 234 349
 159  22 272  94 226 356 301 151 160 270 274 287  74 198 293 352 241 408
 171  52 115 170 186 378 194 145 379 252 303 113  27 163 172 263 214  63
 259  90 302  36  47 139  54   2 409 316 126 124 225 313 196  56 331   5
  77  82 335  99  69 370  67  86 358 239 312 177 231 244 140 128 360 286
 169 386  62  26 307  79 222  14 392 407 178 260 176 238  80 155 100 269
 108 406   9 199 276 297  15 156 197 400  32 135 257  64  48  41 143 173
 416  81 127 296  33 174 150  87 304  72 382  19 187 275 364 207 240  16
  24 157 411 343 183 216 401 208 107 136 367 369  97 318 220  75 405 261
  46 179 109 101 300 273 350 256  66  39 387 105 374  35 146 162  49 193
  76 103  95 253  71 237  12 119 396  40 224 182 317 412 375 215 116 339
 132  17 310 305 125 142   3 202 271 221 397 228 377 385 141 217 223 309
 285 122 402 403  73  55 175 376  45 229 230  84 210   1 192 380 384 205
 388 133  91]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4813
INFO voc_eval.py: 171: [ 43  23 107  14  67  18  42  99  96  85   2   8  41  72 115  37  15  77
  55  50  20  17  54 105  13  89  52  66 104  79  27  93 103  49  98  53
  65  34  48  69   5 106  29  39  62  19  80  87  70  21   1  60 112 110
  16 101  78  94  44  82  75 116  71  84  95  38  61  35  97  36  92   0
  58  33  68   9  46  40  30  64  10  26  31  45 113   3 100 109   7  74
  90  59  24  76  56 111   4  25   6 114 108  73  12  88  11  91  32  22
  63  86 102  47  83  57  28  81  51]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5807
INFO voc_eval.py: 171: [217 223 102 295  56 294 115 313  98  72  22 154  42 227 148 290 282 198
 291 265 235 236  14 140 191 155 274   3  20 133 281 195 296 167 150 240
 242 171 304 178 238  74 117  62 157 156 276  41 283 220 263  49  88 130
 179 273 144  51 231 267 132  64 204  34 182  32 194  10 314 141 190 116
  73  12 197 219 216 287 215 136 277 173 201 250  36 259 213  39 252  58
 111 300 183  30 202  53 151  92 185 196 322 110  43 315 200   8   9  44
 237 269  55 298 255  78 121 184 307 244 221 323  61 108 170  71  52 258
  47 186  18 120 100  77 177 131 275 149  83 138 192 285   0 211 145 272
 143 310 234 266 180 189 256 147 246  85 137  90 302 254 251 158  91  93
 308 187 261 207  87  96 152 279  23 239 297 230 247 224  86 165 128 312
 301   4 245 316 292 212  21 280  25   1 162 181 260  29  11  46 309 112
 160 303 288  15 306  82  24  40 166 208 139  95  76  38 119  69  60 317
  19  33  48  63  66 241 209 168 284 188  45 257  54 163 205 270 127   6
 262 126  13 114 103  27 264  80  84 106 174 175 226  97  57 225 243 233
  50 109 228  59 107   7 293 206 311 123 124 289 268  16  89 169 305 199
 320 299 286  31 232  99 176 105 271  68   5 101 214  35  26 104 153 129
 172 318  70  75  65 118 249 278 248 193  37 134  17 319 122  79 159 142
 161 229  94 203  28 218 146  81 125  67 113 253 321   2 135 222 164 210]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4689
INFO voc_eval.py: 171: [ 7 14 76 65  3 30 22 19 70 20 17 61  0 56 29 74  2 31 50 34 27  5 46 24
 75 45 69 32 39 12 68  1 33 10 28 57  8 60 26 59 73 43 64 72 53 11 35 18
 77 63 52 15 67 37 49 54 40  4 25 55 44 58 36 71 42 41 23 21 13 66 16 38
 62  6 47 51  9 48]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0757
INFO voc_eval.py: 171: [236 990 993 ... 360 150 959]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5530
INFO voc_eval.py: 171: [ 5 29  4 77 50 89 71 95 21 48 27 49 25 82 67 10 94 34 40 60 92 20 36  1
 37 43 55 74 35 47 30 90 75 54 78 85 87 23 41  2 24 17  3 73 38 32 69 12
 26 93 44 65  8 22 80 76 59 14 16 33 56 57 91  7 68 98 53 83 58 15 52 72
 18 63 64 96 62 61 79 19 42 70 46 31 88  6 99 84 51 81 39  0 66 45 28 13
 11  9 86 97]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.3161
INFO voc_eval.py: 171: [329 144 139  30 283  72 103 158 197 196 289 204  43  10 190 310  17 206
 302 245 116 226 230 217 122 210 212 138 235  38 223 118 351 271 280  34
  12  86 291  11 272 117 105 163 239 175 297 106   2  48  59   8 263  52
 276 288 268 170 342   6 184  20  68  75 234 198 156 231 366 305 160 257
 147 348 356 224   9  36  32 349 244 261 113  23 303 252 368 161 255  78
 340 211 200 335  57 108 258  94  95 282 317 112 225 347 260 124 114  62
  25  41 373 213 338  21 339 269  55 145 346 227  90 330 188 326 168  40
 238  19 194 323 186 121 183 100  81 371 152 205 222 135 157 146 270 249
 321 218 153 318 292 142 181 237 228 248  96  47 207 362 293 123 111 233
  82 115  93  15  22 350  13  85 274  66 187  64 132  61 178 232  18 311
 167 307  89 173 120 242 119 273 306  51 182 162 236 199 243 140 312 355
  14 370  88 221 328 133  74 364 298 128 250 365 322 251 369  50  91 277
 148 172 208 164 180 325  76 313 334 141  69 296 294 357 216  99 354 275
 102 130 300 104 229 215 315 150 195  53 179 177   5 336 151 241   3 174
 107  79  73  92   4 332 286 290  44 219 193 202 137 149 189 327 299   7
 129 287 264  60  77 159  29 301  97 279  98 125 192  58  45 201 134 266
 256  54  31 155 143 333  26 165 154 209 191 166  35 358 295  63 343 309
  24 308  27 372 110  28 361 360 345 319 185 344 169 246 316 253 324  80
  83  39 267 176  87 214   1  49 109 127 247  37 331 220 359  70  67 126
 262 337 278  42  65 281 285  84 304 341  56 240  46 352 363 367 131  33
 259 254  71  16 203 101 320 314 284 171 136 353   0 265]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4402
INFO voc_eval.py: 171: [ 69  64 124   2  10   6 126 140  60  30  58  19  29  36  71  33  78  62
  66  59  57  43 103  23 133 131   1  35  80 134 102 105  32   7 141 130
   4  82 115  87  41 127  25  94 111   5  52  50  89  20 120  91 110  18
  55  53  28  70  46 128  49  61 123 125  15 107 112  56  85   8  63  40
  12  34  92  22 117  79  42  17  86  96  27 104 100  83   9  65   0  73
 129  76 109 108  99  37 118  21 101  72 121  45  24  95  16 132 119 116
 106  68  44  14  31  51 135  88  97  11 114  26  98  75  90  38 137  93
 138  74 136  39  54  48  84 143  67   3  13 113  77 122  81  47 142 139]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1846
INFO voc_eval.py: 171: [124  13 109  56  16 102  14 103  86  99  34  90  85 100 113  10  92  32
  76 128  17  78 111  19  65  60  36 122 119  37 107  46  50  61 104 126
  28   3  54  29  55  74 112  91  41 123 101  21 118   1 116  35   2  42
  43  11  73  33  77 110  88  64  51  45  66   9 121  12  31  59  57  87
  27   5  24  97  47  52  20  79  69 115  70  96  83 105  38  23  25 117
  22  93 127  72  49  39  40  82  30  53  81  63  44  95  26   8  62   0
  67  89  94  15  71  48 106  68 125   6  75  18  80 114   7 108  98  84
 120  58   4]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.4561
INFO voc_eval.py: 171: [ 25 115  13  80  60  82 119   6  56 120  66 114  90  26 109  44  32  49
  88 113  95  87  83 101  84  63  70   4  75  91 122  71   8  78 118 106
  97  43  40  24   9  28  69 102  35  68 100   2  86  96  76  53  81  11
  59  72  17  61  12  58  19  93 107  77 103  54  29 110  51  39 111  41
  67  33  45  65   7  89  99  48  15  92  52  50  74  16  31  21  79 117
  46  34  37  62 121  47   3  14  64 104   5  57  38   0  73  42  36  98
  20 105  22 116  10  55 112  27  23   1  85  30  94 108  18]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7725
INFO voc_eval.py: 171: [1783  413  299 ... 1432  710   84]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6909
INFO voc_eval.py: 171: [460 679 210 139  78 129 622  80 532 681 107 720 185 120   8 244  77  88
 298   9 243  79 462 230 292 591 503 739 457 407  87 722 486 654 416  98
 628  85 134 149 446 270 600 158 725 257 675 219  52 115 231 375 104 651
 242 154 731 770 339 525 342 491 232 121 544 706 685 341 354 267 667 485
 663 678 323 269 184 599 496 657 327 475 220 763 376  36 677 565 621 655
 670  10 481 125 504 659 392  92 224 528 419 336 730 126 478 445 723  48
 571 601  14 566 727 540 474 360 127 272 726 156 255 378 340 312 630 625
 661 624 222  47 602 262 614 615 430 174 590  42 664 295  27 268 527 721
 775 280 438 377 143 299  95 769 439 146 226  43 461 451 370 458  44  11
  69 645 259 311 494 437  86 386  39 699 593 406 482 760 136 287 132 680
  72  70  61 512 324 168 367 510 737 108  53 301 742 514 764 166 498  22
 744  82 585 192 470 202 682 409 497 709 603 398 533 700 275   6 546 325
 712 500  21 473 389 570 118 683 117 452 524 632 493 605 320 582  16 523
  74 738 261 459 467 549 695 576 668  13 704 235 551 623 638 606 164 421
 395 733 344  18   3 728 316 417 610 300 629 413 513 732 718 627 646 362
 302 626 286 190 550 155 529  84 536 539 328 778 253 418 238 113 698 199
 697 586 303 383 351 676 443 666 412 178 589 260 373 237  62  59 337 581
 689 643 355 686 711 468 329 372 672 380 569 353 394 433  40 771 660 189
 515  33 545  55 644 313 393 604 773 335 364 594 779 662 183 186 511 572
 761 201 746 144  31 194 487 294 568 307 171 352 290 172 708 471  63 707
  37 740 526 541 147 396 285 637 553 649 250 673 716 472 613 687 273 612
  28 138 276 150  38 399 671 345 483 574 631  90 162 448 236  56  93 428
 652 309 148 548 635 442 476 293  12  17 401 425 455   5 390  89 241 234
 177 402 357  51 505  94  76 560 449 245 647 426 691 304 180 772 447 279
 592 502 450 254 397 306 776 141 747 484 332 617  35 310  96 499 331 694
 233 423 780 124  58 101  34  64 112 130   0 400 296 346 318 387 435 562
 283 674 777 465 535 153 684 579 756  29 385 173 281 575 690 258 658  15
  73 271 422 552 501 578 701 159 207 743 145 229 705 609 384 388 424 634
 103 157 734 334 757 379 584 116 432 558  75 221 516   4 693 656 639 251
 489 102 371 308 239 135 374 263 209 750  23 110 431 411 343 543 106 636
 187 114 696 198 227 480 133 182 365 567 729 580 314 754 218 490  45 434
 492 361   2 530 618 440 765 212 215 479 369 151 724 463 368  68 347 317
 326 573 495 653  57 556 715  60 381  54 596   7  91 611  41 517 119 196
 358 289 179 741 542 128 608 420 403 321 508 169 167 223 284 669 277 315
  66 642 414 766  32 703 534 469 735 587 557 200 751 648 348 288  71  65
  30 710 752 519 208 123 195 131 319 175  97 408 356 441  49 349 160  24
 274 333  99 214 217 338 252 109  67 225 181 597 405 595 719 415 248 206
 464 520 759 188 748  25 713 105 454 216 444 264 641 240 203  46 665 100
 211 256 702 555 507 640 692 122 768 477 193 714 163 577 561 564 161 453
 265 152 429 142 197 767 509 522 774 688  50 330 213  83 736 758 322 506
 781 228 170 598 205 537 538 488 176 305 619 247 111 753 404 140 137 518
  81 165 554  26 755 588 616 563 366 410 382 204 456 436 466 531  19 297
 607 521 266 749 359 391 717 745 246 650 191 427 282 583 291 249 620 278
 547 762 363 350 633   1  20 559]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5701
INFO voc_eval.py: 171: [ 2 70 72  3 39 48 63 49 71 79 81 34 54 23 12 31  4 52 28 27 65 37  9 35
 75 25 16 13 10 78 47 77 82 44 57 51 64 53 40 76 56 41 30 46 15 24 11 42
 45 20 14 50 67  7 66 43 17 18 80 55  1  0 74 68 22  8 61 33  5 62 32 21
 60 38 26 36 73 59  6 29 58 19 83 69]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3325
INFO voc_eval.py: 171: [32 13 33 44 47 48  2 49 68 43 56 27 36 26  7 39 30 53  1  6 19 20 64 57
 37 65  9 25 69 42 14 63 41  4 50 28 51 54 59 60 21  3 12 61 66 35 34 46
 40 31 11 16 10 22 62 17 58 67 24 15  8 23 45 18 29 38 52  5  0 55]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.2566
INFO voc_eval.py: 171: [ 11  69 108  15  34  75 137  24  57  78   8  10  54  95  35 124  81 141
 153  14 134  29 111  91  51  66  13 119  56  73 147  39 120  58  80  46
 148  92  60 154 113  55  28  99   7 128   5   9 144 156  86  62  82  42
 143 149  79  20  59  76  19 129 104 105  90  84  26  48 125 123 107 152
   0  53   6  63  49  89  77  45  33 133 140 103  85 136  47  38  16 142
  74  65  52 146  30  22  83  72 106 122 158 117 115  70  68 131  31  23
  94 157 138  12 114 110 112  37 118  50  18  41 151  64 100 130  36 150
  71  21  25 102  87 132 126 139  88 145  43  61  97 155 127 121  32  96
   4  98   1 116   2  27  67 101 109  40  17  93  44 135   3]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5380
INFO voc_eval.py: 171: [ 96  56  31  69   2 141  13 104  76 101 125  41   9  30  93   8 143 130
 134 108  21  52 114  92 120  24  35 106 133  79  82  46  40 109  85  28
 103  68 148   5  84  97  51  50   4 150 112 115 113  61  10  95  71  49
  32  34  72  55  25  26  91 151  14  74  16  11 111 110  33 122  89  62
  47  59 144  27   6  15  70  75  66  65 147  87  22 135 131  23 121  99
  43  44  38  86  98  18  94 107  45  29 127 116 132 105 149 102  58  73
 124 119 100 136 123  77  90   3  78 142 139   7  12 126  39  17  53  42
 146 118  83 137   0 129  20  64   1  88  48  54  80  19 145 117  37  60
  63  36 128 140  67 138  57  81]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4621
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4431
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.702
INFO cross_voc_dataset_evaluator.py: 134: 0.325
INFO cross_voc_dataset_evaluator.py: 134: 0.327
INFO cross_voc_dataset_evaluator.py: 134: 0.481
INFO cross_voc_dataset_evaluator.py: 134: 0.581
INFO cross_voc_dataset_evaluator.py: 134: 0.469
INFO cross_voc_dataset_evaluator.py: 134: 0.076
INFO cross_voc_dataset_evaluator.py: 134: 0.553
INFO cross_voc_dataset_evaluator.py: 134: 0.316
INFO cross_voc_dataset_evaluator.py: 134: 0.440
INFO cross_voc_dataset_evaluator.py: 134: 0.185
INFO cross_voc_dataset_evaluator.py: 134: 0.456
INFO cross_voc_dataset_evaluator.py: 134: 0.772
INFO cross_voc_dataset_evaluator.py: 134: 0.691
INFO cross_voc_dataset_evaluator.py: 134: 0.570
INFO cross_voc_dataset_evaluator.py: 134: 0.332
INFO cross_voc_dataset_evaluator.py: 134: 0.257
INFO cross_voc_dataset_evaluator.py: 134: 0.538
INFO cross_voc_dataset_evaluator.py: 134: 0.462
INFO cross_voc_dataset_evaluator.py: 135: 0.443
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.494s + 0.001s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.358s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.369s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.368s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.377s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.377s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.380s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.379s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.377s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.374s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.373s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.373s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.374s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.627s + 0.002s (eta: 0:01:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.391s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.379s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.365s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.362s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.366s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.374s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.375s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.373s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.379s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.376s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.375s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.371s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.501s + 0.002s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.376s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.380s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.385s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.380s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.375s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.373s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.372s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.370s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.367s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.367s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.366s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.366s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.625s + 0.001s (eta: 0:01:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.400s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.378s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.375s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.371s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.366s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.367s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.373s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.374s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.373s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.373s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.373s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.373s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.282s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [179 104  52 223  56  45  46 122   2  27  88   5  24 242  47  25 173  73
 108  18  76  84 186 149 165  70 177 110 238  53 116  64 100 114 231  30
 137 193 150   8 237  35 204  86  99 105  10 148 202  28 191   6 157 121
  72  62  33 194  26  74  12 252 143 220  69 128 117 227  65  49  75 222
 127 232   3 124   0  95 106 209 241 254  48 255 182  80 102 200  32 146
  39  11  54 119 131 201 185 126 218  97  89 236  67 160 162 123 172 229
  59 207 212 166 138  40 154 134   1   9 101 211 144 247 221 155 159  34
  37 161 217 249  50 176 226 167  77 133 170 188 230 208 139 256  17  19
 129 234 190 206 219  15  78 203  44  92 168 111 141  83  14 246  23 171
  43  55  90 240 153 245   7 239 244  94 163 135 187 178  22  60 180  71
  16 216 233  63 181 228  81 213 145  31 248  96 113  68 125 132 251  91
 192 156  38  51 235 184 136  57 174 120 195  98 169 215 214 196  36 151
  58  87 112 175 118  66 142  93 158 147  29 115 109  61 103  21 210 183
 164  13 250 152 197   4 107 243  79 189  20 130  82 225 205 199  42  85
 257 198 224  41 253 140]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3439
INFO voc_eval.py: 171: [10 18 66 36 71 55 73 39 79 81 80 85 56 43 35 82 44 63 78 60 14 61 84 25
 58 86  6 72  2 65 62 57  4 42 48 59 23 51  7 11 22 32 69 20 77 29 31 70
 28 26 83 27 15 33 40 12 46 52  1 64 50 38 16 53  8 88 68 37 21 75 34  0
 30 45 47 49 17 41  5 87 19 74 24 13  9 54 76  3 67]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6927
INFO voc_eval.py: 171: [209  73 184   5 187 424 224 427 212 346 147  22 318 148 455 146  70  98
 400 120  90 503 454 456   6 204 223  55 353 326 173 269 512  37 504  60
 270 343 508 369  34  78 421 340 428  93  33 188 480 325 203 100 261 138
 135 483 202  75 472 360 403 150 249 423 105  89 506 112 356 103 161 438
  59 153 290 378 251  23 376   1  43 513  31 296  41 175 337  92 419 460
 312 154 521  58 298 201 406 195 149 482  48 151  96 232 477 448 177 272
 189 198 263 355  20 239 225 214 516 374 227 208 236 210 412 241  91 157
 461  13 304 262 366  63 167 163 113 468 123 447 494 475 350 511 342  83
 288 246 244 418 134 136 501 131 490  50 268 444 295 133  68 338 235  53
 302 459 219 139 230 331 368  99 252 354 158 441 471 222 451 335 327 388
 333 178 168 439 347 291  71 484 125   3 243  11 159 280 190 425  27 205
 143  77 365 470 271 377   4 111 332 437 166 383 310 294 130  79 242 401
 352 361 274 185  87 505 466 370 357 144 297 393 300 488  74 253 126 445
 407 228 258 359  36  47 160 394  21 375 240  61 410 132 336 218  39  42
  80 301 277 279 518 431 463 121 392  26   9 510 339 314 289  38  49 109
 453 479 196  88 390 409  46 397 156 443  76 411 329 172 489 250 408 496
 458  19 311 292 319 194 107 101 417 299 197 495  62 207  51 362 238  18
  24  95  40 191 137 402 171 180 237 345 211 426 324 485  54 265 152 432
  85 140  57 128 102 457 322 229 231  44 429 389 442 309 379 254  32  16
 422 308 462 330  14 118 179 367 284   8 233 358 493  28 486 520 287 285
 155 498  82 315 348 260  72 305  84 220 414 169 398 491 514 492 399 234
 221  17 110 405 476 519   2 259 122  10 281 449 415 509 278 282  15 215
 255 206 465 386  66 306 474 293 164 266  45 119 217  67 316 192  97 174
 507  69 199 116 162 478 247 381 500 499  81 349 142 371 193 256 481  64
 380  52 141 307  56 363 384 303 108 200 413 286 446 283 452 117 469  25
  35   0 351 450 487 467 420 106 497  65 396 440 104 245 435 264 391 226
 115 320 323 186 372 321  94 183 517 387  86 515 395 129 127 176 124 170
 344 216 334 181 317 416 145 213 276 502 257 473 182 313 436 382 273 328
   7 114  12  29 373 341 433 364  30 165 385 434 430 464 404 267 275 248]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.3287
INFO voc_eval.py: 171: [ 71 231 271 230   1 197  37   3 192  62  85 104 195 319 232  39  84 253
 139 246 324 265 283  20 194 185  17 275 129 292 219  16  83  59 233 191
  74 223 211  67 152 286 155 115 367 215 134 310 123 344  97 325 258 312
  36 257  47 317  76 157 214 163  15 220 159 252  99  18 364 301  70 328
 305  50 290 298 266  49 242  95 293 193 274 217 183  87 355 218 306 103
 107 171 337 170 161  72 272 280  69  22 150 228 341 221  93  58 250 189
 187 358 146 321 263 323  31 186 343 181 282 114 289 196 278 174 315 249
 327 166 199 330 234 109  45 102 366 202 295 177  98  40 270  46 132  28
 101 329 307 261  53 308 224 236 180 143 222   4 149  27 332 338  73  94
 360 239 314 244 145 365 113 297 336 140 273 311  96 359 121 130  11 259
  68 285 206  21 124 106 302  14 281 362 353 294 303 184 203  56   8 122
  63  12 334  35 267 247 200  44  89  60 141 322  25 361  19 120 335  32
 299 108 357  30  78 148  86 264 182 117 178 118 176  75  13 167  38 216
 316 227 245 112  34 164 133 128 229  80 173 142 349 342 205  55 288 110
 137 340 256 169 138 326 240 251 126 144 175 162  10 309 296 136 277 204
 241 168 116  64 363  41 153 151 300 160 318   2 276 333 356  65  88 269
 354 347 208 179 279 331 100  90 158 291 304 237 346 201 207  29  33 287
 235 351  42 352 156 254 127  48 147  92 226 131 111 255 135 210 348 368
  43 165  51   9 154  91 339 268  77  66 213  82   5 172 125 119 320 212
  26  23  61 284 248 313 209 198  81 225 238   7  54  24 105  79 345 243
 260  52 190   6 350 188 262  57   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3626
INFO voc_eval.py: 171: [114 216 115 213   2 210 215 133   5 181 283 214 101  99 116 171 180 162
  89 151 254 102  55  90 282 172 100 103 262 219 221 195 194  71 135 225
  62   4 233 248 121 146  60 156  85  80  76  12 184 191 126  21 220  42
  84 148 183 241 223 230  98 137  17  66 190 186 269  20  49 185 155  16
 217  86 222 275 236 234 268 117  65  23 279 272  45 231 173 244  25 209
 226  92  75 169 278 157 257  22   9 196 206  57 245 187 224  14 256  10
 140 276 154  11 255  88  61 149 158 159   8 130 124 218  38  70 202  64
 145  72 197 203 167 263 285 253 246 229  13  37 106 125  67 273  63 119
  36 250  53 227 237  32 113  83  95 139 228  51   0  44   1  47  73 238
 164  59  52 109  26 127 240 161 201 192  39 261 122 152 232 235  24 189
 198 175 193 274 205  79  58 182  97 136 134  91 258  29   3 277 166 107
 104 176 252 211 144  78  87 267  46 120 118 160  43  77  15 200 105 259
  54 208  68 168 247  28 163 284 178  96  33 112  69 153 111 243 280 132
 207  81 199 123  18  19 239 265  74 179  56 174 128 204 129  41  31  50
 249  82 271 147 170 108 188 141 142 150 260 281  48   6  93 270 138  27
 165  40 264   7 266 242 143  34  94 212 131 251  35 110 177  30]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4889
INFO voc_eval.py: 171: [29 82 17 10 28 46 61 66  5 39 11 73 75 24 27 85 43 52 37  2 36 78 70 79
 13 55 38 81  4 80 44 20 26 19 30 34 58 35  9 69 21 57 74 77 63 54 84 48
 22 45 15 60 86 32  1 47 14  7 49 12 41 50 76 59 16 67 56 72 83 68 25 23
 65 71 18 42  8 51 53 62  6 31 33 40 64  0  3]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6303
INFO voc_eval.py: 171: [188  84 184 251  47 252  59  94  82  34 124 132 267 198 190 169  18 248
 243 249 197 227 125 110 253  11 236  15   2 153 143 126 166 259 116 203
  52 130 201 150 109 133 145 244 200  60  95 242  85 237 165  33 223 120
 186  41 117 235 151  74 208 108  25 148  42 174 195  28 131  29 230  96
  61 268  53   6  77 163  30  51 199 272  48 245 183 159 219 111 262 107
  64  99  91  35 209 185 171  90 147 112 226 118 168 207   9 231  43 211
 155 239 234 123 258 106 269 217 256   5 181  62  97 216 238   4  24 172
 127 135 129 144 161 247 154  37 128 162 264  13 246  88  38 204 178 160
  44 266  69 196  12 187 273 139 152 202 134  76 164 254  92  71  81 156
 105  73 113  72  78 167 191   3  66 101 215 114  98  63  26 240 255   0
  39  93 232 214  23 189 136 263  21 222 205 180 261  20  87 157  45 271
  55  46 220  40  32 122 121  17  86 270 138 194 146  56 182  57  31 115
 241 170  50 221 149  79  83 140 213  68 142 141 104  19  49 225  58  80
  67   7 265 158 193 137 100  65   1 257 103 206  36  75 250   8  22  89
 176 179  14  10 229 175  70 233 212 192  54 224 260 218 173  27 228 102
 119  16 177 210]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.5155
INFO voc_eval.py: 171: [ 6 15 78  3 93 23 36 22 86 63 72 19 20 35 85  0 91 52  2 24 58 70 40 37
 48 67 45 39  1  5 54 74 50 65 59  8 10 42 79 92 89 44 82 25 28 71 12  7
  9 69  4 31 83 53 61 43 27 75 90 16 34 88 57 38 81 47 18 17 62 26 32 64
 41 21 94 77 11 60 30 33 73 87 80 14 29 49 46 13 76 95 68 56 84 96 51 55
 66]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1250
INFO voc_eval.py: 171: [805 204  92 807 356 206 856 855 639  93 217  61 570 882 877  39 455 918
 205  62 508 545 518 886 696 339 876 767 359 883 572 822  34 529 546 707
 166 260 141 332 823 695 768 293 301 384 535 602 881 106 691 231 133 114
 538 358 338 109 410 207 640 194 456 577 224 670 571  53 514 165 164 111
 306  22 668 511 557 352 775   7 307  56 264 898 606 380 793 115  87 383
 798 648 491 721 708 269 237 350 357 676 700 851 683 613 223  48 866 444
 262 159 261 118 585 861  63 771 917 156 932 753 245 598 824 597 409 110
  80 323 225 859 154 729 157 819 857 841 312 474 137  35 445 899 172 632
 730 343 373 801 246 442 590 363 419 772  15 466 478  37 908 808 494 926
 791 227 138 399 930 505 295 872 488 139 832 769 626 756 161 212 503 727
 553 121 747 294 369 220 181 667 443 256  86 490 326 806  79  42 496 850
  18  44 685 282 401   8 236 460 860  83 811 241 486 550 569 825 672  11
 289 684 281 547 741 321 723 259 889  78  57 334 362  55 773 616 754 803
 325  64 701 303 183 642 487 390 827 750 180  68 629 658 688 211 728  59
 717 446 500 924 634 375 662 549 810 497 458 162 596 770  36 608 381  28
 631 271 202 185 655 737 915 845 774 675 826 523 196 618 671 650 635 782
 539  96 457 724 654 873 182 184 210 531 610 513 195 562 291 116 119 937
 788 189 844 764 232 697 509 377 429 765 279 104 586 322 847 467 682 310
 186  29  60 900 492 744 125 418 101 507 542 361 251 292  75 862 669 199
 846 440 813 929 431 506 340 188  23 393 575 179 657 849 543 928 177 579
 905  16  91 298 105 694 147 787  54 353 475  27 512  50 214 420 795  26
 796 257 145 712 123 255 124  85 218 477 776 493 441 489 790 863 395 612
 263 360 423 479 554 732 347 233 364 583 128 913 715 630 112 636 433 348
 273 155 113 305 711 160 591 710  99 469 327 551 501 481 324 345 436 603
  71 192  97 209 250 391 221 620 814 313 176 647 745 638  82 766 249 749
  69 272 804 936 720 874 656 367 853  33 901 243 148 397 679 548 484 485
  45 906 812 465  46 219 541 168 564 858 238 743 587  77 468 624 403 589
 599 692 122 382 558 146 239 666  58 762 235 831 315 449 611 413 463 274
 740 526 686 719 385 633 134 600 561 349 584 761 308 240  20 268 663 848
 197 645 540 329 665  12 140 290 893 335 652 646 854 499 150 759 763 387
 344 817 152 891 797 784 833 938 649 171 722 789 935 266 880 432 126 129
  24 283 643 504 842 284 738 135 677 386 173 794 244 470 368 297 374  70
 615 605 911  14   0 201 731 365 871 755  41 517 131 117 594 288 525  90
 392 280 875 716 726 163 734 510 342  72 482 371 560 934 714 604  84 407
 593  81 534 258 320 653 742 559 778 331 216  98 869 406 411 820  88 317
 576 430 158 578   9 376 252 341 226 426 894 760 783 287 751 895 178 354
 311  31 923 884 533 229 149 333 621 175  38 752 625 627  76 835 208 103
 417 151 524 920 912 746 689 687  67 829 563 464 438 402 641  73 428 555
 902 275 706   3 516 190 447 914 400  51 378 888 502 536 302 309 779 415
 248 885 922 925 405 644 704 821   1 213 203 800 837  32 532 568 897 785
 346 472 890 595 370 818 515 698 580 736 892 277 530 143  21 693 725 169
 809 304 651 544 617 424 439 471 394 228 777 909 187 705 637 404 299 607
 815 519 130 314 434 389 127 674 839 215 198 267 758 673 852  47 733 222
  17 735 330 661 234 480 425 142  30 193 102 473 780 366 319 247 495 451
  94 170 379 276  49 628 879 713  95 132 664 904 270 660 414 422 623 718
 933 781 242 265 108 191 483 254 840 461 816 450 552 459 830 355 372   5
 931 528 388 167 903 174 351   2 328 136 699 870 910 200 681 680 878 919
  52 452 556  19 739 296 566 336  74 659 601  10 398  65 498 678 300 828
 916 453 921 100 520 757 834 838  13 864 318 786 703 408 462  43 896 285
 690 454 427 316 609 573 153 588 592 448 120 565  89 522 527 567 337  66
 702 868 435 792  40 843 887 582 581   4 521 614 619 622   6 412 537 416
 867 907 286  25 107 748 278 253 230 865 836 144 802 421 799 437 476 709
 396 574 927]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5417
INFO voc_eval.py: 171: [  5  24  34  41   4  60  61 106  94  90  25  28  26  27 116  32   1  52
 100  47  23  31  87  62  82  95 114 110  45  93  13  75  68   3  43  35
  33  58  38 104 108   2  50  46 111  92  97  74 103  20  71  19  67  72
 101  10  88   8  16 105  44  59  70  17 109   0  14  39  57  98  79  96
  40 117 118  21  78  42 115  64  83  76  29  65  53  69  54   9  99  91
  15  81 112  77   7  89  55  73  12  48  49  63  22  86 102  18  36  51
  80  85  37  66  11  84 107  30  56 113   6]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.4375
INFO voc_eval.py: 171: [327 141 135  30 195 286 288 155  72 307  13 100  40 194 205 230 189 203
 154  85 289 247 300   8 116 119 219 133 217 115  99 223 271 173  57 284
  49 277 236 257   9 102 156 272  79  10   6 229  45 218 269 338 301 260
 259 168   4  86 345  31 291 297 213 264  39  37 197 346 349 184   1 112
 336  98 235 212  20 315 209 157  75 347  34 246 214  95 142 158  62 287
  41 161 131 109  53 241  81  67  29 320  94 120  92  87 325 263 285 200
 319  19 242 258 182 105 211  12 144 243 166 343  27  22  44 150 183  93
 322  28 360 224 251 317 239  76 174 193 270 358 113  47  64 365 204 143
 172 227 305 208 187 149 222 321 303 178  24 364 238 210  63 326 159 314
 339 111 278 308 292 354 262 165 312 341  84 363 233  54 366 139  71 252
 309 332  15  89 276 160 117 293  74  70 250 137 333  23 244  50  80 357
 323 328 207  73 179 151 138 302 106  46 145 298 234 311  25 126  83 167
  68 201  21 331 248 294 130 216  43 146 188 185  35 176 275  78 232 148
  51 104 299 261 170 352 128 348 324 267  82 340 240 121 122 231 249 274
 163 175 273  16 268 362   2 107 351  77  33  14 186 335  11   7 329 118
 220  96 199 353 162 132  61  58 103 265 114   3   5 313 310  91 164 253
 306 359 254  18 127  97 290 140 245  59 355   0 108 123 279 215 318 181
 153 280 134 101 221 337 361 330 177 198 169 180 152  60 125  55  69 295
 129  90 190 255 282 344 136 296 202  65 196 192  17 191 147 304 283 226
  26  48 171 316 334  32 350 266 256  56 225  42 228 124 356 206 237 110
  52 281 342  38  88  66  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4328
INFO voc_eval.py: 171: [ 90  69  81 103 179   8  76 166  12 180 138  26  45 167  74   4  24  94
  36   7 148  10  37 164  27  83  87  41 174   2  75 137  38  79 104 169
 171  29 140 142 173 175  95 117  50 121   6  47 151 122  89 154 109 128
  39 127  63 107  25 168  42  43  13  55  92  62  93 106 136  78  32 124
  66 101  30 129  21 119 135 162 113  56 150 157 144  16  46  96   9  82
 133 115 146  11 141  67  59  18  80 120 177  57  28  52 110  84  70 123
 160  72 176 161  14 102  49   0 116 139  15  44  88 143  64  65  51  61
 170 125 178  33 131 118  48  19   3 156 182 159  86  17 111  97 108  35
 112  53  34  98  22  20 153 147   5 145  40 132  99 152 172 126  23  54
 149  77  31  71 163  58 155 114 100 181  68 134  73 105 158  91 165  85
 130   1  60]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2671
INFO voc_eval.py: 171: [134 115  14  57  31  75  16  15 104 101  34 121 103  92  83  74 102  85
  11 135  37  60 138   4  53 143  95  64  47  18  86  38 130 125 129  63
  44  91  17 116  71  36  55  61  12 126  94 139  20   0   2 120   5 111
  76 106  93 105  28  56  70  50  96 141  78 128  33  84 117  46  65  29
  90  58 124   3  32  59 127 137  79  13  54 100 140 133  73 136 123 142
  26  48  45  24  51   7   9  80  40  30 118 109  35  66  42  27  19  81
  23  97  52  49 132  68  43  22 113  62  77  82 122  10 112 108  89  21
  41   6 119   8 110  72  39 131  98  69  88  87 114   1 107  25  67  99]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.4981
INFO voc_eval.py: 171: [ 9 64  4 37 40 26  2 67 23 46 59 19 10 41 28 45 43 42 48 63 12 17 62 54
 29 50 32 11 20 21 27 44 39 35 69 53 65 68 56  1 36 22 47 38 33 49 14 57
 31 66 51  7 34  5 24 25 30 58 15 52 16 60 55  8  3 61 13 18  6  0]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8403
INFO voc_eval.py: 171: [1841 1826 1703 ... 2073  263 1322]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7103
INFO voc_eval.py: 171: [288  69 130  48 391 436  53  83 151 119  50 433 460  90 181  51   5  49
  77 150   6 319 289 309 463 478 252 375 282 177  58 258 396 139  64 339
 413 270  87  97 157 462 380 430  67 101 230  52 140 100 168  78 471  32
 149  75 411 132 425 212 434 421 165 488 201 211  21 142 378 311 303 435
 229 231 167 332 415 379 271 210 452 416 320 360 204 348 217 336 314 134
 364  30 426 420 301 305 381 432   8 117 390 464 468 262 207 467 466 335
 414 392 276  55  80   7 246 169 133  22 345 299 158 418 461 308 194 209
 393  46  95  91 395  42 178  65 136  18  81 359  88 182 166 388  63 267
 193 484 105 223  26  74 472 161 272 108 489 290 180 240 173 286 409  82
 315  86 285 376  72 437 253  70  44  54 312 327 296 232 249 122 202 243
 198 234 373 203  33 307 480 337 274 331 333 160 384 221 389 382 377 465
 251 482 408  76 109 477 293  56 399 324 227 186 397 255  28  15 369 400
 406 284 401 104 112 476 340 141 146 479 120   9 326 491  43 424 287 351
 362 233 328 237  11  29 208 195  12 387 342 372 259   4 226 449 459 427
  16 419 403 448 440 170 277 175 442  14 213  23 475 184 487 386 370 263
  19 450 185 155 261 407 492 238 344 118  73  27  57 490 302 116 295 174
 220 248 247  84 245 441 394   2 313  17 366  61 215 410  25 350 443 218
 385 451  41 429  13 241 300 469 147 297 125 191 365 483 275  62 114 235
 405 236 454 126 206 106 123  92 156 325 242  31 417 316 423  66  36 317
 486 374 343 260  71 244 273  98 250 163 121 154 145 219 192 318 113 431
 266  47 189 148  37 428  96 222 383  85   3 172 153 128 353 352 183 371
 199 264 265 138 188 176 458 368 110 152  39 159 228 456 306 321 205 412
 398 444   0 322 214 144 445 298 404 162 347 346 485  40  94  59 457  99
  68 107 216 115  35 330 323  20 304 239 402 269  45 470  79 197 453 278
  93 474 349 102 447 473 200 143 329 357 294 355 135  89 279 291  34 164
 354 254 310 171 280 127 358 196 137 103 439 224  60 190 268 446  10  38
 256 493 341 111 257   1 292 187 283 281 481 422 363 438 367 179 129 225
 338 131  24 356 361 455 334 124]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5766
INFO voc_eval.py: 171: [ 94   3  97  56   4  68  69  23  89 111  95  62  49  21 106  75  73  25
  17  40  46  78 104   5  22  45  31  61 107  52 108  26  74  60  85  91
  19  88 105   9  63  77  43  47 110   1  48  71  33  87   2  66 114  80
 113  27  84  67  76  20  42   7  64  32   0  15  35  54  39  16  13  11
  81  59  58  70  37  72  38 101  24  86 102  51  18  30 112   8  90  41
  36  10  53 103  92 115  50  65  98  82  93 100 109  34  79  55  96  14
  57  12  99   6  44  83  28  29]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4543
INFO voc_eval.py: 171: [30 14 29 43 46 33 35 47  1 41 66 56 38  2 27 65 18 48  8 54 39 52 36 49
 17 34 63  7 24 10 64 45 53 13 69  3 58 68 50 11 20 21  9 62  5 31 32 40
 12 55 26 60  4 44 61 25 28 67  0 59 22 51 23 37 57 16 19  6 15 42]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.2983
INFO voc_eval.py: 171: [ 55  84   9  13 103  25  45  60  42  62   8 105  74 115  76  17   6  64
  21  24  95 102  53  12  11 114  39 110  91  85  44  78  34  27 108   5
  43  75   3 116  20  92  63   4  46  29  66  15  31  69 117  83  88 109
 113  37  26 100  48  73  57  99  10   7  86  35  94  89  87  41  28  71
  72 111  70 120  36 104  22   0  82  18  33  93  61  14 107  58  97   2
  81 119  40 101  32  47  79  59  80   1  52  19  98  65  49  50  30  77
  51  96  56  67  23  38 118  16 106 112  90  54  68]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.6065
INFO voc_eval.py: 171: [ 43  73 102   1  23  51  78  10  22   6  71  56  77   7  41  96  34  81
  92  70  89 100  60  33  87  14  61  30  16 105  82  59  79  99  50  75
   3 108  20  63  36  47   2 109  86  18  38  76  88  39  53  17  74  13
  29  37  62  83  24  42 107  90  40  11  80  55  68 101  57  66  44  52
  69  45 106   9 103  19  67   8  12  27   4 104  93  21  35  85   5  91
  15  31  84  98  54  28   0  48  94  72  65  64  58  49  46  25 110  32
  97  26  95]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4731
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4812
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.693
INFO cross_voc_dataset_evaluator.py: 134: 0.329
INFO cross_voc_dataset_evaluator.py: 134: 0.363
INFO cross_voc_dataset_evaluator.py: 134: 0.489
INFO cross_voc_dataset_evaluator.py: 134: 0.630
INFO cross_voc_dataset_evaluator.py: 134: 0.516
INFO cross_voc_dataset_evaluator.py: 134: 0.125
INFO cross_voc_dataset_evaluator.py: 134: 0.542
INFO cross_voc_dataset_evaluator.py: 134: 0.438
INFO cross_voc_dataset_evaluator.py: 134: 0.433
INFO cross_voc_dataset_evaluator.py: 134: 0.267
INFO cross_voc_dataset_evaluator.py: 134: 0.498
INFO cross_voc_dataset_evaluator.py: 134: 0.840
INFO cross_voc_dataset_evaluator.py: 134: 0.710
INFO cross_voc_dataset_evaluator.py: 134: 0.577
INFO cross_voc_dataset_evaluator.py: 134: 0.454
INFO cross_voc_dataset_evaluator.py: 134: 0.298
INFO cross_voc_dataset_evaluator.py: 134: 0.606
INFO cross_voc_dataset_evaluator.py: 134: 0.473
INFO cross_voc_dataset_evaluator.py: 135: 0.481
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.439s + 0.001s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.348s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.364s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.371s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.373s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.372s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.371s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.368s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.367s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.366s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.367s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.372s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.372s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.576s + 0.002s (eta: 0:01:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.388s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.384s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.368s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.369s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.371s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.378s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.377s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.374s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.374s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.372s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.374s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.372s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.431s + 0.001s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.367s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.360s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.355s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.355s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.356s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.357s + 0.001s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.361s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.363s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.365s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.362s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.361s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.360s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.536s + 0.002s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.378s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.379s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.378s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.379s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.373s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.373s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.373s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.376s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.376s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.374s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.373s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.371s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.112s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 51 200 161  94  45  40 110  42  77  22   2   6 219 155  24  53  97 105
  58 134  66  17  71 168  23 215  74 160  63 175  47  89  99  28 103 182
 122  10  90 109  26  32  95 132 135  59 106  12 173 179  76 148  64  25
 141 202   7 214 176  13 127 113  67 227 207 112  55 199 201 115  41  62
 197 205   0 186  36 178 165 164  86 126 228  68 211 114  96 218 131 229
 195   3  37  92  88 198 149 107 123 167 188 206 159 170 116  52 111 143
   9   1 177  65 210  18  11  31 145 181 226  43 172 189  54  16 138 194
 128 120 217 101  33 185 154  78  91 163 119 150 224 117  79 121  15 100
 147 223 204 203 183  44 162  69 137 151  48  30 144 230 139 222 184  14
  85  81  39  21 118 216 124  82  60  70 125 133 191  49  57 209 156 157
 225 208 192 136 108   8 130 187 153  29 104 220 193 146  73 166 152  84
 171  87  34  72 212 196  19  27 140  50  56 221 158 174  61  98  46  75
 129  93 213  38 180 142 169  20 190 102   5  83  80   4  35]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3333
INFO voc_eval.py: 171: [ 9 16 56 30 32 44 61 62 63 65 64 34 67 45 29 35 69 53 70 51 60 20  6 52
 72  4 47 10 46 71 33 55 12 58 19  7 50 24 26 40 66 17 36  3 68 13 11 18
 48 21 27 59  0 38  8 42 31 14 15  5 22 37 49 57 39 73 41 28  1 43  2 23
 25 54]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6986
INFO voc_eval.py: 171: [ 70 180 206   5 182 408 409 141 142 203 218 337 310  22  68 120 435 140
  86  97 485 385   6 217 436 486 437  76 316 343 198  52 168 259  36 359
 261 487  33 334  57 405 331  90 197 184 131 252  99 453 462 367 350 130
  85 144 411 465  72 489 147 111 281   4 388  32 493 102 421 236 104 149
 407 196 148 156 315  41 289 346 201 242  56  91 459  27  42 145 365 327
  46 143 403 288 392 494 430 499  31  88  94 204 442 464 228  87 306 254
 169 208 219  21  55 363 185 193 457 496 221 171 294 263 395 402 229  66
 345 253 356 239 112 279 476 231 450  60 492 161 325 158 441  13 129  98
 126 260 429  48 237 443 472  80 241 372 121 427  96 191 137 224 243 452
 286 349 152 333 199 358 174 293 484 375 471  50 344  11 215 466 282 128
 433 470 320 213 262 321 155 269 319  83 154 133 304   2 329  20 386 127
 376 227 271   3 439  75 413 162  84 488 342  35 317 451 420 351  71 222
  69 311 186 381 498 273  25 123 287 291 424  26 245  47 347 338 160 264
 354 323 110  77  23 292 200 302 428 483  73 328 283 285 238 455 341 108
  64 132  38 280  45  44 150 355 146 214 336 125 477 134  37 124 233 179
 235 445  40 410 382  28 300 426 274  78  93 165 322 151 207 139   9 303
  18 352 491 232 378 360 290 106 425 380 479 396  19 460 190 202  58 299
 166  51 270 468 467  62 103 401 474 225 383 205 246 357 117 212 267 389
 330 277 423 434 308  79 122  17   8 249 183 192 384 314 374 101 394  54
  39 454 406 189 368 414 312 296 391 364 258  63 366 398  15 307  67 387
  14 373 361 362  95 495  82 390 447 175 461 348  59 284 268 431  16 393
 417 209  81 119 490 234 105 422 226 482  65 324  49 500 480 339 370 136
 275 301 115 163 194   1 118 448 399 248 107  43 326 444 371 475 176 240
 223 272  10 135 211 172 473  89 159 157  34 415 379 449 456  92 446 167
 170 114 432 297  74 458 173 404   7 469 250 230 377 257 463 318 419  29
 400 195 116 478 181 251 210 255 313 138 295 278 100  24 247 113 109  30
 244 438 397 340 256 305  53 177 353 309   0 497 298 416 481 332 178 220
 153 187 188 276 418 265 266 369 164  61 335 216 440  12 412]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.3321
INFO voc_eval.py: 171: [ 65 251 216 215   1 183   3  34  31 180 182  56  77 293  92 235 217  76
 129 298 246 228 262 255 122 173  17  60 181 204  14 179 196  15  75 267
  67 208  94 218 142  61 145 261 105 336 279 285 317 200 125 297 197 116
  69  30  70 146  64 239 151  42 291 265  44 232 171 148  91  13 301 238
 287 140 150 268 272 199  86 213  98 160 206 290 275  16  80 203 233 252
  45 311 245 289 159  63 315 176 333  20  90 316 280 328 325 231  84 329
 174 202 335 226 259  48  53 294  89  40 300 188  87 165 270 100 309 254
 175 312  93 209 222 136 288  27 296 250 334  85 207  88 169 205 178 281
 124 167 234 257  41  24 134 185 154 303  33 103 242 332 219 240 189 187
 271 132 123  10 324 283  12  97  23 221 253 302 305  52 193 269 130 164
 138 112  55 126 286 184  39 104 277   4  62   7 128  57 111  82  26 117
  99 131 121 247 113  29 110 227  22 107  21 260 276 149 284  18 190 115
  68  19 263  73 321 307  11 330 139 264 327 314 273  32 152 195 214 147
 109 127 278   2 170 161 155 225   5 274 223  96 108 295 158 326 194 331
 102 258  58 236  38  35 172 308 201 198 230 157 229   9 166  78 337 101
 313 133 318 106  71 292 168  36 144 156 119 319  95 120  28 114 266   0
 249 212 220  81 192 191 248  74  49 143 299  66  83 323  25  59  47 310
 211 237 210 244 141 186 137  72  51  79 177  37   6 320  54  43   8 322
 256 306 304 243 282 162  46 118 153 241  50 163 224 135]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3858
INFO voc_eval.py: 171: [ 98 190 187  99   2 184 113 189 188   6 155  86 246  83 138 100 154  45
 128  75 223 147  76 146  87 247 203  84  85  60 231 194 169 168 115 191
 199  52   5 219  68 107 123  72 166  20  65 131  11 161  71 110 195 160
 197 126  34 157 212 117 205  50 164 192  82 237  19  56 159 196  73 132
  16  40 210  17 148 241  22 103 245 145 207  78 206  37  55  24 204 238
 171 236  64 215 225 136 158 226 201 198  21   4 109 180   9 119 163 112
 134  47 221 135 224   7 244  51  10 242 124  74  14  54 239 170 175 193
 232 176  90  61  13  36  93 216  57 200   1 105  79 118  31  30 202 208
  43  29  41  59 143  97  70  28 165 150 167  62 149 230  42 220  53 101
 183   0   3 211  23  49 172 142  81 162 116 222 141 209 151 228 108 127
 178  88  69  25  48 240 185  91 122  32 114 227 156 133  89  15 173  77
 121  38  39 104 234 129 102 235 106  27 248  26  35  80 182 217 181  58
  95  46   8 139 174 153 130  63  66  67  96 177 137  18 125 243 179 214
 233 144 152  92  44 218 213  33 111  12  94 120 229 140 186]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4817
INFO voc_eval.py: 171: [78 26 15  9 25 63 36  3 10 41 66 44 46 69 81 59 71 35 50 33 34 75 74 24
 22  1  2 31 54 73 77 27 12 76 17 23 18  8 42 65 32 55 82 20 29 61 58 79
 56  5 14 70 52 80 13 47 48 39 72 68 28 43  0 21 16 11 62 19 30  6 57 49
 45 60 53 64 40  4 38 51 37  7 67]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6843
INFO voc_eval.py: 171: [168  71 164 233  41 234  70 246  28 112  51  80 170 118 228 153 177 224
 114 207 235 178  15 232  96  12 216   1   8 116  94 241 136 117 183 128
 150  45 184  72 104 119 134 181 149 223 130  81  52  27 217 203 222 166
 108  33  93 215 105 190 133  67  24 135  22 191  64 156 243 209 253  83
 179  54  37  44  98 163  42 189 175  95 143   6  23  29  76  46 225 147
  99 106 199  84  55 237 197 165  77 247 132 211 111 250  38 115 219 214
 239  66 123 122 120 206 154 218 151  56 137  85 185  19  21 171 152 196
 238   4 113  78 138 182  92   3 102  91   9 249  32 205 145 245 167 162
 212 227 254  34 176 192  97  61 155 146  30 229 148  79  16  69  18 144
  62 121  59  88   2 180  35  39  65 125  75 101 160   7  40  57  86 139
 195 142 110 220 161  63 140 159 169  17 202  25 186  14 226  48 221  10
  26  73 174 103 193 131  43 173 194 141 252 201 242 240  36  50 100 126
 231 244  49  74 236   0 129  60   5 251 200  68  58  87 172 188 198 157
 204 107  82 124 208  53 127 109 187  47  20  11  90  31  13 213 158 230
 248 210  89]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.5721
INFO voc_eval.py: 171: [ 7 75 17  4 90 26 62 38 70 84 23 24 37 21 82 68 56 52  0  3 42 88 47 27
 57  6 48 51 76 39 72 65 40 64 11 46 69 89  1 71 29 12 86 54 36 10 79 30
 44 14 22  8 87 45  5 60 67 73 19 20 63 49 78 13 43 55 34 53 80 85 41 18
 31 32 91 28 59 58 83 35 81 50 25  9 66  2 16 77 74 92 15 93 61 33]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1214
INFO voc_eval.py: 171: [730 729 182  80 320 577 774 772 195  79 184  52  31 793 515 183 788  51
 404 460 463 492 304 628 699 798 231 322 794 787 517 827 745 299 625  26
 125 637 478 149 266 493 629 347 746 697 259 486  98 484 544 303 206  94
 321 119 792 516 174 578 186  99 405 147 200 372 605 272  43 522 607 148
 468 701 316 809  96 588 503  15 459 271   3 229  46 548 349 724 720 240
 105 315  75 341 837 440 212 769  39 650 234 199 619 555 143 636 631 398
 681 232 615 201 747 826 775 220 529 329  53 541 107 698 219 659 732 397
 291 540 777 374 156  97 760 136 139  66 422 456 307 773 203 278  27 810
 399 743 414 363 129 700 820 734 534 453 835 682  69 568  74 443 744 658
 138 331 421 379 832 145 783 726 337 185 752 328 260 500 123 719 652 676
 780  30 565  33 197 124  32   9 109 693  63 262 365 731 227 604   4  11
 396 609 211 768 301 248 776 327 514 289 410  44 497 748 702   6  47 230
 677 216 707 435 255 438 189 595 651  56 441 295 345 127 251 273 157 557
 164 800 158 455 293 662 586 354 494 733 608 825 623 632 480 269 621 142
 627 208 559 736 176 466  83 749  71 190  49 549 567 579 728 695 340 611
 600 179 572  28 593 407 342 110 703 400 448 507 657 667 597 192  89 246
 326 487  20 280 710 112 406 573 473 813 165 387 458 108 712 762 531 207
 539  21 257 447 551 462 140 168 691 496 425 764 181 646 415  67 175 283
 388 359 490  92 235 358 305 630 617 411 620 378 520 842 508 106 767 716
 694 290 737 811 426 224 570 829 104 526 172 671 450 718 325  93  18 380
 362 525 594 324 236 130  16 457 144 215 477 649 673 584 784  78 442 377
 717 722 530  10 355  37 384 576 394 686 170  84  41 309 501 132 439 833
 824 330  45 663 491 574 263  59 709 390 614 498 226 160  86  35 542  65
 188 553 317 469 167 761 436 210 312 302 242 640  19 771 402 445 218 274
 547 401 535 763 552 169 213 241 418 395  50 495 563 114 723 664 706 467
 648 381 738 223 688 198 361  73  25 171 603 375 202 221 281 510 598 314
 571 751 323 150 642 644 292 674 228 596 779 678 434 589   7 102 696 464
 679 392  54 583 506 656 296 177 582 393  57 356 205 256  48 727 511 239
 153 233 840 348 819 173 591 561 612 412 131 431 427 838 113 373 335 690
 318 613 802 141 785 368 161 580 419 250 770 370 685 533 815 556 279 451
 298 545 569 350 120 214 151 416 705 352 389 258 134 834 275  12 137 449
 308 353 217 602 550 759 590 121 704  40 101 485  76 708 193 655 238 482
 654 818 721 641 505 196 366 343 647 622 564 180 346 558 163 778 288 546
 187 483 116 660 750   0 465 133 237 841 475 804 433 249 782 371 194 178
 666 118 367 364 616 765 585 560  62  34 268 839 537 244 386 409 245 267
 428 476 796 713 592 828   8 261 489 668  38 742 502 528 808 277 488 369
 509  87 523  42 474 554 300 270 338 424 408 282   5 254 253  60 823 430
  77 587  61 714 821 313 536  13 103 100 795 781 814 741 339 311 831 806
 754  64 672 755 319 799 791  29 521 420 562  24 805 756 122 146 634 446
 252 334 797 481 643 287 243 513 610 645 740 391 575 830 680 606 665 264
 444   2 715 225 209 111 786 683  82 128  91 822  68 135 843 601 432 499
 382 376 735 803  81 524  22 758  14 504 653 294 222 790  58 286 276 687
 766 725 753 538 332  88 581 265 115   1 638 454 624 166  70 527 336 333
 543 669 403 470 479  85 812 566 154 599 383  23 519 757 711 437 297 739
 675 635 817 417  90 684 472 518 461 816 789 360 247 285 191 351 162 152
 126 284 357 471 423 689 306  55  95 661 633  36 836 801 626 310 670 807
 204 532 344  17 159 429 117 385 512  72 618 452 413 639 692 155]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5512
INFO voc_eval.py: 171: [ 25  46   5  36  27   4  26  28  69  29  35  72  59  24 121 105 109  96
 129   1  66 115  53 110  71 103  32 130  49 125 108  79 118  38  90  58
  56   9  33 123   2   3 111  89 106  44  68  86 119  85   0  19 102 120
 116 124  40 104  51  65  42  21  78  84  18 113  12  88  94  11  70  10
  16  75 132  62  31  14  47  91  93  60  92   7 126  57  55 107   8 131
  87 128  80  61  22  98  64  15  13  97  81  23 127  73  83 101 114  95
  63  76  45 112  67  54 117  41  17  34  30  43  48 122  82  77   6  99
  50 100  52  39  74  37  20]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.5487
INFO voc_eval.py: 171: [142 150 341 207  31 302  76 163 238  41 298 216 201  14 205 319 111 161
 214 303 119 256 311 122   8 228  90 106 139 185 121 120 233 286  61 265
 296  54 281 108 275 165   9 314 350   6 282 245 180 357  91 221 269  46
 268   4 361  11  39 222 237  32 102 105 162 348 219  20 305 244 208 100
 196 360 310 217 257 172 375 329  83 272 230 299  95  79 267 195  36 144
 252 151   1  42  56 169 301 297 101  71 288  99 253 114 250 126 280  30
 215 362  45 110 247  18  68 223  74 333  40 261 374 159 153 321 255  52
  66 342 327 128 306 220 158 320 224 324 273  23 295 178 229  67 355 135
  13 191 232 118  29 334 331 370 372 157 345 194  34 147 145 336 339 326
 199 184 152 285 183  47  86  53 115  37 346 251 367  78 340  81 134 168
 308  57 112  82 171 312 366 266  48 364 363 287 204 358 316 141 198 206
 200 335 130 175 262 239  22 156   7  58  59  84 271  85 192 258 344   2
 176 187 170 154  44 211  35  12 235  27 328  19 116 278 276  62  72  75
 113 218  97 353 323 376 117  73 337  63  25  21 325  65 213  94 138 140
 368 318  87 103 347  33 179  88 260 343 155 133 259  24 186 182 309 167
 109 313 107  77  55 225  80 243 369 322 359   5 193 164  51 104 226 197
 279 148 149 349 231 307  60 371 242 143 283 209 241 210  64 234 293 315
 332  15 181 124 284   3 173 277 289 291 352 212 254 317 246 137 263 129
  49 131 202 190  50 132 166  10 270 356  26 373 304  16  96  69 136  98
  43 274 160 146 330  70 294 338 365 174  93 236 248  89 127 290  28 177
 249  92 125 188 300 292 264 240 123 354 351 227 203 189  38  17   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4538
INFO voc_eval.py: 171: [ 70  90 102  81 170   7  77 157  11  25  87  22  46  75 158 156  37  94
 171   4  23 134   6 143 168 103 133  93  83  38  24  76   2 160  29  88
 167  41 121 162  36  79 147 166  39   9 136 138  63 119  52 164 149  48
  89 125 107 116  64   5 132  59 104  40  43 122 105 126 118  92  12 131
  60 155  69  78  71 111  32 159  49 120 127  96  45 100 137 101  47  30
  20 150 115  55 135 110 142   8  82 108  84  16 128  28  15 140  58  10
  57  65 153 146  80  51  27 106 113  68  62  85 117  98  72  95 109  73
 169 139 114 144  53  19  13 129  50   0  67  18  17 172  61   1  21 163
 161   3 112 123  54 145  86  44  34 165 152 130  42  33 154  56 141  99
  91  97  14 151  66 124  74  26 148  35  31 173]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2587
INFO voc_eval.py: 171: [125 105  16  55  33  70  18  17  97  96 112  93  84  36  79  69 127 126
   4  51  58  95  80 130  13  61 115 119 135  83  87 110  60  53   2  20
  47  66   0  40  38  42  39 106  19 121 103 117  50  22  85   7  86  81
  71  88  65 128 120 111  54   5 100  98  82  34  78  48  59 118  99 133
 132  29   3  57  72  74 114  56  75 134  14 116  90  45  31  62  30  68
  41 124  35  37  94  44  89  15  76 123 102  49 107  10  73  28 104  67
  25 122  23  64  46 131  91   6 113  24  11 108 129  27 109  77 101   8
  52  26  92   9  63  43  32  21   1  12]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.5446
INFO voc_eval.py: 171: [ 8 53  4 30 34  2 38 20 57 18 50 33 36 14  9 41 22 39 35 45 10 44 23 42
 51 25 37 54 47 52 21 15 56 32 28  1 19 31 17 13 11 26 40  0 48 16 43 29
 55  7  3 12 46 27  6  5 49 24]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8864
INFO voc_eval.py: 171: [1690  967 1706 ...  913 1941  856]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7130
INFO voc_eval.py: 171: [240  55  36 367 110 326  42  70  38 100 127 366 386 151  39  10  65  76
   4  37 148 264 256 389 126 402 241 310 234 208  40 330 214 117  87  43
 282 190 226  73 347 132  82 391 315 363  41  54  86 116  64 138 390 119
  27 125 345 356 369  62 357 111 251 189  17 174 313 177 314 368 409 166
 192 360 280 141 324 265 350 257 387 316 143 277 228 355 351 181 300 160
 260 112 396 170 392 393   6 290 382 176  25 327 232 227 279  99 250  52
 365 297 253 388  31 133 340 325 172 140 159 353 113  80 248 348 328  51
 383 191 175  59  18  14  77  35 224 198   5 139 152 114  58 149  74 202
 218 298  92  56 311  67 370 235 246 238  33 288 186 261 331 242  90 209
 342 258  72 230  68 104 205 255 136 281 244 168 219 163  45 135 397 276
  47 150 334  89 343  21 275 410 404 273  28  69 169 401 308 317 145 161
 153 207  34  63 304 193  60 270 134 184 254 178 312 364 102 239 333 236
 266  44 335 358 306 122 272 118 405 406 155 372  94 378 173   8 283 196
 215 269  16 188 323 210   3 233 337  93 292  71 197 321 373 284 187 394
  46 101  13   7 403 217 322 400 299  12  49 385  30  32 379 375 199 305
 231 361 200 204 247 408 201 344 286  53 341 374 339 195 362 154 371   1
  11  61  57 131 262 412 338 157 354 380 171  50  20 225 130 395 183 203
  15 359 301 329   0 245  23 129 302 179 211 407  26  29 271 146 121  97
 106 285 352 252 107  88 123 229 223 216 167 319 144  19 221 158 105  95
   9 156 194  96 165 108 307 291 411   2  24 268 263  84 381 349 103 115
 318  48 259 206 220 346 398  81  98 399  78 320 147 289 274 309 180 185
 120 142 249 296  79 293 294 243 182 109 267 384  85 222 295 237 376  75
  91 303 213  22  83 137 128 287 278 212 164 377 332 162 124  66 336]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5964
INFO voc_eval.py: 171: [101   5 104  62   7  72  24  74  94 120 102  56  53  22  29 118  81  77
  23  18  36  50  46  84 113  30 116  67  78 115  66  49  57  97  45  20
   6  10  75 114  51  98  31   1  83  42  68  95 119  92  76   0  82   3
  37 123  52  38  35   8  85  60  58  69  59  71  91 124  19  13  21  16
 110  40  44  73  86  93  39  17  65  43 117  80 111 112  89  15   9  34
 125  88   2  70  12  41  64 122  28  47  96  26  99 103 108 107  90  61
 105 106  27   4  79  87 121  55  48 109 100  14  33  11  54  63  32  25]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4452
INFO voc_eval.py: 171: [30 13 32 48 44  1 34  2 24 42 71 49 61 70 40 38 27 17 36 39 59  8 55 46
 50 68 15 51 10 12 56  7 69 57 65 31 35 33  4  3 18  9 74 11 53 72 62 25
 75 20 63 45 22 67 29 16 60  0 23 43 41 66 64 58 19 52 73 28 26 54 21 14
  6  5 37 47]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3282
INFO voc_eval.py: 171: [ 51  78  14  10  96  45  25  54  42 100 110   7   9  66  69  56  18  58
  22  26  89  95  12  28  88  49  39  53  44 105 111  85  79  36  43   6
  21  68 107 103   4   3  71  57  65  83  46  32  17  52  92  30  81  61
  86  59  27  77  38  11 108  29  97   8 112  93  99  37  67  84  55  82
  64 114 109  87 115  24  23   0  16  62 106  76  15  74   2 102  47  34
  94  80  73 113  19  63   1  41  20  33  40  91  31 101 104  60  75  90
  13  72  35  48   5  70  50  98]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.6247
INFO voc_eval.py: 171: [ 43  71 103   1  23  48  76  69   9   6  22  74   7  40  55  32  58  97
  33  81  59  68 101  92  88  16  13  30  83  79  54 105  57   3  52  20
 108  75  61 100 110  17  77  82  12  35 107   2  85  36  37  60  18  78
  50  47  90  29  38 102  42  80  73  72  64  10  39  51  49  89  45   8
  25  62  19  11  67  31   4  66 104  28  84  46  87  63  15  14  94  99
  91  34  21  27  86  41  56  98   5  53  24   0  93  70  26 109  65  95
 106  44  96]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5026
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5031
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.333
INFO cross_voc_dataset_evaluator.py: 134: 0.699
INFO cross_voc_dataset_evaluator.py: 134: 0.332
INFO cross_voc_dataset_evaluator.py: 134: 0.386
INFO cross_voc_dataset_evaluator.py: 134: 0.482
INFO cross_voc_dataset_evaluator.py: 134: 0.684
INFO cross_voc_dataset_evaluator.py: 134: 0.572
INFO cross_voc_dataset_evaluator.py: 134: 0.121
INFO cross_voc_dataset_evaluator.py: 134: 0.551
INFO cross_voc_dataset_evaluator.py: 134: 0.549
INFO cross_voc_dataset_evaluator.py: 134: 0.454
INFO cross_voc_dataset_evaluator.py: 134: 0.259
INFO cross_voc_dataset_evaluator.py: 134: 0.545
INFO cross_voc_dataset_evaluator.py: 134: 0.886
INFO cross_voc_dataset_evaluator.py: 134: 0.713
INFO cross_voc_dataset_evaluator.py: 134: 0.596
INFO cross_voc_dataset_evaluator.py: 134: 0.445
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.625
INFO cross_voc_dataset_evaluator.py: 134: 0.503
INFO cross_voc_dataset_evaluator.py: 135: 0.503
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.477s + 0.001s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.369s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.384s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.384s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.378s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.371s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.372s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.373s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.377s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.381s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.379s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.378s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.378s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.611s + 0.002s (eta: 0:01:15)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.394s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.384s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.376s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.378s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.378s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.381s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.376s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.372s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.371s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.368s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.369s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.368s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.449s + 0.001s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.381s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.375s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.383s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.380s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.373s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.371s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.374s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.374s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.371s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.372s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.371s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.372s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.519s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.379s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.365s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.365s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.361s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.362s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.361s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.360s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.362s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.363s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.360s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.361s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.361s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.617s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 42 177  77 141  36  33  64  35  93  21  18  80   2 135   5  54 196  88
  48  19  15  43 116 148  60 192  62  52 140 153  37  74  23  50  90  92
 160  84  22 105  73  78  10  86  30  26 129 114 152 122  11 157 117  53
  20  95 178 110 191  63 154  96  98   6 176  34  79  55   0 203 184  46
 179  29 144 156  51 164 174 145 109  97 205  83  12  71 175 147 113 188
 195 172  56 204  72 150 139  76 143  25  85 182 106 130 166   3  45  44
  99   1 186 194 151 183  94  16 159 104 101 124 127   7  66  91  27  59
 167 155 171 142 111  14 134 102 163 202 103 128 100 181   8 199 131  75
 149  65 180 119 121  67  68 132 168 161 198 200 126   9 108 136 169  39
 193 170  47  40  57  13  38  17 206  70  58  32 165  87  41 138 201 115
  69 107 190 173 137 118  61 133 120 112  28 123 187 158  82  24 162  31
   4 125 185 197 189  81  89  49 146]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3712
INFO voc_eval.py: 171: [14  7 41 22 24 44 32 46 48 51 49 26 54 50 21 33 27 39 53 36 45 18  1 47
 38 55  8  4  3 34 25 17  5 40 42 20 15 52 37 35  9 19 11  2 10 30 16 29
 23 31 28  0 43 13 12  6]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6921
INFO voc_eval.py: 171: [ 65 171 195 173   4 390 132 391 193 293 319 207 131  63  20 130  82 416
 110 464 205  72 463   5  95 366 299 417 341 418 188  33 159  49 245 323
 317 247  30 465 387  53 433 314  87 139 298 186 238  81  37  94 122 349
 134 177 137 442 466 121 103 444 266   3  68 369  96 331 402 394 138 439
  88 148 214 470  99 413 189  29 187 274 389  93 230  38 345 133 135  42
  24 327  52 384 310  83 273 347 194 242 475 206 372  91 443  85 288 471
  61 197 421 175 217  12 183 160  28 383 209 280 227 308 375 420 219 437
 162 473 249 456 468  44 239 264 104 451 325 353 153 338 190 120 429 246
  51 149 223 117  56 128  11 146 450 326 452  92 271 408 432 180 212 166
 303 231 431  10 140  79 143 118 279 248 340  76 289 462  19 226   2 119
 204 410 414 254 302  80 422 267  46 445 355 304 145  66 123 367 316  32
 256  71  59 295   1 467 210 330 440 202 335 112  67 393 156 255 430  43
  22 419 124 272  21 312 356 332 233 324 268 405 461 277 361 385 344 113
 176 275 259  57 407 320 276 401 300 136 474 125 152 305 102  23  69 363
 228 215 141 318 192 328 409 250 454 435  90  25  35  34 457 306 343  40
 447  58 265 337 392 270 287 334 311 283  73 213  86 262  41 364 354 157
 203 333 224  70  18  16 111 426 222 196  47   8 469 339 116 406 446 108
 459  17 167  97 234 191 114  75 282 309 170 182  48 365 359  98  39 229
 362 357 382   7 307  64 476 322 165 101  50 388 100 115 350 129 296 379
 201 253 235 269  54 404 352 342 158 297 397 427  74 436  13 216 374 415
 243 425 292 220 434  60  14  15 257 291 163  36 377  77 252 164 184 411
 396 438 428 174 106 109   6  78 403 232 351  31 225 472 198 412 251 373
 370 321 150 398 258  26 329 244 360 386   0 301 263 286 313  55  89 181
 260 142 346   9 126 376 455 200 127 336 284 315 178 380 168 460 424 161
 381 358 423 294 144  27 179  62 107 240 185 278 236 241  84  45 448 172
 211 199 458 147 155 449 218 281 105 261 453 371 441 290 237 169 399 400
 368 285 348 154 221 151 378 395 208]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.3498
INFO voc_eval.py: 171: [ 62 246   1 180 213 212  33   3 176  30 179  53 288  72 231  84  73 214
 118 127 241 292 250 256 168 225 192 200  54 175  15 178  13  91 204  65
  74 308  11 262 215  68 141 144 255  58 103 275 327 281 123 291 196 113
 193 270  64 145  29 149 235  41 260  39 148  89 167 295 138 286 146 263
  95 210 228  12 240 318 234 267  61 202 158 195  87 302  48 248 282  59
 134 324 157 172 301 285 284 306 317  44 199 257  14 276 307  19  86 169
  37 162 303  32  78 314 223 326 205  75  83 229 173 283 325  90 289  85
 219 184  81 253  79 293  97 265 164  42  50 245 201  23  26 236  82 198
 124 203 100 277 133  38  60 297 230 120 165 152 266  66 320  52 181 249
 237 170  10 313 294 186 136  94 119 264 247 117 188  36 227 128  49 101
 130 251 147 279 108  25 111 125  55 298 319 280 116 110  96  93 191  46
 296 309   6  31  67 109 216  28   4 159 107 258 132 190 311 232   9 274
   5  70 140 242  16 114  20 259 304 104 208 328 174 189 137  18 273 160
 316 105 238  92 254 305 322 183 194 269 211  56  35 185 150   2 106 126
 268 315 261  22 166 156 122 299  71  99 252 153 220 218 163 271 239  34
   7 177 224 143  21 244 217 131  45 182 287 155   8 206 222  47 310 312
  24 300 321  43  77   0 139 243 115 197  63 102 272  51 323 171 154  40
  88  98 226 278  69 221 233 209 207 290  17 187 129 121 135  57 142  80
 112 161 151  76  27]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3964
INFO voc_eval.py: 171: [ 98  99 177 180 174 111   2 179 178 147 234  87  84 132 100   6  47 146
  75 124 140  76 212 139  86  85 194 235  83 186  62 219 159 160 181 113
 184  54 106   5 209 119  68  37  72  23 153 189 187  71  67 128  14 109
 204 152 122 115 148 196  52 150 156 183 188 151 225  22 141  58  19  73
 201  82 127  43 182  25 232  77 103 138 213 117 197  20 229 162  40 195
 214  57 190  13  27 226 207  66 131 191 215 155  24  11   4 170 224 211
 108 110  48 130   9  93 129 175 165  53 192 185 161  61  90 120 220 230
  79 167 227  56 231   1 105 116  16  74  63 193 218 198  17  34  39  44
 157 208  32  33  70 158  97 101   3  31  46 202 163 114 217 199  88 107
 137  42 136 210  26 154 144  64 233  18 173  50  69 216 126 200 135 222
 168  89  35  59  55  91   0 237 228 104  81  78 205 149 112 102 164 118
 171  28  51 203 223  41  36 145  49 142  30  94 172 123 134  80 166  45
  92 169  65  60  29  95  10  96 133  38 125  12   7 206 221 121 236  21
 176 143  15   8]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5042
INFO voc_eval.py: 171: [75 27 16 10 26 61 37 64 41  3 11 57 78 46 66 44 68 36 50 34 72 71 35 25
  2 28 73 32 52 74 23 24 19 70  9 30 21 13  1 53 63 18 79 33 59 42 56  5
 14 47 76 77 31 67 54 29 51 22 15 40 48 69 17 60  6 62 65 39 49 58 20  8
 55  7 43 38 12  0 45  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6677
INFO voc_eval.py: 171: [155 159  67 225 223  37  66 111  26 162  47  77 218 236 113 144 173 213
 197 224 171 108 222  13  11  90   0  91 206   7 110 231 112 177  68  41
 125 141 178 214 122  99 140 130 175 124  78  48  25 208 157  31 196 106
 205  89  63 211 103 183 100 128  20  23 233 131 184 172 243 200  92  38
  40 147  52  82 154  33 135  72 191  93  60 169 215   5  21 101 109 105
 238 139 119 202 240 156  62 117  34 229  95  50  42  80 193  74 146 115
 199 127  27 163 209  97  75 142 179   8  83  53 181  87 176 133  22 207
 203 239 107 143  14 228 198 158 132  73 153 129   3  29 190  17 244  28
 174  76 120 227 136 235  16  57  88 170   9 186  30   4  65   1  55  85
 104 152 116  58  35 210 137  61 219  51  81 149  96 138 150 216 145 189
  15 212 217  94 166   6   2 160 180  44 221 232  36 192  59 187 195  12
  98 194 126  24 182  71 134 168  32 242 165 185 241 161  39 220  69  56
  70 148  45  43  46 151 102 188 114 121 226  64 204  54  84 230 237  79
  19 201  49  18 118  10 123  86 164 167 234]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.5542
INFO voc_eval.py: 171: [ 7 79 26 94 15  4 66 74 88 39 21 48 72 25 60 38 54 86 44 23 27 52  1  6
 61  3 92 80 76 53 41 11 73 75 30 47 68 69 29 93 83 40 37 14 22 46 90  0
 56 31 10 91  8 45 17 67 65 78 50  5 43 13 71 18 12 82 34 24 32 89 95 42
 33 84 58 85 77 62 51 55  9 64 28 19 20 35 16  2 59 87 97 81 63 57 36 70
 49 96]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1818
INFO voc_eval.py: 171: [698 171 697  74 739 302 556 741 183  75 169  50  32 756 498 168 442  49
 389 752 445 666 285 476 604 219 304 757 499 751 602 712 786 279 758 116
 610  27 252 461 329 605 137 713 477 667 469 245 467 286 303 194 108 524
  89 160 497  90 557 390 755 258 582 584  93 135 177  42 451 566 504 136
 356 769 297 188 440 257   3 485 331  98 218 528 671 615 228  44 692 326
 688  15 201 296  69  38 796 187 736 132 220  96 624 738 597 534 143 221
 652 381 189 127 714 422 787 310 511 593 522  51 632 607 379 272 672 700
 288 100 438 727 668 521 358 208 191 209 263 687 405  91 124 312 740  64
 435 346 383 770 743 710 398 795 780 669 635 517 653 711 318 118  70 659
 548 748 170 126 359  35  28 718 313  60 404 628 791 349 483 598 113 185
  33   4 695 647 699 101 583 215 114 587 544  11 308 270   8 333 248 425
 649 496 235 217 670 394 281 174  63 742 327 705 745 480 380  45 164 205
 715  55 574   7 421 242 627 463 417 197 145 586 437 150 237 537 735 600
 259 785  92 144 175 762  77 130 535 706  26 182 449 529 704  47 664 575
 560 274 634 478 696 247 701 571 608 198 323 275 133 716 589 419 199 309
 558 773 322 490 408 673 547 234 265 267 392 167 551  97 369 441 102  84
 631 391  21 513 371 578 568 196 531  19 552 640 253  87 680 340  67 244
 399 444 474 508 287 734 456 342 728 162 410 299 222 502 731 520 363 103
 128 307 471 684 192 249  66 594 623 412 306 119 707 364  37 564 606 798
 592 555 771  18 224 790 429 131 506 572 527 423 663 452 657 362 163 212
 685 271 460 448 619 603 484 290 550 643 336 385 158 439 482 690 784 420
  52 155 367 729 284 230 176 229 146  79 686  73 382 372 386 488 523 512
 427 778 418 311 203 190 645 737 360 637  40 749 409 153  43 553 193 156
  88 121 200 533 516 622 377 677   6 202  81 613 351 591 416 231 479 357
 250 317 147 337 430 354 726  10 300 305  41 581 339 792 719  62 402 432
  46 475 562 691 186 492 400 294 526  25 563 401 214 694 777 797 129 243
 396 658 646 353 536 141 211 298  53 468 493 569 676 149 226 730 276  20
 273 466 650  68 576 278 590 793 636  99 106 365 763 775 661 120 264 779
 324 546 489 376 648 104  57  54 172 665 744  95 559 330 138 618 573 332
 139 375 154 370 269 373 702 109 656 538 115 447 675 207 350 395 159 334
 760  39 530 266 630 289  71 173 110 470 165  48 625 616 431  16 125 148
 260 549 772 216 341 473 446 223 487 781 225 157 204 166 465 348 411  24
 639 761 539 595 255 660 433 254  59 232 620 543  12 123 236 415 794 233
 227 509 181 717  13 567 580 107 633 256 689 599 261 532 561 328 111 514
 206 733   0 609 161 343 210 585 459 655 178 491 152  36  31   9 681 403
 570 709 542 541 765 293 407 320 481 280 355 246 316 283 457 486 642 505
  56  61 626 184 674 195  80  94  83 554  82 241   5 277 344 374 753 774
 724 335 240  78 799 464 455 515 518 651 732 345 366 179 747 393 453 644
 768  14  23 414 151 545  34 397 783 641 788 638 510 759 105 238  22 503
  29 251 500 721 789 428 462 540 472 378 338  72 295 117 764  65 703 122
 565 767 315 683 720 611 693 525 454 384  58 654  76 436 426 268 754 361
 629 588 450 519 347 406 458 291 413 507 596 321   1 314 301 292 424 614
 142 579  30 679 725 239 617 443 776 766 612 434 662 495 494 352 723 368
 387 501 140 134 112 319 750 678 325  17 282   2 722  86 782 213 621 180
 746 577 682 601 708 262  85 388]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5483
INFO voc_eval.py: 171: [ 28  48   7  38  30  31  29   5  37  32  73  27  64  75  99 109  69 114
 136 125  57 120 115   1  62  74 106  34 137 113  52 129 123  12  81  92
  35  60  89   2  46  40   3 127   0 117 110  56  91  87 124  23  49 126
  71  68  90 121  44 100  77 107  41 122  61 118  96  59  80  86  93  11
  21  18  24 140  95  94   9  33 132 111  47  63  70  13  85 101  84  53
  14  72  98  20  17 133  76  15  19 105  82  88 102  65  26  97  66  43
  83  67  39  45 116 104  25 119 112  10  42  36  50 135 134   4 130  22
   8  55 131 103 139  78 138  51  79 108  58  54  16   6 128]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.5701
INFO voc_eval.py: 171: [144 152 208 344  33  74 304 325 162 240  39 202  15 217 300 257 108 207
 215 305 119 117 229  88 102 314 186 142   8 267 287  58 234 298 121 275
 105  20 318 165 282 120  49 224   5 283 225 353 182 161 246 101 163 308
   3  12 269  36  98 270 154 364 350 239 223  80 299  43  96 363 218  93
  13 259 301 231 197 146 209 268 199 334 360 245 313 118  34  51 172  38
  97  76 187 382  95 254 290  65  68  10 211 138 169 255 302 151 273  52
  42 220 332 216  29   0 129 124 112 159  16 323 340  22  71 263 307 250
 176 104 252  63 258 328 310 381 286  75 147 327 322 347 336 230 281  31
  35 148 158 192 379  44 157 136 116 274  45 113 109  79  83 368 196 338
 367 335  21 348  69 168 343 330  14 359 377 320 315 366 370  28 289   6
  59  54   1 242  53 200 130  48  64 156 185  99  56 103 141  82 365 341
  86 125 342 346 189  81 201 153  70 333 294 260  50 143 155 213 278 194
  41  92 110 331 167  37 354 277 107 383 160 205 135 227 173 115 221 329
   9 198 264 276 164 253 339  73  87  27 261  30 140  32 206 375  91 345
 195  78 280 272  60 179 226 349  55 326 376 178 303 222 166 262 321  24
 357 170 351 228 145 235 149 319 150 372 100  25   4 184 181 127 374 309
 171 183  46 210 233 243 312  18 271  89 237 292 247 114 369  61 279  67
 188 193 232  47 203 212 291 293 316 251  26  66  94  11 191 175 219 244
  40 284  85 256 134 190 214 358 361  77   2 362  23 180 296  17 373 123
 380 204 238 378  19 352 128 122 126 288 356 133 311  90  84 131  72 295
 249 174 139 106 111 324 236  62 137 265 285 248   7 371  57 177 241 132
 337 297 306 266 355 317]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4578
INFO voc_eval.py: 171: [ 65  84  94   7  74 157  71  81 145  26  87  11  24 144 124  35  69 146
  23   6  95  44 134 158 123   4  25 153  34  88  77  36 148  82  70   2
 113 150  29  83  39  73 138 139  60 154 128  49   9  52 111  64 121  37
  56  46 114  86 115  61 126  93 110 122 101  67  96  48 108  42 151 103
  38   5 140  50  45 104 112  72  57 143  28  12 155  31 127  79 116  85
  55  32  97  16 147 100  92 132 117  90 125  43  21   8  89 102  30  47
  99  76  20  91  10  51  53  17  54 135 130 129  75 119 142  27  15 156
 109  13  18  41 137 152 107  62  78  59  22  33 105 120 106  63  68 149
 136   0   3  66 131  19  80 133 141  58   1  98 118  14  40]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2561
INFO voc_eval.py: 171: [118 101  15  50  28  67  16  92 105  93  17  89  80  32  74  66 120 119
   3  53  46 103  91 108  78   1 113  77  56  76  55  83   0 131 110  48
 123  63  35  43  12 114  20  37  19  69  40 112  82 102 104  36  81  84
  62   6  79  75  68   4  29 111  49  94  13 126  96  99  44 109  95  70
  25 129  52  38  72  26  54 107 130  33  71  51 117  10  65  87  18  47
  61   2  27  85  57 115  97 116  31  90  41 124  64 100  86 127  22 122
   5  88  45  24  59 128  34  42  21  98  23   8  58 106 125   7  39  14
  73  60  30 121  11   9]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.5286
INFO voc_eval.py: 171: [45  6  3 23 27 31  2 16 47 42 14 26 29  7 32 10  0 34 38 28 18  8 37 19
 30 35 20 15 12 17 46 40 24 43 25  1 21 13 48 41 36 33 22 11 44  5 39  9
  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8864
INFO voc_eval.py: 171: [1592  903  254 ... 1518 1900  361]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7216
INFO voc_eval.py: 171: [207  50  30 314  95  35 276  63  32 313  88  33 111 329  10 131  31  70
   4 221 227 332 110 346  59 202 208 262 128 180  34 279  42 185  77 164
 266 103 193 241  38  74  66 295 116 310 334 104  49  76 333  58 302 105
 293 316  23 109 216 162 304 124  15  56 228 267 265  96 149 152 142 315
 239 163 354 268 237 307 275 123 136 222 122 297 201 156 330 298 254 303
 224 340 336 335  97 194 277  26 198 238   9  21  87 145  45 138 151 331
  13 300 248 214  73 288 312 326  54 121  98 285  51 213  47 327 192 296
 251 218 166  29 147 281 120 197 150 170  68  16 211 317 264  71 132  53
  81 282 129 100 252 225 189   5 203 182  28 178 236 205 209 140 172 246
 223 143  79 338 188  65 119  67  40 159 291 179  37 290 175 165  27 345
 269  24 210  90 341  60 235 133 144 348 183 118  78 257 139 115 130 261
 352 311  55 117 220 233 240  91  57 229  61 306  62 107 153 204 158  36
 232 283 102 305  82 126 219   7 323  43 259 168 181 318 186   8  20 169
 349  89   3 337 263 280 273 127  25 320 284  64  44 344  39 244 199 187
 230 242 322  84 160 173  48 161   0 249 171  11  80 146 196 308 274 177
 200 167 321 206  52  75 114 301 292 355  14 324 309 294 243 134 286  85
 258   1  46 112 339 271 157 217   6 328 353 245 287 255 253 226 176 347
  18 106 174 141 289  12 325  22 125 256 184 260 231  17 250 148 278 212
 350  83 351 270 108 319  93 299 137 272 342 343  99 113   2  86  92 154
 234 195 191 101 155  19 135  41  69 190  94  72 247 215]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5931
INFO voc_eval.py: 171: [ 98   4  61 101   6  73  20  93 113  99  56  54  63  26 112  19  21  35
  81  15  77  84  50  46  27  78   5  66 108  29  67 116  75  57 110   9
  45  52  95  49 109  17   1  96  40  83  68  76   0  82  58  36  37   7
   2  38  60 117  69  59 114  85   3  91  16  97  80  25  71  90  18 118
  42 111  86  92  11  74  39  53 105  13 106  43 107  89  44  14  65  70
  88  79 119  47  12  72 103  33 100  41  31  64   8  23  32  10 115  62
  87  51  22  30  34  55 102  24  94 104  48  28]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4542
INFO voc_eval.py: 171: [29 14 30 47 43  1  2 33 62 42 71 70 24 48 18 26 36 39 53 38 45 35 50 58
  8 16 68 13 10 49 65 31 19 69 32 55 54 74  6 17  5 25  9 28 60 63  3 23
 64 44 34 11 59 73 52 37 41 40 56 72 61 67 51 21 66  0 22 20 12  7 75 57
  4 46 15 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3311
INFO voc_eval.py: 171: [45 67 12  8 81 42 23 39 92 48 84  7 58  5 61 52 16 21 24 76 80 91  9 41
 50 37 93 26 75 44 47 89 40 72 60  4  3 56 78 34 19 69 68 62 87 83 36  2
 43 30 46 54 27 51 10 28 66 73 59 57 90 22 82  6 35 13 74 53 71 98 25 15
 20 96 49 55 14 88 65 95 85 29 86 33 31 63 77  1 38 70 79 11 94 97 32 17
  0 18 64]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5913
INFO voc_eval.py: 171: [37 94 20 63  0 70 41 67  7 62  4 47 19  5 28 34 51 52 29 74 88 92 61 81
 14 11 84 25 76 73 96 50  2 44 18 98 66 71 54 15 68 75 64 31 99 10 78 93
 30 16 43 72 82  1 58 32 24 36 65  8 46 45 40 33 97 42 13 60 38 56  6 22
 17 26  3  9 23 55 83 77 59 80 90 95 86 39 35 49 53 12 69 87 91 21 85 89
 57 27 79 48]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5141
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5085
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.371
INFO cross_voc_dataset_evaluator.py: 134: 0.692
INFO cross_voc_dataset_evaluator.py: 134: 0.350
INFO cross_voc_dataset_evaluator.py: 134: 0.396
INFO cross_voc_dataset_evaluator.py: 134: 0.504
INFO cross_voc_dataset_evaluator.py: 134: 0.668
INFO cross_voc_dataset_evaluator.py: 134: 0.554
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.548
INFO cross_voc_dataset_evaluator.py: 134: 0.570
INFO cross_voc_dataset_evaluator.py: 134: 0.458
INFO cross_voc_dataset_evaluator.py: 134: 0.256
INFO cross_voc_dataset_evaluator.py: 134: 0.529
INFO cross_voc_dataset_evaluator.py: 134: 0.886
INFO cross_voc_dataset_evaluator.py: 134: 0.722
INFO cross_voc_dataset_evaluator.py: 134: 0.593
INFO cross_voc_dataset_evaluator.py: 134: 0.454
INFO cross_voc_dataset_evaluator.py: 134: 0.331
INFO cross_voc_dataset_evaluator.py: 134: 0.591
INFO cross_voc_dataset_evaluator.py: 134: 0.514
INFO cross_voc_dataset_evaluator.py: 135: 0.509
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.514s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.378s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.369s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.374s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.379s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.376s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.375s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.376s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.371s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.374s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.371s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.374s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.377s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.399s + 0.001s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.356s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.376s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.359s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.363s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.367s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.375s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.374s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.373s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.374s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.372s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.370s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.368s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.533s + 0.001s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.386s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.378s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.389s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.387s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.383s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.377s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.376s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.376s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.378s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.380s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.378s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.380s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.595s + 0.002s (eta: 0:01:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.378s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.378s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.380s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.376s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.372s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.374s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.371s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.371s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.371s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.368s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.369s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.370s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.770s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [158  34 125  67  26  29  56  27  18  82  15  69   4  77  40   2 119 175
  13  16 131  35  52 102  48 171  78  41  54  44 124 136  30  20  81  19
 143  64  63  68  73 113  96   9  75 107 101 135  22 140  10 159  84  17
  87  25  46  98 170 103 157   0  85  28 137 165   5  92  24  47 181 139
  38 182  55  72 146 160 129  43  86 156  12 128  61 155 100  71 154 168
 174 130  62 133  37 123  49  60  90 173 132 134 127 166  36 142  88 111
  74 148 115  45  66  95 126  91  14  23  83 112   3 164   6  89 153 183
 162  99 145  58 138 150   7 176  94 180  80  21 149  93   1 118 163  65
  79  42 105 114 151  59  57 161 106 152 177 116 120 110 108 109 178 169
 122  97 172  32 144 147  51  76  31 167 117   8  70  11  53 121 141  50
 104  33  39 179]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3258
INFO voc_eval.py: 171: [38 10  6 19 21 42 40 29 45 48 23 50 43 47 18 46 30 36 24  0 41 44 15 33
 35 51 31  7  2 17  3 22  4 49 39 16  8 13  9 34  1 20 28 26 25 32 12 27
 11 37 14  5]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6745
INFO voc_eval.py: 171: [ 66 165 189 167   5 129 379 283 380 187 308  64 131 203  83 130  21 401
 109  73 201 445 446 331   6  95 358 289 402 181  35 237 154 139 232  49
 312  31 306 403 239 447 377 417  82  94 288 133 180  54  87 303 121  40
 136 103 258 171  96   0 427 429 120  69 387 448 360 137 320 426  88 334
 145 382 183 398  99 209 224 339 132 422 267 452  41  84 374  44 134  93
  32  53 378 182 235  26 316 301 202 188 266 338 457 363  62 279 428  91
 172  86 324  11 168 372 405 177 299 191 206 155 406 271 453 221 343  30
 450 438  46 365 216 184 241 256 104 231 455 416 157 119 238 116 314 149
 215 414 315  92 263 127 346 208 423 122 117 327 435 425 293 146  13  52
   3 160 207  57 434  80 393  34 226 240  12 433  72 415 144  60  81 285
  20 214 270 200 205  24 246 217 118 280 399 329 292 123  45 444 141  58
 294 251 381  22 260 222 395  47  77 449 333 248   2 357  68 198  67 265
  23 432 354 443 349 228 247 323 404 319 373 143 332  71 419 345 321 408
 151  16 274 305 430 242 124  59 278 313 111 269 135 309 392 138 220 186
 268 112 326 352 140 302 307 110 386 148 295  70 102  36 277 169 254 456
 290  37 394  90 297  42 355 439 211  27 185 161 152 257 259 411  18 317
 441 153 100 262 175 298  50 296 351 328 190 218  25 322 347 421 229 113
 272  19 376   9 356 199 300  48 431 384 383 458 230 410 391 451 170 107
 210 371  43 249 397 223   7  51 389 245  65  74 418 158 342 325 282 353
 243 115 413 286  39  76 176 227 212 261  33  61 340   8 424 412 164 255
 287  97  85 350 197  89 101 436 368  14 108  17  28 159 454   1 390  55
 234 310 396 250  98  63 126 375 236 344  78 244 388 105  15 281  75 178
 204  79 291 219 311 128 114 304 362 213 284 162 330 253 400  38 318 193
 341  10 142 196  29 437 273 361 275 192 179 370 106 173 125 147 174 369
 367 252 407 359 150 233 336 348 156 442 163 364   4 366 385  56 166 276
 335 195 420 440 409 194 264 225 337]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.3435
INFO voc_eval.py: 171: [ 57   1 225 162 193 194   3  30 160 163  27  51 266  68 212 108  79  69
 195 221 117 269  50 230 152 235 175  81 206 159 181  14 161  13 186 285
  63  70  11  58 196 241 130 105 234 132  53 252  96 303 257  60 112 267
 249 176 179 137  38 215  26 239 133 136  86 271 127 220 295  36  90 264
 242 151 191 214 134 246  12  82 209  29 228 143 278 279  56 184  80 226
 263  45 124 110  54 280 147 155 300 178 188 157 301  83 142 294 283 236
  78 153  75  72 253  85  16 261  34 260 182 284 302 291  40 114 146 149
  76  22 201 166 183  92 244 216 233  19 185 210 204 270  94  24  49  71
 158 254  55 123  47 274  10 109 167 297  42 245 290 243  35 180 268 135
  88 169 139 217 125  89 227 272 170  77 104 208 211  61 150 218 118 156
 258  46 248 144 101 205  95 296  21 154  39 229 107 103  43 174 172   2
 120 259  33  91 213 281 304 200 171 100 237 251   9 140 298 275  87 288
   4 256  25 238   5 115 293  15  97   6 187 299 102 240 222 277  17 129
 177  28 198 250  18  32  64 126  67 231  99 262 273 282 219  73 106 189
  52 197 192  98 247  31 116 173 165 111  44   7 190 141 203 131 276 199
 138 207 148 232  41 255 122  65 119  74  62 286 202 168 145  84  37 223
  59 265 289  48 121  93  66   8 128 287 164  23 292   0 113 224  20]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4072
INFO voc_eval.py: 171: [ 94  95 174 180 171   2 107 176 175 142 230  82  80 127  96  43   4  83
 141  71 135 118  72 208  81 134  79 231 190 182  58 215 155 156 177 109
 181  50 115 102  12 205  64  35  68  67 187  22 185 200  15 124 105  63
 111 146 117 143 192 179  48 121 147 151 186 136 145 150   5  21  19 220
  54 198 178  69 122  39  78 120  24 196 228  73 183 133  99 209 158   6
 191  37 225 193 210  14  53 113 203 149  26  62  20 222 211  89 165  23
 188 207  11  45 104 154 161 172 184 106 219  49 157  86  75 216 126 125
 116   1 163 189 226 112 101 214 223  16  57 194 173   9  59  52 227 152
  36  70   3  40  97  66 123 103 199 153  30 195  17  84  33 197 213 159
 110  29 204 201  31   7 217 206  93  18 212 233 148  25  65  46  85 131
  41  77 202 130 132 129 170 139  60 100  88  47  74  32  34 168 166  38
  98  51 229 108 140 224 167  55  91  87   0  90 162 144 138 114 169  76
  92 164   8  61 218  42  27 232  28 128  13 137 160  44 119  10  56 221]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5075
INFO voc_eval.py: 171: [67 24 13  8 23 55 57 39 33 35 52 12 70 34 58 42 60 41  2 46  1 63 64 27
 22 25 32 56 16 29 21 65 66 31 49 62  0 71  7 20 18 48  9 53 43 30 15 51
  3 40 10 26 68 19 69 28 11 38 14  6 54 61 59 50  4 47 44 37 17 45  5 36]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6590
INFO voc_eval.py: 171: [ 62 142 146 210  36 211  61  25 100 149  44  70 205 131 102 222 159 212
 200 157  98 186 208  97  11  82  13  81   0 193 216   7  63  40 162 114
 106 201 128 127 164  90 111 113 160 117  71  45 195  24 105 144  58  30
 183  80  91 192 199  94 218 118 170  55  21 158 120 228 187 171  18  83
  37  39 134 178 174  50  76  32 202 122 141  84  65  96  99 110  67 155
  22 225   5 189 126  92 108 215  57 143  86 224  33  88 150 185 133 116
  68  26  72  46 167 180 165 104 129  14  41  48  74  78  19   8 190 209
 194 140  20 184   9 145 130 214 119  28 221   2 101  69  66  75  49 177
 229 213 153  27  16 197 161 196 136  95 107  17  29 139 173  34  85  60
   4  56  47  52  53  73 156  79 124 198  87 204 220   1  15 176 148 123
 203 179 137 166 125 132 168 175 206   6  43  89 217 147 121 226   3 115
  35  54 103  31  23 219 181 227 182  42 154  51 152 207 135  64  12 109
 191 172  38 223  93  77 112 169 163 138 188  10 151  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.5572
INFO voc_eval.py: 171: [ 7 74 24 87  4 15 61 69 20 43 81 34 67 22 54 33 46 25  6 80 23 55 48 39
 75 85  3 71  1 36 47 70 11 42 26 62 77 86 32 68 14 64 60 41  8 51  0 35
 27 84 83 10 66 73 38 44 40 16 13 59 30 21  5 28 65 37 56 17 45 31 88 19
 18 72 29 76 12 82  9 79 78 57 53 49 58  2 52 63 50 89]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1855
INFO voc_eval.py: 171: [643 642  72 163 515 282 172 680 678  73 164  49  30 693 160 461 412 410
  48 357 689 616 266 207 439 561 286 694 462 688 655  26 112 718 236 568
 695 425 262 306 562 129 656 617 559 230 440 432 284 267 431 104 184 152
 460 358 484 517 243 130 540 162 538  88  86 692 191  40 417  91 465 279
 704 329 242 408 177   2 524 308 206 449 216 487 620 569 637  87  42 278
 303 634  36 351 730  14 675 208 176 640 677  67 209 135 492  94 603 580
 178 553 657 127 121 546 283 719 391 471 482  50 566 270 350 586 255 666
 405 644 331 403 247 618 374 481 321 100 199 622 120 679  89 352  62 728
 198 661 366 705 589 653 619 713 541 291 476 610 654  32 604  58 326 682
 554 119 332 646 161 131 686 294   3 298  27 174 113 583 507  31 539 373
 725 109 447 684 165 674 179 598 289 204 621 599  61 304  10  97 253 205
 458 633 110  68 362 649  43 393 681 263 427  54 156 126 234 532 503 641
 582 195   7 311 227 658   5 244 390 189 543 171 137 349 223 557 496 167
 187  75 717 533 406 224 488 143 493  46 650 529 415 614 700 384 138  25
  90 444 520 441 301 645 377 290 257 544 188 251 565 659 623 222 409 186
 588 340 454 516 360 248  83 258  95 159 708  20  98 509 668 506 473 732
 342 490 628 673 526 317 359 585  84 468 231  99 379 318 463 510 211 437
 292  18 154 353 671 334 237 268 181 381 128 193 229  35 411 367 388 535
 280 115 592 648 288 486 421 480 523 182  64 548 720 434 662 414 354 335
 579 155 125 651 285 212 514 122  34  17 385 733 594 452 530 302 435 706
 631 551 563 272 467 378 560 124  65 716 218  41 424 442  51 214 265 168
 333 254   6 313 330 202 389 395 201 297 183 549 327 192 407 324 512 555
 293 613 338 343 595  78 676  15 136 606 636 139  39  45 219 150 281  71
 511 537 397 472  77 123 477 578 609 287 132 602 711 348 491 314 443 639
 341 134 315 369  38 632 731 261 521 228 627 370 572 190 596 148 364 522
 687 630 117 575 175 495   9 259 547  80 710 141 233  24  96 457 456 647
  85 483 528 180 277 430 245  52  19 665 497 697 527 438 600  53  60 116
 597 344 256 133 203 111  66 611 626 300 453 147 518 701 166 726 323 485
 714 709 608 615 670 387  69 505 489 157 325 698 310  23 336 712 669 271
 531 624 534 368 433 217 501 140 252 584 105 347 469 727 101 663 249 386
 345 581 106 574 153 683 210 103  37 220 504 363 307 399  93 451 591 413
 309 550 429 724 145 672 158 213 519 238 346 545 241 240  56 276 508 400
 221 593 607 448  44 320  57 428 502 707 419 107 542 194 151 612 525 246
 169 356 576  12  79  82 494 474 118 401 372   8 450 420 660 567 102 185
 536 587 299 513 500 275 226 197 296 556 635 170  33  29 173 319  11 445
 418 146 423 625 225  55 455 260 652  76  28 149 337 722  47  21 383 376
 723 426 601   0 235  81 305 398 699 316 196 144 466 422 232 664 264 498
 416 721 312   4 475 478 690 590 375 274 322 729  59 273 464 114 577 404
 629 269 365 328 200 573 295 382 436  63  22 715 446 380 250   1 571 108
  13 402 355 499 361 339 142 570 638 479 667 703 702 605 564 685 459 215
 392 394 558 691 696  16 239  92 371 470  70 396  74 552]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5431
INFO voc_eval.py: 171: [ 47  27   7  28  35  29  26  37  58  30   6  31  94  71  69  65 105 132
 110  54 122 111 116   2  70  33 102 133 109  12  51 120  34  87 128  76
   3  53  45   4   1  39  60 113 124 106  83  67  82  86 121 123  22  48
  46  57  56  64  85  73  55  43  66 117 103  95  50  88  91  17  80  96
 119 114 135  10  81  75 107  23  20  72  79 127  78  32   9  89  68 129
  59 101  18  84  93  16  40  61  13  41 108  38  77  97  14  42  25  90
  19  24  92  44   8  49  62  21 118  15 130 125 112 100  11  36  52   5
 104  63   0 126  99  74 115 134 131  98]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.5570
INFO voc_eval.py: 171: [ 32 136 143 199 332 311  74 291 152 230 100  38 193  16 209 248 288 198
 213 116 222  86  99 176 256 134 110   9 300 292 276  58 286 224 102 265
 155 219  21 220 304  47 272 214 215 117 172 153   4 151 273  97 340 207
   6  95  13 114  35 258 236  79  30 144  91 259 206 338 287  43 294 351
  94 229 210 289 250 257 346 190 348  64 320  12  93 200  14 244 115 235
  42 177  37 279  49  67 299 246  73  51 317 159  70 142 132 108  29 202
 166 290 369 208  28 104 124  34 308 216 254 162 275 150 120   0  72  62
 138 313 188 296 293  22 240 249 327  33 315 334  18 368 211 131 309 242
 182 356 139 271  44  77 366 112 148  45 103 186 262 355  96 101 109 306
  82 322  56  59 149 345 325 158 301   7 336  54 146 179 321  78 278   2
 231 314 133 125 328 341 105  23 358 147  84  69  52  48  15 365 335  63
  68 121 135  81 163  40  27 106 331 191 145 316 218  53  80 266 130 318
  11 267 269 370 201 204 175  85 283  89 354 154 330 251 192 329 270 111
 268  76  31 184 173 307 156  55 137 305 312 196  19 225 157 217 333  60
 253 342 169 364 260 185 255  26 337 140 141 223 326  87 174  98 123 343
  75  10 232 353 362 357  46   5 295 233 349 284 361 252 181 281 282 261
 237 212 189 243  66 167 183  20  36 171 241 161 165 194 164 180 221 280
 344 319  25 298 170  92  61 339 118 168 197 360 195 347  39 122 350 297
 107 247   3 302 352   8 228 367 226 160  88 113  50 277  65 310 359 363
  24  83 128 274 239 323 205 238 178 285 203  17  57  41 119  71 324 234
 129 264 126 127 227 303 263 187   1 245  90]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4619
INFO voc_eval.py: 171: [ 61  80   6  91 155  70  77  66  83  22 143  32  20 142   9  19  64 144
 121  92 132   5 156 153  21  40  31 120   3 152  33  73  84  78  45  79
 111 125   1  65 136  47 148  57  26  36 146 151 137  60 118  69  48 101
  90  63 109   7  58  42 112  44  99  25  94 106  82 119  34 113 108  37
 150  75  11  52  35  51   4 124  68  98  27 138  13  53  28 141 100  85
  43  89  10 129  29 102 123  23  41 145 122  97 114 110  17  55  12  16
 133   8  95  72 115  38  46 126  39  87 105  71 127  49 107  15  74  14
  86 154 149 140 103 131  18  24 128 116  59  30  56  81 130 117 135 147
  96  67 134  62  88   0 139  50   2 104  93  54  76]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2604
INFO voc_eval.py: 171: [ 97 114  11  45  24  62  12  86  89  14  75  69 100  28  90  60 115 116
 101   3  48  73 104  88   1  41  51 108  50  72  78 105   0  43  30  71
 126 109  58  16 123  15  32 106  80  64  38  77  74   5  36  76  95  70
  57  63  98  31  10  13 107  25  91  44 122  99  22   2  93  65  67  39
  33 124 121  29  21  56 125  92  26 113 118  61 103  66  47  49 110  84
   8  94  53  46  82  23  96  59  42 111  52  35  27   6  18  87  85  37
  68 119  83 120   4  17 112   7  55  54  19 102  40  34 117   9  20  79
  81]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.5405
INFO voc_eval.py: 171: [39  5  3 20 21 27  2 13 41 36 22 11 24  6 26  0 29 33 23  7  8 35 32 15
 12 16 17 30 40 14 18  9 10 34 31  1 19 42 25 28  4 37 38]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8926
INFO voc_eval.py: 171: [1526  859  883 ...  527  138  601]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7246
INFO voc_eval.py: 171: [ 48  28 291 190  86  32  30 254  61 290  31  82 306 100   9 119  29 203
   3 209 305 322  99  66 185  57  46 191 116 241 166  71  39 257 149 171
  92 245 177  69 223  35 103 272  63 287 309  93  70  47 279  56 308 270
  94 293 147 198  21 281  98 277 112  14 210 246  54 244 135 131 220 137
 148 247 292 111  87 326 219 284 253 274 180 204 110 275 140 124 307 312
 206 233 316 222 178  88  24 255 181  19  81 125  52  42   8 104 138  68
 132  12 197 266  49 176 303 109 263 276 231 311  89  44 134 289 196 259
 228  27 273 302  64 159 108  51 175 294 200 243  15 136 232 120 207  75
 260 218 168 313 118 157 126  90 186 164 128  25  73 165  37 320 188 205
 192   4 227  62 150 268 107 321 248 151  34 194  53  22 121 236 130 144
 288 267 217  84 318 240 174 169 282 314  72 105 215 224 283 211  67 221
  85  96 327 161   0  40  76 153 187 106 143  33 167 214 139 239  58 202
 261  60  83  91 324   2 172  23 184 242 201 300 310  18   7 154 114 295
  45 115  41 271 315 158 323 299 123 251 179   6 195 225 297 155  59 182
 258 163 262 145  78 133  74  36 278 152 298 173 183 102 193  10  50 285
  79 199 252 286 122 212 229 269 264 146 101  38 117 142 189   1  43 141
 301  26  95  55 162 226  13 234 129 325 237 127 208 249 328 235  17 213
 238  20 319  11  77 160 265 304  80 170 113 256 317 296 280   5 250 156
 230  16 329  65  97 216]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5843
INFO voc_eval.py: 171: [ 92   5  57  95  20   7  68  88 106  93  65  50  59  25 105  21  16  33
  76  15  79  72  47  28  44  71  26  64  62 103  73  54 102 101  49   9
  43   6   2  39 100  46  19  90  70  78   0  35  89  77  34  60  24  66
  56  36   4  55   3  75  17 108  53 107   8  91  85  67  87 104  40  29
  69  81  11  98  37  74  97  86  18  84  99  31  45 109  41  80  83  13
 110  63  22  14  51  42  12  96  10  94  32  48  52  27  23  30  58  61
   1  82  38]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4680
INFO voc_eval.py: 171: [28 16 29  0 45  1 61 41 71 32 40 73 24 19 26 37 46 52 43 34 33 36 55  9
 48 17 14 69 64 11 30 47 20 18 76 31 70 22 27  5 25 59 10 53 58 75 49 54
  7 62 42 39 35 38 74 12  2 67 56 66 13  6  4 51 57 60  8 65 50 21 72 68
  3 44 15 23 63]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3465
INFO voc_eval.py: 171: [66 44 13  9 81 40 23 37 92 83  7 47 57 60  6 51 17 21 24 76 80 11 12 93
 39 36 46 26 75 72 38 42 49 55  5  4 59 88 78 69 19 67 89 50 61 41 86 33
 35 31 45 63  3 53 22 56 27 65 58 73 96 28 91 14 74 95 84 10 94 70 34 52
 82 90 32 71 20 15 30 48 29 85 77 16  0 54 62 25 87 79 64 68 43 18  2  8
  1]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5908
INFO voc_eval.py: 171: [37 87 20 60  0 40 67 59 65  7  4 46 19 27  5 49 50 34 70 28 76 82 85 58
 14 11 24 79 73 89  2 43 18 48 52 92 68 64 61 66 15 31 72 93 21 55 10 16
 42 30 77 71 32 44 69 13 45 62  1 36 23  8 75 86 33 41 38  6 39 54 90 17
 22  3 78 25  9 53 57 74 84 51 12 29 63 56 47 83 80 35 88 91 26 81]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5239
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5077
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.326
INFO cross_voc_dataset_evaluator.py: 134: 0.674
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.407
INFO cross_voc_dataset_evaluator.py: 134: 0.508
INFO cross_voc_dataset_evaluator.py: 134: 0.659
INFO cross_voc_dataset_evaluator.py: 134: 0.557
INFO cross_voc_dataset_evaluator.py: 134: 0.186
INFO cross_voc_dataset_evaluator.py: 134: 0.543
INFO cross_voc_dataset_evaluator.py: 134: 0.557
INFO cross_voc_dataset_evaluator.py: 134: 0.462
INFO cross_voc_dataset_evaluator.py: 134: 0.260
INFO cross_voc_dataset_evaluator.py: 134: 0.540
INFO cross_voc_dataset_evaluator.py: 134: 0.893
INFO cross_voc_dataset_evaluator.py: 134: 0.725
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 134: 0.468
INFO cross_voc_dataset_evaluator.py: 134: 0.346
INFO cross_voc_dataset_evaluator.py: 134: 0.591
INFO cross_voc_dataset_evaluator.py: 134: 0.524
INFO cross_voc_dataset_evaluator.py: 135: 0.508
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.469s + 0.001s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.355s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.352s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.352s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.366s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.367s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.364s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.367s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.366s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.365s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.364s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.365s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.366s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.471s + 0.001s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.397s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.388s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.379s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.376s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.373s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.380s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.377s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.375s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.375s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.372s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.374s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.370s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.538s + 0.002s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.368s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.369s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.373s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.368s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.363s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.361s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.362s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.364s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.362s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.362s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.363s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.362s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.442s + 0.002s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.382s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.368s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.358s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.360s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.358s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.358s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.353s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.355s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.356s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.355s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.356s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.357s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 54.533s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 33 152 120  63  26  29  52  27  17  78  15  65  38  73   4   2 114 169
  13  16 125  34  49  98  46 165  74  39  50  42 119 130  77  30  20  18
 137  60  59 108  64  69  92   9 102  71  97 129  22 134  10 153  80  19
  83  25  44  94 164 151  99   0  28  81 131 159  88   5  24  45 174 133
  37 175  68 140 123 154  51  41  82 150  12 122  57  96 149  67 148 168
  58 124 162 127  36 118  56  86  47 167 126 128 121 160  35 136 106  70
  84 142 110  43  87  91  14  62 107  23  79   3   6  85 158 147 156 139
 144  54 176  95 132   7 170  90 173  76  89 143  21   1  61 113 157  40
  75 100 145 109 101  53  55 155 146 171 104 115 111 105 103 163 172 117
 166  93  32  48 141  72 138  31 161 112   8  66 116 135  11]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3263
INFO voc_eval.py: 171: [38 10  6 19 21 42 40 29 45 48 23 50 43 47 46 18 30 36 24  0 41 44 15 33
 51 31  7  2 17  3 22  4  1 49 39 16  8 13  9 34 20 28 35 26 25 32 27 12
 11 14 37  5]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6379
INFO voc_eval.py: 171: [ 66 188 164 166   5 128 282 378 186 379 307  64 130 202 129  83  21 400
 109  73 443 200 444 330   6  95 357 288  35 180 401 138 236 153 231  31
  49 311 305 402 238 445 416  82 376  94 287 132 179  54  87 302 120  40
 135 257 170   0  96 425 427 119 338  69 386 103 446 359 136 319 333 424
  88 144 381 182 397  99 208 131 223 421  84 266  41 373  44 451 133  93
  53  32 234 181 377  26 201 315 300 187 265 337 456  62 362 278 426 323
 171  91  86  12 404 371 168 176 298 342 205 220 270 190 154 405 452  30
 449 436  46 364 183 215 240 255 104 230 454 415 118 156 237 115 313 148
 314 412 214 345  92 121 262 126 207 116 423 326 292 145   3 159  13 206
 432  52  57  34  80 239 225 392  11  72 431 413 414  60  81 284  20 204
 213 199 269 433 245 279  24 216 142 117 122 291  45  58 398 328 442 250
 140  22 380 259 293 332 448 221  47 394  77   2 247  68 356 197  67 353
 430 264 441  23 348 322  71 227 246 331 403 372 418 318 273 344 320  16
 143 150 407 428 241 304  59 123 312 277 268 111 134 137 308 391 185 139
 219 267 110 112 325 351 306 301 385 294 147  70  36 102 276  90 253  37
 167 455 289 354 296  42 437  27 160 184 393 151 210 258 409 152  18 256
 100 439 316 261 297 174  50 295 350 420 189 327 217 321 346 383 228  25
 113 271 375   9  19 382 355 229  48 429 408 198 299 457 396 390 248 388
 209 107 324 370 450 244 222  51  65 341   7 169 157 281 417  43 352 242
 411  74  39 211 226  76  33 175 285  85 260  61 422 254 339 410   8 163
 286 434  89 349 101  97 367 196  17  28 108 453  14 309   1 249 233 158
  55  63 343 389  98 243 125 374 235 290  78 280  79 203  15 387 218 310
 127  75 303 399 161 114 212 395 283 329 361 317 252  38 192 340  29  10
 191 384 141 360 195 358 369 173 172 435 272 440 106 274 368 178 124 146
 149 251 366 177 347 232 162 335 105 406   4 363 419 155 165 275 438 365
 194 334  56 336 447 224 193 263]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.3423
INFO voc_eval.py: 171: [ 55   1 220 158 189 190   3  28 156 159  25  49 261  66 208  77  67 217
 191 113 106 264  48 225 148 230 171  79 202 155 177  13 157  12 182 280
  61  68  10  56 192 236 126 103 229 128  51 247  94 252 298  58 109 262
 244 172 175 133  36 211 234  24 129 132  84 216 266 123 290  34  88 237
 259 187 147 210 130  80 241  27 205  11 223 273 274 139  54  78 221 180
 258  43 143 108  52 275 120 151 295 153 174 184  81 296 289 138 231 278
  76  83  73 256  70 149 248  15  32 255 178 279 297 286 110  38 145 142
  99  19 197  74 162 212 239 179  90 228 181  18 200 265  47  92 206  22
 154  69  53 119  45   9 249 269 107 163 240 292  40 285 238 131  33  86
 176 263 121 213 135 222 165 102  87 166 214 267  59 152 146 204 253 114
 243  75 140 207  44 291 201  20  93 101 150 224 168  41 170 105   2  37
 276 254 299 116  89 209  31 167 196 232  98   8 293 270 246 136   4 283
  85 251 233   5 288 111  23 183 294  95  14   6 235 100 272 218 173  16
 125 194 245 122  65  17 215  97  62  30  26 185 268  71 226 277 161 169
  29 104  50  42 257 242  96 112 193 188   7 199 137 195 186 271 127 134
 203 144 250 118 115  60  39  35 227  63 281  72  82 284 198  57 141 219
 164 260  21  46  64  91 117 160 282   0 287 124]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4089
INFO voc_eval.py: 171: [ 94  95 174 180 171   2 107 176 175 142 229  82 127  80  96   4  43  83
 141  71 135 118  72 208  81 134 230  79 190 182  58 215 155 156 177 109
 181  50 115 102  12 205  64  35  68  67 187  22 185 200  15 124 105  63
 111 146 117 192 143 179  48 121 147 151 186 136 145 150   5  19  21 220
  54 198 178  69 122  39  24 120 196  78 227  73 183 133  99 209 158   6
 191  37 193 224 210  14  53 113 149 203  26  62  20 221 211  89 165  23
 188 207  11  45 161 104 154 172 184 106 219  49  75 157  86 216 126 125
 189 116 163   1 101 225 214 112 222  16  57 173 194  59   9  52 226 152
  36   3  40  70  66  97 199 123 103 195 153  17  30  84 197 213 159  33
 201 110  29 204   7  31 206 217  18 212 232  93 148  65  25  85  46  41
  77 131 202 129 130 170 132  60 100 139  32  88  47  34  74 168  38 166
  98 140 223 228  51 108  91 167  90 138  87 144 162  55   0  76 169 114
  92 164   8  61 218 231  42  28  27  13 128 160 119  56 137  10  44]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5065
INFO voc_eval.py: 171: [65 24 13  8 23 54 56 39 33 10 35 51 68 34 57 42 58 41  2  1 45 61 27 62
 25 55 22 32 16 21 29 64 48 31 63  0 69 60  7 52 20  9 47 18 43 30 15 50
  3 40 11 26 19 66 67 12 28 38 14  6 53 59 46 49  4 37 17  5 44 36]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6606
INFO voc_eval.py: 171: [ 62 143 147 212  36 213  61  25 100 150  44  70 207 132 102 224 160 214
 202 158  98 187 210  97  11  82  81  13   0 194 218   7  63  40 163 115
 106 203 129 128 165  90 112 114 161 118  45  71 196  24  30 105 145  58
 184  80 193  91 201  94 220 119 171  55  21 159 230 121 188 172  18  83
  37  39 179 135 175  50  76 204  32 123 142  84  65  96  99 111  67 156
  22 227 190   5 127 217  92 109  57 144  86  33 226  88 151 107 186 134
 117  68  26  72 168  46 181 166 104  14 130 191  74  48  41  19  78   8
 211 195 141   9  20 185 146 216 131 120  28 223   2  75  69  49 101  66
 197 215 154 231 178  27  16 162 198 199  95 137 108  29 140  17  85 174
  34  60   4  56  47  52  53  73 157 200 125  79  87 206   1  15 222 124
 205 149 180 177 167 126 138 169 133 176   6  43 208 219 228 148  89 122
   3 103 116  31  23  54  35 229 221 182  77  42  64 136 153 183 192  51
 209 173 155 110 225  38 113  12  93 170  10 189 164  59 139 152]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.5578
INFO voc_eval.py: 171: [ 7 75 25 89  4 16 62 70 21 45 83 35 68 55 23 34 49 26  6 40 82 24 56 51
 77 87  3 72 50 37  1 71 11 44 27 88 79 63 33 15 69 61 65 43  8 28 53  0
 36 86 85 10 67 74 39 47 41 17 14 31 60 22  5 29 66 48 57 38 18 20 90 32
 73 30 78 13  9 84 81 19 58 54 42 80 52  2 59 91 76 46 12 64]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1976
INFO voc_eval.py: 171: [635 161 634  72 507 279 670 170 672  73 162  50  31 685 158 406 404 454
  49 354 608 681 264 205 553 432 283 686 455 680 647  27 110 710 234 560
 687 419 260 303 554 127 648 551 609 228 433 426 281 265 425 102 182 150
 453 355 476 509 241 128 532 530 160  87  85 684 189 411  41  90 458 276
 696 240 326 402 175   3 305 516 204 442 214 479 612 561 629  43  86 275
 300  37 626 348 722  15 667 206 174 632 669  68 207 133 484 595 545 176
 649 572  92 125 119 539 280 711 387 463 474  51 268 558 347 578 253 658
 399 636 328 397 245 610 370 473 318  98 197 614 118 671  63  88 720 349
 653 196 364 697 581 645 611 705 533 288 468 602  33  59 323 646 596 546
 117 637 329 674 159 291 129 678 172   4  28 295 111 575 499  32 531 369
 107 717 163 440 177 676 590 286 666 202 613 301 591  62 251 203  11 451
  95 625 108  69 359 640  44 389 421 673  55 261 154 124 232 524 574 193
 633 495   8 225 308 650 243   6 187 169 386 535 135 549 165 488 346 221
 185  74 709 525 400 222 485 409 521  47 141 692 606 380 136  26 437  89
 512 434 639 298 372 287 255 537 249 186 557 651 615 220 403 481 184 337
 580 447 508 357 246  82  93  96 157  21 256 700 501 465 498 339 724 660
 620 518 314 665 482 356 577  97  83 461 374 315 229 456 502 209 289 152
 430 331 266 350 663 235  19 179 376 126 191 227  36 527 405 113 365 277
 641 584 384 478 285 472 515 415  65 712 180 541 654 427 351 408 282 571
 153 332 123 643 210  18 506  35 381 445 725 120 428 299 522 586 698 270
 623 555 544 373 122 460 552 708  42 216 418  66 212 263  52 166 435 330
 310 327 199   7 385 252 200 294 391 542 181 324 321 190 290 401 335 504
 547 668 605 340  77 587  16 628  46 278 598 217 137 134 529  40  71  76
 503 148 464 121 392 284 469 601 130 338 311 594 570 703 436 132 483 638
 631 366 312 345 513  39 259 723 226 367 564 619 514 642 624 146 588 361
 188 679 622 173 540 115 487  10 702  94 139 257 178 475 450  79 231 424
 567 242  25  53 520 489 449  84 689 657  20 131  54 607 431 519 254 592
  61 109 603 114 589 297 341 201 618  67 510 164 446 706 320 693 322 718
 690 662 480 145  24 701 600  70 269 497 477 155 307 616 333 250 383 704
 493 652 215 462 661 523 138 363 526 576 103 344 719  99 573 655 247 382
 342 208 151 104  38 101 566 360 496 218 675 583 444 394 304 716 543 538
 511  91 156 407 664 423 306   1 236 211 239  57 238 343 599 500 143  45
 441 274 317 585 413  58 534 219 494 192 105 422 395 604 699 167 149 244
 414 517 568  78 353   9  81  13 486 559 466 443 116 505 492 296 183 368
 273 528 224 293 548 100 168 438 316 195 171  34 627  30  12 412 617 579
 258  29 223 334 714  22 417  75 715 644  80 448 147 656  56 378 233 420
 144 691   0 593 262 313  48 416 230 142 459 302 393 410 470 194 309   5
 582 713 467 682 319 371  60 272 271 112 292 398 569 490 721  64 457 377
 267 198 621  23 707 325 362 439 375 379 352 429 248 396 106 556 563  14
 565   2 140 562 550 536 659 491 452 358 471 336 237 677 390 683 695  17
 694 597 630 688 213 388]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5431
INFO voc_eval.py: 171: [ 47  27   7  28  35  29  26  58  37  30  31   6  95  72  70  66 107 134
  54 112 124 113 118  71   2  33 103 135  12 111  51 122  34  88 130  53
  45  77   3   4   1  61  39 115 108 126  84  68  83  87 125 123  22  46
  48  56  57  55  65  86  74  43  67 119  89  96  50  92  81  17  97 116
 121  10 137  82 109  23  76  73  20  79 129  80  32   9  90 131  18 102
  69  85 105  59  94  16  62  41  13  40 110  38  78  98  14  42  25  91
  24  44  19  93   8  49  21  63 120 127  15 132 114 101  36  11   5  52
 104  64   0 128 100  75 106 136 117  99 133  60]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.5656
INFO voc_eval.py: 171: [136  31 144 200 334 313  74 291 153 230 100  38 194  16 210 248 288 199
 214 116 223  86  99 177 256 134 110   9 292 302 276  58 286 225 102 156
 265 220  21 306 221  47 272 215 216 117 154 173   4  97 152 273 208   6
  95 342  13 114  35 258 236  79 145  29  91 259 207 287 340  43 294 353
  94 229 211 289 250 257 348 191 350  64 322  12  93 115 244 201  42  14
 235 178  37 279  49  67 246  73 301  51 160  70 319 143 108  28 167 132
 203 209 290 371 104 124  34 217 310 254 163 275 151 120  62  72 138   0
 315 189 297 293  22 249 240  33 336 317 329 212 131  18 370 311 183 358
 318 140 242 271  77  44 368 112  45 103  96 149 357 187 101 262  56  82
 109 308 324  59 303 327 347 150 159 147   7  54 180 338 323 278   2  78
 133 231 316 105 343 330 125  23  84  69 360 148  52  48 337  15 320 367
 121 135  63 164  40  81  68 219 106 146  27 333  53 192 266  11 130  80
 202 270 372 267  89  85 205 176 251 155 332 283 193 356 331  30 174 268
 111  76 157 309 307 314 185  55 137  19 158 197 226 218 253 335  60 344
 260 170 339 366 255 141 186  26 224 328 175 142  87  98 123  10  75 232
 345 351 233 359  46 299 363 284   5 355 364 252 281 282 182 168 237 261
  66 213 184 243  20 190 241 166 165 195 227  36 172 181 162 171 222 321
 346  25 280 300 341 118  92  32 269 296 349  39 122  61 362 196 169 298
 352 107 247 354 198   3   8 361 369 228 295  50 277  88 304  65 161 312
 113  24 239 128  83 285  41 206 274 365 204 179  57 119 326  71 238 325
  17   1 234 264 127 139 245 263 129 126 188 305  90]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4615
INFO voc_eval.py: 171: [ 61  80   6  91 155  71  77  83  67  22 143  32 142  20   9  19  64 144
  92 132 121   5 156  21 153  40  31 120   3  33 152  74  84  45  78  79
 125 111   1  65  47 136  57 148  26  36 146 151  60 118 137  70  48 101
  63  90 109   7  58  42 112  25  43  94  99 106  82 119  34 113 108  76
 150  37  11  35  51   4 124  52  69  98 138  27  13  28  53 100 141  44
  85  89 129  10  29 102  23  41 123 145 122  97 114 110  55  12  17 133
  16   8  95  73 115  46  38 126  39  87  72 105 107 127  49  75  15  14
 149 154  86 131  24 103 140 128  59 116  18  30  66 130  56  81  96 117
 135 134  68 147  88  62 139   0  93  50  54   2 104]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2626
INFO voc_eval.py: 171: [ 98 115  44  10  23  62  11  87  90  13  76  69 101  27  91  60 116 117
 102   3  47  73 105  89   1  39  50 109  49  72  79 106   0  29  42  71
 127 110  57  15 124  14  31 107  81  64  74  37   4  78  35  70  96  56
  63  99  30   9  12 108  24  92  43 123 100  21   2  94  65  67  32  38
 125 122  28  20  55 126  93  25 119 114  61 104  66  46 111  48  85  75
  95   7  45  52  83  22  97  59  40 112  51   5  34  26  17  88  86  36
  68 120  77  84 121   6  16  54 113  53  18  33  58  41   8  80 103  19
 118  82]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.5406
INFO voc_eval.py: 171: [38  5  3 20 21 27  2 13 39 36 22 11 24  6 29 26  0 33 23  7  8 35 32 15
 12 17 16 37 30 18 14  9 10 34 31  1 19 40 28 25  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8926
INFO voc_eval.py: 171: [ 842 1499  901 ... 1403 1446  851]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7259
INFO voc_eval.py: 171: [ 47  27 286 186  84  31  29 249 285  60  30  80  97   8 300 116 199  28
   3 205 301 317  96  64 181  56  45 187 113 236 162  69  38 253 145 167
  90 240 173  67 218 100  34  62 267 282 304  91  68  46 274  55 303 265
  92 288 143 194  20 276 272  95 110  13 206 241  53 239 127 131 215 133
 144 242 287 108  85 321 279 214 248 176 269 200 107 270 121 136 306 302
 202 228 310 217 174  86  23 250 177  18  79 122  51  41 101   7 134  66
 128  11 193 261  48 172 298 106 258 271 305 226  87  43 130 192 284 255
  26 268 223  63 155 297 105  50 171 289 196  14 238 132 227 203  73 256
 117 164 213 153 307 115 123 182 160 124  88 161  24  71 315  36 184 201
 146  61 188 222 316 263   4 104 243 147  33 190  52 118 309 126 231  21
 283 140 262 212  82 235 312 165 170 277 308 102 211  70 219 278 207  65
  94 216   0  83 322  39 157  74 149 183  32 139 210 163 135 103 234 257
 198  89  57  81 319  59  22   2 168 237 180   6 295 197  17 150 154 290
 266  44 112 294  40 111 175 318 120 191 220 246 151 313 178   5 292 129
 254 159  58  76 141 148 273  72 293  35  49 189   9  99 169 179 195  77
 247 280 281 119  98 224 264 259 208  37   1 142 114 138 185 137  42 296
  93  54 158  25  12 221 229 232 125 230 244  19 204 209 314 320  10  16
 251 233 323  75 260 299 156 166  78 109 311 252 152 245  15 275 291 225]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5872
INFO voc_eval.py: 171: [ 93   5  57  96  20   7  68  89 107  94  65  50  59  25 106  21  16  33
  76  15  79  72  28  47  44  71  26  64 104  62  73 103  54 102  49   9
  43   6   2  39 101  70  46  19  91  78   0  35  77  90  34  60  24  66
  56   4  36   3  75  55  17 109  53 108   8  85  92  88  67  29  40 105
  69  99  11  81  74  37  98  18  87  84 100  31  45 110  41  83  80  22
 111  51  13  63  42  14  12  10  86  97  95  32  48  27  30  23  52  58
   1  61  38  82]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4704
INFO voc_eval.py: 171: [28 16 29  0 46  1 62 41 72 32 40 74 24 19 26 37 47 53 44 34 33 56 36  9
 49 17 14 70 65 11 30 48 20 18 77 71 31 22 27  5 60 25 10 76 54 59 50 55
  7 42 63 35 39 75 38 12  2 68 57 67 13  6  4 58 52 61 21  8 51 66  3 73
 69 45 23 15 43 64]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3459
INFO voc_eval.py: 171: [65 43 12  8 80 39 22 36 91 82  7 46 59  6 50 16 21 56 23 75 79 92 10 11
 38 35 45 25 71 37 74 41 54 48  5 58  4 87 77 68 18 66 88 49 40 85 60 34
 32 30 44 62  3 52 55 26 64 57 72 95 27 90 13 19 94 93 83 73  9 33 51 69
 89 81 31 70 20 14 29 28 84 47 76 53 15  0 61 24 78 63 86 42 17  2 67  1]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5815
INFO voc_eval.py: 171: [38 88 21 61  1 41 68 60 66  8  5 47 20 28  6 50 51 71 35 29 77 83 86 59
 15 12 25 80 74 90 44  3 19 49 53 93 69 62 65 67 16 32 73 56 94 22 17 11
 43 31 78 72 45 33 70 14 46 63 37  2 24  9 76 87 34 42 39  7 40 55 18 91
 23  4 79 26 10 54 58 75 85 52 13 30 64 57 48 27 84 92 89 81 82 36  0]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5267
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5071
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.326
INFO cross_voc_dataset_evaluator.py: 134: 0.638
INFO cross_voc_dataset_evaluator.py: 134: 0.342
INFO cross_voc_dataset_evaluator.py: 134: 0.409
INFO cross_voc_dataset_evaluator.py: 134: 0.507
INFO cross_voc_dataset_evaluator.py: 134: 0.661
INFO cross_voc_dataset_evaluator.py: 134: 0.558
INFO cross_voc_dataset_evaluator.py: 134: 0.198
INFO cross_voc_dataset_evaluator.py: 134: 0.543
INFO cross_voc_dataset_evaluator.py: 134: 0.566
INFO cross_voc_dataset_evaluator.py: 134: 0.462
INFO cross_voc_dataset_evaluator.py: 134: 0.263
INFO cross_voc_dataset_evaluator.py: 134: 0.541
INFO cross_voc_dataset_evaluator.py: 134: 0.893
INFO cross_voc_dataset_evaluator.py: 134: 0.726
INFO cross_voc_dataset_evaluator.py: 134: 0.587
INFO cross_voc_dataset_evaluator.py: 134: 0.470
INFO cross_voc_dataset_evaluator.py: 134: 0.346
INFO cross_voc_dataset_evaluator.py: 134: 0.582
INFO cross_voc_dataset_evaluator.py: 134: 0.527
INFO cross_voc_dataset_evaluator.py: 135: 0.507
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.489s + 0.002s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.377s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.379s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.372s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.370s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.367s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.367s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.367s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.368s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.365s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.363s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.365s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.367s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.474s + 0.002s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.389s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.379s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.367s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.369s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.370s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.379s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.376s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.376s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.377s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.377s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.378s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.377s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.486s + 0.001s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.392s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.378s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.379s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.375s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.372s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.371s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.371s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.370s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.369s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.370s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.371s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.371s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.650s + 0.001s (eta: 0:01:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.400s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.380s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.375s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.378s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.373s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.370s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.369s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.368s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.367s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.365s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.365s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.364s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.654s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [150  33 119  25  63  28  52  26  16  78  14  65  38  73   4   2 114 167
  12  15 124  34  49  98  46 163  40  75  50  42 118 129  77  29  19  17
 135  60  59 108  64  69  93  10 102  71  97 128  21 133 151  80  11  18
   0  83  44  94 149 162  99  27  81 130 157  88  23   5  45 132 172  24
  37 173  68 122 138 152  82  41  51 148   9 121  96  57  67 147 166 146
 126  58 123  56  36 160 117 136  86  47 165 125 127 120  35 158  70 106
  84 140  87 110  43  92  13  62 107  22  79  85   3   6  91 142 154 156
 145  95  54 137 174 131 168   7  90 171  89  76 141  20   1 113  74  61
  39 100 155 143 101 144  53 109  55 153 104 169 115 111 103 105 161 170
 116 164  48  31 139 112  72  30 159  66   8 134  32]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3265
INFO voc_eval.py: 171: [38 10  6 19 21 40 42 29 45 48 23 43 50 47 46 18 30 36 24  0 41 44 15 33
 51  7 17  2  3 22 49  4  1 39 16  8 31  9 13 34 20 28 26 35 25 12 32 27
 14 11 37  5]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6254
INFO voc_eval.py: 171: [ 66 165 188 167   5 129 376 281 377 186 306  64 131 201  83 130  21 398
 110  73 199 441 442 329   6  96 356 287  35 180 139 399 234 230 154  31
  49 310 304 236 400 443 414  82 374  95 286 133 179  54  87 301 121  40
 136 256   0 171  97 423 425 120 337  69 384 104 444 357 332 137 318 422
  88 145 379 182 395 100 207 222 419 132  84 371  44  41 265 449 134  94
  53 232  32 375 200 181  26 299 314 187 264 336 454 322  62 277 361 424
 172  92  86 402  12 369 169 297 176 341 204 219 269 155 190 403 450  30
 447 434  46 183 363 214 238 105 254 206 229 452 413 119 235 157 116 312
 313 344 149 410 122 213  93 261 127 117 421 291 325   3  34 430 205 146
 160  80  13  57  52 224 237 390  11  72 411 429 412  60 283  81 212 203
  20 198 431 268 243 278 123  24  58  45 215 118 143 290 327 396 249 258
  22 440 331 378 141 293 446 220 392  47 245 352   2  77  68 196  71 439
  67 428 355 321 263  23 347 226 330 416 244 401 370 317 343 272  16 319
 151  59 144 426 124 239 405 303 311 276 267 135 112 140 138 307 389 185
 111 266 305 217 350 113 324 294 300 383 148  36  70 103 275  90 252  37
 353 288 453  42 184 168 161 295  27 435 391 153 152 257 209 407  18 437
 101 255 296  50 418 174 349 315 260 173 381 326 189 216 320 345 432 227
  25 114 270 380 373 228   9  19 354  48 427 394 197 246 323 298 455 448
 386 158 388 368 108  51 280 340 415 221 242  65 208   7 170 240  43 351
  91 409  39 225 210  33 175  85  74 284 253   8  76 420  89  61 408 259
 285 338 164 348  98 102 365 195 451  17  28 109  14 159   1 292 247 360
  63 308 342  55 126 372 387 406  99 289 241  78 309 233 202 211 302  79
 382 279 218  15 385 397 282 393 162  75 359 316 115 251 128 328 191 339
  38 192  29  10 142 358 433 194 367 271 438 147 366 178 163 273 150 334
 125 107 231 346 274   4 193 250 364 106 166 404 156 417 177 335 362 436
 223  56 333 445 262 248]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.3414
INFO voc_eval.py: 171: [ 55   1 219 157 188 189   3  28 155 158  25  49 261  66 207  77  67 216
 190 106 113 264  48 224 147 229 170  79 201 154 176  13 156  12 181 279
  61  68  10  56 191 235 125 103 228 127  51 246  94 251  58 296 109 243
 262 171 174 132  36 210 233  24 131 128  84 215 288 266 123  88  34 236
 259 186 146 209  27 129  80 240 204 222  11 138 273  78  54 220 179 258
 142 274 108  43  52 272 293 150 152 183 173  81 294 287 137 277  76 230
  83  73 247 256  70 148  15  32 254 177 278 110 295 285 141 144  38  99
  19 196  74 161 211 238  90 227 180 199  47 265  18  92 153  22 205  69
 119  53   9 107 269 162  45 248 239  40 290 284  86 237 130  33 263 121
 164 175 151 212 213 102 221 267 134 165  59  87 252 242 203 114 139 145
  44  75 289 206  20 200  93 101 178 167 169 223  41 105   2 275 297 149
 253  89 116  37 208  31 166 291   8 231 195 245  98 135  85 270   4 282
 232   5 286 250 111  23 292 182 234  14   6  95  16 172 124 217 100  62
 193  97 244  17 214  30  65  26 184 122  71 160 276 268 168  29   7 198
  42 104 192 225 187  96  50 112 241 185 271 136 257   0 194  60 120 126
 249 115 118 143 280  63  39  82  72 133  35 226 218 163  57 202 283 197
 140  46 260  64 255 159  21  91 281 117]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4098
INFO voc_eval.py: 171: [ 93  94 173 179 170   2 106 175 174 141 228  81 126  79  95   4  43 140
  82  70 134 117  71 207  80 133  78 229 189 181  57 214 154 155 177 108
 180  49 114 101  12 204  63  35  67  66 186  22 184 199  15 123 104 110
  62 145 191 116 142 178  47 146 185 120 150 135 149   5 144  19  21 197
 219  53 176  68 121  39 195  24 119 226  77 182  72 132  98 208 157   6
 190 192  37 222 209  52 112 148  26 202  61  88  20 210 220 164 187  23
 206  11  44 160 153 183 103 171 105  48 218 156  74  85 215 125 124 188
 162   1 115  14 100 213 111 224 221  16 172  56   9 193  58 225  51 151
   3  36  40 198  65  96  69 102 194 152 122 196  30 158  83  17 200 212
 109  29   7 203 205 216  18 211 231  31  33  64 147  92  84  45  25 130
 201  76 128  41 129 169  99 131  87  59  34 138  32 167  73  38  46  97
 139 165  50 227 166  90 107 161  89 137 143  86   0  75  54 168 113  91
 163   8 223  60 230  42 217  28 127  27  13 136 159  10 118  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4971
INFO voc_eval.py: 171: [64 24 13  8 23 54 56 39 33 10 35 51 66 34 57 42 58 41  2  1 45 27 60 61
 25 55 22 16 21 32 29 48 31 63 62  0 67 59  7 52  9 18 20 43 47 30 15 50
  3 40 26 19 11 65 12 28 38  6 14 53 46 49 37  4 17 44  5 36]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6606
INFO voc_eval.py: 171: [ 62 142 146 213  36 211  61 100  25 149  44  70 132 206 102 223 159 212
 201 157  98 186 209  97  11  82  81  13   0 193 217   7  63  40 162 115
 106 202 128 129 164  90 112 114 160 118  71  45 195  24  30 105 144  58
 183  80 200 192  91  94 219 119 170  55  21 158 229 121 187 171  18  83
  37 178  39 135  76  50 174 203  32 123  84 141  96  65  99 111  67 155
 226  22 189   5 127 216 109  92  57 143  86  33 225  88 150 185 107 134
  68 117 167  26  72  46 165 104 180  14 130 190  19  74  48   8  78  41
 210 194   9 140 184  20 145 215 131 120  28 222   2  49  75  69 101 196
  66 153 214 230 177  27  16 161 197  95 205 108 198 137  29 139 173  85
  34  17  60   4  56  53 156  47  52  73 199  87 125  79   1  15 124 148
 221 176 179 204 166 126 138 168 175 133   6  43 147 218 207 227  89 122
   3 103 116  31  23  54  35 181 191  77 228  42 136 172 152  64 220  51
 182 208 224 154 113 110  38  93  10 188  12 169 163 151  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.5572
INFO voc_eval.py: 171: [ 7 75 25 89  4 16 63 70 21 46 83 36 68 56 23 35 50 27  6 41 82 57 24 52
 77 87 72  3 38 51  1 71 11 45 28 79 88 64 34 15 69 62 65 44  8 29  0 54
 86 37 85 10 67 74 48 40 42 17 14 32 22  5 49 30 58 61 66 18 39 20 90 33
 31 78  9 84 13 19 81 43 59 55 73 80  2 53 60 26 76 47 12 91]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1976
INFO voc_eval.py: 171: [633 632  73 162 506 668 279 670 171  74  50 163  31 683 159 403 405 453
  49 355 606 679 264 551 206 431 283 684 454 678 645  28 111 707 234 558
 418 685 260 303 552 128 646 549 607 228 432 425 281 265 424 103 183 151
 452 356 508 475 241 129 531 529 161  88  86 190 682  41 409  91 456 276
 694 240 327 401 305 515   3 176 205 214 441 562 478 610 627  43  87 275
  37 300 624 349 719 665  15 207 667 630 175  69 208 134 483 593 647 543
 177 126 570  93 120 537 280 708 388 465 268 473  51 556 348 576 253 656
 634 398 329 396 245 608 319 372 472 198  99 119 612 717  63 669 350  89
 651 365 579 197 695 643 609 288 532  27 702 467 601  59 324  33 644 594
 118 635 544 330 160 672 291 131 676   4 173 295 573 112 498  32 530 370
 108 714 164 439 178 286 588 611 664 589 301 203  62 251 204 450  11  96
 360  70 623  44 638 109 420 389 671  55 155 125 261 232 572 523 631 193
 225 494 674   8 307 170 648 243 387 188   7 534 136 547 487 166 347 221
 186  75 524 706 399 520 222 484  47 407 604 690 142 381 137 436  26  90
 511 433 637 374 287 298 255 536 249 187 649 555 402 613 220 480 185 338
 578 446 507 358  83 246  94  21 158  97 500 463 256 697 618 497 238 340
 315 481 663 517 721  98 658 575 357 316 376  84 460 229 455 289 501 210
 153 332 429 266 661 410 351 180  19 192 127  36 227 404 114 277 366 526
 639 582 477 285 385 514  65 471 414 709 539 181 282 154 352 652 426 641
 569 124 333 211  18  35 505 444 427 371 722 382 121 521 270 584 696 299
 621 375 553 123 542  42 550 216 705 459 417  52 263 213 167 328 434 310
 331 386 294 200 252 540 325  67 390 201 322 191 182 215 290 400 503 666
 545 336 603  78 341 626 585 217 278  46 596 528  16  40 138   6 135 502
 122  77  72 284 600 339 132 468 592 149 312 435 636 391 567 130 699 367
 482 313 384 259 629 720 512 346 226  39 617 368 362 640 561 513 622 147
 620 586 189 677 174 486 538 116 257 242 140  95  80  10 474 179 423 448
 488 449 519 687  25  20  54 231  53 565 655 133 605 518 254 342 115 430
 590 110  85 587 602 297  61  68 616 703 445 509  66 165 321 688 323  24
 691 479 599 698 156  71 715 614 660 269 306 701 461 496 476 146 250 139
 650 364 492 522 574 659 334 571 152 653 104 716 100 525 345 247 383 343
 209 105 581 564 361 102  38 218 495 393 510 443 673 662 304 406 541 713
  92 422   1 274 318  57 311 212 235  45 157 412 598 344 440 239 583 499
 533 106 219  58 413 421 168 194 462 144 493 244  79 516 442 597 394 150
 354   9 309 566  82 464  13 184 557 485 117 369 504 273 172 169 293 491
 101 296 224 546 317 236 527  34 411 437  30 711 196 258  29 335  12 615
 577 625  22 654 447 642  76 202 416 233 712 223 379  56 419  81 230 314
 689 148 591 408 195 143 145   0 415   5 458 580 262  48 308 320 373 271
 392 302 710 272 469 466 680 113 568 457  60 378 700 199  23 718  64 292
 489 438 397 267 380 377 363 326 107   2 554 560 704 353 395 619 248 657
 548 141 490  17  14 451 470 359 675 559 563 237 428 595 692 681 628 337
 535 686 693]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5433
INFO voc_eval.py: 171: [ 47  27   7  28  35  29  26  58  37  30  31   6  95  72  70  66 106 133
  54 111 123 112 117  71   2  33 103 134  12 110  51 121  53  34  45  88
 129   4  77   1  61  39   3 114 108 125  84  68  83  87 124  22 122  46
  48  56  55  74  65  57  86  43  67  81 118  89  92  96  50  17  97 120
 115 136  10 109  82  23  73  79  76  20 128  80  32   9  90 130 102  18
  85 105  59  62  94  16  41  38  13  40  78  98  14  42  25  91  24  44
  93   8  19  69  49  21 131 119  63 126 113 101  15  36   5  11 104  52
  64   0 100 127  75  60 107  99 135 116 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.5636
INFO voc_eval.py: 171: [138  32 146 202 335  76 314 292 155 232 102  40 196  17 212 250 289 201
 216 118 225  88 101 179 258 136 112   9 293 278 303  60 287 227 158 104
 267 222  22 307 223  49 274 217 218 119 156 175  99 154 275   4 210   6
  96 343  14 116 260  81 238 147  30  92 261 209 288  45 341 354 295  95
 231 213 290 252 259 349 193 351  66  13 323  94  44 246 117 203 180 237
  15  39  51 248  69  75  53 302  72 320 169 162 110  29 145 133 205 211
 291 371 106 126  35 219 311 256 277 165 153 122  64  74 140 316 191   0
 352 298 294  34  23 251 242 330 337 318  37 214 132 312 185  19 370 319
 358 142  46 273 244  79 368 114  98  47 105 103 151 189 357  58  84 264
  61 309 111 325 348 149 328 305  56   7 161 152 182 339 280 344 317 331
 107  80 324   2 135 233 127  71  86 148  24 150 360  54  50 338 321 137
  16  42  65 367 166 123  83 221  55 108  11 268  70 334  28 194 204  82
 131 372 272  91  97 269  87 157 253 178 207 333 284 195 176  31 332 270
 356 310  78 160 113 139  20 308  57 315 159 187  36 228 199 220 336 255
  62 172 262 366 340 345 143 257 188 226  89  27 144 329 177 100 125 234
  10 363 285 235 346  77  12  48 359 300 184 170   5 254 282 283 364 355
 239  68 186  21 263 243 215 229 245 167 183 197 168 192 174 322  38  26
 347 120 173 301 164 224 281 271  93 342 350 362 198 297  33  41 124  63
 353 299 109 249   8 171 230  52  67 200   3 279 369 296  90 361 304  85
 115 313 241  25 163 286 276  43 129 365 206  59 240 208 327 326  18  73
 181 121 128   1 266 141 265 190 236 247 306 130 134]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4613
INFO voc_eval.py: 171: [ 62  81   6  92 156  72  78  84  68  22 144  32 143  20   9  19  65  93
 145 133   5 122 157  21 154  40  31 121   3  33 153  86  75  45  79  80
 126 112   1  66  47 137  57  26 149  36 147  61 152 119 138  71  48 102
  64  91 110   7  58  25 113  42  43  95 107 100 120  83 109 114  77  34
 151  37  11  35  51   4 125  70  99  52  13  27 101 139  28  53  44 142
  90 130 103  23  10  29  41 123  98 146 124 115  12 134 111  55  17  16
   8  74  85  96  46 116  38 127  39  88  73 106 108 128  76  15  49 155
  24 132  14 150  87 104 129 141 131  67  30  18  56  59 117  82  97  60
 118  69 135 148  89 136 140  63  94  54   0  50 105   2]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2647
INFO voc_eval.py: 171: [ 96 113  44  10  23  62  11  86  88  13  76  69  99  27  89  60 114 115
 100   3  47  73 103  87   1  39 107  50  49  72  79 104   0  42  29 125
  71 108  57  15 122  14 105  81  31  74  37  64   4  78  70  35  94  56
  63  97  30  12 106   9  90 121  43  98  21   2  65  92  32  38 123 120
  55  28  20 124  91  25 112 117  61 102  66  24 109  46  48  84  75  93
   7  45  52  22  82  67  95  59 110  40  51  34   5  26  85  17  36  68
 118  77  83 119  16   6  54 111  18  33  80  53   8 101  41  58 116  19]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.5408
INFO voc_eval.py: 171: [38  5  3 20 21 27  2 13 39 36 22 11 24  6 29 26  0 33 23  7  8 35 32 12
 17 15 16 37 30 18 14  9 10 31 34  1 19 40 28 25  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8926
INFO voc_eval.py: 171: [ 835 1489  893 ...   45 1436 1391]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7246
INFO voc_eval.py: 171: [ 46  26 283 185  83  30 247  28  59 282  29  79  96   8 297 115 198  27
   3 204 298 314  95  63 180  55  44 186 112 234 161  68  38 251 144  89
 166 238 172  66 217  99  33 265 279  61 301  90  67  45  54 300 272 263
  91 285 142 193  19 273  94 109 205  13 239  52 237 126 130 214 132 143
 240 284 107 276 318  84 213 246 271 175 199 267 106 120 268 135 303 299
 201 226 307 216 173  85  22 248 176  17  78 121  50  40  65 100   7 133
 127 192  11 259  47 171 295 269 256 105 302 224  86  42 129 253 281  25
 266 222 154  62 191 294 104  49 170 286 195  14 236 131 225 202 254  72
 116 212 304 163 152 122 123 159 181 114  87 160  23  35 312 183 200 145
 313 187  60 221 261   4 241 103 146 189  51  32 229 117 306  20  70 125
 280 260 139 233 211  81 309 275 164 169 305 101 274 210 218 206   0  64
 215  93  82 319  37  69  73 148 156 182  31 162 138 209 134 102 232 197
  80  88 255 316  56  58  21   2 167 235   6 179 292 196 264 149 153 174
  43 291  16  39 287 111 119 315 110 190 310 219 177 150 244 289 128 158
   5 252 140  75  71 147 270  57 290  34  98   9 188  76  48 168 178 194
 278 277 118 245  97 262 223  36 257 113 207   1 137  41 141 136 184 293
  92  53  24 157 220 227  12 228 230 242 124 203  18 317 311  10 231 208
  15 320  74  77 249 155 108 165 296 258 308 250 151 288 243]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5834
INFO voc_eval.py: 171: [ 92   5  57  95  20   7  68  88 106  65  93  50  59  25 105  21  16  33
  79  15  76  72  28  47  44  71  26  64 103  62  73  54 102 101  49   9
  43   6   2  70  39 100  19  46  90  78   0  35  77  89  34  60  24   8
  66  56   4  36  75   3  55  17 108  53 107  84  91  29  87  67  40  69
 104  98  81  11  74  37  97  18  86  83  99  45  31  22  80  41  82 109
  51 110  13  63  42  14  12  10  85  96  32  94  48  27  23  58  30  38
   1  52  61]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4782
INFO voc_eval.py: 171: [28 16 29  0 46 62  1 72 41 32 74 40 24 26 19 37 53 44 47 34 33 56 36  9
 49 14 17 70 65 11 30 48 20 18 77 71 31 58 27 22  5 60 25 10 76 50 59 54
 55  7 42 63 35 39 38 75 12 57 68  2  6 67 13  4 61 52  3  8 21 51 73 66
 69 45 23 15 64 43]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3438
INFO voc_eval.py: 171: [63 41 10  6 77 37 20 88 34 79  5 44 57  4 48 14 19 54 21 72 76 89  8  9
 36 33 43 23 68 35 52 71 39  3 46  2 56 84 74 65 16 64 85 47 38 58 32 82
 30 28 42 60  1 50 53 24 62 55 69 92 25 11 90 87 17 91 80 70  7 49 66 29
 31 86 78 18 67 27 12 26 81 45 73 51  0 75 13 59 61 22 83 15 40]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5815
INFO voc_eval.py: 171: [38 88 21 61  1 41 68 60 66  8  5 47 20 28  6 55 50 71 35 29 77 83 86 59
 15 12 25 80 74 90 44  3 19 49 52 93 69 62 65 67 16 32 73 56 22 94 17 11
 43 78 31 72 45 14 70 33 46 63 37  2  9 24 76 34 42  7 39 54 40 18 23 91
  4 79 26 10 53 58 87 75 85 13 64 51 30 92 27 48 81 84 57 36 89  0 82]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5271
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5060
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.327
INFO cross_voc_dataset_evaluator.py: 134: 0.625
INFO cross_voc_dataset_evaluator.py: 134: 0.341
INFO cross_voc_dataset_evaluator.py: 134: 0.410
INFO cross_voc_dataset_evaluator.py: 134: 0.497
INFO cross_voc_dataset_evaluator.py: 134: 0.661
INFO cross_voc_dataset_evaluator.py: 134: 0.557
INFO cross_voc_dataset_evaluator.py: 134: 0.198
INFO cross_voc_dataset_evaluator.py: 134: 0.543
INFO cross_voc_dataset_evaluator.py: 134: 0.564
INFO cross_voc_dataset_evaluator.py: 134: 0.461
INFO cross_voc_dataset_evaluator.py: 134: 0.265
INFO cross_voc_dataset_evaluator.py: 134: 0.541
INFO cross_voc_dataset_evaluator.py: 134: 0.893
INFO cross_voc_dataset_evaluator.py: 134: 0.725
INFO cross_voc_dataset_evaluator.py: 134: 0.583
INFO cross_voc_dataset_evaluator.py: 134: 0.478
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.582
INFO cross_voc_dataset_evaluator.py: 134: 0.527
INFO cross_voc_dataset_evaluator.py: 135: 0.506
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.632s + 0.001s (eta: 0:01:18)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.376s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.359s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.361s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.365s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.364s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.362s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.363s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.361s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.361s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.361s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.364s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.366s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.504s + 0.001s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.377s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.371s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.364s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.364s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.371s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.379s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.377s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.376s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.377s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.376s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.377s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.375s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.448s + 0.001s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.396s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.390s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.381s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.377s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.372s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.370s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.368s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.369s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.369s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.369s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.369s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.369s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.516s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.385s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.383s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.367s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.362s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.358s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.359s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.359s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.360s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.361s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.360s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.363s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.361s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.179s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [147  31 117  24  61  27  50  25  15  76  13  63  36  71   4   2 112  11
 163  14 122  47  32  96  44 159  38  73  48  40 127 116  75  28  18  16
 132  58 106  57  62  67  91   9 100  95  69 126  20 148 131  78  10   0
  17  81  43  92 146 158  97  26  79 128 153  22  86   5  42 130 168  23
  66  35 169 120 135  80  39 145  49   8 119  94  55  65 144 162  54 143
  56 124  34 121 156 115  84 133  45 161 123 125  33 118 154  68 104  82
  85 137 108  41 105  12  90  60  21  77  83 139 150   6 152  89   3 142
  52  93 134 170 129   7 164  88 167 138  74  87   1  19  72  37 111  59
  98 140 141 151  99  51 149  53 107 102 165 157 103 101 113 109 114 166
  30 160  46 136 110  70 155  29  64]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3254
INFO voc_eval.py: 171: [11 37  7 19 21 39 41 29 44 47 23 42 49 46 45 18 30 36 24  1 40 43 15 33
 50  8 17  3  4 48 22  5  2 38 16  9 31 10 13 34 20 28 26 35 25 12 27 32
 14  0  6]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6254
INFO voc_eval.py: 171: [ 65 166 189 168   5 129 282 377 378 187 307  63 131 202  82 130  21 399
 110  72 443 200 444 330   6  96 357 288 139  35 181 400 235 231 154  31
  49 305 311 237 401 445 416  81 375  95 133 287 180  54 121  87 302  40
 136 257   0 172  97 425 120  68 338 427 385 104 446 358 333 137 424 319
  88 145 380 183 396 100 208 421 223 132  83  44 372  41 265 450  94 134
  53 233 201  32 376  26 404 182 300 188 315 264 323 337  61 455 278 362
 426 173  92  86 403 370  12 170 342 177 220 205 269 155 191 448 451 436
  30 184 364 215 105 239 255 230 207 414 453 119 298 236 345 116 157 314
 313 122 148 412  93 214 262 423 127 117 292 432   3  34 326 206 161 146
  79  56 238 225  13  52  46  11  71 391 413 431  59 284  80 213 204  20
 199  57 244 433 123 268  45  24 415 250 279 216 291 118 328 143 259 397
  22 332 379 442 353 447 141 293 393 221 246  70   2  47  67 197 322  76
 441 430 348 263  23  66 356 331 418 227 245 402 371 273 318 344 320  16
  58 151 144 124 240 428 304 406 312 140 277 135 267 138 308 112 111 186
 306 390 351 266 295 218 113 325 384 301 149  90  36 276  69 103 253 162
 354  37 185 294 289 454  42 153  27 258 296 169 437 152 392 408 210 101
  18 439 297 256 420 382  50 175 434 174 261 321 346 327 316 190 217 374
 228 270  25 381 229 355 114 324   9 247  19 395  48 429 158 198  39 387
 341 299 456 281 417 369  64 243 108 449 389  51 209   7 241 222 411 171
 352  43  91  33 226  84 211 176  73 254 422  89   8 285  75  60 260 339
 410 366 286 165  98 102 349 109  62 248 309  17 361   1 196  28  14 452
 343 159 126 383 350 373  55 290 407 242 388 234 310 219  99 212 360 394
  78  77 303 283 203 163  15 398 386 280  74 317 252 192 128 142 193 329
 340  29 115  10  38 435 368 359 272 440 150 335 107 164 195 179 367 232
 125 147 274 194 419   4 224 347 251 178 106 365 167 275 156 271 405 336
 438 363 249  85 160 334 409]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.3420
INFO voc_eval.py: 171: [ 55   1 221 159 190   3 191  28 157 160  49  25 263  66 209  67  77 218
 192 108 115 266  48 226 149 231 172  79 203 156 178  13 158 281  12 183
  61  68  10  56 193 237 105 126 230 128  51 248  94  58 253 298 111 245
 173 264 176 133  36 212 235  24 132 129  84 217 290 267 124  88  34 238
 188 261 148 211  27  82 130 224 206 242  11 275 140  78 222  54 144 260
 181 110 276  52  43 274 185 154 295 152  80 296 175 289  76 139 279 232
  83  73 249  70  15 150 258  32 112 256 146 280 179 297 143 287  38  20
 100  74 198 213 163 240  90 182 229 201  47  92 155 268  18  22   9 109
 207 164  69 271  53 250  45  41 241 292  86 286 131 239 153 151 215  33
 166 103 223 265 122 254 214  59 177 269  87 136 244 205 167 141 116 147
 291  44  19 180  75 202 169 107  93 171 208   2 277 299  97 225  89  31
 210 118  37 168   8 293  40 233  99 137  85 247 197 234 272   4 284 113
   5 288  23 294 252 184  14 236  95   6 101 174 219 102 104 125  16 216
  62  65 195  30 246  71  26 123  17 186 162  29  42 200 278 270 189   7
  50 170 120 194 114  96 106 273 243 138 227 117   0 127 259 187 196  60
 121  35 251 145  72  81  63 134  39 285 282 228  57 220 204 165 161 257
  46 199  64 142  21  98 135  91 119 262 283 255]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4081
INFO voc_eval.py: 171: [ 90  91 176 170 167 103   2 172 171 138 225  79 123  77  92  43   4  80
 137  68 131 114  69 204  78 130  76 226 187 178  56 211 151 152 174 105
 177  49 111  98  12 201  61  35  65 183  64  22 196 181  15 101 120 107
  60 142 139 189 112 175  47 182 143 147 117 132 146   5 141  19  21  53
 215 173 190  66 118  39  24 116 223 179  70  75 129  95 184 205 154   6
 191 188  37 219 206  52 145 109  26  59 199  86 207  20 217 161 185  23
 203 180  44 150 157  11 168 100 102  48 216 153  72  83 212 122 121 186
  55 159   1  14  97 210 108 221 169 218  16 192   9  57 222  51 148 113
   3  36 195  40  93 193  63  99 149 194 119  67  81 197  30 155 209  17
 106  29   8 202 213 200  18 208 228  62 144  31  33  82  89  45  74  25
 198 125 127  41 126 166  34  96 128  32 135  71  85 164  38  46  94 136
 162  88 224 163  50  87  84 104 134 158 140  73 165   0 110  54   7 160
  58 227 220  42 214  28 124  13  27 156 133 115  10]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4971
INFO voc_eval.py: 171: [64 24 13  8 23 54 56 39 33 10 35 51 66 34 57 42 58 41  1  2 45 27 60 61
  7 25 55 16 21 22 32 48 31 63 62  0 67 59 52 43  9 47 18 20 30 15 50 29
  3 40 26 19 11 65 12 28 38  6 14 53 46 37  4 49 17  5 44 36]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6606
INFO voc_eval.py: 171: [146 142  61 213 211  36  60  99  25 149  44  69 132 206 101 159 223 212
 201 157  98 186 209  96  11  80  81  13   0 193 217   7  62  40 162 114
 202 105 128 129 164  89 111 113 160 117  70  45 195  24  30 104 144  58
 183  79 200 192  90  93 219 118 170  55 158  21 229 120 187 171  18  82
  37 178  39  75 135  50 174 203  32 122  95  83  64 141  97 110  66 155
 226 189  22   5 127 216 108  57  91 143  85  33  87 150 225 185 106 134
  67 116 167  26  46  71 165 103  14 180 130 190  19  73  48  77   8   9
 210  41 140 194 184  20 145 215 119 131  28 125 222   2  74  49  68 100
 196  65 153 214 161 177 230  16  27 197  94 107 205 137  29 198 139 173
  84  34  59  56  17   4  53  72  47  52 156 199  86 124  78   1  15 123
 148 204 221 176 166 126 179 168 175 138 133   6  43 147 218 227   3 121
 102  88 207 115  23  31  54  35 181 191  76 228  42 152 172  63 224 136
  51 220 112 208 182 154 188  92 109  10  38 169 163  12 151]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.5574
INFO voc_eval.py: 171: [ 7 75 25 89  4 16 63 70 21 46 83 36 68 56 23 35 50 27  6 41 57 82 24 52
 77 72 87  3 38 51  1 71 11 45 28 79 88 34 64 15 69 62 65 44  8 29  0 54
 86 37 85 10 67 74 40 49 42 17 32 14 22  5 48 58 30 39 61 18 66 20 90 33
 31 78 84  9 19 43 13 81 59 55 73 80 26 76 53  2 60 12 47 91]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1980
INFO voc_eval.py: 171: [632 631  73 162 667 279 504 669 171  74  50 163  31 682 159 403 405 452
  49 355 603 678 265 206 549 430 283 683 453 677 644  28 111 704 235 556
 418 684 303 261 550 128 645 547 604 266 229 431 425 281 424 103 183 151
 451 506 474 356 242 129 529 527 161  86  88 681  41 409 190  91 455 276
 691 241 327 305 513 401   3 176 273 205 216 560 477 440 607 625 126  43
 275  87  37 622 300 349 716 664  15 666 207 175 627  69 134 209 482 590
 646 541 177 567  94 120 535 280 705 360 388 464 472 554 348 573 254 655
 398 633 329 396 246  51 605 319 372 471 198  99 119 609 714  63 668 350
  89 650 365 576 692 197 642 606 288 530  27 699 466 598 324  59  33 643
 591 118 330 634 160 542 291 671 131   4 173 675 295 570 112 497  32 528
 370 108 164 711 286 438 178 585 608 301 586  62 203 204 252 663 449  11
  70  95  44 637 621 420 109 670 389 125 155  55 262 233 569 521 193 629
 226 170 673 493   8 244 647 188 387 532   7 545 166 136 486 186 347 222
 522  75 703 399 518 407  47 483 223 601 142 688 381 137 435  26  90 432
 509 636 374 287 298 250 534 256 648 187 307 610 402 221 479 553 338 185
  97 445 358  83 575 505 247  21  96 158 208 499 462 615 694  98 315 239
 257 340 496 662 515 480  93 718 316  84 572 357 459 376 454 230 657 289
 211 498 153 332 410 428 267 127 660 180  36  19 351 228 114 404 638 366
 277 524 579 476 285  65 706 512 470 385 414 537 282 352 426 154 566 181
 640 651 124 333  35  18 443 503 371 427 618 382 719 270 121 299 693 375
 581 619 123 551  42 702 218 540 458 548 328  52 417 167 264 294 200 331
 215 433 325 310 538 390 386 322 253 182 201 212  67 192 290 501 217 543
 400 336 665 624 341 600 526 278  46 593  40 138 582  16   6 135 339 122
 500  77 597 284  72 467 130 132 589 434 635 191 260 696 312 149 367 564
 481 313 391 510 717 227 384 628  39 614 639 346 368 362 511 559 147 676
 536 189 485 583 174 620 243 116 140 487 179 258  80 685 473 517  10 423
 447 448  25  54  53  20 232 133 110 516 654 562 342 297  85 115  78 584
 587 617 255 599 429 602  68 700 630 613  66 323 165  61 507 686 321  24
 444 478 689 611 156  71 269 596 712 460 659 695 495 491 251 364 649 475
 306 698 139 152 334 658 520 146 713 568 571 343 345 100 210 523 652 248
 104 383 105 508 578 219 361 102 494 561  38 318 393 661 710 442 539 672
  45  57 213 406 439 236   1  92 422 595 157 412 240 304 311 531 344  58
 580 168 413 106 461 421 492 220 245 441  79 194   9 514 309 594 354  82
 563 144 150 484 369 184  13 117 274 463 172 555 502 169 394 490 296 293
  34 519 101 317 411 525 225 544  30 708 436 237 335  29 259 623 612  22
  81  12 419 416 653 574 234 379 709  76 588 196  56 224 446 314 641 687
 408 202 148 271 143 231 214 415 707   0 272 320 195 577 373 457   5  48
 468 263 308 679 113 392 378 377 302 437  60 465 697  64 199 565 145 456
 397 268 292 353 249 701 326 380 363   2 558 552 656 488 715 450  23 395
 489  17 546 616 141 107  14 626 690 337 469 533 557 592 238 680 674 359]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5413
INFO voc_eval.py: 171: [ 47  27   7  28  29  35  58  26  37  30  31   6  94  71  69  66 105 132
  54 110 122 111 116  70   2  33 102 133  12 109  51 120  53  45  34  87
 128   4   1  61  76  39   3 113 107  83 124  68  82  86  22 123 121  46
  48  56  55  73  65  85  57  43  67  80  88  91 117  17  50  95  96 108
  10 119 135 114  72  23  78  81 127  20  75  79  32 129   9  89  84  18
 101 104  41  59  62  93  16  38  13  77  40  97  14  42  25  90  44  24
   8  49  92  19  21 130 118 125  63  15 100 112  36   5  11 103   0  64
  52  99  60 126  74 106 131  98 134 115]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.5631
INFO voc_eval.py: 171: [135  31 143 199 333 312  74 291 152 229 100 193  39  17 209 248 288 198
 213 116 222  86  99 176 256 133   9 110 292 275 302  58 286 224 155 102
 265 219  21 305 220  47 272 214 215 117 153  97 172 151 273   4 207   6
  94  14 341 114 258  79 235 144  90  29 259 206 287  43 339  93 352 294
 210 289 228 250 257 347 190 349  64  13  92  42 244 115 177 200 234  15
  38  49 246  51  73  67  70 166 319 301 159  28 108 142 317 130 202 208
 290 369 104 124 216  34 254 309 162 150  62  72 137 120 188 314   0 350
 297 293  33 328 335 249 240 316  36  23 211 129 310 318 181 356 368  19
  44 139 271  77 242 366 101  96 112 103  45 186  56  82 355 148  59 262
 307 323 109 346 146 277 326 304  54 179 158 342   7 149 337 329   2 278
 105 230 125 132 315  78 322 170  69  84 361 145 147  22  52 358  48 336
  41 320 163 134 121  16 365  63 218  81 266  53  11 106 332  27  68 201
 128 191 370  80  89 267 270  85 251  95 154 204 175 331 173 283 268  30
 192  76  20 308 136 111 157 354 330 313 156 306  55  35 184 225 196 217
 253 334  60 260 364 338 255 223 140  87 185 174 343 327 141  26 123 284
  10 279  98  12 231 357 232 344  75  46 299 167 180 281 362   5 236 282
 252  66 183 212 164 226 261 241 243 165 353 194 169  25 189 280 171 300
 221 118 340 321 269 161  91 345  32  37 348 195 296 360 122 238 107  40
 182  61 351 247 298  88   8 359 367   3 227  65 303 276  50 295 168 311
 113  83 239  24 197 160 274 203  57 285 127 363 205 325 237 119 138  71
 324   1 264 178  18 126 263 187 131 245 233]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4610
INFO voc_eval.py: 171: [ 60  79   5 153  90  70  76  82  66  21 141  31 140  19   8  18  63  91
 142 130   4 119 154  20 151  39  30 118   2  32 150  44  84  73  78  77
 123 109   1  64  46 134  25  55 146 144  59 149 116  35 135  69  47 100
  62  89 107   6  56  24  41 110  42  93 104  98 117  81  75 106 111 148
  10  33  36  34  49 122  68   3  97  99  12  50  26  43  27 136  51  88
 127 139  22 101  28   9  40 120  96  11 143 131 112  53 121 108  15  16
   7  83  72  45  94 113  38  85 124  86  37 103  71 105  74 129 125  14
  23  48 147  13 152 126 102 128  65 138  54  57  29 114  17  95  80 115
  67 132 145  58  87 133  61  52 137  92   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2675
INFO voc_eval.py: 171: [ 92 110  42  10  21  59  11  82  84  13  72  66  95  25  85  57 111 112
  97   3  45  70 100  83   1  37 104  48  47  69  75   0 101  27  40 121
  68 105  55  15 118  14 102  77  29  71  35  61  74   4  73  67  33  90
  54  60  93  28 103  12   9  86  94 117  41  19   2  62  88  30  36 119
  53 120 116  18  26  87  23 113 109  58  22  99 106  63  44  46  80  89
  43   7  50  78  20  64  91  56 107  38  32  49   5  24  17  81  34  65
 114  96  79 115  16   6  31 108  52  51  76   8  98  39]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.5398
INFO voc_eval.py: 171: [38  5  3 19 21 27  2 12 39 36 22 10 24  6 29 26  0 23  7  8 35 33 32 11
 16 15 37 14 30 17 13 20  9 31 34  1 18 40 28 25  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8926
INFO voc_eval.py: 171: [ 833 1481 1385 ...  384  686  986]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7271
INFO voc_eval.py: 171: [ 46  26 285 186  83  31 249  28  59 284  29  79  97   8 298 116 199  27
   3 205 299 315  96  63 181  55 187 113 236  30 162  68  38 253 145  89
 167 240 173  66 219 100  34 267 281  61 302  90  67  45  54 301 274 265
 143  91 287 194  19 275  95 110  13 241  52 239 127 216 131 134 144 242
 207 286 108 278 215 319  84 248 273 176 200 269 107 121 270 136 304 202
 300 228 308 218 174  22  85 250 177  17  78 122  50  65  41 101   7 133
 128 193  11 261  47 172 296 271 303 258 106 226  86 192  43 255 130 283
  25 268 155 224  62  14 295 105  49 171 288 196 238 227 132 203 256  72
 117 214 164 305 153 124 123 160 182 115 311  87 161  36  23 184 313 201
 146 314 188 263  60 223   4 243 104 147 190  51  33 231 118 126  70  20
 282 262 235 140  81 213 277  94 310 165 170 306   0 102 220 212 276 208
  93 217  64 206  37  82 320  69  73 149 157  32 183 163  40 139 211 234
 135 103 317  80  88 257 198   2  21  58  56 168 237 180 175   6 266 293
 197 154 150 292 289  44 112  16  39 316 307 111 221 191 120 178 151 129
 246 159   5 141 290 254  75 148 272  71 291  35   9  76  57 189  99 179
  48 195 169 247 119 279 280 264  98 225 114 259   1 138 185  42 142 209
 137 294  92 222  24 158  53 229 232 244  12 125 230  77 318 204 312  10
 210  18 233  15 251  74 309 156 109 321 166 252 260 297 152 245]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5813
INFO voc_eval.py: 171: [ 93   5  57  96  21   7  70  89 107  94  65  51  59  26 106  22  17  34
  80  77  73  16  29  48  45  72  27  64 104  62  54 103  74  50 102  44
   2   6  71  40 101  20  47  91  79  10   0  36  78  90  35  60  25   8
  66  56   4  37  76   3  18  55  53 109 108  85  68  92  30  88  67  41
 105  12  82  99  75  38  19  98  84  87  46 100  23  32  81  83  52 111
  42 110  14  63  43  11  15  13   9  86  33  49  69  97  95  28  39  24
  31   1  58  61]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4796
INFO voc_eval.py: 171: [28 16 29  0 47 63  1 73 42 33 75 41 24 26 19 38 54 45 48 35 34 57 37  9
 50 14 17 71 66 11 31 18 20 49 78 72 32 27 59 22 61  5 25 51 10 77 60 55
 56 43  7 40 36 64 76 39 12 58 69  2 21 13  6 68  4 62 53  3 52  8 74 67
 46 15 23 70 44 30 65]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3432
INFO voc_eval.py: 171: [63 41 10  6 77 37 20 89 34 80  5 57 44  4 48 14 19 54 21 72 90 76  8  9
 36 33 43 23 68 35 52 71  3 39  2 56 46 74 85 65 16 86 64 47 38 83 32 58
 28 30 42 60  1 50 53 24 62 93 55 69 91 25 17 11 88 81 92 29 70  7 31 49
 66 87 78 18 67 12 27 26 82 45 73 51 75  0 13 61 59 22 15 84 79 40]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5815
INFO voc_eval.py: 171: [38 88 21  1 61 41 68 60 66  8  5 47 20 28  6 55 50 71 35 29 77 83 86 15
 59 12 25 80 74 90 45  3 19 49 52 93 69 62 65 67 16 32 73 72 56 22 94 17
 11 44 78 31 14 33 70 46 63 37 43  9 24  2 76 34 42  7 39 58 54 23 18 40
  4 79 91 26 10 53 87 75 85 64 13 51 30 92 84 81 48 27 89 57  0 36 82]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5267
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5059
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.325
INFO cross_voc_dataset_evaluator.py: 134: 0.625
INFO cross_voc_dataset_evaluator.py: 134: 0.342
INFO cross_voc_dataset_evaluator.py: 134: 0.408
INFO cross_voc_dataset_evaluator.py: 134: 0.497
INFO cross_voc_dataset_evaluator.py: 134: 0.661
INFO cross_voc_dataset_evaluator.py: 134: 0.557
INFO cross_voc_dataset_evaluator.py: 134: 0.198
INFO cross_voc_dataset_evaluator.py: 134: 0.541
INFO cross_voc_dataset_evaluator.py: 134: 0.563
INFO cross_voc_dataset_evaluator.py: 134: 0.461
INFO cross_voc_dataset_evaluator.py: 134: 0.267
INFO cross_voc_dataset_evaluator.py: 134: 0.540
INFO cross_voc_dataset_evaluator.py: 134: 0.893
INFO cross_voc_dataset_evaluator.py: 134: 0.727
INFO cross_voc_dataset_evaluator.py: 134: 0.581
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 134: 0.343
INFO cross_voc_dataset_evaluator.py: 134: 0.582
INFO cross_voc_dataset_evaluator.py: 134: 0.527
INFO cross_voc_dataset_evaluator.py: 135: 0.506
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.601s + 0.001s (eta: 0:01:14)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.399s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.399s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.385s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.384s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.383s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.380s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.382s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.381s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.380s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.379s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.382s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.380s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.381s + 0.001s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.370s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.381s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.373s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.367s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.375s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.381s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.382s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.379s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.379s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.382s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.381s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.380s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.479s + 0.002s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.368s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.354s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.358s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.357s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.357s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.362s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.362s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.361s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.361s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.361s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.362s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.363s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.454s + 0.002s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.369s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.373s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.369s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.368s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.364s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.364s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.363s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.369s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.369s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.368s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.369s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.369s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.456s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [147  31 117  24  61  27  50  25  15  76  13  63  36  71   4   2 112  11
 163  14 122  47  32  96  44 159  38  73  48  40 127 116  75  28  18  16
 132  58 106  57  62  67  91   9 100  95  69 126  20 148 131  78  10   0
  17  81  43  92 146 158  97  26  79 128 153  22  86   5  42 130 168  23
  66  35 169 120 135  80  39 145  49   8 119  94  55  65 144 162  54 143
  56 124  34 121 156 115  84 133 161  45 123 125  33 118 154  68 104  82
  85 137 108  41 105  12  90  60  21  77  83 139 150   6 142  89 152   3
  52  93 134   7 170 129 164  88 167 138  74  87   1  72  37  19  59 111
  98 140 141 151  99  51 102  53 149 107 165 113 157 109 103 101 114 166
 160  30  46 110 136  70 155  29  64]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3255
INFO voc_eval.py: 171: [11 37  7 19 21 39 41 29 44 47 23 42 49 46 45 18 30 36 24  1 40 43 15 33
 50  8 17  3  4 48 22  5  2 38 16  9 31 10 13 34 20 28 26 35 25 12 27 32
 14  0  6]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6254
INFO voc_eval.py: 171: [ 65 166 189 168   5 129 282 377 378 187 307  63 131 202  82 130  21 399
 110  72 200 443 444 330   6  96 357 288 139  35 181 400 235 231 154  31
  49 305 311 237 401 445 416  81 375  95 133 287 180  54 121  87 302  40
 136   0 257 172  97 425 120  68 338 427 385 104 446 358 333 137 424 319
  88 145 380 183 396 100 208 421 223 132  83  44 372  41 265 450  94 134
  53 233  32 201 376  26 182 404 300 188 315 264 323 337  61 455 362 278
 426 173  92  86 403 370  12 170 342 177 205 220 269 155 191 451 448 436
  30 184 364 215 105 239 255 207 230 414 453 119 298 236 345 116 157 314
 313 122 412 148  93 214 262 423 127 117 292  34   3 432 326 206 161 146
  79  56 238 225  13  52  46  11  71 391 413 431  59 284  80 213 204  20
 199  57 244 268 123 433  45  24 279 415 250 291 216 118 143 328 397 259
  22 379 332 442 447 353 141 393 221 293 246  70   2  47  67 197 322 441
  76 430 348 263  23  66 356 331 418 227 245 402 371 273 318 344 320  16
  58 151 144 124 240 428 304 406 312 277 140 135 267 111 112 138 308 186
 390 306 351 266 295 218 113 325 384 301 149  90  36 276  69 103 253 354
 162  37 185 294 153 289 454  27  42 169 296 258 437 152 392 408 210  18
 439 101 297 420 256 382  50 175 434 174 327 261 321 346 190 217 316 374
 228 270  25 381 229 355 114 324   9 247  19 395  48 158 429 198  39 387
 341 299 456 281 417 369  64 243 108 449 389  51 209   7 241 222 411 171
 352  43  91  33 226  84 211 176  73 254 422  89   8 285  75  60 260 339
 410 366 286  98 165 102 349 109  62 248 309  17 361   1 196  28  14 452
 343 159 126 383 350 373  55 290 407 242 388 234 310 219  99 212 360 394
  78  77 303 283 203 163  15 398 386  74 317 192 252 280 128 142 193 329
 340  29 115  10  38 435 368 359 272 440 150 335 107 164 195 179 367 125
 147 274 194 419   4 232 224 347 251 178 106 365 167 275 156 271 405 336
 438 363 249  85 160 334 409]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.3421
INFO voc_eval.py: 171: [ 55   1 221 159 190   3 191  28 157 160  49  25 263  66 209  67  77 218
 192 108 115 266  48 226 149 231 172  79 203 156 178  13 158 281  12 183
  61  68  10  56 193 237 105 126 230 128  51 248  94  58 253 298 111 245
 173 264 176 133  36 212 235  24 132 129  84 217 290 267 124  88  34 238
 188 261 148 211  27  82 130 224 206 242  11 275 140  78 222  54 144 260
 181 110 276  52  43 274 185 154 295 152  80 296 175 289  76 139 279 232
  83  73 249  70  15 150 258  32 112 256 146 280 179 297 143 287  38  20
 100  74 198 213 163 240  90 182 229 201  47  92 155 268  22  18   9 109
 164 207  69 271  53 250  45  41 241 292  86 286 131 239 153 151 215  33
 166 103 223 265 122 254 214  59 177 269  87 136 244 205 167 141 116 147
 291  44  19 180  75 202 169 107 171 208  93   2 277 299  97 225  89  31
 210 118  37 168   8 293  40 233  99 137  85 247 197 234 272   4 284 113
   5 288  23 294 252 184  14 236   6  95 101 174 219 102 104 125  16 216
  62  65 195  30 246  71  26 123  17 186 162  29  42 200 278 270 189   7
  50 170 120 194 114  96 106 273 243 138 227 117   0 127 187 196  60 121
 259  35 251 145  72  81  63 134  39 285 282 228  57 220 204 165 161 257
  46 199  64 142  21  98 135  91 119 262 283 255]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4081
INFO voc_eval.py: 171: [ 90  91 176 170 167 103   2 172 171 138 225  79 123  77  92  43   4  80
 137  68 131 114  69 204  78 130  76 226 187 178  56 211 151 152 174 105
 177  49 111  98  12 201  61  35  65 183  64  22 196 181  15 101 120 107
  60 142 139 189 112 175  47 182 143 147 117 132 146   5 141  19  21  53
 215 173 190  66 118  39  24 116 223 179  70  75 129  95 184 205 154   6
 191 188  37 219 206  52 145 109  26  59 199  86 207  20 217 161 185  23
 203 180  44 150 157  11 168 100 102  48 216 153  72  83 212 122 121 186
  55 159   1  14  97 210 108 221 169 218  16 192   9  57 222  51 148 113
   3  36 195  40  93 193  63  99 149 194 119  67  81 197  30 155 209  17
 106  29   8 202 213 200  18 208 228  62  31  33 144  82  89  45  74  25
 198 125 127  41 126 166  34  96 128  32 135  71  85 164  38  46  94 136
 162  88 224 163  50  87  84 104 134 158 140  73 165   0 110  54   7 160
  58 227 220  42 214  28 124  13  27 156 133 115  10]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4971
INFO voc_eval.py: 171: [64 24 13  8 23 54 56 39 33 10 35 51 66 34 57 58 42 41  1  2 45 27 60 61
  7 25 55 16 21 22 32 48 31 63 62  0 67 59 52 43  9 47 18 20 30 15 50 29
  3 40 26 19 11 65 12 28 38  6 14 53 46 37  4 49 17  5 44 36]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6586
INFO voc_eval.py: 171: [146 142  61 213 211  36  60 149  99  25  44  69 132 206 101 159 223 212
 201 157  98 186 209  96  11  80  81  13   0 193 217   7  62  40 162 114
 202 105 128 129 164  89 111 113 160 117  70  45 195  24  30 104 144  58
 183  79 200 192  90  93 219 118 170  55 158  21 229 120 187 171  18  82
  37 178  39  75 135  50 174 203  32 122  95  83  64 141  97 110  66 155
 226 189  22   5 127 216 108  57  91 143  85  33  87 150 225 185 106 134
  67 116 167  26  46  71 165 103  14 180 130 190  19  73  48  77   8   9
 210  41 140 194 184  20 145 215 119 131  28 125 222   2  74  49  68 100
 196  65 153 214 161 177 230  16  27 197  94 107 137 205  29  84 139 198
 173  34  59  56  17   4  72  47  53  52 156 199  86 124   1  78 123  15
 148 204 176 221 166 126 179 168 175 133 138   6  43 147 218 227 102  88
 121   3 207  31  23 115  54 181  35 191  76 228  42 172 152  63 224 136
 220 112  51 182 154 208 109  10 188  92  38 169  12 163 151]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.5576
INFO voc_eval.py: 171: [ 7 75 25 89  4 16 63 70 21 46 83 36 68 56 23 35 50 27  6 41 57 82 24 52
 77 72 87  3 38 51  1 71 11 45 28 88 79 34 64 15 69 62 65 44  8 29  0 54
 86 37 85 10 67 74 40 49 42 17 32 14 22  5 48 58 30 39 61 18 66 20 90 33
 31 78 84  9 43 19 13 81 59 55 73 80 26 76 53 60  2 12 47 91]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1980
INFO voc_eval.py: 171: [632 631  73 162 504 667 279 669 171  74 163  50  31 682 403 159 405 452
  49 355 603 678 265 206 549 430 283 683 453 677 644  28 111 704 556 235
 418 684 261 303 550 128 645 547 604 266 229 431 425 281 424 103 183 151
 451 506 474 356 242 129 529 527 161  86  88 681 409  41 190  91 455 276
 691 241 327 305 513 401   3 176 273 205 216 560 477 440 607 625 126  43
 275  87  37 622 300 349 716 664  15 666 207 175 627  69 134 209 482 590
 646 541 177 567  94 120 535 280 705 360 388 464 554 472 348 573 254 655
 633 398 329 396 246  51 605 319 372 471 198  99 119 609 714  63 668 350
  89 650 365 576 692 197 642 606 288 530  27 699 466 598  59 324  33 643
 591 118 330 634 160 542 291   4 671 131 173 675 295 570 112 497  32 528
 370 108 164 711 286 438 178 585 608 301 586  62 203 252 663 204 449  11
  70  44  95 637 621 420 109 670 389 125 155  55 262 233 569 521 193 629
 226 170 673 493   8 244 647 188 387 532   7 545 166 136 486 186 347 222
 522  75 703 399 518 407  47 483 223 601 142 688 381 137 435  26  90 432
 509 636 374 287 298 250 534 256 648 187 307 610 402 221 479 553 338 185
  97 445 358  83 575 505 247  21  96 158 208 499 462 615 694  98 315 239
 257 662 340 496 515 480  93  84 316 718 572 357 459 376 454 230 289 657
 498 211 153 410 332 428 267 127 660  36 351 180  19 228 114 638 404 366
 277 579 524 476 285  65 706 512 470 385 537 414 282 352 566 154 426 640
 181 651 124 333  35  18 443 371 503 427 618 382 719 270 121 299 693 375
 581 619 123 551  42 702 218 540 458 548 328  52 417 167 264 294 200 331
 215 433 325 310 538 390 386 322 253 182 201 212 192 290  67 501 217 543
 400 336 665 624 341 600 526 278  46 593  40 138 582  16   6 135 339 122
 500  77 597 284  72 467 130 132 589 434 635 191 260 696 312 149 367 564
 481 313 391 510 717 227 628 384  39 614 639 346 368 362 511 559 147 676
 536 189 485 583 174 620 243 116 140 487 179 258  80 685 473 423 517  10
 447 448  25  54  53  20 232 133 110 516 562 654 342 297 115  78 584 587
 617 255 599 429 602  85  68 700 630 686  66  61 323 165 613 507 321  24
 444 478 689 611 156  71 269 712 596 460 659 695 491 698 495 649 475 306
 251 364 152 139 146 658 520 568 713 571 343 523 248 652 345 104 100 210
 334 105 383 508 578 219  38 361 102 494 561 318 393 661 710 442 539 672
  45  57 213 406 439 236   1 422 595 157 412 240  92 304 311 531 344  58
 580 168 413 106 461 421 220 245 441 492  79 194   9 514 594 354  82 563
 144 150 484 369 184  13 117 274 502 463 172 555 309 394 169 490 296 293
  34 519 101 317 411 525 225 544 436 708  30 237 335  29 612 623 259  22
  81  12 419 416 653 574 234 379 709  76 588 196  56 224 446 314 641 408
 202 148 271 143 687 214 231 415 707   0 272 468 308 263 457   5  48 373
 577 195 320 378 392 302 437 377 113  60 465 697  64 199 565 145 292 397
 679 456 353 268 249 701 326 380 363   2 558 552 656 715 450  23 395 488
 489  17 546 616 141 107  14 626 690 337 469 533 557 592 238 680 674 359]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5413
INFO voc_eval.py: 171: [ 47  27   7  28  29  35  58  26  37  30  31   6  94  71  69  66 105 132
  54 110 122 111 116  70   2  33 102 133  12 109  51 120  53  45  34  87
 128   4   1  61  76  39   3 113 107  83 124  68  82  86  22 123 121  46
  48  56  55  73  65  85  57  43  67  80  88  91 117  17  50  95  96 108
  10 119 135 114  72  23  78  81 127  75  20  79  32 129   9  89  84  18
 101 104  41  59  62  93  16  38  13  77  40  97  14  42  25  90  24  44
   8  49  92  19  21 130 118 125  63  15 100 112  36 103   5  11   0  64
  52  99  60 126  74 106 131  98 134 115]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.5633
INFO voc_eval.py: 171: [135  31 143 199 333 312  74 291 152 229 100 193  39  17 209 248 288 198
 213 116 222  86  99 176 256 133 110   9 292 275 302  58 286 224 155 102
 265 219  21 305 220  47 272 215 214 153 117  97 172 151 273   4 207   6
  94  14 341 114 258  79 235 144  90  29 206 259 287 339  43 352  93 294
 210 289 228 250 257 347 190 349  64  13  92  42 244 115 177 234 200  15
  38  49 246  51  73  67  70 166 319 301 159  28 142 108 317 130 202 208
 290 369 104 124 216  34 254 309 162 150  62 137  72 188 120 314   0 350
 297  33 293 249 328 335 240 316  36  23 211 318 129 310 356 368 181  19
  44 139 271  77 242 366  96 101 112  45 103 186  56  82 355 148  59 262
 307 109 323 146 346 277 326  54 304 179 158   7 149 342 329 278 337 105
   2 132 322  78 315 125 230 170  69  84 361  22 147 145  52 358  48 320
  41 336 163 134  16 121 365  63  53 218 266  11  81 106  68  27 332 128
 201 370 191  89  80  85 267 270 251  95 154 204 173 331 175 268 283  76
 157  20 192  30 136 308 111 330 354 156 313 306  55 184  35 225 196 253
 217 334  60 260 338 140 364 223 255  87 185 174 141 343  26 123 327  10
 284 279  98  12 231 232 344  75 357  46 299 167 180   5 281 362  66 236
 282 252 183 226 212 261 165 164 241 243 194 353 169 118  25 171 280 189
 221 300 345 321  32  91 340 161 269  37 360 296 122 195 348  40 238 182
 107 298 351  61 247 359   8  88 276 367  50   3 295 227 303 168  65  83
 311 113 197  24 239  57 274 160 203 285 127 363 205 237 325 119 138  71
   1 324 264 126 178  18 187 245 131 263 233]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4610
INFO voc_eval.py: 171: [ 60  79   5 153  90  70  76  82  66  21 141  31 140  19   8  18  63  91
 142 130   4 119 154  20 151  39  30 118   2  32 150  44  84  73  78  77
 123 109   1  64  46 134  25  55 146 144  59 149 116  35 135  69  47 100
  62  89 107   6  56  24  41 110  42  93 104  98 117  81  75 106  10 111
 148  36  33  34  49 122  68   3  97  99  12  50  26  43  27 136  51  88
 127 139 101  22  28   9  40 120  96  11 143 131 112  53 121 108  15  16
   7  83  72  45  94  38 113  85 124  86  37 103  71 105  74 129 125  14
  23  48 147  13 152 126 102 128  65 138  54  57  29 114  17  95  80 115
  67 132 145  58  87 133  61  52 137  92   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2675
INFO voc_eval.py: 171: [ 92 110  42  10  21  59  11  82  84  13  72  66  95  25  85  57 111 112
  97   3  45  70 100  83   1  37 104  48  47  69  75   0 101  27  40 121
  68 105  55  15 118  14 102  77  29  71  35  61  74   4  73  67  33  90
  54  60  93  28 103  12   9  86  94 117  41  19   2  62  88  30  36 119
  53 120 116  18  26  87  23 113 109  58  22  99 106  63  44  46  80  89
  43   7  50  78  20  64  91  56 107  38  32  49   5  24  17  81  65 114
  96  79 115  16   6  31 108  52  51  76   8  34  98  39]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.5400
INFO voc_eval.py: 171: [38  5  3 19 21 27  2 12 39 36 22 10 24  6 29 26  0 23  7  8 35 33 32 11
 16 15 37 14 30 17 13 20  9 31 34  1 18 40 28 25  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8926
INFO voc_eval.py: 171: [ 831 1480  257 ... 1425 1219   18]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7273
INFO voc_eval.py: 171: [ 46  26 285 186  83  31 249  28  59 284  29  79  97   8 298 116 199  27
   3 205 299 315  96  63 181  55 187 113 236  30 162  68  38 253 145  89
 167 240 173  66 219 100  34 267 281  61 302  90  67  45  54 301 274 265
 143  91 287 194  19 275  95 110  13 241  52 239 127 216 131 134 144 242
 207 286 108 278 215 319  84 248 273 176 200 269 107 121 270 136 304 202
 300 228 308 218 174  22  85 250 177  17  78 122  50  65  41 101   7 133
 128 193  11 261  47 172 296 271 303 106 258 226  86 192  43 255 130 283
  25 268 155 224  62  14 105 295  49 171 288 196 238 227 132 203 256  72
 117 214 164 305 153 124 123 160 182 115 311  87 161  36  23 184 313 201
 146 314 188 263  60 223   4 243 104 147 190  51  33 231 118 126  20  70
 282 262 235 140  81 213 277  94 310 165 170 306   0 102 220 276 212 208
  93 217  64 206  37  82 320  69  73 149 157  32 183 163  40 139 211 234
 135 103 317  80  88 257 198   2  21  58  56 168 237 180 175   6 266 293
 197 154 292 150 289  44 112  16  39 316 307 111 221 191 120 178 151 129
 246 159   5 141 290 254  75 148 272  71 291  35   9  76  57 189  99 179
  48 195 169 247 119 279 280 264  98 225 114 259   1 138  42 142 209 137
 185 294  92  24 158  53 229 222 232 244  12 125 230  77 318 204 312  10
 210  18 233  15 251  74 309 156 109 321 166 252 260 297 152 245]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5813
INFO voc_eval.py: 171: [ 93   5  57  96  21   7  70  89 107  65  94  51  60  26 106  22  17  34
  80  77  73  16  29  48  45  72  27  64 104  62  54 103  74  50 102  44
   6   2  71  40 101  20  47  91  79  10   0  36  78  90  35  59  25   8
  66   4  56  37  76   3  18  55  53 109 108  85  68  92  30  88  67  41
 105  12  82  99  75  38  19  98  84  87  46 100  23  32  81  83  52 111
  42 110  14  63  43  11  15  13   9  86  33  49  69  97  95  28  39  24
  31   1  58  61]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4796
INFO voc_eval.py: 171: [28 16 29  0 47 63  1 73 42 33 75 41 24 26 19 38 54 45 48 35 34 57 37  9
 50 14 17 71 66 31 11 18 20 49 78 72 32 27 59 22 61  5 25 51 10 77 60 55
 56 43  7 40 36 64 76 39 12 58 69 21 13  2  6 68  4 62 53  3 52  8 74 67
 46 15 23 70 44 30 65]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3432
INFO voc_eval.py: 171: [63 41 10  6 77 37 20 89 34 80  5 57 44  4 48 14 19 54 21 72 90 76  8  9
 36 33 43 23 68 35 52 71  3 39  2 56 46 74 85 65 16 86 64 47 38 32 58 83
 28 42 30 60  1 50 53 24 62 93 55 69 91 25 17 11 88 81 92 29 70  7 31 49
 66 87 78 18 67 12 27 26 82 45 73 51 75  0 13 61 59 22 15 84 79 40]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5815
INFO voc_eval.py: 171: [38 88 21  1 61 41 68 60 66  8  5 47 20 28  6 55 50 71 35 29 77 83 86 15
 59 12 25 80 74 90 45  3 19 49 52 93 69 62 65 67 16 32 73 72 56 22 94 17
 11 44 78 31 14 33 70 46 63 37 43  9 24  2 76 34 42  7 39 58 54 23 18 40
  4 79 91 26 10 53 87 75 85 64 13 51 30 92 84 81 48 27 89 57  0 36 82]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5267
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5059
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.325
INFO cross_voc_dataset_evaluator.py: 134: 0.625
INFO cross_voc_dataset_evaluator.py: 134: 0.342
INFO cross_voc_dataset_evaluator.py: 134: 0.408
INFO cross_voc_dataset_evaluator.py: 134: 0.497
INFO cross_voc_dataset_evaluator.py: 134: 0.659
INFO cross_voc_dataset_evaluator.py: 134: 0.558
INFO cross_voc_dataset_evaluator.py: 134: 0.198
INFO cross_voc_dataset_evaluator.py: 134: 0.541
INFO cross_voc_dataset_evaluator.py: 134: 0.563
INFO cross_voc_dataset_evaluator.py: 134: 0.461
INFO cross_voc_dataset_evaluator.py: 134: 0.267
INFO cross_voc_dataset_evaluator.py: 134: 0.540
INFO cross_voc_dataset_evaluator.py: 134: 0.893
INFO cross_voc_dataset_evaluator.py: 134: 0.727
INFO cross_voc_dataset_evaluator.py: 134: 0.581
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 134: 0.343
INFO cross_voc_dataset_evaluator.py: 134: 0.582
INFO cross_voc_dataset_evaluator.py: 134: 0.527
INFO cross_voc_dataset_evaluator.py: 135: 0.506
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.413s + 0.001s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.357s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.369s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.368s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.373s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.372s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.370s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.368s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.366s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.369s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.369s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.371s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.369s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.472s + 0.002s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.388s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.381s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.379s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.377s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.382s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.390s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.387s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.388s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.387s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.385s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.386s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.379s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.421s + 0.001s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.387s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.387s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.382s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.377s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.378s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.378s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.377s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.380s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.376s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.376s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.376s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.373s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.408s + 0.001s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.363s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.358s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.351s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.350s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.347s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.346s + 0.001s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.345s + 0.001s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.349s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.348s + 0.001s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.345s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.346s + 0.001s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.346s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead_unlock-rpn-conv/Nov16-19-16-09_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 56.118s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [147  31 117  24  61  27  50  25  15  76  13  63  36  71   4   2 112  11
 163  14 122  47  32  96  44 159  38  73  48  40 127 116  75  28  18  16
 132  58 106  57  62  67  91   9 100  95  69 126  20 148 131  78  10   0
  17  81  43  92 146 158  97  26  79 128 153  22  86   5  42 130 168  23
  66  35 169 120 135  80  39 145  49   8 119  94  55  65 144 162  54 143
  56 124  34 121 156 115  84 133 161  45 123 125  33 118 154  68 104  82
  85 137 108 105  41  90  12  21  60  77  83 139 150   6 142  89 152   3
  52  93 134   7 170 129 164 167  88  74  87 138   1  72  37  19  59 111
  98 140 141 151  99  51 102  53 149 107 165 113 157 109 103 101 114 166
 160  30  46 110 136  70 155  29  64]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3255
INFO voc_eval.py: 171: [37 11  7 19 21 39 41 29 44 47 23 42 49 46 45 18 30 36 24  1 40 43 15 33
 50  8 17  3  4 48 22  5  2 38 16  9 31 10 13 34 20 28 26 25 35 12 27 32
 14  0  6]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6254
INFO voc_eval.py: 171: [ 65 166 189 168   5 129 282 377 378 187 307  63 131 202  82 130  21 399
 110  72 200 443 444 330   6  96 357 288 139  35 181 400 235 231 154  31
  49 305 311 237 401 445 416  81 375  95 287 133 180  54 121  87 302  40
 136   0 257  97 172 425 120  68 338 427 385 104 446 358 333 137 424 319
  88 145 380 183 396 100 208 421 223 132  83 371  44  41 265 450  94 134
  53 233  32 201 376  26 404 182 188 300 315 264 323 337  61 455 362 278
 426 173  92  86 403 370  12 170 342 177 205 220 269 155 191 448 451 436
  30 184 364 215 105 239 255 207 230 414 453 119 298 236 345 116 157 314
 313 122 412 148  93 214 262 423 127 117 292  34   3 432 326 206 161 146
  79  56 238 225  13  52  46  11  71 391 413 431  59 284  80 213 204  20
 199  57  45 244 268 123 433 250  24 279 415 291 216 118 259 328 143 397
  22 379 332 442 447 353 141 393 221  70 293   2 246  47  67 197 322 441
  76 430 348 263  23  66 356 331 418 227 245 402 372 273 318 344  58 320
  16 151 124 240 144 428 304 406 312 277 140 135 267 111 138 308 112 186
 390 306 351 266 295 218 113 325 384 301 149  90  36 276  69 103 253 354
 162  37 185 294 153 289 454  27  42 169 296 258 437 152 392 408 210  18
 439 101 297 420 256 382  50 175 434 174 327 261 321 346 190 217 316 374
 228 270  25 381 229 355 114 324   9 247  19 395  48 158 429 198  39 387
 341 299 456 281 417 369  64 243 108 389  51 209 449   7 241 222 411 171
 352  43  91  33 226  84 211 176  73 254 422  89   8 285  75  60 260 339
 410 366 286 165 349  98 109  62 102 248 309  17 361   1 196  28  14 452
 343 159 126 383 350  55 290 373 407 242 388 310 234 219  99 212 360 394
  78  77 303 283 203 163  15 398 386  74 317 192 252 280 128 142 193 329
 340  29 115  10  38 435 368 359 272 440 150 335 107 164 195 179 367 125
 147 274 194 419   4 232 224 347 251 178 106 365 167 275 156 271 405 336
 438 363 249  85 160 334 409]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.3421
INFO voc_eval.py: 171: [ 55   1 221 159 190   3 191  28 157 160  49  25 263  66 209  67 218  77
 192 108 115 266  48 226 149 231 172  79 203 156 178  13 158 281  12 183
  61  68  10  56 193 237 105 126 230 128  51 248  94  58 253 298 111 245
 173 264 176 133  36 212 235  24 132 129  84 217 290 267 124  88  34 238
 188 261 148 211  27  82 130 224 206 242  11 275 140  78 222  54 144 260
 181 110 276  52  43 274 185 154 295 152  80 296 175 289  76 139 279 232
  83  73 249  70 258 150  15  32 256 112 146 280 179 297 143 287  38  20
 100  74 198 213 163 240  90 182 229 201  47  92 155 268  22  18   9 109
 164 207  69 271  53 250  45  41 241 292  86 286 131 239 153 151 215  33
 166 103 223 265 122 254 214  59 177 269  87 136 244 205 167 141 116 147
 291  44  19 180  75 202 169 107 171 208  93   2 277 299  97 225  89  31
 210 118  37 168   8 293  40 233  99 137  85 247 197 234 272   4 284 113
   5 288  23 294 252 184  14 236   6  95 101 174 219 102 104 125  16 216
  62  65 195  30 246  71  26  17 186 123 162  29  42 200 278 270 189   7
  50 170 120 194 114  96 106 273 243 138 227 117   0 127 187 196  60 121
 259  35 251 145  72  81  63 134  39 285 282 228  57 220 204 165 161 257
  46 199  64 142  21  98 135  91 119 262 283 255]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4081
INFO voc_eval.py: 171: [ 90  91 175 170 167 103   2 172 171 138 225  79 123  77  92  43   4  80
 137  68 131 114  69 204  78 130  76 226 187 178  56 211 151 152 174 105
 177  49 111  98  12 201  61  35  65 183  64  22 196 181  15 101 120 107
  59 142 139 189 176 112  47 182 143 147 117 132 146   5 141  19  21  53
 215 173 190  66 118  39  24 116 223 179  70  75 129  95 184 205 154   6
 191 188  37 219 206  52 145 109  26  60 199  86 207  20 217 161 185  23
 203 150 180  44  11 157 100 168 102  48 216 153  72  83 212 122 121 186
  55 159   1 210  14  97 108 221 169 218  16 192   9  57 222  51 148 113
   3 195  36  40  93 193  63  99 149 194 119  67 197  81  30 155 209  17
 106  29   8 202 213 200  18 208 228  62  31  33 144  82  89  45  74  25
 198 125 127  41 126 166  34  96 128  32 135  71  85 164  38  46  94 136
  88 162 163  50  87  84 104 224 134 158 140  73 165   0 110  54   7 160
  58 227 220  42 214  28 124  13 156 133  27 115  10]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4968
INFO voc_eval.py: 171: [64 24 13  8 23 54 56 39 33 10 35 51 66 34 57 58 42 41  1  2 45 27 60 61
  7 25 55 16 21 22 32 48 31 63 62  0 67 59 52 43  9 47 18 20 30 15 50 29
  3 40 26 19 11 65 12 28 38  6 14 53 46 37  4 49 17  5 44 36]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6586
INFO voc_eval.py: 171: [ 61 142 146 211  36 209  60  25  99 149  44  69 132 204 101 159 221 210
 199 157  98 184 207  96  11  81  80  13   0 191 215   7  62  40 162 114
 200 105 128 129 163  89 111 113 160 117  45  70 193  24  30 104 144  58
 182  79 198 190  90  93 217 118 169  55 158  21 227 120 185 170  18  82
  37 177  39 135  50  75 173 201  32 122  83  95  64 141  97 110  66 155
 224 187  22   5 214 127 108  57  91 143  85  33  87 150 223 106 134  67
 116 166  26  71  46 164  14 103 179 130 188  19  73  48  77   8   9  41
 208 140 192 183  20 145 213 131 119  28 125 220   2  49  74  68 100 194
  65 153 212 161  16 176 228  27 195  94 107 203 137  29  84 139 172 196
  34  56  59  17   4  53  72  47 156  52 197  86 124   1  78  15 123 148
 202 175 219 165 167 126 178 174 138 133   6 147  43 225 216 121  88 102
   3 205 115  31  23 180  35  54 189  76 226 171 152  42 222 136  63  51
 112 218 206 181 154 186  92  10 109 168  38  12 151]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.5575
INFO voc_eval.py: 171: [ 7 75 25 89  4 16 63 70 21 46 83 36 68 56 23 35 50 27  6 41 57 82 24 52
 77 72 87  3 38 51  1 71 11 45 28 88 79 34 64 15 69 62 65 44  8 29  0 54
 86 37 85 10 67 74 40 49 42 17 32 14 22  5 48 58 30 39 61 18 66 20 90 33
 31 78 84  9 43 19 13 81 59 55 73 80 26 76 53 60  2 12 47 91]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1980
INFO voc_eval.py: 171: [632  73 631 162 667 504 279 669 171  74 163  50  31 683 403 159 405 452
  49 355 603 678 265 549 206 430 283 681 684 453 677 644  28 111 704 556
 235 418 261 303 550 128 645 547 266 604 229 431 425 281 424 103 183 151
 451 506 474 356 242 129 529 527 161  86  88 682  41 409 190  91 455 276
 691 241 327 305 513 401   3 176 273 205 216 560 477 440 607 625 126  43
 275  87  37 622 300 349 716 664  15 666 207 627 175  69 209 134 482 590
 646 541 177 567  94 120 535 280 705 360 388 464 554 472 348 573 254 655
 633 398 329 396 246  51 319 605 372 471 198  99 119 609 714  63 668 350
  89 650 365 576 692 197 642 606 288 530  27 699 466 597 324  59  33 643
 591 118 330 634 542 160 291 671   4 131 173 675 295 570 112 497  32 528
 370 108 164 711 286 438 178 585 608 301 586 203  62 252 204 663  11 449
  95  44  70 637 621 420 109 670 389 125 155  55 262 233 569 521 193 629
 226 170 673   8 244 493 647 188 387   7 532 545 166 136 486 522 186 222
 347  75 703 399 518 407 483  47 223 601 142 688 381 137 435  26  90 432
 636 509 374 287 298 250 534 256 187 648 307 610 402 221 553 479 338 185
 445 358  97  83 575 505 247  21 158  96 208 462 499 694 615 315 239  98
 257 340 662 496 515 480  93  84 316 718 572 459 357 376 454 289 230 657
 211 498 153 332 410 428 267 127 660 351  36  19 180 228 114 404 638 277
 366 579 524 476 285  65 512 706 470 385 414 537 282 352 154 640 566 181
 651 426 124 333  18  35 443 371 503 427 618 382 719 121 375 270 693 299
 619 581 123  42 551 218 702 540 548 458 328  52 264 417 167 200 331 215
 294 325 433 310 538 322 386 390 253 192 212 201 182  67 290 501 217 543
 624 400 665 336 341 526 278  46 593 600  40 582 138   6  16 339 122 135
 500 284  77 596  72 467 589 132 130 191 434 635 367 564 696 312 260 149
 481 313 628 391 227 510 717 384  39 639 346 614 362 368 511 559 147 485
 676 620 536 583 174 189 243 140 116 179  80 487 258 685 423  10 517 473
  25  54 448 447  20  53 232 133 110 516 654 342 562 255 617 584 297 587
 429  78 115 602 598  68 700  85  66 323 507 613  61 165 686 630  24 444
 321 478 156 611  71 689 460 712 269 659 595 491 495 251 695 306 364 698
 649 152 475 139 571 146 713 568 658 520 210 100 104 523 652 334 248 345
 383 343 105 508 219 578 361 102 561 494  38  45 672 442 539 661 710 318
 393 422 406   1 439  57 236 213 594 412 157 240 304 311  92 344 531  58
 106 168 413 580 421 461 220 441 492 245  79 194  82 354 514 599   9 369
 502 144 309 150 563  13 117 184 274 555 484 463 172 394 293 296 490 169
 101 225 544 525  34 411 519 317 436 237 708  30 612  29 623  22 335 379
  81 653 574  76 234 416 419  12 259 709 196 408 202 641 314 446 224  56
 588 415 231 271 687 143 148 707   0 457  48 468 214 195 308 272 263 373
 577   5 320 378 113 392  64 377 437  60 465 199 145 697 565 302 292 397
 456 353 679 268 249 380 656 363 395 450  23 552 715 558   2 326 701 141
 107 488 489 546  17  14 626 337 690 469 533 557 592 616 238 680 674 359]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5413
INFO voc_eval.py: 171: [ 47  27   7  28  29  35  58  26  37  30  31   6  94  71  69  66 105 132
  54 110 122 111 116  70   2  33 102 133  12 109  51 120  53  45  34  87
 128   4   1  61  76  39   3 113 107  83 124  68  82  86  22 123 121  46
  48  56  55  73  65  85  57  43  67  80  88  91 117  17  50  95  96 108
  10 119 135 114  72  78  23  81 127  75  20  79  32 129   9  89  84  18
 101 104  41  59  62  93  16  38  13  77  40  97  14  42  25  90  24  44
   8  49  92  19  21 130 118 125  63  15 100 112  36 103   5  11   0  64
  52  99  60 126  74 106 131  98 134 115]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.5633
INFO voc_eval.py: 171: [135  31 143 199 334 313  74 292 152 229 100 193  39  17 209 249 289 198
 213 116 222  86  99 176 257 133 110   9 293 276 303  58 287 224 155 102
 266 219  21 306 220  47 273 215 214 153 117  97 172 151 274   4 207  94
   6  14 342 114 259  79 236 144  90  29 206 260 288 340  43 353  93 295
 210 290 228 251 258 348 190 350  64  13  92  42 245 115 177 235 200  15
  38  49 247  51  73  67  70 166 320 302 159  28 142 108 318 130 202 208
 291 370 104 124 216  34 255 310 150  62 162 137  72 188 120 315   0 351
 298  33 294 250 329 336 241 317  36  23 211 319 129 311 357 181  19 369
  44 139 272  77 243 367  96 101 112  45 103 186  56  82 356 148  59 263
 308 109 324 146 347 278 327  54 179 305 158   7 343 149 330 105   2 279
 338 323 125 230 316  78 132 170  69  84 362  22 147 145  52 359  48 321
  41 337 163 134  16 121 366  63  53 218 267  11  81 106  68 333 128  27
 201 371 191  80  89 271 268  85  95 252 154 204 332 173 175 269 284  76
 192 157  20  30 331 111 309 136 307 156 314 355  55 184 225  35 196 217
 254  60 335 261 339 140 365 223 256 174  87 185  26 344 141 285  98 280
 328  10 123  12 231 345 358  75 300 232 167  46 180   5 363 282  66 283
 237 253 183 226 212 242 165 164 262 244 194 354  25 118 169 171 301 281
 346 189 221 341  32 161 322 270  91 361 349 195  37 297 122 239 182  40
 107 352  61 299 248 360  88   8 296 168 227  50 277   3 304  65 368 312
  83 113 197  24 240 203 160 275 127  57 286 205 364 238 326   1 138 119
 325  71  18 126 265 178 264 246 131 187 233 234]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4610
INFO voc_eval.py: 171: [ 60  79   5 153  90  70  76  82  66  21 141  31 140  19   8  18  63  91
 142 130   4 119 154  20 151  39  30 118   2  32 150  44  84  73  78  77
 123 109   1  64  46 134  25  55 146 144  59 149 116  35 135  69  47 100
  62  89 107   6  56  24  41 110  42  93 104  98 117  81  75 106  10 111
 148  36  33  34  49 122   3  68  97  99  12  50  26  43  27 136  51  88
 127 139 101  22  28   9  40 120  96  11 143 131 112  53 121 108  15  16
   7  83  72  45  94  38 113  85 124  86  37 103  71 105  74 129 125  14
  23 147  13  48 152 126 102 128  65 138  54  57  29 114  17  95  80 115
  67 145  58 132  87 133  61  52 137  92   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2675
INFO voc_eval.py: 171: [ 92 110  42  10  21  59  11  82  84  13  72  66  95  25  85  57 111 112
  97   3  45  70 100  83   1  37 104  48  47  69  75   0 101  27  40 121
  68 105  55  15 118  14 102  77  29  71  35  61  74   4  73  67  33  90
  54  60  93  28 103  12   9  86  94 117  41  19   2  62  88  30  36 119
  53 120 116  18  26  87  23 113 109  58  22  99 106  63  44  46  80  89
  43   7  50  78  20  64  91  56 107  38  32  49   5  24  17  81  65 114
  96  79  16 115   6  31 108  52  51  76   8  34  98  39]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.5400
INFO voc_eval.py: 171: [38  5  3 19 21 27  2 12 39 36 22 10 24  6 29 26  0 23  7  8 35 33 32 11
 16 15 37 14 30 17 13 20  9 31 34  1 18 40 28 25  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8926
INFO voc_eval.py: 171: [1479  830  853 ... 1218 1131 1424]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7256
INFO voc_eval.py: 171: [ 46  26 285 186  83  31  28 249  59 284  29  79  97   8 298 116 199  27
   3 205 299 315  96  63 181  55 187 113 236  30 162  68  38 253 145  89
 167 240 173  66 219 100  34 267  61 281 302  90  67  45  54 301 274 265
 143 287  91 194  19 275  95 110  13 241  52 239 127 216 131 134 144 242
 207 286 108 278 319 215  84 248 273 176 200 269 107 121 270 136 304 202
 300 228 308 218 174  22  85 250 177  17  78 122  50  65  41 101   7 133
 128 193  11  47 261 172 296 271 303 258 106 226  86 192  43 255 130 283
  25 268 155 224  14  62 105 295  49 171 288 196 238 227 132 203 256  72
 117 214 164 305 153 123 124 160 182 115 311  87 161  36  23 313 184 146
 201 314 263  60 188   4 223 243 104 147 190  51  33 231 118 126 282  70
  20 262 235 140  94 213  81 277 310 165 306   0 102 170 276 220 208 212
  93  64 217 206  82  37 320  69  73 149  32 183 157  40 163 139 211 234
 135 103  80 317 257 198  88  21   2  58 237 168  56 180 175   6 266 154
 197 293 292 289 150  44  16  39 112 307 316 178 221 120 191 111 151 159
 246 129 290 141  75 272   5 148 254  71 291  76  57  35 189   9  99  48
 169 195 179 119 247 279 280 264  98 259 225 114   1 138 142  42 137 209
 185 294  24  53  92 158 229 222 244 232  12 230 125 204 318 233  18  15
  10  77 312 210  74 251 109 156 309 321 297 260 252 166 245 152]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5826
INFO voc_eval.py: 171: [ 93   5  57  96  21   7  70  89 107  65  94  51  60  26 106  22  17  34
  80  77  73  16  29  48  45  72  27  64 104  62  54 103  74  50 102  44
   6   2  71  40 101  20  47  91  79  10   0  36  78  90  35  59  25   8
  66   4  56  37  76   3  18  55  53 109 108  85  68  92  30  88  67  41
 105  12  82  99  75  38  19  98  84  87  46 100  23  32  81  83  52 111
  42 110  14  63  43  11  15  13   9  86  33  49  69  97  95  28  39  24
  31   1  58  61]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4796
INFO voc_eval.py: 171: [28 16 29  0 47 63  1 73 42 33 75 41 24 26 19 38 54 45 48 35 34 57 37  9
 50 14 17 71 66 31 11 18 20 49 78 72 32 22 27 59 61  5 25 51 10 77 60 55
 56  7 43 40 36 64 76 39 12 58 69 21 13  2  6 68  4 62 53  3 52  8 74 67
 46 15 23 70 44 30 65]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3432
INFO voc_eval.py: 171: [63 41 10  6 77 37 20 89 34 80  5 57 44  4 48 14 19 54 21 72 90 76  8  9
 36 33 43 23 68 35 52  3 71 39  2 56 46 74 85 65 16 86 64 47 38 32 58 83
 28 42 30 60  1 50 53 24 62 93 55 69 91 25 17 11 88 81 92 29 70  7 31 49
 66 87 78 18 67 12 27 26 82 45 73 51 75  0 13 61 59 22 15 84 79 40]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5815
INFO voc_eval.py: 171: [38 88 21  1 61 41 68 60 66  8  5 47 20 28  6 55 50 71 35 29 77 83 86 15
 59 12 25 80 74 90 45  3 19 49 52 93 69 62 65 67 16 32 73 72 56 22 94 17
 11 44 78 31 14 33 70 46 63 37 43  9 24  2 76 34 42  7 39 58 54 23 18 40
  4 79 91 26 10 53 87 75 85 64 13 51 30 92 84 81 48 27 89 57  0 36 82]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5267
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5059
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.325
INFO cross_voc_dataset_evaluator.py: 134: 0.625
INFO cross_voc_dataset_evaluator.py: 134: 0.342
INFO cross_voc_dataset_evaluator.py: 134: 0.408
INFO cross_voc_dataset_evaluator.py: 134: 0.497
INFO cross_voc_dataset_evaluator.py: 134: 0.659
INFO cross_voc_dataset_evaluator.py: 134: 0.557
INFO cross_voc_dataset_evaluator.py: 134: 0.198
INFO cross_voc_dataset_evaluator.py: 134: 0.541
INFO cross_voc_dataset_evaluator.py: 134: 0.563
INFO cross_voc_dataset_evaluator.py: 134: 0.461
INFO cross_voc_dataset_evaluator.py: 134: 0.267
INFO cross_voc_dataset_evaluator.py: 134: 0.540
INFO cross_voc_dataset_evaluator.py: 134: 0.893
INFO cross_voc_dataset_evaluator.py: 134: 0.726
INFO cross_voc_dataset_evaluator.py: 134: 0.583
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 134: 0.343
INFO cross_voc_dataset_evaluator.py: 134: 0.582
INFO cross_voc_dataset_evaluator.py: 134: 0.527
INFO cross_voc_dataset_evaluator.py: 135: 0.506
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
