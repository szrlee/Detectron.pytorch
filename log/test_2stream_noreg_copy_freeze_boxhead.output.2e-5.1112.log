Start testing on iteration 499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.864s + 0.002s (eta: 0:01:47)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.439s + 0.002s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.416s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.398s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.400s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.405s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.401s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.407s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.406s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.404s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.403s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.408s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.407s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.499s + 0.001s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.414s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.407s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.390s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.387s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.396s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.405s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.402s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.401s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.399s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.396s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.396s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.395s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.514s + 0.001s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.394s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.405s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.407s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.400s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.394s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.395s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.395s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.395s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.393s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.393s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.392s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.392s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.907s + 0.002s (eta: 0:01:52)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.422s + 0.002s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.396s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.392s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.394s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.388s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.393s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.389s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.390s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.389s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.387s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.384s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.384s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.576s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [196 250  64   6 314 103  58  21 237 140  50  60  56 153 163 321 188 134
 193  49 197 164 332 198  90 338 346  55  51 190   1 349 129  24  96 279
 136 152 222 294  93 168  33 143  43 295 259 119 148  80 167 282  52 112
  89 308 205 344  74 191  12 262  32 315 173  65 248 337 353  63  97  29
  39 260  20 266  22 219  57 166   3 285 304 181 340 171 328 283 107 128
 194 231  26 291 170 229 301 123  46 182 149  81 249 142 330 180 264 345
 159 305  37 169 144  62  27   7  87  42 226  48 311 316 351 217 179 303
 121 202 147  79 281 187 242 275 358 101 218 322 247 127 234 245 355 334
 227  98 108 186  19 270 299 162  68 210 223 235 178  61 298 114 204 154
 111 271 201 276 317  23 131  10  72 296 212  41 319 150  82 174 310 277
 215 233 347  91 300 214 341 352 327 261 269 354 313  92   4  30 120 203
 184 339  25 139 228 255 135  13   0 361 185  16 263   2 151  83 138   8
 258 225 208 289 137  66 130 267 256 320 126 230  38 278  71 236 251  28
 265 110  95 156 104 274 155 243 331 221  11 348 309  76 172 329  99 252
 206 102 183 286  35 284 213 220  78 146 106 254 280  14 113 244  84 290
 105  54  70 241  59 273 161 232  44 160 125 360 158  15  67 100 246 302
 109 177 336 200 288 157 224   9 240 165  88 325 209 307 116 272 335 145
 268 324 257   5  47 216  77  31 253 342 287 293 326 195 350 192 175  73
 117  85 176 306 238 292 122 132  17  18 356 133  69 189  75 199 333 141
 211 357 118 323 343  45  53 312 239  36 124 115 297  86 318  34  94  40
 359 207]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3455
INFO voc_eval.py: 171: [ 69 242  31 154 278 210 289 292 273 160 291 143  32  28 177 277 293 288
 213  20 260 224 115  44 279 120 202 211 105 283 294  80 206 188 285 118
 149 218 148 235  19 272  13  66 192 262  10  46 181 300 110  86 290 295
 237 104 172 258 296 101  55  54  93 231 239 116 253 214 127 212 269 121
 169 158 112 186 178 161 271 113  34 226 302 198 155   2   7 265 159 102
 123  42 307  76 257 267  48 133 162 270  47 306 254 185 287 236 150  99
 153  78 132 166  11 256  71 100  61 157 299  30  33 182 168  26 230 286
 176 180 167 303 247  62 266 194 301 308  35 232  16 263 129 144 152  88
 297 114  90 145  49 109 240  22 264 203 137 216 234 284 140 139 233 187
 164 223  38 298 281 255 225 208 248 179 189  12  84 147  72 244 165  75
   6 246   8 201  56 107  65  40 156  96  59 108 134  81 135 222  89 243
 249  64 227 245  57 209 142  27 126 173 221  52   4 136  92  77 128   1
  18 195 259 275  67  37  23  43  14  63 146 130  68 174  21  97 241 151
   0 305  95 304 138  53 215 261 103 163  39 268 124  25  94 207 220 252
 117 280  50  91 141  85 238 229 125 191  41 196  74  83 217  98 204 197
 122  15   9  79 205 106 170 193  60  17 276  36 228  82 184   3 131  87
  51 119 171 250 219  45 200   5 251 282 190  24  58  70 274 175 199  73
  29 111 183]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6198
INFO voc_eval.py: 171: [ 73  32  92  56  58  80  88  57   1 197 149 179 196 181 198 113  68  28
 224   2  70  45 110 182 160  60  87  65  95 216  11  63 163 156  81 219
 199 210  71 126  47 129  16 231  93 174 188 217 167 201   4  69  38 230
 159 165  83  79  90 140 170  82 162  26 206 184 195 143 212  84 147 161
 164  66 103  89 191 202  10  36  49  19 229 186  48 112   5 106 172  22
  15  77 122 166 189  12 152 200 127 132  25 205  52  44 193 207 134 222
 221  61 183 228 169 121 137  55  85 157   8 214 155  50 153 211 107 109
 225 175 142  98 194  40  34 145 144  21  99 146 111  78 130 213 204  54
  46 101 218  97  62 133  29  30   6  74  27  13 125  75 104 138 192  35
  67  24 115 105 203 226  53  72 208  41 119 178 108  86 215   9  31 141
 136 128 148 120 116  33  20 190 223 220 177  23 139 114 154 176 158 102
   0  18  91 180   3  37  51 131  59 123  94 135 117  42 232 151 173 124
  14 168  39 118 100 209 187  96 150 171   7  43  76  17 227  64 185]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2290
INFO voc_eval.py: 171: [394 464 395 127   0   9 396  57 102  30 322 173 567 321  62 433 143 316
 428 453 319 558 496 320 142 239 328 245 310  10 172 397 144 323 498 566
 121 233 106  32 294 214 368  41 103  35 497 422 488 385 495  24 591 123
 539 204 261 146 128 628 465 302 327 244  72 420 555  73 486 490 225 217
 569 187  28 513 594 518 509 389 485 588 287 451 401 475 325 333  34 597
 135 524 562 374 264 547 312 487 366 440  37  31 577 370 576 489 574 155
 382  43 625 104 234 215 568 271 304 532 391 344 580 213  15 306  68 174
 317 605 434 339 417 421 472 619  80 520   3 230 197 228  44  16  33 338
 378 208 313  27 112 337 358 476 237 354 601 622 533 164 109 514 218 405
 291 318 301 357  39 535 258 295 477 361 593 154 178 346 268 570  54 238
 336 388 153 596 612 145 608 263 587 275   5  61 611 516  88 375 536 610
 466 152 467 286 175 352 274 404  51 439 129 115 560 177 189 133 246 550
 523 445 408  86 369  52 285 350  83 198 406 324 528 469 210 579 376 607
 170 554 278 627 249 503  50 347 303 447 329 108 200 543 551 227 150 160
  18 273 309  60 149 284 331 461 265 589 571 525 276 212 219 590 512 427
 307 585  56 251 110 199 438 501 416 578 410 335 384 330  93 223 534 403
 616 468  38 383 484 505 620   7 280 151 575  74  13 493  58 342 156 614
 457  97 296  48 147 259 393  49   2 450 409 289 563 435  77 254  81 602
 573 527 308 392 430 130 270 379 157 559 530 209 283 236 120 542 362 183
 442  36 141 248 508 618  67 188 425 526 377 398 483 351 572  19 561  22
  23 180 600 148  25 545 510 462  90 288 419 117 519 552 540  65 386  26
 266 184  71 343 191 478 456 206 414  92  95 529 400 126 615 158 586 224
 449 299 257 281 125 340 241 221 432 122 598 134 353 491 167 162 262 541
 603 549 624 479  85 334 116 531  66 548 311  69 235 481 100 473 407  91
 176  75 118 205 182 436 297   1 139  55 242 341 114 626 326  11 604  42
 494 132 470  82 105  96 190  40   6 599 441  84 371 556 179 226 364 537
 499 380  17 124 314 349  14 186 367  76 240 426 169 538 171 544 231 272
  98 413 621 355 201 553 418 194  89 606 429 623 256 584 459  94 448 372
 583 592 345 111 253 292 455 332 390 107 137 522 515 252 504 211 232 363
 229 260  45  29 500  63 216 402 305 113  47 165 247 365  87 207 203 181
 356 168 506 423 293 192 300 437 185 517 411 617 564  20  70 140 131 243
  64 480 119 202 557 492 444 609 431 222 454 269 279  78 196  99   8 387
 399 373 482 511 220  79 463 136 502 255 595 507 195 166 282 521  59 452
 250 267 359   4  53 277  12 460 474 443 298 381 193 315 163 471 159 412
 161 415 348 138 565 360 582 458 546 290 446  21  46 581 101 613 424]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3466
INFO voc_eval.py: 171: [509 279 510 247 511 250 286 513   4 516 246 427 285 248 313 349 627 138
 230 626 420   7 404 423 495 405 157 231 249 372 422 315 581 537 521 421
 514 425 569 523 208 548 529 517 451 300  43 515 576 533 531  97 512 335
 540 417 424 520  96 327 150 535 534 479 545 292 219 354 429  21 460 345
 544 190  18 188  31 355  42 595 542 607 288 438 357 178  71  59 310 241
 297 446 319 518 363  94 525 606 450 605   8 406 455 173 428 253 166 305
 100  58 205 291 211 480   5 426 536 466  57 105 347 503  38 346 359 149
 457 254 209 419 195 239 265 389 528 526 571 473 445 597   6 448 220 433
 245 385 215 170  22 289 543 566 302 175 524 192 263 608 360 491 628 314
  10 469 565 583 320 456 564 567  66 120 547 458 476 552 109 430 326 256
 287 307 141 551 588  53 156 356  67  33 222 296  19 416 434 468 193 189
 464 432 361 555 409 309 252 323 508  44 251  48 499 471  74 159 145 383
 271 577 257 293 408 282 221 318 441 237 493  20 437  82 333 625 482 198
 443  89 559 610 311 362 504 611 620 119  72 321 378 185 199  84 207 381
 452 366  14 298 325   0 133 367 203 106 554 585  45 391 234 294 373  88
 200 114 261  36 233  90 411 377 549 398 115 364 553 227 329 505 619 596
 592  75 151 295 268 570 174 407 139 477  65 299 353 462 183 266 102 339
 506 332 226 519 177  52 204  11 440 489  91  28 290  78 163  83 586 129
 336 500 243 614 218  77 475 563 214 169  50 283 591 240 587 122 617  30
 262 390 546 184 589 260 342 630 467  29 370 603 431 162 478  69 442  41
 276 116 501 558 575 259  13 418  63  95   9 465 165  70 593 137 631 164
 232 358 374 449 317 101 395 181 484 578 579 488 274 278 112   2 228 171
 340 453  60  46 125 392 379 472 110  39 415 210 568 487 160 324  55 225
 609 142 132 267 584 153  81 337 557 235 497  62  99 602 618 414 270 238
 308 351 135 344 376  87 530 126 334 242 155 527  17 167  92 140 264 562
 485 306 632 410 341 301 622  85 180  86 196 490 396  76 403  54 481 615
 201 272 387 143 447 561 179 612 412 224 486 399 304  40 598 117 161 507
 556 206 444 258 502 604 316 194 123  49 213  24 459  35 496 388 613 600
 284 375 338 369 277  34 532 172 223 229 212 275 124 273  79  27 474  37
 401  56  25  64 365 152  73 118 328 560 494  12 186 136 312  32 394  68
 368 386 303 582 144 550 130 236 633 107  23 621 435 380 108  61 343 330
 601 348 134 131 147 104   1 154 158 182 255 217 244   3 572 397 146 197
 202 216 280 439 331 522 121 616 103 623 498 187 148 463 371  80 436 176
 281 483 574 111 580 470 492 573 624 454  26 128 413 127 594 393 538 400
  51 382  98 113  93 599 384 539 629  15  16 168  47 590 350 352 402 269
 191 461 322 541]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5650
INFO voc_eval.py: 171: [ 33  20  63  60  92  27 152   6  69 145  77 113  99  19  39 117 150 110
  95 122 109 104  88   3  31  62  26 159  75 144  93  29  65  73  21 125
 120 151  89 108 147 112 148 103  68  10  59  14 119 154 134  11 118  28
 102  25  45 138  49 115  80  16  42  56 139 116 137 155   1  97  24  15
 153  96 128  79  41 126  55  87  76 101  84  35 135  66 142  91  81 114
 123 105  22   7 143   2  30  72  17  70  47  34  67  40 131 156  36  50
 136  86  85 124 157  74  61  23  83   4  98 158  64   8  58 130  44 106
  53 132  82  12  48  71  38 133 111 141 140  18  90  13   9 127 100 149
  37 129  32  51   5  52  78 146  46  94  43 107  54  57   0 121]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5941
INFO voc_eval.py: 171: [351 378 162 468  89 517 557 156 276  14 506 187 111 384  28 518  66  21
 313 247 254 410 227 497 248 496 413 286 519   4 272 510 211 417 465 406
 533 408 186 110 479 282 310 369 542 470 144 476 436 523  22  41 352  99
 285 502 319 318 100  72 385 107 246 213 280 304 241 288 225 574  60 328
 278 177 475 407  42 210 567  32 314 438 295 345 394 505 212  62 214  44
 490 201 296 199 292 113 189  81  64 441 253 439  73 544 338 235 140 464
 283 494 527 172  58 230 277 514 165  74 287 421 353 578  35 297 333 348
  17 244 290 206 393 350 552 388 478 131  52 509 306 239  40 299 257 145
 226 454 522 208 332 477 129 481 122 109 221 501 432 183 526 216   3 179
  45 184 559 132 429 251 207 480 493 215  63 245 362 289 103 236  92 383
 528 168 347 336 511 374  31 258 255 428 155 405 342 531 561 317 463 541
 256 301 443 461 364 504 209 204 564 556 366  90  69 309 482  20  67 444
 250 534 191 115 379 264 192 498  65 116 324 164 488  13 194 118 148  37
   8 457 243 166 138 180  84 171 367 569 422 516 197 121  30 508  56  29
  27  18 387 484 469 152 424 233 471 190 114 558  70  46 549 453  91  61
 403 157 431 392 142 169 536 433 229 560  48 330 323 326 521  53 486 401
 349 222 218 108 512 503 327 188 305 404 151 112 360 375 434 500 203 396
   2 205  12 325 128 524 160   9 483 242 281  36  19  83  93 489 416 451
 176 178 576 238 554  57 539 445 491 344  71 141 331  98 371 224  34 415
 450 291  43 198 515  85 442  49 529 167  82 565 163 466 449 553 150 321
 159 346 124 335 269  51 381  95 391 102 575   6 232  15 174  26 302 382
 260 577 136 357 473 427 161 170 117 193 231  39  78 185 234 456 543 359
 507 365 153 143 105 426  54 273 566 548  80 467 538 181 308 334  77 418
 303 551 540 311 532 340  23 355 437  38  24  33   5 130 106 320 440 547
 377 315 368 259  87 423 474 430 452 135 525 275 535 146 373  86 337 562
 546 412   1 262 123 316 223 202 240 158 356 270 358 274 520 530 261  97
 390 572 399 419  79 263 487 101 361 137  75 389 298 267  16 414 343 300
 370   7 460 402 555 200  76 485  68 196 120 127 571 363 217 458 182 237
 104 354 420 513 380  11 435 550 294 455 409 134 462 447  96 265 568 266
 573 570 139  10   0 472 126 397  25 386 499 492 322  59  55 195 459 563
  50 495 425 125 119 293 249 411 175 252 219 220 173 395 154 149 537  94
 228 147 329  88 133 341 279 446 448 400 284 376 372 339 307  47 398 271
 312 545 268]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4652
INFO voc_eval.py: 171: [ 6  2 13 51 14 46 23  4  0 49 37 48 29 39 19 21 27  9 16  7 50 10 28 12
 43 44 30 47 52 32 38 31  1 22 36 26 20 18 17 11 15 35 40 45 34 42 24 53
 33  5 41  8  3 25]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0048
INFO voc_eval.py: 171: [1944 1946  442 ... 1521 1817 1664]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5272
INFO voc_eval.py: 171: [ 3 33  4 63 25 50 26 20 57 68 73  5 23 46 71 32 74 17 40 62 34 52 29 70
 22 75 42  7 69 36 60 56 58 54  6 61 49 30 72 55 66 38 14 21 48 24 27 39
 31  9 11 53 16 28 65 35 43 18  8 67  0 44 19  1 13 12 10 37 41 15 59 45
 51 47  2 64]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2110
INFO voc_eval.py: 171: [223 491 121 213 427 241 308 315 294 316  63  45 437 190 225 367 521 176
 188 441 216  21 103  61  23 492 268 425 304 187 457   1 333 436 404 323
  79 515 364 522 345 453  83 145 547 221 403 109 319 470  49 247 330 246
 444 191 127 175 144 400 458 174 550 479 356 438 527 476 358  80  39 101
 339 420 210 278  34 342  32  12  36 347 374 484  48 463  18 514 336 149
 555 501  78 520  84 202 536  27 122 487  92 182 163 513 318 178 300 357
 385 245 365  71 156 332 531  44   0  16 545   5 123 541 309  52 146  20
 269 393  24 511 262  56 477 380 482 184 169 558 557 128 173 389 508 338
 410 376 129 535 499 431 291 256 355  14  65 464  76   7 186 525 200 311
 185 369 454 460 306 496 452 506 195 387 205 507 451 264 480 299 468 159
 446 524  35 360 183 337 324 170  17 126 289 414 151 110 552 141  93  74
 281 136 488  19  25 113  94 139 325 379  40 434 226 224 448 516 335 301
 474 219 405 288  85 440 189  77 391 544 418  89 274  41 117 131 107 283
 239 115 486 154  95 181  47 116 244 485 408 215 378 209 147 344  10  43
 455 153 540 125 370 351 354   6 523 212 240 465 293 135 204 248   4  51
 193 249 439 542 423  58 528 443 519 132 220 263 130  97 489 307 442 556
 211  82   2 510 502 383 292 361  87 148  26 512 538 353 120 419 549 198
 261 119   3 160 228 253 537  29 277 298 371 282 372 242 497 430 155 429
 331 237 398 100 297 285 462 265 254 396 546 359  28 236 490 302 206 529
  91 251 166 509  31 295 227  69 362 104 133 218 142 503 231 290 250 158
 252 199 417 134 384 296 498 326 255 517 471 321 402 203 310 493 432 532
 165 272 533 494 343 526 157 196 137  46 341 407 258 214 368 435 377  86
 350 415 118 260 416 450 426 473  75 352 238 257  54  57 229 518  62 467
 233 320 222 284  99 472  50 504 399 500 197  98 177 162  70 143 286  72
 235 553  38 530 413 433 466 230 475 138 534 102 167 456  88 112 412 194
 164   8 397 349 548 179 287 280 322 388 312 201 303 445   9  42 124 340
 422 276 152 192 334 234  68 386 554 208 271  66  81 394  59 317 243  73
  33 266 217 329 348 481  22  96 328 150 313 161  53 168 447 259 114 108
 270 314 382 390 483 106 395  55 180 421  15 428 461 373 207 459 478 551
 559  64 172 424 305 409  37 401 232  67 366  90 171 140 273  60  30  13
  11 105 543 279 539 327 346 363 381 392 406 411 449 505 275 111 375 469
 267 495]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3979
INFO voc_eval.py: 171: [ 94  51  77  46  12   1  49  24  52 105   2 106  17 103  20  28   0  26
  70  82  44  47  16  61  80  43   7  25  72  99  31  40 109 110 107  39
  22  15   9 108  14  96  21  75  32 102  42  76   6  83  97  69  18  88
  66  57  63  60  93  79  35  54   4  36  53  55  27  30  50   5  86  37
  95  10  92  67  23  73  89  29  19  64  90  48  11  65 111  78   8  84
  81   3  71  87  98  41  56  85  74  59  45 104  58  13  33  68 101  62
  91 112 100  38  34]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0746
INFO voc_eval.py: 171: [75  9 63 12 88 64 90 50 92 28 53 61 10 81 62 29 49 52 66 38 11 82 86 51
 76  4 83 41 57 33 67 20 45 21 85 27 15 89 19 25  5 23 74 18 35 65  7 22
 80  2 73 55 36 58 77 87 37 84 91 32  1 14 34 69 16 47 24  8 46  3  0 43
 26 39  6 60 13 42 68 17 30 93 54 59 72 70 48 40 78 56 79 44 71 31]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3881
INFO voc_eval.py: 171: [ 37 259  25 167 120 174 270 196 231  17  41  10 126 204 248   8 175 111
 193 189 265 100 112  62  69 247 222 192 180  55 228 135 168 163 113  18
 207 155  54 185 104  77  86 275  38 109  73 152  89 154 195  95 138 132
 147 142  70 218 208 177 206   1  78   7 122 278  68  31 268 271  83  99
 197  33 226 161 217 229  23 219 254 203  45 190 230  12  30 251   3  88
 223  42 134 129  93 117  72 105 149 262 162 188  52  43 209 269  19 227
  91  87  47 205  75 141 183 110 233 148 143 108 273  22 137 241 240 128
  44 166 187 124 255 280 264 215 249 173  94 266 118 199 281 282  20 276
  13  66  39  63  14 246 235 139 101  67  49   0 130 159 250  85 211 156
 267  53 170  15  65  56 272 242 146 165 253 198 261 164  36  98  80 184
 178 224  11  34 140 114  60 107 234 181 252  21 258  82 263 106 123  90
 236 225 151  29 125  92  61 186  71 214 160  51  28  57 202 179 158 201
   4 274 182  26 277  84  35 237 172 133 212 279 116  74 145 257 150 171
 245  27 256 238 210 121  46  50 157 244 216 169  48   6  97 239 153  81
  58 232 221 200 243 176 127 103  32   2   5  16  96 191  79 136 115 220
 102 131 194 144 260  76 119   9  24  59 213  40  64]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7340
INFO voc_eval.py: 171: [2396 2413 1504 ... 1110 2704 1616]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6201
INFO voc_eval.py: 171: [159 186 558 273 908 306 172 197 844 192 390 781  98 286  99   9   7 102
 377 238 913 847  66 123 544 477 101 591 727 288 532 736 310 620 512 103
 951 307 809 100 334 909 673 287 845 814 347 560 604 774 662 217 208 351
 215 912 828 619 454 125 738 843  73 841 182 740 607  81 923 212 277 911
 745 934  82 111 173 348 294 443 195 519 851 424 291 755 290  44 181 392
 530 633 925 622 666 834 336 129 479 735 525 237 107 408 538 104 154 120
 431 427 784 949 444 914 386 817 824 634 833 160 846 311 581 741 480 362
 737 585 832  56  15 671 137 293 690 890 533 849 210 446 549 850 739 891
 803 608 775 744  84 501 395 916 279 482 977 649 922 313 520 478   8 765
 548 460 748 561 540 980 743 611 621 670  22 628 709 819 810 370 461 179
 579 592 773 115 450 428 527  86 918 939  11 578  70 264 109 981 825 354
 445 280 836  45 226 371 298 868 953  60 106  47 214 350 113 397 782  85
 400 213 681 848 783 730 577 984 331 507 162 609 168 760 779 920 776  58
 940 456 648 821 684 638 240 455 641 562 497 612 584 639 132  21 410 442
 518 946 190 341 960 465 631 134 882 919 746 339   1 150 239 857 550 353
 108 278 888 979 799 886 174  76 625 588 488 935 510 335 674 837 885 295
 503 243 245 580 986 282 402 655 865 629 969 535  36 858 759 114  16 565
 724 414 892 338 387 623 566 689  38 193 895 534 391 429   6 289 418 563
 692 230 314 928 780  79 364 966 906 630 161 626 128 761 747 718 340 495
 317 440 146 815 680 467 222 349 227 202 171 900 771 489 700  93 368 917
 693 352 835 742 422 990 180 481 543 976 381 345 499 557 469 169 861 816
 315 853 965 223 767 229 756 574 269 667 978 792  31 117 304 719 359 255
 130 127 943 363 947 379 975 265 877 188 409 476  17 250 312 344 426 958
  83 513 421  40 247 301  23  75 768 216 697 688  25 433 521 708 305 720
 416 385 302 367 252 187 365 542 225 470 376 869  18 962 432 536 887 568
 907 185 459 750  43  53 516 961 860 205 842 733 704 453 163 624 854 451
 838 752 731 695  61  67 786 797 405  78 438 988 594 982 903 640 436 176
 866 458 375 299 804 554 878 967 983 985 957 437 517 728 203  14 553  37
 964 398 855 412 668 798 711 796 537 281 863 859 904 514 164 196 355   0
 852 963  88 177 831 777  27 300 308 384 149 328 926  33 937 382 897 751
 654 539 936 616 234  32 545 332 434 915 569 713 191 663 758 694 893 573
 954 552 439 636 763 924  90 659 500 457 200  77 415 571 664 726 524 807
 650 787 251 973 483 494 373 388 801   2 155 575 587 485  28 572 598 419
 184 931 635 233 677 492 319 175 822 599 880 333 152 526 839  62 647 644
 788 275 484 219 523 462 148 725 110 324 511 464 209 119 802 468 361 795
 256 826 613 710 606 872  13  46 875 241 372 879  94  30 318  87 823 141
 679 682 220 263 270 956 813 675  24 166 778 435  68 642 564 933 506 921
 864 441 653 716 126  51 757 905 131 596 883 151 346 714  89 595 605 491
  65 309 955 396 656 259 474 261 618 899 785 138 876 253 142 643 218 124
 987 254 158 266 498 910 811 582 246 274 657 493 228 637 399 342 393 551
 463  97 932 105 423 769 136 889 116 547 691  52 651 930  49 378 380 135
 425 358 323 615  64 242 707 447 862 508  12  54 221 283 567 676 357 472
 901 490 466 122 343 121  10 292 660 600 143 948 502  42 194 206   4 420
 522 702 829 665 870 830  74 678 627 337 764 721 471 325 257 258 201  63
 989   3 235 178 139  71 881  69 661 473  92 717 449  80 840 703 366 118
 144 207 303 856 145  20 646  48 531 789 374 762 586 970 699 487 873 871
  19 403 231 974 722 902 683 971 417 896 260 597 583 614 285 555 236 686
 369 156 165 754 244 972 211 952  91 330 705 529  57 715 356 576 753 790
 766 559 696 556 327  59 267 723 884 942 645 296 389 528 486   5 570 404
 941 430 321  50 297 772 183 224 712 603  95 133 276 475 701 452 950  96
 812 411 406 147 749 383 360  34 652 698 867 590 248 541 874 632 818 448
 687 320 672 959 729 284 112 968  55 509 734 515 153  41  29 601 669 794
  39 808 232 770 820 401 322 157 610 167 329 262 658 827  35 938 685 189
 204 898 706 249 326 496  26 589 617 268 793 546 800  72 791 407 271 272
 945 806 199 413 927 944 504 602 170 198 140 593 394 805 929 732 505 316
 894]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5934
INFO voc_eval.py: 171: [62 57  1 36  2 44 72 55 51 54  8 70 79 71 32 59  9 13 39 34 64  4 25 17
 42 76 27 40 35 58 65 12 69 68 14 41 24 37  5 67 29 38 16 26 49 60 43 18
 20 48 45 75 46 74 73 19 22 11 56 50 61 47 15 63  6 52  0 33 10 78 31 21
  7 28 77 53  3 66 30 23]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3068
INFO voc_eval.py: 171: [ 80  81  25 119 115  67 180 120  77 137 131  90  92   4   5  40  31  36
 116  58 126  63  85 158 143 163 169 105 100 103  91 182  32  48 127 141
  59 111 165  98  57 170 128 146  16  72  35  84 107 124 136 104  12 151
  71  47  23 176  56  27  39  30  69   8  20 108  51 117  17 160 130  87
  68  70  14  38 181  65 166  46 118 112  45  73 106 101  34 179 156  94
  62  60 133 150  83 132  95   2  42 113  11  44 167  13 138 122  52 139
 152  29 159  86 175   1  21 121  82  93 144  66 161 134 154 135  50  61
 129 168 155  18 172  76 157   9 173  88 110  75 123 164  37  49  55 174
  19 125 162  96 140 177 147  54  41   6   7 109 149  53  26  78  79  43
 178 142 145  15  28 102  24   3  33  89 148 153  64  22 171  74  99 114
  97  10   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3740
INFO voc_eval.py: 171: [136  84  19  15  66  40 118  30  98  38  70 169 171  99  41  94 181  13
  67 121  72 153 186  80   8 117  16 175 100  64 108  35  18  39 156 154
  25  89  50 114  44 122  28 110 128 150 160  22 129 126  90 187  86 184
  83  65  87 191 177 120 107  31  36  47 134 143  11 193  63 147  55  34
  61 127  79  91  59 163 152 166  96  14  74  42 188 145 164 105 172 178
 132 111  10 131   2 170 151  71  68 179   4 119 167 137 159 174  58 139
  93  85 189  26 135 123  12  24  52  17 194 162 115  48  33 138 130 101
  95   7 182 144  21  57  73 183 103  92  37 168 195 116 157  56 112 146
 124 142   3  53  49 113  32   5 141  82 102 106 133 180  46  29 158   0
  69   6  81 104   1  23  62 176 155  97  43 161 192 173  76  27  60 109
  78 140  88  45  75 148  51 125 185 165  20   9 190  54  77 149]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3799
INFO voc_eval.py: 171: [168  94 248 265 371   6 198 238  28 340 324 202 153  24  41  88 266 364
  55 349 273 380 131  45  31 261  72 310 249 203  80 246 226 212 237  12
 130 251 392 147 376  91 119  59 291 110 274  44  56   5 256 281 236 387
  69 331  62 172 260 133 365 271 352 205 268 257 239 178 288 206 289 196
 350  82  87 395  38 149  61 305 184  60  74 111 282 284 182  57 283 102
 319 393 290 359  52 366 143 201 351 255  76 311 157 163  90 129 388  30
 292 221  92  27 181 233 138 220  75  73 333  20 390 285 346 232 222 317
 362 210  58 223  42 361 151 294 193 161 254 106 262 336 356 104 124 165
 170  93 296  22 145  46  19 227  48  10 312 329  81 286  50 164 116 360
 353 396 197 269  29   7 194  63 247 267 135 244 314 160 341 174 334 188
 171 208 383 298 258  54 158 391 152 101  34 347  18 316  32 259  84 115
  83 225 339 297 370 200 109 358 379 264 250 142  85 214  49 338  26 128
 342 394  17 136 332 307  25 234 315 122  39 154  16 398  13 367 173  36
 313 363 231 219 191  99 108 321 156 185 357 241 114 326 159 146 372 280
 176 229  33 211 166 322 192 384  71 113 228   8 120 204 355 180 112  51
 378 382 167 144 230 287 199 306 123  96 302 304  47 381 389 374 243 187
 252  70  65 253 169  64   9 215 107 141 337 375 240 224 299 118 345 335
  98 139  15 303 105 301 134  68 140   3 162 323   4 148 179 189  14  53
  97 183 275  11 245  78 369 209   1  40 276 348 344 277 295 186 309 328
 386 175 293   0 270 263 354 213 195 117 377  35 150  79 242 126 127 132
 385 330 121 320 327 103 100 177  21 235 207 318 137  67 308 343 397  95
  77  86 278   2 325 218  89 272 125 217  23  37 216 373  66 368  43 300
 279 190 155]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4765
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4127
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.346
INFO cross_voc_dataset_evaluator.py: 134: 0.620
INFO cross_voc_dataset_evaluator.py: 134: 0.229
INFO cross_voc_dataset_evaluator.py: 134: 0.347
INFO cross_voc_dataset_evaluator.py: 134: 0.565
INFO cross_voc_dataset_evaluator.py: 134: 0.594
INFO cross_voc_dataset_evaluator.py: 134: 0.465
INFO cross_voc_dataset_evaluator.py: 134: 0.005
INFO cross_voc_dataset_evaluator.py: 134: 0.527
INFO cross_voc_dataset_evaluator.py: 134: 0.211
INFO cross_voc_dataset_evaluator.py: 134: 0.398
INFO cross_voc_dataset_evaluator.py: 134: 0.075
INFO cross_voc_dataset_evaluator.py: 134: 0.388
INFO cross_voc_dataset_evaluator.py: 134: 0.734
INFO cross_voc_dataset_evaluator.py: 134: 0.620
INFO cross_voc_dataset_evaluator.py: 134: 0.593
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.374
INFO cross_voc_dataset_evaluator.py: 134: 0.380
INFO cross_voc_dataset_evaluator.py: 134: 0.477
INFO cross_voc_dataset_evaluator.py: 135: 0.413
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.427s + 0.001s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.362s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.374s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.384s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.385s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.384s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.379s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.383s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.384s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.382s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.383s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.388s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.390s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.525s + 0.001s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.389s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.408s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.391s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.386s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.386s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.396s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.395s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.393s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.397s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.393s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.394s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.391s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.519s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.412s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.400s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.399s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.391s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.394s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.398s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.400s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.402s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.401s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.397s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.398s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.399s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.480s + 0.001s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.398s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.390s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.388s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.388s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.383s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.384s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.383s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.383s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.382s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.379s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.379s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.383s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.300s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [251 322  78   7 403  69 134  25 302 180  71  67  61 197 207 238 175 413
 244 252  60 427  66 253 208 116  62 449 240 435 445  29 125 183 360 168
   2 214 196 379 120  53 284  39 380 191 102 186 335 213  63 262 156 115
 148 365 241 404  80 220  14  95 339 395 443  38 211 126 279  27  68 336
 319  75 369  48 391 232  34   4 434 218 453  31 217 344  23 388 366 169
 293 184 139  73 437 422 291 425  32 248  43 376 320 233 405 258 192 341
 103 216   8 392  51 288 112  57 313 162 231 203 355 444 101 130 190 399
 451 187 278 390  85 363 415 300 298  59 230 158  28 459 257  22 151 127
 236  72 277 224 316 406 383 384 269 237 317 194 105  90 140  12  50 409
 430 285 206 452 349 274 260 456 146 118 234 350 381 289 157 179 337 167
 229 421 117   5 356 446 296  30 454 171 387 287 340 176 411 261   1 398
  82 439 324 195 106 178 198 447 358  35  10 301 368  46 271 401 290 438
 348 200 342 235 282  16 334 374 465 177 273  97  19 345   3 123 426 170
 330 292 323 397 267 136  88 424 150 144 359 219 375 128 166  13 332  83
 265  87 199 328 314 353 315  55 255 165  70 189  11  33 286  65  18 433
 346 309   6 281 272 129 362 143 351 108 352 210  58 370 201 205 373 464
 133 204 227  17 114 138 100  37 432 173 450 242 417  41 154 386 295 160
 202  21 418 428 276 137 109  56 327  98 372 394 333 407  92 402 225 172
 153 239 268 228  20 159  84 307 303 393 174 245 209 275 122 188 400 182
 419 385 378  64 113 163 382 135 193  79 325 326 367 111 440  26 104 149
 297 420 312 306  86 441 429 462 347  93 250  76  40 185 377  45 263 181
 458 457 463 223 304 338 396 461 215 416 222 152  96 256 254 243  47  99
 308 311 436 299 119 164 354 412 212 410 155 455 361  42 264 423  36 389
 107  44   9  54 259 266 124 270 121 294 448 145  15 142  24 318 460 247
 246   0  49 147  94 431  74 249 357 364 132 161 321 343 408 310 141 414
 226 283  52 331  77 221  91  89 442  81 305 110 131 280 371 329]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3481
INFO voc_eval.py: 171: [ 91 318  43 211 372 276 357 376 352 218 374 200  44  39 238 280 377 371
 356 338  29  56 157 293 277 378 165 358 267 365 104 251 367 138 206 163
  28 205 272 309 242  20 286  88 350  57  17 115 340 373 379 256 380 137
 231 384 304 134 336 146  72 281  71 126 315 312 279 347 158 228 216 331
 239 295 176 349 249  46 387 155 212 154 219 184 168 335 348 341 217 345
 263 210  99 207 135 310 394   5 391 241 183  58 370  93  14 170  47 332
  54  60 220 132 248  80  18 133 334  84 268 215 284  45 102 225 344 395
 342 202 156 227  25 118 201  50 388 383 307  42 316 145 258 343 209  62
 320 243 240  94 319 366 253 325 386 237 311 204 381 368 359 301 326  37
 180 351 226  19 250 382  75  32 305  38  98 185 122 143 124 195 308 324
  30 222 214 283 297 224  52 144 232  12 291 333 274 292 188 119  49 390
 322 275 290 208 194  10 294 203 136 107 233 339 257 113 354  27  15  51
 129  68  89 181 266   4  87 187 114 175 317  90 123 346  78 327 337  86
  85 186  64 353  36  33  48  53  21 289  97 273 300  76  55 128 196 229
 162 199 355 264 230   2 369 130 389   3 260 140 131 265  16 101 364 392
 111 298 254 179 375 169  82 190  74 234 288  59  96 109  69 221  34 103
 362 271 270 360  41 117 361 127  95 261  92 164 172 330 246 393 121 166
 213   1 197  66  40  73 285 396 160 255 313 173 269  77 299 125  70  24
 328 302 193 262 363  67 287 153 167  35   8 149 314 385 282 329  22 182
 259 223  61 112 192  26 321 151 148   6 141 142 247 244 106 152  65 159
 303 161 278 191 189 120 110 177 150 105 116 174 296 323  31 178 245   0
  81 236 235   9  11 108 100  13 306  79   7 198  83 139 147 171  63  23
 252]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6255
INFO voc_eval.py: 171: [103  43 125  74  76 113 121   1  75 256 236 196 255 257 238 152  38  95
 284   2 149  99 212  59 239  78  87 120  81 276 130  15 217 205 114 258
 270  97 279  62 170 126 174 260   6  20 297 111 231 277 211  96 296 222
 215 246 116 266  50 123 115 219 225  36 184 272 117 194 241 261 254 214
 218  88 189 122 294 141 250  22  48  14  65 151 108 245   8 259 267 145
 264  27  79  63 292  16 240 165 201 221  57 282  66 186  32 274 281 227
  69 177 172  19  73 271 146 148 134 247  11 286 202 179  45 252 244 204
  26 224 263  52 133 110 150 164 232 190 273 135   9 191 155 253  41 208
 262 118  80 278  61 104  40  31  71 105 175  37 178 268 138 188  42 100
 182 192 143 288 147 251  53  94 144  46  44 169  17 234  67 210  25 181
  49 173  70  77  54 195 249  24 183  12 275 163 153 159  30 119 187 235
 124 203 106  10 283 140  89  84 280 197 136  82  51   3 237 198 289   7
  91 180 157  18 109 220 223 176 156 167 129  92 168  90 233   0 137 230
  68   5  93 248  34 131 112 154  72 298 242  21  56  39 209 243 285  58
  83  29  85 269 291 229 226 216 158 139  28 207 160 193 185 287 102 206
  47 213  35 265 166  98  55  60   4 132 162 228 293  33  13 171 290  23
 295  64 142 101 200 107 128  86 161 199 127]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2271
INFO voc_eval.py: 171: [506 597 507 162   1 508  13  73 127 420 218  40 725 419 559  79 182 411
 554 715 637 585 417 181 305 418 405 426 509  14 316 217 185 421 640 724
 134  42 156 270 382  52  45 128 296 545 639 475 636 627 759 691 163 158
 495 187  32 257 808 425 339 598 543 393  93 314 621 285 629  94 727 515
 659 273 710 664 233 655 762 500 623 407 431 739  38 375 172 482 624  44
 610 755 766 477 740 582 699 343 423 196  47 738 673 567 297 735  56  41
 271 445 395 726 743 801 473 628 414 396 539 130 491 351 437 560 502  22
 145 486  89  57 776 436 660 544 269 392 219 681 138 460   4  43 606 207
 798 456 292  23 435 276 609 415 300 102 195  50 464 248  37 666 223 793
 261 147 447 459 164 434 498 289 728 682 184 169  65 454 186 385 765 771
 304 784 317  69 381 483 334 222 599   9 194 452 602 521 600 220 761 341
 111  67 347 685 422 566 709 612 783 373 662 394 484 717  78 357 476 265
 524 288 647 672 213 785 105 137 402 677 356 684 251 742 448 192 235 781
 577  63 754 372 429 202 108 658 427 398 191 141 322 751 370 729 756 360
 703 519 704 683 268 428 433 779 517 253 807  72 344 564 283 575 758 741
 642  77 522 363 674 527  25  49 553 649  61 736 594 193 354 556 252 165
  95 118 619 695 324   3  11 198 526  20 733 229 189 494 487 601  74 103
 123 790 154  62 399 511 465 611 299 716 263 337 441 676 537 721 369 225
  98 505 278 365  46 150 453 321  29 234 656 618 654 386 569 794 327 720
 561 378 787 707 485 633 349 589 694 503 551 438 492  92 345 675 442 792
 513 333 149 770  30 406 200 455 679 236 119 496 259 227 171 115 180  85
 678 190 731  83 665 692  34 157 230 580 232 772  87 439 478  96 451 588
 168 204 630 146 377 768 117 533 595 161 424  15 774 432 613  26 471 697
 620 603 693 310 131 151  70 298 488 160 237 562   2 542 340 607 258 805
 366 306  35 638 104 272 135 387  24  10 408 281 568 457 143 701 796 680
 284 555 446 705 641  53 224 397 221 750 753 470 390 558 800 116 576 634
 700 516  97  58 106 226 799  90 126 788 631 210 523 512 294 113 430 338
 383 650 215 167  51 352 139 722  81 211 579 689 775 107 541 592 501 563
 295 254 120 177 657 546 256 479 748 267 653 311 458 159 121 777 384 614
 319 616 760 663 552 404 769 291 110  91 152 175 205 287 648 531 332 231
 688 326 652 410 474 214  21 481 255 696  59 661 557 583 260 462 238 712
 380 391 723 791  99 203 586 124  60 201 570 548 669 617 240  39 101 364
 367  19 571 308 744 173 325 497 346 282 745 645 782 318 133 528 449 472
 443 463 625 286 279 174 136 803 587 312 183   7 535 228 206 510 698 199
 247 490 178 687 209 668 125 348 763 330  88  27  76 615  75  16 409 574
  17 208 323 804 353  28 388   8 622 241 795 307 730  82 719 591 536 764
 605 413 342 368 100  64 166 400 780 778  68 734 320 280 593  12 197 293
 718 239 786 212 529 530  33 112 713 690 746  31 122 142 362 309 797 635
 549 757 371 584 243 401 274 440 250 578 547 643 374 313 336 355 534 802
 525 651 538 489 480 450 532 376 581 329 520 686 596 747  80 249 359 702
 129 608 550 469  18 246 144 518 277  71 504  55   5 389 350 711 737 806
 646 732  66 773 109 467 573 266   0 708 301 303 331 358 412 461 264 176
 499 514 540 170 466 140 604 132 632  48  54 275 244 242 216 361 379  84
 416 444 667 767  86 179 403 262 153 188  36 789   6 809 290 302 706 749
 335 493 565 670 572 245 752 626 644 671 148 590 155 468 714 315 114 328]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3507
INFO voc_eval.py: 171: [640 364 641 328 642 331 372 643   6 547 646 327 329 371 404 778 455 777
   9 306 540 181 521 543 522 622 210 330 307 542 484 406 719 668 541 651
 705 545 644 280 653 679 571 659 647 387 645 663  59 714 536 661 671 436
 666 544 129 199 650 421 128 604 664 665 676 378 294 462  27 549 580  24
 449 675  39 673 737 463 752 260 374  58 240 318 558  78 257 465  98 400
 566 383 123 570  10 410 648 471 750 655 751 523 548 222 334 575 233 377
 605   7  77 392 277  80 546 283 667 586 132 139 198 453  50 577 335 266
 452 315 348 469   8 281 505 565 568 553 595 631 656 295 707 739 375 236
 389 539  28 326  12 658 576 702 346 499 674 654 411 721 598 781 590 228
 337 701 618 467 703 753 678 184 373 290 578 395 683 209 698 682  86 161
 550 258 554 728 464 297 405 589 382 333 584 420  25  89 552 687  42 332
 527 414  72 259 191 145 626 593 263 338 468  60 525 409  64 313 101 367
 212 379 354 620 497 557  26 535 296 561 639 563 398 774 607 434 412 693
 402  17 757 109 756 274 269 160 279 493 572 470 103 111 491 384 117 174
 685 716 489 723 343 766 474 270 380 418 530 310 475 271 508 488 632 351
  61 200 115 733 234 349 140 381 524  48 765 684 309 680 582   0 441 153
 706 600 386 738 118 461 155 472 182 219 247 427 301 102 135  85 376 252
 616 628 322 238 649  38 560 433 106 724  13 633 635 119  36 437 538  71
 697 748 514 597 304 170 105  37 344 341 587 507 124 288 629 760 317 249
 729 276 486 110  11 227 726 677 551  15 602 293 163 445 763 783  66 340
 220 691 731 562 216 442 308 594 785 466 358  57  92 179  83 734 479  93
 715 713 569 361 585 717 244 363 615 704 485 573 490 108 221 755 611 764
 509 311 205 624 722  51 229  79  62 512 300 208 369 444 173 157 353 166
 439 415 533 302 614 408 487 146 350 520  74 282 314 689 214 355 133 657
 388  82 513 435 393 529 758 660 167 347 448 770 696 272 321 186 339 183
 612  21  56 786 114  73 243 516 567 113 112 606 104 407 531 761 121   2
 164 695 158 617 131 564 579 357  30 264 613 362 688 150 241  45 503 440
 278 267 534  43 662 623 749 741 636 630 769 744 500 788 107 215 298  76
 747  14  84  65 759 223 596 511 177 458  33 473 708 359 286 323  29  99
 138 172 502 268 211 396 204 303 299 365 171 518 336 159   3 232 165 621
 694 162 681  41 285 196   1 352  91 492 743 710 187 403 141 610 366 390
 519 206 477 273 669 391 556 143 510  35 583 762 245 312 425 175 292 494
 625 720 291 136 652 168 746 254  49 154 188 195 169  68  81 670 413 151
 773 532 574 429 619 498 581 555 672 148 194 735 255 224 718 782 457 360
  34 246 559 320  53 368 481  19 127 454 447 385 130 459 122 771 588 251
 591 178 370  18 202 261 501 460 478 506 450 732 287 592 137 727 242 438
  63 686 134 399  67 526 730 451 120 638 417 144  88 156  47 305 325 740
 601 768 419 780 284 142 700 356 324 482 176 504 262 401 316 319 394 345
 779 256 476 239 237 784 226 225  90 423 515 537 690 289 190  69 235 185
  70  94 147  44 430 116  23 126 253 709 248 711 754 230 637 275 189 207
 627 608 603 431 180 397 197 496 456  87 692 599 149 231  32  95 424  20
  16 446 528 517  31 634 265 712 218 725 217 203 201  75 426 432  54 422
 100 787  96  40   4 480 250 483 495 742 699  97 776 775 443 772  22 152
 192 416 736 342 213  46  52 767 125 193   5 609  55 428 745]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5605
INFO voc_eval.py: 171: [ 37  21  76  73  28 112   6 185  82  92 178 138 123  45 144  20 135 134
 183 128   3  27 117 105 151  35  75 177  30 193  90 114 149  78  87 184
  14 181 156  22 133 109 127  71 147  81  10 180  29 137 187 166 126  26
 143  60  54  95  11 141 146 188  16  50  94  68 171  99 170 103 169  15
 121 157 174 159 120 186  25 125  49  67 111   1 129  39  23  31   2  91
  86 167 139   7  79 152  38  96 176  47 162 189  88  61   4  84 102  17
 101  40  57 190 163 122  85  64 192  77  80 155  53  44  74 161  18  24
  98   5 168 136 172 160   8 182  70 158  36  59 130 164  97  12   9 116
  42  13 179  62 110 118 119 140 115  93  65 191 100  56 124 173  43  41
  63  69  51  72 150  48  66  32   0  89 107 142  33 153 132  52  58 148
 113 145  46  34 106  55 175 131  83 104 108 154 165  19]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5883
INFO voc_eval.py: 171: [450 479 211 597 103 665 713 204 357  21 649 243 137 488  36 666  28  80
 398 318 326 292 526 319 639 633 529 367 667   5 351 655 272 521 535 523
 684 593 612 363 136 242 393 469 182 695 601 608 366  29 671 557 116  52
  86 451 644 274 117 403 404 361 489 317 369 132 359 522 607  74 387 232
 376 312 399  53 505 733 724 287 273 276 419 442 373  76 647  41 269 377
  56 559 563  95 258 139 245 325 560 625 260  78 223  87 431 675 630 364
  72 179 215 358 303 368 697 659 452 540 378  88 296 592 708 371 737 265
 315 611 447  23  44 380 162 636 426 493  65 310 389 504 581 183 449  50
 175 654 330 288 239 716  57 278 267 160 234 610 614 674 670 552 163 425
 445 643 283  77 134 277 219 151 323 316 370 305 266 613 487 548 565 331
   4 461 329 429 382 109 474 240 719 268 201 646 328 656 392 105 465 681
 589 615 694 123 677 144 463 250  83  40 566 685 147 225 253 520 722 321
  39 256 263 150 480  10 437 591 216 248  79 547 467  27 142  98  81 714
 653 619 585 338 235 402  46  20 712 599  69 222 662 726 314  37  59 188
 413 717 299 247 141 543 623 206 492 637  75 602 230  24 541 553 503 106
 422  66  61  35 280 518 516  84 550 645 196 244 138 688 133 580 294 412
 459 448 704 669 508 264 313 617  45 388 657 231   3 641 519  17 621 414
 567 735  26 475  71 193 362  97 415 372 444 110 710  12 262 691 233 483
 594 346  64 441 284 178  43 628 554 605 159 660 471 208  85 564 533 709
 214 218  54 333 672 385 428 624  96 227 112 298 209 534   8 424 650 307
 190  89 434 576 145  99 574 736 251 454 352 734 153  67 478 456 575 286
 220 115 678 501 172 696 236 458 558 355 702 386  30 409 595 177  62 683
  94 486 400 180 120 184 257   7 391 427 583  34 707 197 464  42 546 703
  49  92 332 129 693 241 301 210  47 130 297 408 101 542 536 336 261 673
 311 720 430 723 396 468  31 562 379 549 537 207 692 167 401 606 545 100
  93 161 255 631 152 149 528 354 524 416 457 481 114 473 375 285 279 588
 570 381 577   1 118 700 680 530 455 173 259 515 582 440 725 453 254 731
 721 148 509 640 686 658  90 571 668  82 460 224 620 470 706  91 497  68
 347 320 544 166 729 603 156  33 476  63 111 238 462 343 337 334 732 365
 622 507 113 324  14 498 228  58 341 102 360 108 590 586   9 569   0  60
 472 539 587 598 383 374  73 485 348 629 494  13 200 632 433 185 126 730
 510 176 711 164  38 124 573 527 146 477 189 198 511  22 339 252 555 604
 432 293 168 616 638 513 281 635 532 491 121   6 306 420 682 390 411 556
 496 609 213 423 642 327 436 705 568 302 304 195 158 155 435 205 345 199
 295 690 698 514   2 131 127 438 572  70 410 217 676 202 282 104  55 170
 538 349 135 271 171 237 122 384 194 418 466 506 107 517 551 181 309 290
 443 322 525  11 500 203 405 584 275 191 689 600 174 140 246 499 350 125
 648  16 394  19 482 490 291  25 344 618 340  32 417 356 502 512 186 664
 727 728  18 531 407 561 701 353 679 289 165 229 157 421 221 119 718 397
 699 439 212 128 308 192 687 187  15 626 446 627 154 579 406 663 300  51
 484 652  48 661 270 715 249 335 143 395 169 634 596 495 578 226 651 342]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4674
INFO voc_eval.py: 171: [ 7  3 14 59 15 52 27  5 57  0 55 42 34 44 24 21 32 10 11 18  8 58 13 49
 33 48 54 35 43 60 37  2 36 41 26 20 31 19 40 22 12 47 39 29 45 50 61 17
  4 38 56  9 30 53  6 46 25 23 16 51 28  1]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0051
INFO voc_eval.py: 171: [2492 2494  259 ...  174 1533 1238]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5279
INFO voc_eval.py: 171: [ 3 37  4 69 27 56 28 21 63  5 74 79 25 52 77 36 80 18 46 38 68 58 32 76
 24 82 48  7 62 33 75 42 66 64 60  6 67 72 55 61 78 23 44 54 26 15 29 12
 31 71 59 35 49 10 45 39 17  8 14  1 19 20 50 43 73  0 11 40 57 51 81 13
 65 47 70 34 16  2  9 30 53 22 41]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2061
INFO voc_eval.py: 171: [302 675 166 290 582 324 412 394 423  87 424  62 598 304 261 717 489 238
 294  30 259 139 603 406  83 677 578 597   1  32 626 255 365 432 447 711
 111 486 461 545 113 757 543 196 617 718 428 300 146  65 641 331 262 330
 606 175 627 536 195 236 760 444 235 112 454 600 656 137 286 378 476  53
  49 727 692  45  43 458 451 646 478 666 500 463  26 570 120 740 276  64
 670 201 633  38 248 240 218 716 329 427 709  15 477 401 766  99 125 110
 487 210 708 109  60   6 747 167  23 514 197 446 526 168  71   0 706 355
 768 648 177 507 227 555 414 366 739 502  33 753 690 251  29 731 702 588
 618   9 176 417  89 493 254 272 391 346 280 147 769  19  48  77 520 106
 250 664 453 204 475 634 725 699 630 480 410 700 682 228 268 399 658 359
 234 639 214 562 190  25 594 104 450 616 174 762 517 115 298 402 185 609
 253 611  27 389 723  54 614  35 116 383 506 435 433 127 179 452 107 157
 712 323 129 151 388  55 207 524 669 189 327 245  63 469 260 305 671 566
  68 293 289 547 474 206  12 601 751 303 744 668 285 505   2 373 728 622
 130  79 299 184 602 382   8 178 715 279 155 604   4 322 645 198 143 180
 510 377 287 308 332 156 741 722 568 356 576 495 393 172 705 162  37 133
 693 460 473 684 673 161 208 360 742 635 392  59   3 411 607 551 481 533
 281 703 119  42 270 759 353 632 334 729 674  40 404 339 318 587 319 531
 312 381 200 748 496 595 192 396 271 596  96 589 215 181 136 277 187 118
  39 395 398 337 211 679 306 415 335 445 397 264 678 221 497 511 390 733
 585 325 470 437 504 694 644 292 755 267 565 182 341 430 707 105 734 159
 315 456 758 124 205  74 549 726 534 482 713 459 642 348 563 301 685 296
 479  85 558  81 384  66 643 559 467 386 194 370 140  11 265 429 349 309
 636  90  10 490 317 239 763 213  67 217 579 222 122 284 220 351  78 696
 141 135 380 405 344 387 448 468 243 613 338 637 311 138 101 442 117 530
 465 564 647  56 714 170 462 519 761 624  72 655 542  31 316 134 612 123
 273 320 350 376  82  93 363 574 103 730  16 628 216 592  97 426 743 756
  52 532  44 237 231 691 229 557 150 333 369  18 573 203 538 219 745 142
 372 525 591 498 660 659 439 680 367 485 364 266 148 665  14 522 374 421
 186  94 572 488  22 767 144 681 491 552 676 314 509 242 537  88 615  46
 638 188 508  75 752 347 575 738 418 554 472 515   7 503 431 379  47 610
 132  41 149 160 577 295 697 441 556 269 438 345 413 408 698 623 629 434
 416 583  51 400 653 202 667 100 326 548 275 249 721 652 683 735 770 283
 407 403 375 313 282 274 126 193  98  69 501 153 719 436 710 223 233  61
  57 661  28  24  21 121 593 523 569 368 455 256 358 171 527 720 158 342
 749 571 581 765 640 278 263 354 651 357 443 385 209 484 625 754 199 165
 257 152 310 336 128  50 704 619 492 471 746 483 464 154 226 701 750  20
 512 732 649 553  92 663 529  17 102  86  34 695 440 528 422 688 224 657
 449 409  95 662 672 608 737 244 736 164 343 550  91  36 307 297 620 580
 288 584 516 212 183 687 246 425 586 544 561 499   5 513 163 567  73  70
 247 232 535 518 546 230 621 689 352 466 654 362 340 328 321 590 540 419
 541 420 560 631  13  76  58 371 191 252 291  80 457 605 173 258 169 650
 539 131 361 114 108 724 686 764  84 241 599 145 494 521 225]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4039
INFO voc_eval.py: 171: [110  51  57  90  13   1  55  28  58 120 123   2  19  23 119  32  30   0
  79  95  48  52  18  69  93   7  29  47  81  44  35 115  43  25 126 127
  17 124 112  26  10  84   6  36 125 118  46  16  20  24  96 113 108  85
  78  72  92  75  66  39 102  63   4  31 100  56  40   5  60  34  61  59
  91  22  76  41  11 128  73 106  33 103  82  27 107  80  45  74   8 104
 114   3  12  98  54  94 111  99  70 117 122 101  83  37  77  14  64  62
  49  65  42 116 105 129 109  89  38  15  86  71  68  67  88  50 121  97
  53  21   9  87]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0647
INFO voc_eval.py: 171: [ 84  11  70  15  99  71 102  54 104  32  67  58  90  12  69  33  53  42
  73  13  57  85  55  91  97   5  92  76  45  37  62  18  24 100  96  49
  25  31  23   6  27  29  39  83  22  72  60  89  86  40   2  98  82   8
  26  63  19  17   1  28  41 103  95  50  78  38  36  51  87   9  43  77
  61  21  16   0 105   3  88  30  34  47  59  44  56  46  48  52  66  35
  74  81   7  64  94  79 101   4  80  75  93  14  20  10  65  68]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3901
INFO voc_eval.py: 171: [ 39 296  27 184 132 312 192 221 262  18  44  10 140 229 282   9 120 217
 193 212 304 124 108  66 281  75 215 251 199 149 204 125 258  58 185 179
  19  83  40 233  57 170 317 113  94  79 167 121 169  97 103 246 220  76
 151 161 146 320  84 156 313 232  33 235  73 195   1 255   8  24 135 248
 106 260 213 288 307  91 177   4 222 228 245  35  12 285 129  78 252  96
  32 261 300 143  48 178  46 256  45  81 164  22 265 148 211 114 101  99
 315 150  95  20 310  47  50 234  55 230 155 208 157 323 202 163 102 290
 119 118 272  41 318 137 273 130 283 183 303 142 327  13  21  15 280  71
 243 190 305  72  93  52 109 187  67 325 224 298  70  16 284 152 306   0
 267 175 314  23  11 144 153  86 182 253 287 196 126 274 247  28 223 200
 266  56 203  64 286 301 319 154 254 171  59  54  36  38  31 100 321 160
  80 197 136  98 128 181 159 201  51  37 295 237   5  30 139 117  90 174
 166 238 294 194 249 240 186  77 263 316 206 269 214  53  60 107 268  65
 271 293 147 115 176  82  29 227 165 134  92 226 168  42  17  61  26 299
   7 189 244  25 158 219 112   6 277 145  89 188 311 180 236 225  43 275
 210   3 297 270  69  85  34 110 309  49 198 292 276 264 291 239 242 250
 105 289 127 162 173 322  62 172 259 111 191 218 141 131 326 324 216 241
 278 116 207   2  68  63 302  88 279 104 122 205 231  87  74  14 308 123
 209 133 257 138]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7307
INFO voc_eval.py: 171: [3128 3153  284 ... 1022 1599  515]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6222
INFO voc_eval.py: 171: [187 235 714 ... 246 588 720]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5887
INFO voc_eval.py: 171: [71  1 66 41  2 48 82 63 58 62  9 79 89 81 36 68 14 43 10 39 73 27  4 46
 18 86 44 67 74 40 30 15 13 45 77 26 78 42 56  5 28 76 49 69 33 17 47 22
 85 84 53 21 83 19 50 64 70 24 12 51 16  6 57 72 88 52 87 37 31 35 11  0
 34 60 75  7 20 65 61  3 23 25  8 90 55 32 38 80 54 59 29]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3210
INFO voc_eval.py: 171: [ 93  94  31 142 135 215  77 143  88 105 154   4 107 162   5  45  69  36
 149 136  41 187 106 171 203  72 118 195 122  98 124  97 219  37 130 204
 168  55  67 197  20 126  65 174  33 116 123 150 100  52  81  28 151 153
  82  64  40  16 161  44 147  79  63  80  58 179  11  43  24  78 218 138
 141  96 211 214  51  74  68 127 163 190  21  35 131 125 156  50  84 112
 198  95  18  99  49 199  25 180 113 178 144 133 172  76  71 210  47 157
 119  57 110  14   2 159  39  87  59 186 185  34 165  70  17 152 182 160
   1 183  86 145 146 188 208 200 177 207 196 191 175  56  61  32  10  62
 193  60 114  12  23  22 148 170 167 194 212 213 129  89  48  92  13 102
 121 155 173  38 176 108 134 104  42 128 137   9   3 181  30 201 103 209
  46  29  26 115 140 217  73 101 158  53   6  83  85  19 117  91 189   8
 184 202 206 111 120 216 139 169 192 166   0  66  54 205 109 164 132  90
  75  27  15   7]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3787
INFO voc_eval.py: 171: [174 109  26  22  56  90 151  41 127  54  94 128 216 219  57 234  19 123
 155  91  23  13 195  97  48 150 240 138 129  25 105 227  55  87 200 197
  70  36  39 142 118 163 113  32 147  62 112 119 204 164 108 238 137 154
 191  43  88 161  47 241  82 246 248  86  66  75 229  58 120 170  79  20
 188 184  99 125  17 104  50 162 207 210 217 167 135  92 242 175 208 194
   2 152  16 230 186 203 110  24 226 166   5 244 193 221 144  78 214 173
  18  96 179  35  45 232 249 165 130 122  67 148  53  37 206  72  98 124
 201  77  68  44  14   3 181  30 146 156 237 202 121 185 215 177  73 236
 149   0 145  59   9 131 187  83  40  12  76  93 180 133   1 136 247  80
 169 106 140 239 250 223 205 158 178 228 134 189 198 103 114 107 102 245
 224  71  63 233 143 182 171 132  34  42  95  74 141 126  21 159 218  64
 100  38 172 222 101  89 116 220  61  84 117   6 192  15 209   8 225  65
 153  85 231  10 111 211  52 139  33   4  81   7  31  49 157  51  60 176
 243 168 212 190 183 235  29 160  69  46 196  11  27  28 115 213 199]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3989
INFO voc_eval.py: 171: [121 218 318 468 336   7 254 305  33 428 409 261 197  50  29 114 337 460
  73 345 441 479 170  54  36 332 392  92 319 262 105 314 289 304  15 272
 169 189 321 474 494  72 117  71 154 366 142 346  68  53   6 223  76  85
 326 417 303 173 343 339 330 356 461 489 264 306 444 265 363 230 192 442
 252 500 357  47 143 386 327 237  75 113 108 130 364  95 234 359 462  69
 497  62 167 403 185 258 443 365  35 325 283 393 358  99 116 368 394 455
 119 201 212 492  93 299 360  70  23 233  96  32 398 419  51 207 436 370
 285 249 195 270 284 451 324 457 187 458 333 179 282 120 297 133 399 220
 456  25 415 341  57 424 290  55 137  34   8  22  13 315  59 372 213 106
 361 214  77 491 446 225 149  66 267 160 253 175 395 501 420 250 397 222
 338 328 206 148 484 493 329 109 375  38 202 288 274 437 129 312 429 373
 242 176 110 257 196 320  18  81 335  31  58 224  30 418 111 184 431 141
 478 127 411 467 396  41 427 300  21 188 166 454  16 426 459 157 463  48
 307 506 499 387 147 247  19   9 140 239 246 186 198 469 146 406 296 483
  45 205 227 158 485 448 200 311 355 383  60 490 407 190  80  78 138 293
  20 323 381  89 241 125 477 215  86 134  40 281  12 309  56 452 322 362
  79 123 209 255 263 144 271 481   4 488 421 473 155 295 102 183 232 217
 286 292 439 219 231 153  14 371 182 472 244 476 235   5 347 385  65 377
 269 447  49 174 434 405 465  84 408 382 313 124 251 342 180 308 486   1
 353 425 369 165 275 238 101 156 351 273 380 400 391 236 352  46 310 416
 266 498 301 410 177 128 132 464 171 104 194 413 135  26  17 298 374 229
 245 505 145 278 139  52 122 433 115  83 103 404 402   3  87 163   0 340
 450 226 349  82 414 445 221 118 422 164 211 291  37  27  24 503 151 334
 390 240 208 152 412 277  61 449 423 495 302 279 193 168  94 430 432 466
 126  28 112 280  98 287 161 367 344  44  90  39 475 260 401 438 504 471
  43 480 389 203 487   2 317 496  88 376 378 348 350 388 482 379 100 107
 248 294  11 256 178 331  10 316 268 162 453 204 136 470 276  64 354 210
 259 131 199 502 435  74  97  91 150 159 243  67 181 228 440  42 191 216
 384  63 172]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4809
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4143
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.348
INFO cross_voc_dataset_evaluator.py: 134: 0.626
INFO cross_voc_dataset_evaluator.py: 134: 0.227
INFO cross_voc_dataset_evaluator.py: 134: 0.351
INFO cross_voc_dataset_evaluator.py: 134: 0.561
INFO cross_voc_dataset_evaluator.py: 134: 0.588
INFO cross_voc_dataset_evaluator.py: 134: 0.467
INFO cross_voc_dataset_evaluator.py: 134: 0.005
INFO cross_voc_dataset_evaluator.py: 134: 0.528
INFO cross_voc_dataset_evaluator.py: 134: 0.206
INFO cross_voc_dataset_evaluator.py: 134: 0.404
INFO cross_voc_dataset_evaluator.py: 134: 0.065
INFO cross_voc_dataset_evaluator.py: 134: 0.390
INFO cross_voc_dataset_evaluator.py: 134: 0.731
INFO cross_voc_dataset_evaluator.py: 134: 0.622
INFO cross_voc_dataset_evaluator.py: 134: 0.589
INFO cross_voc_dataset_evaluator.py: 134: 0.321
INFO cross_voc_dataset_evaluator.py: 134: 0.379
INFO cross_voc_dataset_evaluator.py: 134: 0.399
INFO cross_voc_dataset_evaluator.py: 134: 0.481
INFO cross_voc_dataset_evaluator.py: 135: 0.414
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.556s + 0.002s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.390s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.393s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.386s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.392s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.397s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.392s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.396s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.394s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.392s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.393s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.395s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.395s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.571s + 0.001s (eta: 0:01:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.406s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.408s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.407s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.407s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.406s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.409s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.408s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.404s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.402s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.399s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.398s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.394s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.424s + 0.002s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.354s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.394s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.394s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.387s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.383s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.386s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.388s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.387s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.385s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.383s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.383s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.383s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.556s + 0.001s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.406s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.402s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.399s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.393s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.395s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.399s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.396s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.398s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.396s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.392s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.390s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.388s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.017s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [303  93 391  10 491  84 162 217 370  31  86  82  76 253 240 212 289 295
 504 304  75  81 522 550 306  77 291 140 254  35 531 150 260 545 220 436
  66 239 205 145  48   2 461 346 231 123 316  78 259 462 492 408 292 226
 139  95 191 180 266 257 443 340  17 412 151  83 115  33 409 450 477 280
 542 482  47 263  90   5 389  37 264 474  88 224  61  42 530 494 206  38
  54 418 358  29 555 518 444 167 383 356 390 262  11  64 478 431 281 415
 155 534 102 458 352 136 515 300 125 233 230 122  34 310 339 367 270 553
 506 487 476 364 184 496 439 247 279  71 200 543  28  87 127 235 227 314
 562 152 287 194 277 554  63 465  15 108 333 282 386 216  74 468 142 192
 326 447 500 337  36   6 387 350 394 168 410 348 547 514 413 288 252 141
 546 213 463 526 423 178  98 361 557 369 305 425   1 215  58 128 236 432
 519 243 473 344 559  13 276 416 117 208 354 355 393 536 183 204 486  43
 535 457  99 214 407 315 148 485 517 456 489 419 284 265 105  19 153 434
 357 571   9 164  22 349 207 329 308 173 311 422 420 241   4  14 106 399
 324  68 528 321  16 435 402 331  85  72 202 385 203  21 426 274 405 508
 154 210 242  80 229 256 498 171 378  45 384 244 293 552 521 197 455 490
  27 187 438 428 427 132 138 343 371  69 196 570 330 249  41 100 296 137
 248 529 211 469 395 451 396 133 147 334 255 163 471 511 181 454 336  94
 446 290  20 110 209 161 397 363 119 272  32  91 121  24  57 406 360 464
 382 479 112 525 166  50 312 126 268 234 219 201  79 365 218 488 246 421
 294 411 480 225 325 261 460 499 186 258 565 566 483 135 269  89 275 532
 309 165 475 507  92  12 319 376 381 548 146 104 359  60 375 400 567 430
 392 228 149 198 539 540  49 520  96 160 512 175  30 564 320 143 267 169
 509 379 516 131  55 297 524 318 561  18 527 558 497  44 560 113 433 109
 513 323 179 134 377 505 157  67 302 107 502 459  70 373 495 185 238 130
 437 114   8 103 493  25 299  51   0 120 156 544 159 442 388 417 124   7
 116 307 301  26 273  73 541 380 188 232 285 533 572 429  59   3  23 170
 404  46 448 322 278  65 195 328 313 452 176 335 158 549  53 563 347 351
 345 362 401  56 245  62 193 190 445 118 538 177 283 286 341 470 569 441
 424  39 111 353 366 342 568 449 182 440  52 298 174 332 221 374 501 372
 250 503  97 466 551 144 414 556 403 223 472 189 101  40 481 172 537 199
 222 237 251 317 129 484 510 453 467 327 368 338 523 398 271]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3482
INFO voc_eval.py: 171: [115 385  57 260 444 337 428 422 448 267 446 249  58  48 341 449 443 292
 427  72 407  34 199 452 338 357 209 307 429 327 132 436 255 296  33 438
 254 207  24 374 176 149 112 332  73 445 453  21 454 350 420 370 175 410
 284 172 342 313 405 381 163  91  92 458 417 340 359 201 186 279 265 293
 419 377  60 461 230 305 399 261 222 418 197 404 259 295 415 411 266 256
  62 268 195 375 322 125 229 213 117 442 466 328 345 173 470  74  65 106
 400   7 102 387 386 372 153 215 251 198 414 412 472 294 403 170  18  59
 250  76 382  22  69 413  29 118 185 304 264 462 309 269 130 278 257 437
 316 376 253  78  47  54 275  35 344 430 394  96 457  23 161 297 200 306
 232 463 182 393 460  64 456 455 285 124 314 258 362 168 263 392 184 291
 226 355 174  14 252  39  32 409  67  16 390 154 439 286 274 371 421 336
 276 366  66 354 148  46 424 244 227 145 356 272 373 334 401 158 425 157
 113  80  63  68   5 384 416 468   6 138 114  88 236 441  45 406 324 235
 283 243 435 246 433 282 166 221 107 358  19 426  95 469 167 325 310 122
 326 365 447 353 179 287  82 110 329 206  38  20  75 121 108 165 431 389
 434 395 333 363  99  25 302 378 119 465 339 352 432 262 233 194  83 162
  90 104 143  42 210  13 245  70 214   4 360 367  53  94 369 131 151 318
 364 141 156  52 116 343   2 331 330  97 188  77 203 319 241 379 150 193
 351 189 459 187  26 238 128 208 135 391 288 473 225  15 320 224 271 248
  36  44  89  98 273 212 127 180 368 240 192 239 220 204 160 298 397 398
 164 105 181 396 216 218 347 380 155 142  86 223   0 281 321 423 126 103
 101 311  71  17 312 147 219   8 471 217 159 133 120 317  28  11 146   1
 299 228 134 137  10  87   9  50  27  93 100 323 211 202 237 361  61 205
 169 270 308 144  84  81 247  49 280 177 300 191 303 196  30 183 315  43
  56  41 109 301 349   3 234 123 139  51 408 450 231 346 402 129 290 388
 467 140 464  12 383  31 171  37  79 277 178 451 289 136 348 335 152 242
 440 190  40  55  85 111]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6287
INFO voc_eval.py: 171: [137  56 166 102 104 162 150   1 103 327 326 302 257 328 306 204  50 124
 275 201   2 359 130 304  80 106 116 109 161 348 267 176 282 329  17 151
 341 127 167 331  84 353   6 231 227 147 273 279  23 125 372 337 349 373
 295 153 286 332 164 317 152 343 157 255 277  46  67 289 281 283 243 325
 117 370 143 203 163  26 316   8 338 330  87 250 321  65 191  15 368 335
 307 107 196  88  18 180  34 262 356 197 345  76 199 246 342 222  85 285
 179 361  60  32 334 355  41  69 145 100 207  95 263 202 234 333   9  11
 344 229 266 251 291 181 305 298  54 108  22 138 252  39 339  55 352 318
 315  53 236  83 288 139 324  47 323 131 235 220  98 232 188 364  57  70
 198 239  89 270 300 159 105  71 195  66 193 272 249 322  31 123  63  29
 118 238  10 141 110 253   7 113 226 240  19 365 320 230 120  38 205 183
 144 258 256  68   3 165 190 206 284 347 259  96 149 264 217  12 303  90
  20   4 186 357 122 247 212 119 160 121 112 336 210  43 354 175  36 287
 301 310 237 214 233 142 225 319 271 269  52 224  81 308  99 340 367  45
 177 366  24 244 268 294 189  79 208 360   0 192 223 215 216 209  86 299
 261  75 293 111  59 171 134  35 362 292 374 280  73 254 170 132 148 312
 174 184  49 178   5 274 228 172  14 129 185  42 290  37  30 114 115 276
  72 168  64 158 371  77 128 221  28 211 133 146  62  51 242 156  97 101
 218 126 358  82  27 369 278 241 297 169 194 313  44  40 260 182  91 350
 363 155 219  92  93 245  21  33 200 248 136 187 140 154  61 173 346  25
 314 351  74 311 309  78  16  94  58  48 265 296 135 213  13]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2289
INFO voc_eval.py: 171: [639 748 640 ... 921 760 534]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3510
INFO voc_eval.py: 171: [756 445 757 402 758 405 454 759   8 658 762 453 403 401 491  11 912 911
 654 555 376 221 653 630 631 734 258 404 377 652 493 587 843 783 651 826
 656 760 767 682 346 769 794 763 793 471 761 647 777 837  72 776 525 786
 244 781 655 508 715 165 362 766 461 790 779 780 164 563  33  29 661 692
 544 789  49 564 865 389 456 297 882 157  98 669 677 466 681  71  12 566
 122 321 486 764 497 880 659 572 318 771   9 660 716 276 881 633 408 460
 100 687 289 342  97 477 657 243 697 782 349 176 424 689 386 330 423  10
  61 548 676 168 547 679 611 570 706 347  14 457 688 292 363 473 710 421
  34 867 498 846 410 828 772 821 400 605 916 227 770 701 788 822 455 820
 257 792 481 729 690 568 774 799 665 883 283 798 650 365 700 407 744 565
 814 855 695 106 465 199 662 357 406 664 803 637 501 319  30 411 109  52
 235 635 496 739 492 384 705 507 449  89 320 732 462 668  77 432 126  32
 603  73 260 684 569 674 324 499 718  21 339 672 364 523 908 182 887 344
 487 128 683 418 467 597 198 801 640 850 886 143 463 214 571 141 334 381
 428 591 594 592 575 245 336 425 290 645 576 615 754 335 505 897 860 151
 484 271 634 464 839 694 530  74 149 896 800 306 562  48 712 649 741  59
 459 222 395 470 727 795 827 866 380 745 671   1 765 171 295 177 877 152
 573 105 190 158 192 851 132 127 515 526  15 812 742  47 369 708  46 698
 130 416 153 522 419  13 373 614  88 210 748  19 663 355 532 308 856 273
 713 589 378 388 431 890 853 437 415 282 807 673 746 796 920 791 536 825
 567 142 918 267 201 893 312 219 680 139  79 838 862 341  70 622 361 250
 840 895 113 443 726 115 858 103 885 593 303 685 535 382 254 588 696 629
 736 836 849 440 616 720 581 430 643 590  62 433 213 621 368  99 275 528
 773 472 639 502 385 888  75 284 183 724 478 620 524 412  69 778 337 426
 805 206 775 904  94 494 811 371 263 207 348 422 435 721 102 625  92 441
 495 225 641 717 543 196 203 394 691 921 678 923  38  26 195 902 326 129
 147 302 829 144 146 722 891 606 609 529 675 804 169  54  16 728 810 133
  53 155 366 229 396 735 299 343 104 429 878   5 618 743  36 175 628 871
  96 869 200 211 872 447 451 332 409 259 707 749 574 889 167 205  43 123
 266 448  78 832 331 241 719 784   3 797 197 249 191 372 438 627 253 287
 733 617 809  51 595 204 338 111   4 667 353 251 475 892 178 500 208 737
 598 731 785 490 305 768 485  82 439 351 304 604 579 209 188 277 642 557
 450 686 693 359 311 187 907  64 612 391 172 469 383 180 185 360 608 215
 636 875 558 644 367 278 607 350 845 876  45 239 513 162 917 217  58 247
 101 863  23 802 316 228 842 238  44 231 859 914 315 518 854 527 610 480
 585 560 702 482 584 179 108  22 156 296 322 387 545 670 666 704 901 300
 166 399 506 354 170  28  60 154 291 751 230 648 884  80 434 194 504 714
 476 174 561 483 242 216 420 181 116 546 740 905 317 397 750 919  76 868
 375 857 787 816 830 577 510  20 285 323 248 922 390  39 488 184 711 512
 281 539 356 552 218 236 873 806 835 280  87 327 110 520 808  83 186 340
 252 220 638  27 246 124  41   6 699  95 237 582 160  63 233 452 815  65
  18 725 602  17 599 286  57  31 234 913 261 417 709 121  24 256 864 274
  50 328 414   7 269 468 723 503 910 120 446 533 118 623 307 313 703 310
 813  68 509 899 915  42 580 294 398 521 150 852 135  56  85 847 392 898
 747 542 534 586  86 125 531 329 844 596  35  55 436 224 223 161 738 818
 114 112 900 861  25 427 107 538 226 906  81 583 833 272 519 514  90 458
 894 831   0 730 550 474  84 537 358 309 189 193  37 755 159 556  40 265
 131 870 489 270 817 279 600 325 240 874 613 314 541 624 148 145 834  91
 517 511 553 752 601 393  67 909 540 333 442 559   2 516 379 626 212 288
 632 140 202 824  93 413 134 138 879 549 551 554 352 298 262 232 841 119
 301 374 370 646 117 848 903 819 264 823 255 619 444 293 479 345 137 268
 753 578 163 173 136  66]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5575
INFO voc_eval.py: 171: [ 49  25  88  91  34   6 136 224  98 110 171 216  57 151 176   3  32 166
  23 165 159 126 221  46 142  90 215 185  39 108  17 234 183 219 138 222
  86 191 164  93 158 104 179  37 132  12  96  26 201 175 156  74 227  31
 120 115 218 169 112  67 124 173 228  13 212  19  82  62 207 178  18 134
 206 153 194 192  81  61 160 205 147  40 225 103   2  27 148  30  50   8
 187 170  51   1 197   4  59 105 230 203 116 214  94  75 109 101 123 198
 195 122  78 231 149 100   5  52  56  20 233  92  65  21  71 167 208 196
 220 190  89  47 193  95 118  28 232 141 139 143  10 145  84 204 172 130
 121 217  79  73  55  66  87  76  72  70 199 174  54 111 107  15 133  97
  14 117  22 161  99 188  53  45  42 168 129  64  63 125  83 177  58 152
 186 184  60  80 189  77  68   9  44 226   0 137  41 182  11 213 163 162
 211 200 144  24   7 127 131 128 146  85 209 210  38 154 180  35  33 150
  69 223  48 235 113 114 229  43 119 155 157  36 135 140  29 202  16 106
 102 181]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5861
INFO voc_eval.py: 171: [553 584 261 724 122 806 863 253 442 166 306 785  26 595  44  34 807  96
 396 493 405 364 634 397 770 764 637 808 455 435   6 793 339 631 629 647
 828 741 451 719 305 165 485 573 224 454 841  36 737 728 139 103 812  63
 341 449 672 776 554 140 498 457 499 447 596 630 395 464 736  90 290 461
 162 494 613 478 875  65 343 340 466 680 390  92 887 112 544 357 169 309
 783  68 403 676 518 322  50 278 336 675  94 816 761 756  87 270 532 104
 324 446 452 456 467 855 555 220 798 652 379 106 459 843 470 369 330 740
 388 200 702 480  79 767 226 600 718 393 892 549  29 866  52 526 612  61
  69 345 302 358 410 547 551 792 216 293 201 274 197 332  93 815 344 458
 682 382 394 743 666 739 401 594 411 409 472 811 774 129 331 525 333 352
 483 163 742 530 782 574 181 870 564 579 314 174  47 744 661 248 124 320
 864 180  49 407 177 317 829 282 794 683   4  12  99 271 715 571 399 824
 749 116 790 585 867 303 840 567 726  95  72 312 172 710 873  83  97 327
 295  33 255  45 277 818 150 171 311  48 800 374 877  54 420 717  25 628
 392 538 656 288  91 768 232 510 599 660 862 667 730  80 497 347 754 781
 308 168 164 522  30 626 611 127  74 289 391 747  53 616 562 685 664 329
 834  86 654 550 624 890   3 508 546 100 460 479  32 772 511  43 131 701
  21 366 734 115 588 795  78 810 852 627 450 242 720 430 859 580 837 476
 218 413 543 238 535 284 292 752 799 681 273 269 759 258 856 440  14 643
 575 513 529 786 557  66 114 133 371   9 107 101 674 437  81 175 315 326
 849 227 495 477 891 196  37 298 275 217 827   8 234 213 559 668 561 353
 842 721 385 694 755 111 524 854 221 117 813 889 609 468 528 646 482 871
 183 325 412 389 655 850 109 632 243 259 707 120 418 814 568  55 531 504
 839 138 649 820 695 319 110 158 593 696 179 586 157 505 648 463 356 145
 304 346  60  42 690 691 679 318  76 178 659 376 496 321 257 572 872 119
 439 471 876 370 714 214 560 491 260 143 762 279 137 663  38 705 638 206
 735 771 323 182 617 556 874 636 541 581 822 132 847  71 558 796  82 623
 398 577 198 881 102 838 853 453 751 731 885 657 355   1 402 725 569 615
 697 514  98 448  77 108 286 604 563 205 769 658 601 809 192 154 121 592
 474 316  46 619 826 582 176 576 688  16  73  41 886 693 618 147 745 247
 766 566 533 406 432 135 425 423 419 534 245 228 202 831 301 507 207 851
 727 716 711   7 733 578 219 431 713   0 539 130 155 462 641 421  15 671
 233 545  88 753 763 686 195 381 738 587 265   2 414 240 523 365 272 635
 209 760 625 773 537 475 605 254 362 481 250 651 590  67  11 603 848 677
 670 519 748 170 367 500 310  84 187 160 342 400 287 614 387 239 633 377
 708 246 817 152 883 821 350 299 438 598 488 784 429 433 844 441 860 640
  28 665 123 173 313 249 650 692 128 503 204 620  23 520 149  18 153 210
 536 215 223 501 570 384 276 879 607 597 589 428 363 835  22 231 836 610
 360 490 622  58 338 144 283 880 787  17 621 190 236 184 868  27 869 778
 208 765  31  13 422 237 434 386 337 156 833 780 608 865 698  70 230 351
 521 241 687 465 517 803 789 801 857 229 758 506 264 424 359  56 732 194
 193 225 662  39 445 235  20 700 540 489 334 723 268  89 263 416 825 262
 757 642  40 704 361 222  62 191 167 788 775 307 845 118 296 372 515 888
 706 606 136 832 375 211   5 380 203 146 408 487 285 159 777 492 709  59
 373 199 722 469 300 802 256 639 348 830 443 846 266  24 335  19  85 444
 415 548 509  75 161 142 861 426  10 699 328 113 673 291 669 349 280 294
 404 185 542 188 703 297 583 858 602 141 267 805 368 750  57 473 779 281
  51 354 653 436 417 484 746 878 729 516 486 134 125 819 591  35 383 565
 378 804 791 645 502 512 251 644 797 684 427 678 186 151 712 527 126 189
 105 244 823 212 148 882 252 689  64 884 552]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4811
INFO voc_eval.py: 171: [ 8  4 16 71 17 64 30  6 69 67  1 50 39 52 26 23 37 13  9 11 20 70 66 51
 40 60 15 38 58 72 48  3 43 41 22 49 29 32 56 45 68 21 14 34 53 24  5 62
 73 33 44 18 65 27 10 19 31 47  7 55 25 42 12 63 57 54 46 36 35  2 61 59
 28  0]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0091
INFO voc_eval.py: 171: [3047 3049  327 ... 3063 2341 1392]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5274
INFO voc_eval.py: 171: [ 4 44  5 79 32 66 33 25 73 86 91 30 89 61 43 92 45 22 55 78 68 28 88  8
 38 39 72 95 57  9 87 70 84 49 74 76  6 77 71 65 90 31 27 64 52 83 37 58
 18 19 69 15 46 34 10 13 42 54 21 24 51  2 59 47 94 23 85  0 14 67 60 41
 16 48 80 75 29 82  3 11 35 81 26 56 17 20  7 63 36  1 50 93 53 40 12 62]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2043
INFO voc_eval.py: 171: [388 888 212 ... 896 265 276]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3928
INFO voc_eval.py: 171: [124  61  67 101  16   2  65  35  68 139   5 136  23  27  40  37 134  62
   0  58 108  89  22  78  43  10  36 104  53  29  57  91  52  21 129 143
 144 126 140  95   9  31  44 133  13  56  24 122 141 103  82  28  48  20
 109  76 127  96 114  88   7  39  85   8  66 102 115  73  49  26 145  83
  42  70  86  71 120  14  69  54  41  50  90 116 121  92  79  11  84 132
 125  33 128 117   6  87 111 138 112 107  15  64  46 100 123  18  93  97
 113 130  51  80  77  74  63  59 146  25 105  75 118  99  19  60  72   1
  47  98 137  45  55 106  38  12  17   3   4 110 119 131  94  81 142  34
 135  30  32]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0648
INFO voc_eval.py: 171: [ 87  12  73  15 104  74  56 107 110  32  93  70  14  61  72  88  33  43
  76  13  60  94 102  58   6  18  79 105  95  46  37 100  24  65  50  25
   7  31  22  39  27  63  89  29  75  23  86  92 103  57  40   2  19  17
  85   1   9  28  51  26  66  90  42  64  99 108  21  80  44  91  81 111
  38  16   0  36  10  53  62  34  98  77 106  45   4  30   3  35  48  49
  55  59  47  20  78  71  11  69  84   8  67 101  96  82  83 109  97   5
  68  52  54  41]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3936
INFO voc_eval.py: 171: [ 51 354  37 221 161 374 229 268 315  26  56  14 171 278 336 147 263  13
 230 257 363 152 135 335  88 246 180 154  95 261 303 240 307  52 106 222
 381  75  28 214 282  74 203 120 296 140 148 200 202 100  96 382 129 122
 107 182 266 193  45 310  34 177  93 281 188 298 284 258 342 313 232   6
   1 375  12 132 164 212  99 158 370 115 358 339 277 305 103 121 213  32
 295 311  16  58 271 174  47 318  44 376 181 314 196 124  59  57  60 251
 386 119 256  67 127 141  29  53 126 179 378 186 373  70 279 350 283 189
 243 159 194  18 334 167 337 390  21 146 325 145 224  30 361 118 356 326
  91 380  92  85 220 173  38  89  22  33 136  15 227 367 292 369 338 379
 110 306 183 273 219 241   0  65 155 388 383 233  69 210 297  82 341 319
 340 309 101 175 359  43 320 270 191 327 235 125 272 157 184 231 244  66
 300 223 185 259 123  71 166 316  48  64 357  36  50  68 352 242  76  49
 204   7 105 324 287  54 286 133  41 207 192 253 199 218 322 170 289 349
 377 190  35 353  97  40  23  55 215 114 201 265 137  78 248 144 178 163
 198  10 299 176  79  11 321 385 294  83 139 317 344 211 276 345 142 226
 238 117 372 113 331 355  86 275 312 328 274 302 249 291   5 288 111 197
 343 109 205 225 285 252 216  84 262 247 108 323 308  81  46 371 264 280
 143 269  94 138  61 156 151   3  80 162  90 330 333 131 387 160   4 290
 209 301 116 360 102  63 130 228 304 332 389  39 169 165  19 255 239 384
 150  87 206 134  73 260 112  20 168 347  77 153 366 172 234  62 250  27
 217   9 128 364 208 362  31 104   2 351 348 267 365  25 368 149 346 293
 187  42  24  17 236 329 237 245 254  98  72   8 195]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7314
INFO voc_eval.py: 171: [4027 2550 4059 ... 2832 3566 3373]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6200
INFO voc_eval.py: 171: [ 217  273  880 ... 1492  811  836]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5882
INFO voc_eval.py: 171: [ 84   1  78  45   2  52  96  73  67  72  10 106  93  95  80  40  16  47
  11  43  31  87   4  50 102  48  22  79  86  44  17  34  30  15  49  91
  64  92  81  32  46   5  53  90 100  26  37  99  51  97  25  82  20  74
  59  28  54  14  23   6 105  19 103  35  56  85  41  58  39  66  13  76
 107   7  24  38   0  29  88  62  33   8  36   3  69  61  63  70  77  27
  68  42   9  89  65  94  57 104  71 101  12  60  21  98  83  18  75  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3208
INFO voc_eval.py: 171: [109 110  38 161 170 253  92 171 123 184   5 103 126 194  82 179   6 124
  53 222  43 162 206 240  48 146 114 141  87 232 148 115  40 241 156 117
 234 150  44  25 147 203  80  66 259 196  63  78 209  96 183  35 136 180
 112  77  95  94  18  52  51 181 252  69  81  76  97  47 258 192 169 214
 111  93  13  89 177  62 157 186 164  30 116 149 248  31 153  91  99 131
 236 226  26  68 173  61 207  59 215  42 129 221 247 102 133 213 235 159
 190  56  19  83 101  21 212 188  41  16 182  39 210 176 220  84 217 191
 198 218   3  70 143  74  46 245  11 229 233  72 134  67   2  15 185 205
 244  75 227 237 118 174 127 223 250 163 251 202 104  28 231 120 211 108
 178  64  14  57 145  37  98  45 160 107 216  27   9 208 122 238 155 165
 168 135 125   4   7  32 154 225 189 255 119  10 254  55 228  50 130  36
 199 246  88  34 239  90 106 197 140 242 128 100 219  65  79 200 204  17
 121 230 257 113  60 144 167 138 137  24 158   1  22 187 243 132  73  20
 172   8 256  86   0 201 152 139  23 193  49 249  29  85  12  58  71 105
 175 195 224  33 151  54 166 142]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3746
INFO voc_eval.py: 171: [205 128  36  32  68 107 177  52  66 149 111 150  69 263 268 287 181 108
  28 143  33  59  19 163 230 176 115  35  67 295 151 237  82 232 277 103
 124 132  51 168  47 131  43 193 127 180 139 160  54 293 138 243 194 105
  58 173  74  97 225 304  70 102 302  30 189  78 140  94 296 265  88 206
 117 109 129  34 147 197 220 244 200 216 280 178 123 158 253 249   4 247
 276 300  23  24 190 297 281  62 218  93 228   8 204  27 196  56 195 227
  46 260 152 305 170 210 271  65  80 172  55  71 239  79   5 113 238  20
 174   0 213 294 285 116 142  98  92  60  86 246  17  13 291 146 153  41
  95 303 110  85 166 171  48 175   3 262 159 141 219 211 273 245 217 125
 199 222 214 209 184 274 208  99 112 167 301 121 290 169  90  53  84 154
 136 201 133 155 122 157 179 279 130 106  31 266  87 186 164 272  75 235
 137 270 275   7 299  73  77 306 100 207 187 202  44 126  11   9 118  45
 148 226 101  64 286  72  42  63  61 257  16 231 284  76 267 188  81 120
  96  49 236  83  22  10 185 298 215  14 259 198 234 250 241  57 288 258
 135  38 191  37 162  26  15 229 223 221 134   2 248 233  89  50  18  40
 161 119 144 289  12 292 255 145  25   6 256  21 203 192 183 182 156 224
   1 283 269  91 261 251 240 212 278  39 104 114 264  29 165 282 242 254
 252]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3997
INFO voc_eval.py: 171: [140 266 387 573 411   7 311 374 519  40 494 318 239  59  35 131 412 561
  85 421 538 587  43  65 197 475 388 404 105 121 352 383 319 373  18 227
 196 331  84 390 580  83 134 177 604 443 164 422  80  62 273  88  98 505
 419   6 415 200 395 372 321 562 375 401 230 322 435 599 441 165 283 539
 610  55 542 149 434  87 467 293 309 130 563 124 437 288 398 109 193  42
  73 442  81 344 216 541 315 607 446 476 394 477 487 133 114 445  82 602
 106 137 366 438 243 482 436  28 252  60 556 287 258 448 110 347 550 306
 507 225 236 530  39 393 328 138 346 558  41  10 417 557 406 153 384  68
 502 601 353  16 559  30  70  27  66  78 270  89 325 276 439 259 364 483
 209 513 122 343 158 171 481 125 450 544 172 400 479 272 310 508 399 260
 203 334 453 603 592  46 414 307 351 205 611 250 244  22 496 451 275  94
 148  36 146 532  37 226 314 184 389 506 127  69 298 409 480 520  12 381
 522 215 128 304 162 221 560 376 367 238  49 583  19 170 591 565 192 616
 228 169 572  57 180 518 295 181 380 161 517 555 574 464  26 609 159  23
 303 600 143 546 594  92  90 154 392 469 254 280  71 490 378  14 297 240
  91  99 461 598 363  25 391 248 585  67 433   4  53 142 492 440 536 118
 242 509 359 102 582 312 579 262 489 166 589 596 284 320 349  17 290 449
 117 377 608 214  48 341 545 308 291 213 423 418 362  58 300 430  77 327
 379 269 144 567   5 176 551 382 191 179 330 265 178 431 202 466 286 484
  54 528 452 493 155 447 457   1  97 294 578 368 323 566 365 504 463 355
 495  31 160 333 119 488 206 147 152 301 135 426 416 210 100 168  44 428
 486 500 615 581 548  61 516 132 338 108 474 257 282 317 543 498 198 187
 385 547 141 120 233 369 605 185 588  95 335   3  96 253 511 512 296 174
 444  32 113 194 271  72 460 326 471 454 613 535   2   9 427 111 354 232
 350 577 116 337 606 145 402 597 568  47 199 614 123  51 361 207 182 151
 186  21 339 521 305 313 103  33 104  29 472 527 485   0 459 336 229 403
  52  13 595  86 190 115 526 540 501 590 279 470  79 397 211 552 413  75
  24 224 420 173 424 157 425 478 101 529 264  50 386 129 432 245 136 612
 462 525 285 429 302 219 195 163 220 523 175  93 246 458 407 263 234 255
 316 370  45 497 299  38 112 499  20 410 533 531 231 584 281 251 345 268
 289 554   8 465 235 156 564 491 292 237 150 358 342 524 204 267 455  76
 274 223 218  11 503 201 593 553  74 247 241 188 340 576 534 126 549 408
 570  63 139 217 189 256 329 107 510 348 396 278 515  15  34 249 222 277
 208 371 324 537 571 332 575 586 360 514 456 468 357 405 167 356 183 473
 261  56  64 569 212]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4854
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4147
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.348
INFO cross_voc_dataset_evaluator.py: 134: 0.629
INFO cross_voc_dataset_evaluator.py: 134: 0.229
INFO cross_voc_dataset_evaluator.py: 134: 0.351
INFO cross_voc_dataset_evaluator.py: 134: 0.557
INFO cross_voc_dataset_evaluator.py: 134: 0.586
INFO cross_voc_dataset_evaluator.py: 134: 0.481
INFO cross_voc_dataset_evaluator.py: 134: 0.009
INFO cross_voc_dataset_evaluator.py: 134: 0.527
INFO cross_voc_dataset_evaluator.py: 134: 0.204
INFO cross_voc_dataset_evaluator.py: 134: 0.393
INFO cross_voc_dataset_evaluator.py: 134: 0.065
INFO cross_voc_dataset_evaluator.py: 134: 0.394
INFO cross_voc_dataset_evaluator.py: 134: 0.731
INFO cross_voc_dataset_evaluator.py: 134: 0.620
INFO cross_voc_dataset_evaluator.py: 134: 0.588
INFO cross_voc_dataset_evaluator.py: 134: 0.321
INFO cross_voc_dataset_evaluator.py: 134: 0.375
INFO cross_voc_dataset_evaluator.py: 134: 0.400
INFO cross_voc_dataset_evaluator.py: 134: 0.485
INFO cross_voc_dataset_evaluator.py: 135: 0.415
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.393s + 0.001s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.375s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.389s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.385s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.387s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.382s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.382s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.384s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.384s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.386s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.388s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.391s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.390s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.413s + 0.001s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.390s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.390s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.392s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.397s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.401s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.408s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.405s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.401s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.403s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.400s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.400s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.398s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.514s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.394s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.385s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.383s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.380s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.383s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.383s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.379s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.384s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.383s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.383s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.384s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.387s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.625s + 0.002s (eta: 0:01:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.426s + 0.002s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.410s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.403s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.396s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.388s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.389s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.385s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.384s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.384s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.381s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.381s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.379s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.195s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [360 113 460 572  12  97 260 192 435  37 100  95  90 255 302 342 287 349
 361 586  89  94 636  91 607 344 363  41 166 303 309 178 264 618 632  78
 512 373  55 285 172 247 145 541 277 573  92   3 308 409 345 115 542 482
 306 315 165 228 216 403 270  96  39 179  19 521 312 485 528 557 104 481
 332 137 268  43 554 313   7 107 576  54 458 629  44 563  61  72 452 603
 248  49 123 421 506 183 617 320 367  40 311 491 522  13  76 459 419  33
 201 558 642 431 415 276 144 488 402 161 333 578 221 588 371 147 428 281
 279 149 536 639 103 525 621 356 556 600 568 634 516  32 259  75 334 582
 180 229 395 545  17 169 339 129 465 294 652 331 231 630 413 329  42  85
 237 455 548 272   8 385 486 483 362 434 256 118 598 633  69  88 407 290
 167 258 150   2 456 282 411 543 202 644 425 463 400 139  15 301 489 214
 604 341 220 119 496 535 611 500 507 418 552  11 126 566 176 368 412 602
 257 365 480 623 492 328 647  16 251 181 613 314  50 622 567  86 534 417
  81 590 471 420 196  21 324 336 570  98 372 207 241 580 253  26 246 501
 250 662 379  18 454 127 509 571  24  52   5 382 436 182 511 606  93 495
 346 391 234 305 638  31 466 233 120  82 467 474 163 350 224 274 205 291
 288 549 245 289 446 393 254 533 195 478 304 217 164 369 396 175 616 524
 502 114 157 515  66 453 155 427 108 429 661 406 133 532 297 132  38 503
 318 296 392 347 252 451 343 399 493 553 238 484 106 610 261 322 110 307
 581 593 310 469 148 280 269 544 472 564 462  28 105 605 263 319 529 619
  48 366 141 655 559 116 569 479 555 235 494  57 447  22 160 591 376  14
 635 190 657 504 191 174 589  83 317 209 143 423 177 383   9  34 424 575
 540 609  10 561 152 654 377 284 124 352 658 450 198 640 378 203  71  87
 293 381 186 223 587 612 579 130  29 153 327  63 158 577  30 508 443 146
 278 125 170  20 627 215  56 601 620 526 663 128 185 111  45  51 438 189
 265  53 337 444 645 370 631  27 211 140 445 273 232 426 626 197 405  70
 397  65 358 505  46 230 519 117 650 594 121 653  79 490   4 323 584 643
 550 135  59 187 292 477 330   1 660 437 527 628 637 648 353 583 355 359
 487 457 212 218 414 208 514  60 222 596 448 416 539 151 283 227 226 374
 335 523 659 624 122 625  58 530 430 517 131 585 338 142 134  64  77 348
 440 266 267 380  99 204 375 398  84 364 410 473 225 497 546 239 299 354
 389 432 608 531  80 138 461 408 210 171 316 513 468 442 109 321 388 518
 325 199 249 470 614 538 168 300 404 242 193  35  73 101 615 565  68 551
 340 547 236 262 286 656  74 646 102  62   0 275 599 401 154 156 394 162
 387 136 243 386 240 464 433 560  23 295  67 390 441 213 562 206 592 184
 449 597 422  25 651 649 476 351 298 510 498 574 194 159 173 475 200  47
 271 219 384 244 439 595 537   6 520 326 188 357 641 499 112  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3545
INFO voc_eval.py: 171: [140 457  71 314 533 406 515 508 538 323 536 303  72 410  61 535 539 354
 514  89 485 407 245  41 371 428 161 358 524 257 396 309 517  39 308 527
  31 254 182 445 543 534 137 441 542 212  90 211 411  28 399 208 343 421
 506 488 453 430 199 111 498 409 483 113 380 337 321 355 247 548 502 281
 357  74 499 551 225  76 316 313 482 368 242 495 310 448 473 273 489 322
 446 414 142 324 280  79 237 151 532 458 459 390 132 556 443 356 262 305
 560 186 143 490 209 244  91 493 454 128 563 304 491 475  95 374  73 224
 413  42  36 206  60 307  10 552 311 480 447 265 525 319  29 196 553  96
 467  87 518 119 367 336  24  30 544 381 383  78  85 159 284 312 220 325
  68 344 369  40 210 511 306  19 150 487 546 332 320 246 433  81 181 547
 345 359 426 465 333  47 466 558 223 550   8 545 442 463  83 404  77 187
 331  22 425 204 278 192  98 559 178  84 277 512 531 456 521 353 138 392
 462 397 497 408 342 507 529 300 118 427 523 437 100 522 297  55   9 402
 393 375 139 477 341 107 329  58 444 513 537 148 541 431 168 449 346 236
 484 364 318 101 217  92 287 110 191 133 198 144 147 436 519 440 520 253
  27 424 272 298 434 288  18 555 258 438  46 202 423 412  26 385 401 228
 394 135 226 183 429 201  52 203 234 130 115  65 296 347 435 176 189 439
  67  32 451 360 249 165 361  33 464 293 468 275 134   4 229 184 141 263
  21  43 171 125 549 160 195 180 152 561   7 155 105 285 509 398 452 422
 266   2 131 377 255 340 565  64  86 146 233 193  54 123 470 330 432 271
 218 291 382 290 106 114 461 164 219 388 156 250  75 179 292 387 122 540
   0 334 205 326 260 129 328 283 127 103 339 267  23  63 259 384 126  12
 172 274 469 391 188 167  62 276 207 252  34  53 200 222 108 486 389  11
 348 286 420  14 243 302 416 554  38 455 373 158 471   6  50 163 104 268
 557 417 248 415 289  70  88  35 378 478 269  15 279 301 235  48 317 194
 170 213 149 177 221 185  80 365   3  93 403 460 153 386 239 157 154 362
  37 366 231 476 169  99 400 214 116  57 492 282  82   5 315 215 450 270
 241 120 505 294 503 327 338  69 352  45 166 516 526 238 216 419  59 474
 251 175 530 372  13 112  97 174 117 418 256  56 494 240  44 162  51 501
  17  16 379 232 395 335 405  94 496 562 564  25 230 227 197  49 102 481
 136 109 528 504 350 510 370  66 479 295 376 351 264 261 124 190 173 121
 363   1 299  20 500 145 349 472]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6341
INFO voc_eval.py: 171: [ 65 159 190 123 125 186 174   2 124 379 378 346 293 380 350 232 315  59
 146 229   1 419 152 348  94 128 131 138 185 404 381 305 323 200  22 397
 191 383 175 149 170  98   7 409 312 262 320 257 392 436 147 405  29 384
 177 437 337 328 399 181 318 291 188 322 176   8  53 166 363 231 433 331
 393  78 324 382 139 377  11 275 362  32 187 429 390 351 105 129  76 106
 367 204 224  20 284 217 225 227 398 401 203 279  23 412  69 236  88  39
 388 385 421 300  40 168  80  12 230 252 400 411 325 120 301  48  64 102
 285  14 395 130  46 265 304 114 205  63 349 160 260 287 341  62 408 153
 161  54  66  97 376 333 425 361 107  82 266  81 140 330 344 127 118  77
 226  28 263 214 365 267 163 270 311 132  13 135  10 375  37 223  34 250
 142 221 167 234 269 372 426 172 145  73 272 294 183 309 207 391 134  79
 283 366 233  45 261 326 292 256 189   5 289 144 165 109  24 295 212 141
 143 216  42  95  25 302 347  50 237 244 355 308 247 403  16  52 115 427
 310 428 104 245 239 313 184   4  68  61 396 329  58 307 268 280 413   6
 218 368 133 195 242 199 119 246 277 410 171 253 352 299  36  30 192 357
 196 264 255 198 208  83  90  84 211 215 194  96 345 169 420 254  71  44
 251 156 206 334  92  41 336 201 182 422 154 193 121 150 228 137 180 202
 290 343 179 321 364 316 358 335   0  70 258 155  86 116  19  49  47 418
 273 274 406 278 151 111 197 435 438  51 319 238 110  67 222  75  38 158
 360  60 148 136 248 423 387 162 356 249 402 332 340 353 157 407  85 112
  33 126 303 241  21 327  57 178  91 306 213 235 288 430 431 417 220 373
  27  31  43 101 103 374 219 298 122 370  18  93 281 414   9 173  35  87
 282 259 117 424  89  99  15 432 338 439 240  26  55 394 164 113 271 210
 314 243 389  72  74 286 276 100 415  17 354 339 440 296 434 297  56 371
   3 317 416 386 108 342 209 369 359]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2287
INFO voc_eval.py: 171: [ 757  884  758 ... 1191  190 1213]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3492
INFO voc_eval.py: 171: [901 542 495 ... 131 318 130]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5549
INFO voc_eval.py: 171: [ 53  28  97 100  35   9 149 253 122 108 193  62 243 199   3  32 167 188
 187 179  26 138  49 250  39 241  99 208 155  20 247 120 206 266 251  95
  37 209 151 215 202 184 178 227 145 132 197 116 103  15 124 106  80 175
  31 127 136 256  29  72 238 259 245 191 195 147  16  40 180  22 218  91
  67  21 233 170 115   2  43 216  54  90  66  44 201 163 232 210  11 231
   6 192 254 117 222  64  33 164 261  82 113 223   8  55 135 240  87 128
   1  61 134 165 229 263 104 220  70  56 121 112 102 265  24 249  23 234
 189  50 264 221  76 142 217 214 152  98 157 194 160 154  78 107  60  96
 133 130  25 105  30 109 244  88  13  71 196 141 119 137  48  93  83 230
  75 211 123  79  63 213  42  59  41 139 140 200  12 224  18  69  73 146
  57  68  92 255  10  17 203 129 207 190  94 159 181  34 150  65  89 239
 236  14  47 235  85  51 168 182  27 166 205 173 226 162  38 252 143 118
 126  45   0 114 204  36 174 183 153 219 267 225 257  81 176 237  74  46
 131 260 172 228  77 125  52  86   4 110 148 169 171 156 185 248   7 262
 198 246 111 212 144  58 242  19   5 101 258 158  84 186 161 177]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5727
INFO voc_eval.py: 171: [637 673 301 ... 523 212 667]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4802
INFO voc_eval.py: 171: [ 8  4 18 79 19 71 33  6 77 74  1 56 44 29 58 41 26 14 57  9 73 40 11 22
 78 45 67 16 80 65 42 54  3 55 35 25 63 76 48 46 32 50 20 17 15 59 23 36
  5 49 37 81 34 27 30 69 72 10 53 21 28 61 47 12  7 39 60 70 51 52  0 64
 75 13 38  2 68 66 24 31 43 62]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0087
INFO voc_eval.py: 171: [3685  404 3687 ... 2011  616 1834]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5268
INFO voc_eval.py: 171: [  4  49   5  92  35  77  36  27  85 100 107  32 104  70  48  50 108  24
  64  91  79  30 102  44   8  84  43 112  65  97   9  81 101  87  55  88
   6  83  90  75  34  29  96 106  74  41  20  67  59  51  80  15  21  10
  37  52  26  58   2  68  13  47 110  61  23  19  53  95  78  14  31  99
   0  25  11  46  38  28  94  69  93  17  45   3  89   7  57  66  60 109
  39  33  98  54  72  12  22   1  62  42  86  18  16  40  82 105  71 103
  63  73  76 111  56]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1995
INFO voc_eval.py: 171: [ 485 1106  260 ...  973 1079  446]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4006
INFO voc_eval.py: 171: [141  69  75 112  17   2  73  39  76 157   5  26 154  44  30  70  41 152
   0  66 121 100  24  48  32  88  40  59  10 115  57  23  65 102 144 161
 147 162   9 106 158  34  49 139  27 151  64 114  14 160  93  53 129  86
 123   7  43  31 113 145  22 107  96  99   8  29  74  54 163  94  83 130
  97  62 137  46  80  81  15  89 101  45 150  79 143  55 138 131  98  11
 132 146 103  95 156 140   6 126  37 125 116  51 111 108 120  87  71  90
  19  16  28  72 118 127 104 110  50   1 148  68  56  63  84 164   4  21
 109 149  42  67  91 133 142  18 155  52   3  82  85 105  12  92  33 153
  13 134 124  60  61 119 117 128  77  47  35 159  36  20  78  25  38 135
 122  58 136]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0647
INFO voc_eval.py: 171: [ 97  82  13  17 117  83  63 124 128  38 103  15  79  98  68  49  14  85
  39  81  67 104 115  20  65 120  88   6 105  53  43 112  27  73  57   7
  45  28  37  99  25  71  33 116  84  35  26 102  64  46  96 100  19  21
   2   9  58   1  72  34  95  74  89 101  24  31  50  48 111 129 125  18
 121   4 108  86  90  40  44  70  51   0  10  66  23  80  60  41  42  36
  56  11 113  55   3  87  62 106  54   8  94  78 126  76  59 107 118  91
 123  93 114  30   5  69 119  32  47 127  12 109  77  29  52  75  22  61
 110 122  92  16]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3868
INFO voc_eval.py: 171: [ 61 398  44 253 190 422 262 305 356  30  68  17 315 200 378 176 298  15
 263 291 407 280 181 377 103 159 183 209  62 111 295 124 347 433 274 342
 335 254  38  89 246 319 234  87 434 112 141 177 168 233 230 118 125 150
 211  40  54 350 143 337 302 223 108 292   8 383 423 318 353 320 206 217
 116 187  37   1 153 265 121 402 193 243  14 351 245 344 380  70 359 142
 210 314 424 203 418 136 334  20 439  71  63 308 285 145  57 225 148  53
 355  80 428 140  32  69 290 394  72 215 169 147 188  83 376 421 208 316
  22 325  25 256 218  45 400 379 277 432 442 139 196 224  34 107 366 106
 405 430 174 175 104  26  18 101 435 417 367 130 260  82 160 119 389 275
 252 345 202 264 184 349 220 414 401 381 339  76 266 251 331  97  43  79
 336 186  51 360 293 269 382 307 212 146 255  39 403   0 241 357 310 204
 123 441 368 287  81 278 309  66 144 195 365 361 396 214 213  59 155 276
  84   9  58  77  42 324  60 163 219  67 247 323 393  48 437 238  27 363
  91 301 199 229 425  47 328 235 250 232 358 205 222  12 385 113 397 338
 386 286 228  13 248 283 132 192 135  93 352 166  94 207 333 272 282 281
 420 126  99 102 341 419 348 384 236 173 362 399 327 317  98 330  96 226
 134 369 180 294 372 297 258 311 138  36 306 242 105 313 436 191 128 120
 300 100   7 110 340 312 172   4 171 321 343 390 364   5  75 240 137 257
  46 375 404 289  55 182 151 406 164  35 284  24 122  95 158 198 157 156
  33 149 304 329 415 267 194 391  92 332 185 431 440 373 443 152 239  73
 271  21 371 261 395 189  10 288 178 273 411 179 197  74 387  88  65 161
  23 115 392 114 410 413  85 416 165 133  86 127 270 249 438  28 279 412
   3 409 259 354 408 429  29  50 303  31  90 231 167 237  11  16  19 322
 131 326  64 374 296 227 117  78 201 268 216   6  49 299 346 426  41  52
 154 162 170 244 370 427  56 109 129 388   2 221]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7319
INFO voc_eval.py: 171: [5086  434 3237 ... 2527 4913  285]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6206
INFO voc_eval.py: 171: [ 245  308 1038 ... 1146  280   74]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5892
INFO voc_eval.py: 171: [ 91   1  84  47   2  52 103  78  72  77  11 117 100 102  86  42  17  48
  12  32  45  51  94   4 112  49  93  85  23  46  18  31  35  69  50  16
  98  87  33  60  99 110  53 109  27   5  97 105  89  26  80  39  29  64
  15   7 113  21 116  54  36  20  92  24  43  41  58  63  61 118  14  71
  57  25   9   8  40  82  34  30  67  95  83   0  68  38  66  96  70 115
  90   3 111  65  75  81  73  44 106 107  74  13  62  19  59  55 114  28
  88  22  76  10   6 101 104  56  37 108  79]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3178
INFO voc_eval.py: 171: [135 136  53 194 206 299 115 150 207  13 221 153 232 129 103 216 151 264
  14  71 195 245 284 139  60  56  65 173 168 285 108 275 187 178 176 277
 138 174 142  82 220  38 119  49 101 241  61  99 249  85 308  98 234 118
 298 217 102 163 137  69 117  30  70  88 309 143 205  97 223 230 218 116
 110 188 254 121  81  64 177  25 113  43  87 199 209 246 280  42 213 263
 159 157 128 255 293 125  78 292 251  80  31 184  39 268 127 160 227 252
 104  55 145 253  59 192  75 222 219 279 212  94  27 229  58 259  20 289
 271 225  28 258 262  83 243 236 161 154  91  89 122 105  86 196 276  11
  96  33 147 133  18 288 152 130 170 269 281  63 297 201   8  52 296  26
 250 240  41 273  62  76 134 282 193  15 214 210 172 256 265 149 300 204
 248 267 301  48 226 270 162 186 158 237  40  12  44 185 235 132   6 183
 111 148 156 203 165 286  19 283 233  90 305  51 224  32 109  79 238 190
 107 272  84 208 100  35  74  34 146 126  67  93 290  29 242 167 164 261
 302 179 239  16  21 198 169 295  66 141  17  77 166   4 202   7 171 266
 189 197 231 155 140  95  68 287 106 215 182 123 131  73  22  37 200 191
 114   1 228  54 211 181 260   3   0  10 274  72 294 307 306 304 303 180
 278 291  50   5  23 247  46 257 124  24 144 112   2  92 244  36  45 175
 120  57  47   9]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3892
INFO voc_eval.py: 171: [243 154  45  40  84 132 210  64  81 177  85 180 136 319 313 214 133 341
  41  34  71 171  24 195  44  83 209 273  99 141 280 158 274 349 181 200
  63 330 127 157 150 213  58 153 192  66  53 347 228 167  70 130 229 119
 289 166  86 360 206  91 155  43  38 126 265 315 291 134 168 116 358 143
  95 232 211 222 106 175 329 356 351 189   5 259 296 149 238 302 255 333
  28 293 334 114 352  33 242 230 224 257  68  30   9  87  75 268 182 204
  57  97  67  80 231 282 363 348   6 267 309 247 281   0  25  96 121  72
 202 117 251 198 322  22 207 359 184  17 104 135 142 345   3 122 208 252
 292 138 199 113 137 324 203  50 170 261 174 191 325 246 164 248 311 338
 357 147  65 212 201 151 103  59 185 169 156 258 217 101 236 196 239  37
 131   8 148 123 327  54 354 161 317 165  94 323 256 244 245 321 109  92
 105 344  90 188 118 332 318 240 100 278  14 220 186  89 279  21 275  51
  77  78 277 219 353  73 125 284  11 266  98 144 308 337 364 221  32 294
 306 176  55  93 193 152  12 162 254 218  69 339 346 276 215  27 225 129
 173 260 146 271  23 233 342  19  18 172  60 107  48  15 163 307   2 145
  61  49 320  31 297  36 283 301 194 304 102 286  47 216   1 139 223 226
 128 299 250 241  39  26   7 187 335 178  42  76 290 316 115 336 249 124
 262 328 253 343 305 285 227 331  16 234 303 205  82 362 272  56 295 263
 179 108 197 314   4 310 350 111  79 159 355 288 112  10 235 326  88  29
  52 300  13  20 312 237 190 361  46 340 120 269 270  62 160 298 264 287
 140  35 110  74 183]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.4005
INFO voc_eval.py: 171: [171 327 467 688 499   8 384 450 627  48 597 296 392  71  42 160 500 674
 100 509 648 703  51  78 242 569 470 490 130 146 427 462 393 449  21 282
  97  99 163 241 473 696 220 406 534 200 720 510  98 337 104 119  75 503
 507 612 248 285 452 394   7 526 675 395 201 486 448 532 478 181 728 649
  65 349 102 561 360 715 382 652 676 238 528 158  50 355 525 150 419 134
  88 651 268 389  96 537 483  95 571 572 723 477 718 162 536 533 139 131
 579 529 442 588 167 311  72  35 539 300 279 662 422 378 354 135 292 527
  49  11 505 614 637 669 317 476 169 668 463 403 717 671  47 492  81 185
 421 608  92 428  19 400  83  37 341  32  79 318 333 212 530 153 672 578
 147 485 409 599 484 253 620 574 340 654 583 615 214 336  26 426 708  55
 383 177 112 719 440 541 280 544 193 251 259  43 542 302 502 379 274 418
  14 613  44 376 309 179 729 319 387 283 472 640 576  82 155 453 227 707
 497 673 459 368 209 630 443 156 267 208 198 224 628  22 195 314 174 186
 103 460 557 716 678 735 109 363  58 699 295 475 237 689 197 223  68 657
 456 710 714 624 367 108  17 687  84 122 625 375  27 667 553 701 106 474
 346 727 698 594   5 725 306 143  29 590 646 563  80 712  31 358 297 142
 616 455 173  63 694 524 439 540 531 357 457 655 385 350  20 705 521 381
 506 435 424 125 266 299 512  69 202 543 321 371 187  93 234 263 396 222
 697 585 164 589 402 175 144 461  52 196 680  38 416 522 397 438 444 332
 441 679   6 133  57 538 391 598 123 250 515 362 464 659 218 256 504 206
 611 373 596   1 560 635 326 587   9 118 184 413 408 178 602 734 704 316
 228 445 549 653  73 353 221 401 535 555 138 658 161 604 663 405 545 721
 136 564 693 619 312 348 516 216 141 618 239 365 645  87 172 247  39 517
 230 260 116 225   2 430 183 731 334 692   4 245 568 487 488 711 289 117
 573 722 288 128 713 284 623 425 411 145 240 271 412 437  56 429 482 140
 682 229 149 257 650 733 386 514 277 176 631 551  94 101  28  60 501 377
 410 113 351  40 215 166 126 374 629 324 554 111 638 633 199 520  62  59
  16 552 191 261 414 567 281 446 137 706 603 323 586 636 664 601  90  54
 730 420 249 359  45 310  36  12 498 170 523 565 269 634 252  10 643 513
 294 329 546  25 665  24 496   0 290 233 328 508 182 433 270  18 303 639
 189 157 466 479 632 124 132 606 287 610 291 407 304 390 369 345 154 356
 404 593 417 107 661 566 275 231 709 315 666 276 550  64 617 342 700  89
 398 447 621  91 558 511 677  76 338 423 347 609 344 491 469 562 622 110
 372 644  23 307 217 313 471 232 683 168 592 547 702 207 436 493 192  41
 685 203 243  15 194 320 148 322 724 258 432 188  77 273 595 519 399 305
 105 451 352 580 301 335 434 656  86 226 690  85 660 575 262 556  46 691
 686 272   3 582 581 205 481  53  74 732  34 681 219 330 121 213 152 120
 308 246 607 415 210 298  67 278 159 431 380 264 244 114 570 641 339 468
 670 370 127 600 325 489 548 577  13 180 647 204 265 494 591 361 458 626
 190  30 366 235 642 695 726 331 286 454 584 605 254  66 343 495 480 559
 236 115 129 211  33 151 255  70 684 518  61 165 364 388 465 293]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4874
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4149
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.354
INFO cross_voc_dataset_evaluator.py: 134: 0.634
INFO cross_voc_dataset_evaluator.py: 134: 0.229
INFO cross_voc_dataset_evaluator.py: 134: 0.349
INFO cross_voc_dataset_evaluator.py: 134: 0.555
INFO cross_voc_dataset_evaluator.py: 134: 0.573
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 134: 0.009
INFO cross_voc_dataset_evaluator.py: 134: 0.527
INFO cross_voc_dataset_evaluator.py: 134: 0.200
INFO cross_voc_dataset_evaluator.py: 134: 0.401
INFO cross_voc_dataset_evaluator.py: 134: 0.065
INFO cross_voc_dataset_evaluator.py: 134: 0.387
INFO cross_voc_dataset_evaluator.py: 134: 0.732
INFO cross_voc_dataset_evaluator.py: 134: 0.621
INFO cross_voc_dataset_evaluator.py: 134: 0.589
INFO cross_voc_dataset_evaluator.py: 134: 0.318
INFO cross_voc_dataset_evaluator.py: 134: 0.389
INFO cross_voc_dataset_evaluator.py: 134: 0.400
INFO cross_voc_dataset_evaluator.py: 134: 0.487
INFO cross_voc_dataset_evaluator.py: 135: 0.415
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.312s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.369s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.379s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.386s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.392s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.390s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.389s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.387s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.386s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.389s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.387s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.390s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.391s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.503s + 0.002s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.412s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.408s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.394s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.397s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.401s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.411s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.407s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.405s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.405s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.401s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.400s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.398s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.466s + 0.001s (eta: 0:00:57)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.379s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.374s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.396s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.394s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.391s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.390s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.392s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.390s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.388s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.389s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.390s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.390s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.541s + 0.001s (eta: 0:01:07)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.409s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.391s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.385s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.381s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.386s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.389s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.384s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.385s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.380s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.378s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.377s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.377s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.870s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [411 131 527 651  15 114 299 224 496 117  41 112 105 293 391 348 413 398
 330 668 109 104 723 106 393 691 415  45 355 194 349 207 303  88 702 718
 586 425 652  63 325 174 200 328 133 107 394 615 354 282 352 463 361   4
 617 552 412 456 193 427 113 262 250  43 358 208 121 312 309  22 635 381
 603 551  47 630 655 556 595 359 159 110   9  48 516  72 368 143 686 525
 418  60  44 285 579 714  82 213 642 490 357  86  16  54 476 599 470 422
 319 168 526 636 657 455 474 559 562 596 701 255 187 233 721 670 485 382
  38 298 661 732 120 726 532  85 383 263 323 172 611 634 590  37 647 197
  20 467 448 209 149 620 407 414  46 683 388 705 495 557 743 294  10 137
 554  80 461 265 521 333 339 624 378 380 681 530 439 716 297 138 161 720
   2  98 273 314 254 522 560 195 610  13  18 175 326 734 480 618 420 465
 102 687 146 466 234 563 347 248  19 473 697 453 645  99 111 205 673 685
 581 573 390 568 628 211 296 695 360 550 659 372 540 291 708  93  56 650
 707 115 497 288 574 277 533 377 646 609 737 534 228 475  58  27 189 519
 689 267 433  24 139 239 625 268 725 399 395 472  94  35  29 287 385 212
 421  21 649 351 147 227 753 258 292 108 251 424 335 350 585 598 449 204
 132 283 436 486 583 327 176 543  77 237 123   7 484 700 608 192 317 154
 152 509 181 281 567 122 688 127 365 445  42 396 332 353 575 541 660 607
 555 529 515 356 300 694 548 564 447 134 752 274 429 310 289 674 179 510
 367 460  95 331 173 589 566 270 703 643 324 654 417 343 619  11 518 392
 746 452 222 370  17 629 342 302 631 364 144 722 676  31 727  12 202 576
 446 693 101 536 206 577 648 241 499  36  49 185 637 401 671  32 748 164
 478 235 435 604 549 217  33 171 304 745  53 322 669  65 140 656 432 658
 162  50 150 431 696 483 600 749 513 704 754 182  25  81 423 582 245 216
 614 437 498 177 266 458  59 135 128  73 167 450 717 221  23 249 223 733
  68  76 148 386 479 198 744  69 558 141 257 639 724 264 626  30 403 338
 337 578 246 252  64 662 506 601  57 240 145 684 218 712 376 751  74   1
 230 405 397 471 260 735 468  92 547 428 426  75 508  96   6 409 116 126
 155 384  71 305 593 371 379 316 709  89 451 667 487 528 711 713 561 156
 591 261 665 524 710 587 507 307 275 698 301 151 622 597 740 410 406 750
 511 692  79 231 493 505 501 531 373 345 242 443 362 677 387 606 605 256
 569 229 588 738 286 612 225 247 178 278 679  87 747  39   0 464 400 542
 180 638  66 613 535 199 653 329 318 118 166 389 369  51 196  61 279  97
  78 346 236 462 340 320 226 434 500 129 736 682 253 169 742 210 416 184
 366 119 699 259 504 538 592 188 512 374 459 477  40 728 570 160 690  91
 295 124 644 232 623 280 457 454  34 627 203 441 503 130 336 201  84 308
 584 739 731 271  28 404 408 158 481 565 572 438 402 440 442 142  26 183
 276  14 444  83  70 492 494  52 663 520 640 284 219 311 313 469 125 186
 489 190 215 594 220   8 729 344 621 544 553 632 243 641 675 419 664 136
 482 334 680 545  55 491 315 672 165  90 537  62 580 238 602 571 517 321
 730 666 502 290 214 633 678 363 514 170 269 546 103 430 341 719 523 375
 706 539 157 244 306  67   3   5 191 488 163 715 741 616 153 100 272]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3575
INFO voc_eval.py: 171: [155  81 517 361 603 464 582 574 608 370 606 350  82 468 605 609  69  99
 581 402 546 465 280  47 408 424 180 592 356 294 453 488  45 355 585  36
 204 595 613 502 507 289 612 604 469 151 239 100  34 236 240 390 513 490
 456 467 561 123 223 549 479 569 407 545 125 384 323 368 403  86 565 282
  84 434 562 360 623 617 363 544 357 421 558 277 257 472 550  89 369 314
 518 535 519 510 157 321 509 404 146 505 168 371 602 158 270 352 208 445
 471  48 552 514 628 279 553 555 427 351 635 142  68 105 632 256 101  83
 237  42 354 625 537 303 219 358 435 593 233 131 614  88 624 529 366 586
 106  14  35 359 542  37 327 578  46 391  97 437 383 238 250 306 630  23
 548 353 380 420 203  91  11 178 392 167 422  77 493  95 631  28  87 367
 616  53 486 254 522 466 527 372 504  94 281 601 523 589  93 454 108 462
 214 484 528 379 590 409 319 618 209 200 621 378  26 447 110 579 491 516
 389 615 130 347 560 591 428 448 230 269 388 318 111 152 222 122 607 393
  62 365 417 580 501 291  12 165 551 117 159 153 258 587 102 611 247 487
 401 411 588 570 344 459 499 164 205 260 345 539 439 267 500 496 189 470
 599 376 295  16 410 498  33  22 394 147 627 331 482 483 494 506  66 633
 508 202 495  59  72 458 127 316 576 218  73 169 144 526 213  52 313 115
 521 149 211  49 284 512 186  25   2 226 198 225  39 449 174 381 340  75
 307  31 261 430 162 387 215 145 436 333   4 492 156 620 489 229 610 193
 206  38 179 116 126  96  85 304 326 343 185 201 266 481 373 530 532 234
 148 455 231   9 312 377 386 637 332 337  61  71 290 113 395 297 139 137
 285 249 438 143 248 253 446  70 308   0 329  27  15 188  60 515 339 141
 140 114 547 288  44 364 443 626 475 478 175 194 299  40 336 315 278 177
 330 210 217 629 251 520  90  18 268 441   3 531 375 172 135 473  57 128
   8 176  55 362 426  13 584  19 173  50 540 118  80 324 133  65 311 444
 440 207 103 554 224 317 457 192  98  67 274 568 243 533 184 474 564  92
 538 286 385 335 348   6 283 425 460 166 129 241 309 397 259 511 636 320
 322 374 536 476 244 310 112  41 594 418 182 433 577 432 567 349 273  74
  30 242 132   1 124 341  17 573  78 346 276  63  58 275  29 559 264 556
 197 293 199 419 187   5 414 171 485 598 134 543  20 255 342  43 109 571
 429 196 477 191 412 597 600  56 265  51 287 497 634 525 463 400 572 399
 596 415 195 120 220  64  10 246 325 104 583 396 183 216 563 272  24 382
 423 334 480 450  54 298 398 235 575 136 227 292 160 107 232 406 300 451
 163 161 150 262  76  21 296 263 301 413 328 524 212 138 221 405 619 541
 534  79 170 181 121 461 119  32 442 503 154 190   7 566 305 228 252 245
 622 557 431 302 416 452 271 338]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6270
INFO voc_eval.py: 171: [ 72 180 216 141 143 212 198   2 142 441 440 404 442 333 408 266 360  66
 263 165   1 171 406 485 146 149 107 156 443 370 468 349 210 445 217 459
 192 168  26 227 199   7 113 366 357 474 454 298 446 293 502 166 201 469
  33 461 364 188 206 369 330 455 503 265 375 388   8 214 499 200 444  60
  11 380 495 424 425 409 452 157 371 439  36  87 147 213 122 231 311 121
 257 230 260 255 460  85 270 447 465  78 450 429 316  43  24 487 190  12
 477  89 322  27 100 248 264 343 462 344  71  44 457 476 372 323  52 138
 288 148  70  14 181  54 232 407  69 172 347 473 325  73 396 132 182 301
 117 158  91 123  61 296 145  10 491 112  86 184 401 153 150  13  90 438
 302 259 356 267 137 189 160 299 245 423 196 306  41  38 453 383 492 378
 152 254 303 186 334  32 427 305 308 252 437 235  88   5 271 269 286 108
 373 434 163  82 125 164  50 428 242  46 159 297 331 215 278 161 336 353
 358  57 414 250 321  65 354 208 493   6 292  29  59  77 279 405   4 151
 120 327 345 249  28 222 109 494 193 233 352  40 281 355 103 225  80 219
 430 191 223 280 313  92 289 342  68 417 458 241 273 221 467  19 287 261
 376  94 237 135 304 220 139  34  79  49 203 205 226 418 133 247 207 209
 479 410 384 169 402 317  53 486 276 176 300 174  45  76 291 449 470 224
 475 488 315 134 105 155 420 484  58 127 290 309 175 426 310 228 328 229
 365 178 361 351 489 415 126 294 387 367 253  55 183 144 385  23 177 412
 106 466 504 170  98 471 285  42 400 501   9  67 483 496   0  96 268 346
 116 251 167 395 374  64 284 326  84 319 197 140 456  48 118 104 246 436
  25 130 114 202 154 435 432 101 382  39  37  99 272 185 480 244  81 505
 275 136 295  62  22 413  30 448 498  35 274  18  31  16 338 239 318 394
 341 307 481 187 339 236 490 497 391 500  83 451  15 419 433 359 363 482
 381 320 283 506 211 377 314 312 124   3 397 332 472  47 431 411 324 131
 238 362  21 390 398 110 329 282 277  63 348 115 179 389  51 218 256  56
 335  20 379 386 393 194 262 204 129  75 243 416 195  97 119 128 350 337
 111 162 240 403 258 340 464 463 234 368 173 102  74 392  93 478 422  95
  17 399 421]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2290
INFO voc_eval.py: 171: [ 877 1022  878 ...  253 1332 1377]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3523
INFO voc_eval.py: 171: [1033  630  583 ...  740  121  771]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5529
INFO voc_eval.py: 171: [ 56  30 107 110  37  11 163 282 133 118 212  67   4 221  33 271 183 207
 206 196  42 152 269  51 280  28 231  22 109 275 171 229 131  40 105 281
 252 144 225 195 238 202 296 218 232 135 165 159  86  17 150  32 138 127
 192  39 113 116 265 286  77 289 161  43 197 214 125  45 244 273   3  58
 210  23  47 186  18  24 100 240  72 260   8 233  99  71  13 128 247 179
 211  69 290  10 123 259 224 248  88 283 257  34 148  94 180  66 146 181
  75 139 267  59 293   2 245  26  61 295 279 112 156 255 114 294 122 261
 117  52 208 132 241 166  83 173 176  27 213 119 246  25 170 106  65  81
 237 145 108 216 151 155  50 130  76 272 153 154  96  44  31  15 141 236
  68 234  14 115  80  36  78 222  89 134 226  64 102 103  54  12  74  84
 256  62 285  20 175 129  35 263 101 262 160  73 249 250 243 182  48 227
 124 230  16 137 164  29 191 190 251  98  41  70 201  87  49 266 178 209
 167 140  19 228  38 287 203  92 184  82 157  46   9 200 297   5   6 185
 193   0 172 120 187 189  55 274 291 219 149 254  93 270 204 292 142 235
  79 264 136 162  63  85  90 278 277 205 194 121 199  95  57 284 158   7
 253 104 174 168 242 217 126 276 143   1 258 288 169  53  21 215  91  60
  97 268 111 223 147 177 198 188 239 220]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5713
INFO voc_eval.py: 171: [ 746  786  352 ...  452  726 1170]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4732
INFO voc_eval.py: 171: [ 9  4 19 88 20 79 37  6 85 82  1 62 50 33 65 47 63 29 15 81 10 12 46 51
 86 24 75 17 60 89 73 48  3 61 84 40 71 56 22 28 18 36 52 54 39 41 66 16
 34 55  5 26 90 80 59 77 42 30 11 32 45 13 53 67 23 68 83  7 57 78 58  0
 49 72 25  8 27 64 21 14 44 70 74  2 76 69 35 87 31 43 38]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0079
INFO voc_eval.py: 171: [ 454 4188 4190 ... 3076 4413 2276]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5302
INFO voc_eval.py: 171: [  4  55   5 101  41  84  34  31  93 111 118 115  37  77  54  57 119  28
  71  99  86  49  35 113   8  92 106  48 125  72  88   9  95 112  91  62
  96   6  98  24 105  39  46  33  81  82  74 117  66  58  18  87  59  65
  11  30   2 123  25  23  75  42 104  52  60  16  68  36  27  50  12  85
 103  43  32  51  17 110   0  29 102   7   3  76  64  20  97  38 122  67
  61 109  73  44 114  69 116  45  47  40  79  22  14  19  83  10  94 107
  90  26   1  80  21  56 121  15  70  63 120  78  13 124  53  89 100 108]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2052
INFO voc_eval.py: 171: [ 567 1297  309 ...  811   96 1393]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3915
INFO voc_eval.py: 171: [ 74 147  80 119   3  18  78  44  81 165   6  75  29 162  49  33  46 160
  71   1  27 128  54  35 106  64  45  11  94  25  62 125 150 108  70  10
 112 169 153 170 145  53  30  37 166 158 121  69  26  99  58  15 168 120
 136  92  48   8 130  32 151  34   9 117  24  79 102 100  52 105 113 171
  59  67 143 103 137  89  95 156  51 107  86  16 149  50  87  85 104 146
 138  60 164 144  12 114 152 124  93  76 118 101 109  56 133  96   7  31
  55  40 132 148   2  20 127 116  68  17 122 134   5  73  77  97 155  61
 154 110 115  47  98 123  23  19 172   4  90  66 111  65 139 163  72  57
  36 161  82  14  88  13  39 135 126  91 131 140  22 167  84 141 129  38
  28  63  21   0 157  41  83 142 159  42  43]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0646
INFO voc_eval.py: 171: [104  91  16  20 126  69 133  94  90 137  43 111 105  18  86  75  54  17
  44  89  74  23 112  71 129 123   7 113 120  92  58  48  31  49  80   8
 107  62  32  78  42  29  37  93 124 108  39  22  24  70  51  30  79 110
  64 103   2  10   1  38 109  28 102  55  81 138 130   4  95  53 116  35
 119 134  21  27  45  73  88  77  97  56  50   0 121  14  46  11  61  66
  41  47  68 114 127   3  60 132  65 122   9 135 115  59  34  98 101  76
 128 117  85   6 136  83 100  36  15  19  25  72  52 131  87 106  26  33
 125  57  82  40  96  13  84  67 118   5  12  99  63]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3822
INFO voc_eval.py: 171: [ 78 455  60 294 225 485 304 351 410  42  88 361  26 435 211 342 236  23
 305 322 335 468 216 434 127 218 246 190  79 388 153 493 339 401 138 494
  53 316 396 295 140 367 273 287 112 110 172 212 155 272 269 203  55 146
 404  71 248 181 390  13 174 336 440 132 483 262 144  51 347 407 222 149
 365 462 368 405 286 247 185 255 243  90 229 484 413   3 398 283 437 188
 500 307  82 173  20 239 328  91 360 489 176 384  29 479 167 103 264  45
 354 170  74  70 451 409  61  89 334 298 433 223 492 253  34  37 457  92
 205 184 436 373 106 503 482 362 180 256 245 131 495 319  47 128 232  38
 458 263  27 130  58 147 306 105 466 392 478 161 421 317 446 125 258 209
 210 101 399 297 337 403 219 221 438 119 302 311 193  68 308 330  54 292
 353 179  98 423 411 414 389 151 293  85 463 439 238 475 104 249 381   1
 281 419 175 453 240  86 197 320 496 231  57 424 356 355 186 257 502 498
 288 318  76  14 372 252  99 415 450  75  39 250 107 371 346  77 329  65
 412 289 235 417 114 278 268  64 163 241 442 486 326 271 443  18 156 406
 377 338 323 122 480 391  19 274 291 314 141 402 267 200 124 454 363 497
 116 227 261 481 383 394 126 118 215 441 244 115 129 166 447 275 352 189
 376 341 265 148 380 226 418 325  50 467 456 393 217 165 171 425 280  63
   8  46 397 150 416  49 345 136   7  36 207  96 332 491 429 120 168 327
 357 300 476 182 448 312 169 382 350 159 432 499 208 331  10 449 359 282
 213 309 194  15 187 464  84 279 369 452 198  33 490 358 234 471 157 142
 460  25 301 299 472 408 206 348 230 444 143  72  83  93 111 117 469 477
 504 109  28 162 378 430 199 473 233 113 501 315 220 108 313  56 196  95
 183 303  66  35 375 214 224 324  40 321 400 428 270 260 290  80 310 370
 470 431 474 201 340   6 344 266 160 202 102  44 164  67 488 145 158  24
  59  11  41 296   9  16  12 251 100 387  69  21 178 445  87  43  48 487
  94 135  32 366 154  17 133 385 364   2 461 204 285 276 123 259 254 395
 426 284  81 349 191 386 139  52  30 379  73 465 121 134 277 237 195 427
  62 422 228 374 343   4   5 333  22 177  31 459 192 242 152 137  97 420
   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7231
INFO voc_eval.py: 171: [6177 3943 1143 ... 6330  900 1565]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6194
INFO voc_eval.py: 171: [ 275  340  525 ...   53 1343  636]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5905
INFO voc_eval.py: 171: [ 98   1  91   2  53  58 117  85  77  83  13 133 111 116  93  47  19  54
  14  57  36 102  51  55 128 101  92   4  52  20  25  35  74  56  39  18
  94  37 109 126 125  65  96  59  31 119 110  29  87 107   5   8  33 129
 132  69  44  17  40  22  60  23  49 100  46  27  70  84 134  10  26  68
  16  28  66  63   9  38  89 105  73  45  72  34  90   0  76  71 103  75
 131  42  97  88 127 121 122   7 130  15  95   3 118  61  64  78  21  50
 123  67  41  81 106  80  24  62  79 120 114  32  82  12 112   6  48 115
  86  30  11  43 104  99 113 124 108]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3005
INFO voc_eval.py: 171: [154 151  55 213 226 329 166 128 227  13 244 113 255 171 168 144 239 288
 155  60  77  14 313 268 214  66 192 153  70 314 187 206 121 197 243  89
 300 303 193 133  39  52 195 158 152 108 273 264 111 328 107  67 337  92
 112 132  75 240 257 182  31 131 338 246  96  76 225  94 126 106 207  45
 123 253 130 196  88 230  25 269 277 287 160 241 142 307 175 135  69 274
 278 177 219  32  44 275  85  57 245 321 141 235 139 323 115  90  87  27
 178 250 103 136 234 276 203 170 293 242  40 318  82 211 172  18  20 282
 215 149 252 265 296  64  62 180 221 100 306 163 281 339 286  29 146  93
 105 302 248  15 259  26  97 309  54 327  10 317  68 116 212 326 294 311
 263  83  43 189   7 298  34  65 330 279 190 202 165 236 256 224 331   5
  51 272 295 231 292  98 260 289 184 181 176 258 249 164 223 198 148  12
 334  33  36 157 124 247 120  46 156 209 229  86 174 310 205 312  41  35
 315   6 261 297 204   4 102 110  16  91 216 188 183 332 122 173 217 208
  30  19  53 162 325 266  21 262 140 222  81 285 238 186  71 201 159 104
 185 319  72 137  84 254  74 291 333  22  17   3 220 191 210 138  78 324
   9 127  80 299 117 280 147 305  56 251 194 101   8 320 199  95  47  24
 284 200 335  37 134  42 316 114 336  50   0 232 267  23  28 125 167   2
 270  38  48  61 119   1 143  11  63 322  49 169 179 161 118  58  73 145
 304 308 218  59 283 129 150 228 237 290 271  99 109 233 301  79]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3736
INFO voc_eval.py: 171: [278 177  46  51  95 242 154  94  73 203  98 207 159 246 155 388  47 362
 354  80  40 227  30  50 195  97 181 116 241 317 310 311 232 164 208 399
  72 180 245 176 147 375  77 222 397  60  79  67 173 138 260 192 150 100
 262 327  49 178 191 410 329  44 357 156 146 238 107 193 302 135 265 243
 166 406 374 408 111 201   5 218 123 335 172 254  34 296 343 263 401 133
 272  81 101  39 292  85 276 378 380 332 398 236 402 319 114 294  93 209
  13   8  66 257  36 413   0  82 264 305 141 136  87 318 230 142  31 409
 282 350 287  27 304 113 231 289 213 158 160  22 121   3 240 165 369 244
 239 330 189 394 179 298 234 407 368  74 281 228 170 365 220  61 233 235
 214 132 283  12  57 249  43 352 143 118 151 174 404 372 196 200 110 161
 279 359 273 190 295  68 367 194 270 137 120 361 171 117 384 364 186 122
 108 316 321 106 104 403 314  26 312 293 274 280  18 333 127 224  89  38
  58 396 217 377 392  91 252 149 187 349  83 303 145  15 315 115 247 253
 383  55 215 199 313 167  19  42 198 347 251 297 258 363  28 119 341  78
  16 109 386 124  37  24 202 291 308 168 320   2  48 255 324 250   1  63
  70 204 414 162 175 148 188  33  54 345 328  45 389 248  23  88 266 286
 339 358 334 225  96 412 259 348 381  69 275 373 144 322 169 134  10 337
 285 216  32  53 290 182  64 309  21 139 405 267 382  25  92 125 219 237
 376 353 344 411 129 102 371 346 400 205 306   7  41 340 163  59 391  71
 131 299  17  35  84 185 300 210 206 387 152 326 370  29 223 271 212 269
 395 336  52  14   4 356 229 342 211 256 130 157  11 301 128 288 379  20
 331  90 351 140 221 338 183  86 277 112 325 390 126  99 307 360  75  65
  76 103   9 105   6  62 366 153 355 184 284 268  56 226 385 197 261 393
 323]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3889
INFO voc_eval.py: 171: [189 357 514 763 548   9 420 495 691  52 661 321 429  75  45 549 177 747
 109 558 717  55 779 626  83 262 517 539 143 161 465 508 430 494  24 307
 106 108 180 239 521 771 261 585 219 443 796 560 107 367 113 129 552 556
  79 310 676 269 576 497 431 432 220 748 199   7 582 806  69 718 111 535
 616 393 381 493 527 749 257 418 721  54 791 456 578 105 388 174  95 720
 165 147 291 426 629 590 794 628 525 575 144 636 104 800 588 179 153 532
 579 338 487 584 185  76 648 304 591 732  38  53 459  12 554 414 325 317
 739 509 387 187 148 524 793 678 703 440  99  86 577 742 437 345 204 466
 671 738 541  22  88 458 373 347  51 231  40  35 168  84 635 580 446 663
 534 371 363 274 305 195  29 162 533 121 631 298 743 308  16 679  46 464
 366  59 798 724 784 411 598 233 596 419 684 272 677 327  47 595 551 642
 415 783 633 505 212 424 197 520 498 485 807 246  87 335 707 744 169 280
 342 112 243 192 214 205 228 227 546 348 455 792 612 118 488 695 290 171
 523 406 790  25 217 501 398 727 801 813 117 402 764 692 786  20 751 216
 133  62 775 242 506  89 115 773 688 650 777 522 320 607  72 256 332 391
   5 788  30 715 157 762 156 410 378 500 655  85 502 689 593 737 680 390
 571 769 191 725 618 417 382 581 772  67  23 597 574 555 206 181 421 322
  34 649 462 562 781  56 289  73 100 484 644 510 241 158 253 146 428 479
  10 137 215 286  41 193 486 433 729 221 434 134 752 564 324 662 553 489
 225 753 572  32 350 780 507 408 438 247 586 666 277 149 647 397 490 450
 722 675   6 360 344 482 203 587 599 152 271 812  77 196 453 268 728 565
 237 620 797   1 702 445 660 630 294 244 259 615 155 178 339  61 127 668
 804 683 537 714 258 201 356 787 235 603 682  42 767  94   2 140 309 536
 448 719 531 609 696 400 380 799 154 249 809 803 386 122 563 125 789 301
 240 439 190 550 313 481 463 364  31 248 768 442 278 409 567 164 733 126
 449 103   4 423 412 265 314 120 110 281  60 756 218 384 608 699 470 811
 184 234 570 625 160 605 704  64 491 667 270 306 353 473 150  63 392 292
 665 194  13 694 138 352 336 283 188  58 457 687 624 210  43  48 545  18
 273 735  21  66 547  11 782 600 734 528 358 319 202 359 476  97 444 116
 623 712 808 621 451 200  68 646 170 573 447 145 697 561 299 293 315 492
  27 435 672 606 441 516 681 250 559 374 685 673 211 119 731 316 297 785
 705  39 312  17 617 540  96 389 186 654  80 263 226 300 461 518 701 340
 407 730 454 557 385 427  26 207 757 404 513 601 252 351 224 776 213 329
 330 778 376   0 114 480 136 569 436 163 641 726 736 365 750 638  98 368
 172 251 686  49 333  28  82 637 132 403 657 349  57 475 711 765 296 669
 632  78 176 604 613  90 379  93 229 238 802 326 123 279 496 503 538 377
 478  37 361 222 530 302 287 369 664 232 610  44  14 512 515  33 741 634
 355 124 759 543 416 236 230 264 627 245 653 375 499  70 223 183 139   3
 131 405 198 334 208 810 395 331 362 255 652 295  19 284 267 303 422 529
 754 209 167 542 504 592 166 511 805 159 760 394 709 401 544 670  65 142
 425  71 399 372 519 276 766 723 254 700 474 706 413 708 651 182 288 469
 611 583 311 343 477 761 770 260 135 452 745 175 318 151  36 622 656 468
 602 594 275 568 659  91 337  81 640 323 740 396 266   8 645 282 285 690
  50 128 483 341 354 173 526 643 716 658 370 346 774 614 141 101 693 566
 639 130  92 758  74 383 460 698 795 710 102 619 589 755 467 328 674 713
 746 471  15 472]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4887
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4115
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.358
INFO cross_voc_dataset_evaluator.py: 134: 0.627
INFO cross_voc_dataset_evaluator.py: 134: 0.229
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.553
INFO cross_voc_dataset_evaluator.py: 134: 0.571
INFO cross_voc_dataset_evaluator.py: 134: 0.473
INFO cross_voc_dataset_evaluator.py: 134: 0.008
INFO cross_voc_dataset_evaluator.py: 134: 0.530
INFO cross_voc_dataset_evaluator.py: 134: 0.205
INFO cross_voc_dataset_evaluator.py: 134: 0.392
INFO cross_voc_dataset_evaluator.py: 134: 0.065
INFO cross_voc_dataset_evaluator.py: 134: 0.382
INFO cross_voc_dataset_evaluator.py: 134: 0.723
INFO cross_voc_dataset_evaluator.py: 134: 0.619
INFO cross_voc_dataset_evaluator.py: 134: 0.591
INFO cross_voc_dataset_evaluator.py: 134: 0.300
INFO cross_voc_dataset_evaluator.py: 134: 0.374
INFO cross_voc_dataset_evaluator.py: 134: 0.389
INFO cross_voc_dataset_evaluator.py: 134: 0.489
INFO cross_voc_dataset_evaluator.py: 135: 0.411
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.463s + 0.002s (eta: 0:00:57)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.381s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.399s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.388s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.389s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.387s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.388s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.392s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.387s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.386s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.383s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.385s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.384s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.539s + 0.002s (eta: 0:01:07)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.419s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.407s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.394s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.385s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.389s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.394s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.394s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.391s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.391s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.389s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.393s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.389s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.486s + 0.001s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.374s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.374s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.381s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.378s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.379s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.381s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.383s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.387s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.384s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.387s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.387s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.388s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.565s + 0.001s (eta: 0:01:10)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.378s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.374s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.374s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.376s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.379s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.385s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.389s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.389s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.391s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.387s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.386s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.388s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 56.638s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [457 137 724 580  17 120 333 123 118 548 251  42 327 111 437 459 390 444
 369 115 815 748 110 112 439 461  46 779 397 231 338 216 391  93 471 725
 792 649 809 364 188 139  66 440 394 113 222 367 396 403 684 316 504 512
 458 608 682 473   5 119 215 400 127  44 232 344 281 293 728 704 427 670
  49 699  24 607 350  50 569 401 611 411  76 116 150  10 464 659  45 170
 772 638 238 663 578 541 319 468  19 399  88  63 730  91 813 805 503 181
 520 357 286 527 735 332 579 711 705 750  56 585 617 536 525 207 660 126
 294 620  90 260 429 516 428 789 818 460 219  22  39 496 653 546  48  38
 824 159 233 717 186 362 688 679 328  85 144 434 510 372  11 583 618 610
 145 837 454  15 172 331 769 796 365 189 766 574 678   2 285 692 296 812
 466 486  20 423 378 575 515 425 621 105 807  21 217 155 785 352 827 531
 686 754 306 104 117 714 733 514 773 524 227 325 417 261 771 235 723 389
 108 277 594 586 549 402 501 330 587  99 606 642 632 697 311 467 121 300
 209 626 633 146 783 693 436  60 776 799 446 301  57 100 798 817 441  28
 254  36 572 715 255 677 282 322 237 326 537 526 480 422 393 267 138 392
 289  82 226 497 662 831 791  26 366  23 190 129 321  30 133 374 775 721
 128 162 535 114 431 596 165 156 847 788 522 407 442 395 213 582 598 264
 675 734 476 101  43 647 200 727  12 141 563 398 470 483 614 355 756 674
 622 568 334 562 644 345 782 317 410 624 304 819 151 634 551   8 315 371
 712 249 307  14 363 406 187 107  51 463  18 625 323 814 840 493 781 700
 846 687 339 337 147  52  37 224 438 604 196 509  33 413 230 500 361 698
 269 185 384 652 448 482 636 665 469 383 173 703  32 495  34 571 529 550
 244 534 262 759 142 589 751 729 205 719 274 148 848 370 494 842 839 731
 749 160 506 201 242 297 635  72  61 826 498 706  81 479 134 784 605 643
 816  73 478 452 616 176  87 843 450 132 838 566 474 283  69 668 694  77
  78  98 295 248 443 671 738 191 376  55 432 808 683 158  31 268 275 102
 279  25 472 166 845 122 484 245 291 581 521 637  27 584 650 220 499  79
 336   1 523 517  59 559 340 800 180  68 613  84 786  75 770 258 250 603
 430 708 530 747 154 288 342 455   7 538 654 803 829 690 424 447 421  64
 308 377 276 657 544 780 103 801 404 414 561 252 292 418 270 844 167 161
 726 680  53 804 354 619 320 490 673 194 312 745 661 359  94 183 577 841
 257   0 409 387 707 554 284 456 135  40 433 313 453 253 836 356 802 552
 198 560 368 834 124 564  83  35 451 380 627 507 672 234  41 558 221 588
 204  97 777 820 830 182 149 358 130 136 225 419 435 565 343 287 329 628
  92 218 412 597 349 768  54 513 651 832 528 623 532 761 739 823 763 125
 259 375  16 208 202 449 314 131 388 223 591 821 744 177 533 787 206 543
  70 256 681 658 241 691 246 709 143 179 511  29 557 612 833  65  74 263
 573 140 640 645 630 502 405 556 689 481 518 168 701 485 318 656 465 540
 302 696 713 669 210 272 547 290 462 567 667 508 600 351 741  58 303 488
 239 505 420 169 106 615 171  96 487 309 341 542 590 810 247 475 753   3
 702 629 212 360 184   9 570 492 599 737 740 742 822 305 746 386 666 236
 416 373 797  89 825 324 164 835  13 489 695 273 790 539 592   4 153 545
 229 243 793 576 764 197 757  95 408  71 346 732 710 228 240 685 266 794
 381  62 778 519 553 385 664 765 175 743 755 811 477 195 199 347 353 382
 828 655 795 718 163 762 265 555 109 271 157  67 192 601 716 152 720 676
  47 203 646 760   6 278 609 648 214 806 641 379 445 595 193  80 298 335
 602 280 593 310 722 415 491 426 631 752 348 736 299 174  86 758 767 774
 211 639 178]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3574
INFO voc_eval.py: 171: [175  92 588 407 688 529 654 665 417 693 691 396  93 534 690 112 694  79
 459 664 530 623 317  54 478 453 202 402 677  52 331 516 401  42 228 557
 572 699 669 577 535 680 698 689 327 270 171 114 559 267 584 437  40 460
 639 532 272  98 139 253 364 432 452 519 626 142 622 643 454 415 548 649
 640 406  96 319 709 410 618 403 489 540 636 101 703 589 312 475 590 627
 416 293 455 362 177 354 575 609 166 580 178 538  55 581 398 687 234 188
 585 630 418 631 481  78 397 633 711 307 714 721 506 119 490 162 400 292
 249  94  48 404 100 718 115 700 678 149 659 611 405 268 315 600 716 710
  53 670  41 370 413 341 120 264 269 439  17  14 427 625 286 399 227 717
 103 531 616  27 593 441  43 430  99 493 109 187 517 563 673 476 686 344
 674  61 555 290 414 474 106 200 702 560 125  88 123 598 574 594 527 243
 105 107 509 438 148 552 359  33 394 306 224 318 675 660 587  30 704 571
 482 138 425 252 463 419 236 511 127 638 328 599 412 436 470 707 442 692
 701 294 426 663  70 570 179 229 462 296 261 628 392 671 672 569 304  82
 185 495 172  15 536 443 282 116 358 719 697 226 184 133 592  26 657 173
 332 189 578 428 566 556 248  39 356 713 523   4 583  19 551 131 391  56
 451 613 597 564 565 211 167 553 144 650  67 695 485  83 374 195 492 208
  29 245 522 238 182 423 345 435 321 561 684 165 164  45 568 369 576 386
  76 298  60 143 221 132 170  97 353 256 108 512 207   5  86 706 342 265
 225 444 420 241 257 262 176 375 215  37 411 232 434 201 303  44 603 129
 130 168 334 260 352 586 376 381  81 424 591 558 550 543 247  80 289 102
 456  51 518 494 287 285  57 508 160  69 712 624  18 163  68 723 668 408
   0  31 390 305 347   6 210 324 157  11 323 192 322 199 161 601 283 145
  46 546 197 159 373 384 715 216 151 365  75 355 351 194  73 648 337  63
 541 237 372 642  21 632 363 498 446 380 521  65  10 503 117 196 233 276
 479  22 722 602 325 295 614 464 311 480  35 433 104 658 146   8 422 501
 214  84  91 150 544 128 153  16 316 488 610 612 204 525 504 393 110 421
 647   1 277 666 653 651 582 206 134 637  34 191 255 291 679 554 395 542
 652 379 310 357 186 320  20 457  66 634 313 567 604  13 141 683 682  71
 330 274 326 275 389 152 596 220 483  62 449 681  89  64 361 314 387 348
 368 471  47 617 209 246 217 371  87 280 461 181  72 329 183 349 514 487
 218 472 302 378  23 309 720 528 136 335 250 467 545 466 205 595 549 300
 685 213 641  28 222 174 477 644 635 445 137 667 251 696 190 266  58  49
 491 258 573 515 124 447 288 661 539 547 259 180 360 263 297 448  38 469
 429 656 450 203 118 333 339 122 340 473 465 705 440 135 537 271 155 533
 620 254 526 212 140 662 169 336 513 646 219 382 338 284 308 484 607 121
 409 385 458 676  85 350 497 111 496  50 486 655 507 230  25 193 431  77
 301 126   2 708  90 158   3   9 239 242 154 562 147 619   7 383  32  24
 367  36 615 645 468 500 198 299 278 113 605  95 621 240  12 505  59 281
 520 343 510 231 235 524 606 156 346 223 279 366 502 377 579 608 499 629
 273  74 244 388]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6238
INFO voc_eval.py: 171: [ 88 207 247 165 167 243 229   4 166 507 508 468 509 385 472 311 418  82
 308   3 192 174 470 198 170 557 126 183 510 512 429 405 248 536 221 526
 241 195   9  30 230 262 424 134 414 521 544 513 348 575 343 193 216 232
 522 528 422 538 310 428 237 380  41 511 572  10 434 577 245 449 231 568
 473  73 519 489 171 440 266 143 184  45 305 506 490 265 430 299 317 244
 514 150 105 527  95 532 517 361  53 297 219 366  15 559 107 102 309 547
 494  87 118 529 398  28  31 524 397  65 373 289 374  54 546 172 431 161
  86 185 208 199  89 267 109  84  17 337 543  67 471 375  14 402 209 179
 459 175 169 211 312  16  74 103 565 464 153 351 520 225 217 187 108 138
 133 346 413 303 304 352 567 504 178 145 214  47 564  51 160 285 349 356
 319 387 128 488 296   7 270 355 106 190 358 438 432  55 316 443 415 279
 294 353  80 326 409 186  60   8 492 493 389 188  40 268  94 500 327 503
 129  72  70 191 246 100 381 347 478 176 335 142   6  98 222 292 291  50
 254 121 220 306  33 250 257 255 110 469 566 408 278 342 482 251 396 372
 495 336  97 338 253 239 412 363 400 328 163 234 410 273 483  85 236 516
  32  58 112 525 329 377  93 147 157  66 321 540  13 365 435 256 485 238
 407 196 287  42 354  23 535 156 445 474 555 125 260 341 465 201 359 168
  71 558 205 203 479 181 560  59 423  11 202 154 240 350 562 476 360 523
 204 148 549 210 124 367 554 491 315 344 324 264 545 376 136 569 419 295
  68 534 378 334 370 541 293 263 226 401  57 139 114 340 515 433 286  78
 448 135 426 164  52  27 502 446 579 574  83 197  99 212 119 457 122  29
 497  75  49  20 477 194 233 463 333 501 117 116 581 550 391 159 271 215
 151   1  34 345  22 456 123  18 553 571 484 130  46 392 275 284  26 322
 551 180 475 421 442 441 323 573 383 132 357  43 364  64 499 552 420 563
  39 320 101 436 518 331   5 395 368 224 242 371 451 460 467 149 281 542
  56 249 206 274 453 146 223 416 481 531 582 388 111 461 403 362 496 300
 298 390 530  92  63 570  21 189 115  69 369 141 394 235 330 455  90 466
 200 406 379  61   2 404  12 427 454  25 450 576  91 314 152 548 486 177
 127 113  77 269 276 393  38 307 462 318 439  76 447 382 155 288 561  24
 313  44 325 301 120 282 158 302  37 332  35 578 104   0 384 227 437 162
 556  81 252 228  36 452 417 411  19 487 399 173 277 290 283  48 280 386
  62 425 480 261  79 144  96 218 137 533 580 537 539 140 272 182 213 258
 259 339 505 444 458 131 498]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2272
INFO voc_eval.py: 171: [ 993  994 1154 ...  841  100  752]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3543
INFO voc_eval.py: 171: [1134  650  699 ...  361  397  911]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5518
INFO voc_eval.py: 171: [ 61  36 119 122  43  13 188 314 149 130 240  75   6  39 249 300  46 210
 235 234 224 298 174  55  25 312 259  33 121 306  45 198 257 164 116 281
 313 147 253 153 246 222 230 266 172  44 327  96 182 260 192  19 156 294
  38 219 140 128 125 318  86  47 185 321  49 225 138  63 271   4  51 242
  10 213 261  26 304 238  80 109  15 141 267 276  12  81  27 110 289  20
 135  78 322 239 277  98 206 104  74 170 288 252 315  84 168 286  40 208
 207 179 323 157  29 296 129 311  64 325  67 272 326 124   3  92 290  56
  30 131 193 236 134 284 126 268 200 203 241 117 197  71 148 175 275 176
 173 178  54 244  48 167  28 146  90 301 264 265 106 120  85  76  16  87
 254 262 250  42  17  89  58 144  37 160 279  99  14 113 270 151 127  41
 202  83 136 255 317  70  52 292 291 302 209  94 112  97 155  68  22  11
 111 285 218  82   8 184  50 194   7  35  18 205 171 190 231 229 258 319
 278 280  91 214 295 212  53  79 108 256 199 305 247 299 328  21 180  60
 132 237 216 102 158 220 211 232 103 283 263 282 310 228   0  62  69  95
 316 221   9 233 161 105 195 100 308 201 245 226 287 114 133 163  88 181
 309 187 107 196 243  24 297 154  57 251 293   2 101 139 169 143 269 248
  66  32 165 191 215   1   5 159 150 320  77 227 152 118 162 145  23 186
  65 142 115 324 223 123 166  34 307  72  93 204 137  31 274 183 303 189
  59 177 217 273  73]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5706
INFO voc_eval.py: 171: [838 883 384 ... 849 229 972]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4688
INFO voc_eval.py: 171: [ 9  4 21 92 22 82 37  6 89 86  1 64 50 33 67 65 47 15 85 29 10 12 46 51
 62 90 24 78 17 63 93 88  3 76 74 48 40 57 18 28 39 36 34 41 56 52 68 55
  5 16 61 83 94 26 32 45 80 11 42 87 13 30 53 69  8 59 71 25 49 23  7 81
 66  0 60 27 75 44 73 72 77  2 38 14 31 91 79 70 35 43 54 20 19 84 58]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0084
INFO voc_eval.py: 171: [4692  507 4694 ... 2552 3482 1121]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5255
INFO voc_eval.py: 171: [  5  58   6 108  43  88  36  33  97 118 122 125  39  57  80  60 126  52
  30  74 104  90  37 120   9  96 113  51 133  92  75  10  99  26  95 119
  65   7 112 101 103  49  41  77  35  84  86 124  61  62  25  69  32  91
  21 131  68  12   3  53 111  63  78  29  38  45 110  19  14  46  55  34
  28  71  54  89   8  67  20  40   4 117   1 109  64  79  31  70 116 102
 130 121  11  47  22  42  87 123  48  72  24 114  76  50  98 129  82  17
  94  23  18  59   2  83 115  73 105 128  27  93  44  56 127  15  66 100
  85 107 132  13  81  16   0 106]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2010
INFO voc_eval.py: 171: [ 653 1477  361 ... 1095  950 1702]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3836
INFO voc_eval.py: 171: [ 81 157  88 127   3  18  85  47  89  82 176   6  33 173  52  36  49 171
  77   0  39  57  31 138 114  69  48  26  12  67 134 160 101  11 116  76
 120 155  56 182  34 164 183 129 169  41  75  30 177  62 106 128 145  15
 181  51   9  99 140  10 107 162  86  38 184 125  55  73 109 113 121  25
  63 102 153 110 167 146  96 159 156 115  53  54 111  16 133  94  93  83
 122  95 100 158  87 147 175 126  58  13 163 103  35  64 154 130 108  60
 117 142   2   7  22 124  74   5 141 104 166  44 137 132 123  80 105  50
  72 143 165  65  17  71  21 118  27  84 119  37   4  90 185  14  40 174
  97 172 148  43 144  79 136  61  91 168 150  98 151  24  20  92   1  19
 178 139  42  70  32  66  68 131  28 170 180  45  46 149 135   8 152 112
  23  78 161  59 179  29]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0599
INFO voc_eval.py: 171: [113  98  17  22 137  74 101 145  97 151  47 114 120  19  92  59  81  18
  48  95  25  79 141 122  76 134   7 123 131  99  63  52 116  53   8  84
  34  86  67  35 100  46 135 117  41  32  85  24  26  43  68  56  75  33
 119 118   1  10  42   2 112  30  60 142   4 111 102 127  29  87  78  58
  94  49 130  23  38 147  83 132  61  14   0 104  54 138  50  66 133 144
  69  11 124  65  45  73  70 125 149  51   9   3  93  82  37 128  21 140
  77 150 105 115  39 109  64  27   6 143 136  16  28  91  89 108  44  36
 103  88  13  62   5  12  96  90 121  80  71 126 106 107 139 129 146 148
  20  57  15  40  55  31  72 110]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3896
INFO voc_eval.py: 171: [ 84 483  66 313 240 516 325 376 438  45 387 225 367  95 463  28 254  25
 347 326 360 231 497 462 233 127 266  85 203 414 161 525 526 428 364 144
  58 147 340 117 422 394 314 292 163 306 182 118  60 226 291 431 154 288
  76 216 192 416  11 268 514 468 361  54 151 138 198 184 237 434 282 267
 490 432  88 532 305 372 153 392 515 395  97 441 425 246 465 352  98 275
 302 183 201 520   3 263 259 328  22 386 186 109  49  67 479 410  31 284
 180 509 177 379 318 524  79  75 461 238 485 436  40 359  96 196  37 273
 486 527  63 464 137 535  99  51 276 327 400 134 219  41 112 418 513  29
 111 317 388 344 191 250 265 107 136 355 170 362 474 341 508 283 430 495
 236 426 159 334 466  59 234  92  73 439 378 125 190 449 224 329 223 311
 323 104 110 206 131 491 442 467 415 528 210  93 447 451 481 269 312 504
 258 530  62 279 185 307 407 199 300   1 249 277 345 260 382 353 452  81
 343  15 380 308 399 478  42 172 440 105 371 534 272 363 398 120 350 470
 164 253  82 261  71 471  80 113  70 443 445 510 129 270 287 297 517 433
 348 529  20 156 290 202 429 278 389 338 475 403  21 230 124 158 420 417
 286 377 213 135 488 496 132 512  50 310 148 409 469 155 293 122 446 294
 241 366 232 531 402 181 285 244 523 482  55 419  39 281 477 423 406  69
 351 357 476  53 299 336 264   9 121 505 193 176 522  27 408   8 178 207
  90 356 375 103 142  17 484 370 453 500 221 175 354 435 460  89 149 227
 166 349 480  36 171 373 331 298 457 200 444 322  61 381 320 492 501  30
 126 209 179 168 150 472 498 252 119 337 100 116 115 211 507  72 195 280
  14 427 384 222  12 301 396 502  86 212  26 248 536 215 383 332 108 123
 114 251  13 319 169 167  64 404  77  94 401 533 369 458 315 459 289  43
 339 413 397  34 220  19 489 365 309 393 235  38 346 391 102 499 152 228
 324 271 519  23 239   7 503 214  74 374 139 189  57  46  52 454 473 456
 303  87  10 411 217 368 173 140 405  47 106  44 487 101 130 205 208 141
 162  33   4 143 187 493 412  32 146  18 421 518 160 316   2 245 197  91
 145 358 194 304 321 511 204   6   0 229 296 424 274 188 330 218  68  65
  24 242 448 450 295 335  35 455  78 128 333 494 256  48 521 165 262 157
 390 133  16  56 506 437   5 385 342  83 255 247 243 257 174]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7139
INFO voc_eval.py: 171: [7310  572 1340 ... 2697 7579 4763]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6244
INFO voc_eval.py: 171: [ 298  371  569 ... 1541  695 1484]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5913
INFO voc_eval.py: 171: [107   3 100   4  59  64 126  92  84  90  15 144 121 125 102  52  21  60
  63  41  16 110  61 138 111 101  57   6  58  23  80  39  28  62  44 103
  20 136  42 134 118 105 129  34  71  32  65  94 120  10 139  36 143 116
  45   7  19  75  49  25  66  55  51 109  12  91 145  26  30  76 114  31
  74  43  18  11  96  79  72  29  69  97   2  82  78  38  50  77 142 106
  95   9 131 137 112  48 141 140 132  83 104 115 128  46  70  17  67 130
  24  86 123  73   5  85  56 122  68  27 108   8  33 113  53  93  88  89
 133  47 117  13  35   1 124  14   0  81  37  87 127  22 135  99  54 119
  40  98]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2989
INFO voc_eval.py: 171: [166 163  56 230 244 357 179 133 245  12 116 264 275 181 184 259 155 312
 168  60 341  78 290  13 165 231 208  66 342 263  71 204 141 223 215  92
 164 210 331 125  53 327 356 167  39 115 213 110 111 296 114 138 285 365
  67  95  76 142 260 266  31 366  99 137 172  98  77 243 131 152 199 277
  46 224 127 109 248 311 273 214 291 297 136 189  91 335 265  25  32 301
  58 298 302  93 261 192 151 143 183 236  69  27 144  88 349  45 118  16
 106 149 186 161 238 253 270 193 351 254 346  90  18 232 262 300 287 323
 197 176  84 306 318 228 103  40  14 220 157 272  62 305 367  97 219 276
  26 334  64 108 337  55 329  29 310 358 268 339 355 279  68 229 100 354
 345   4 170 101 284 209  85 303 169   9 359 321  43 325  52   6 178 216
 119 313  65 242 322 316 206 256 201  36 190 280  33 278 241 123 362  34
 294 177   5 226 269 198 160 267   3 247 249 128  15 187 233  11 171  35
  89 188 205 105  47 234 225 343 338 340 324 281 113 174 221  41 360 240
 222  94 258 288 200 218 353  20  30 282 139 148  86 207 361 126 146  17
 107  54 304 352 150  28 309  72 117  75 211  79  83   8   7 203 202 326
 315 237 274  24 332  21  42 145 227 132 217  57 271  48  51 104 348 347
 363  81  37  73 130 182 364 120  49 159 180 289 153 308   2  23 122 121
 307   0  61 185  74  10 292 250 344 350 257 235  63 328 314 147 112 336
 154  59 283 252 195 102 299 135 286   1 162 293 191  38  80 320 251 333
  44 196 173  50 129 140 246  22 156  70  96 158 194 317 175 295 239 212
  82 134  87 330 255  19 319 124]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3655
INFO voc_eval.py: 171: [322 201  52  57 110 279 175 109  84 231 113 235 283 182 176  53 447  89
 421 408 261  46 205  35  56 112 133 222 367 360 278 266 361 204 282  81
 187 236 459  87 200 255  88 456 158 166  69 434 169  55  76 219 115 202
 301 197 381 303 218 378 414 474 177  50 168 469 280 307 433 220 155 275
 189 351 122   7 471 126 249 229 116  94 304 388 196  97  90 457 140  40
 153  45 272 369 396 319 343 131 293 463 108   9 315 440 237 339 384 437
 365 464  15 341  75 162  93 156 477 161   1 264 298 473 265 335 368 183
  42  36  31 332 180 306 281 242 203 130 355   4 327 215 277 262  25 428
 138  70 404  99 470 354  85 346 188 326 382  14 427  49 453 193 244 276
 163 268 466 252 431 171 323 135 288 157 271 125 270 134 419 328 406 417
 198 217  66 316 223 151 426 424 372 228 423 386 342 313 194 366 170 211
 455 364 184 221 257  77  44 465 362 139 137  30 123 119 212 121 284 101
  21  63 317 444  67 403  95  48 136  22 106 225 226 132 165 340 324 291
 394 353 363 436 146 248 422  54 292 299 232  32  17 443 451 294 345  43
 375 141 380 370  27 167 185 401 445 387   2 191 190 476   3  51  18 358
  79 245 124 111 100 290 285 415 159 289 206 398 373 338 213 432 331 164
 392 230  61 448  72  29 300  73 233 250 330 199 318 356 308 359 478 475
  10  39 441  24 154 148 258 247  37 467  26 336 117  47 186 402  12 107
 309  82 173 468 461 172 430 407  60  78  80 274 142 192 216  68  96 393
 390 442 397 239 256 150  19 160 429 210 435 454 241  41   5 334 400 178
 377 446 389  23  33 147 395 296 234 114 418 253 450 207 312 348 349  59
 128 118 321 383 102 143 311 240  16 350  13 439  86  74 371 263   8 347
 287   6 449 179 413 224 425 302  71 174  65 260  64  91 295  83 472  34
 103  98  92 310 329 149 391 254 405 452 243 227 352 286 120  38 267 399
 410 376 209 181 246  28 105 411 273 305 357 374 297 379 259   0  20 238
  62 208 438 416 460 333 385 314 144 409 145 127 214  11 337 458 104 420
 152 344 195 251 269 320 462  58 129 325 412]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3803
INFO voc_eval.py: 171: [211 407 580 852 620  11 481 559 777  62 366 743 491  86 621  55 199 833
 124 630  65 802 869 702 299  94 583 610 161 527 182 574 492 558 121 347
  32 123 202 270 588 247 657 861 298 632 122 886 419 128 505 145 623 629
 351 650  90 561 760 306 248 493 494 221 834 655 897  79 126 690 803   9
 450 437 835 605 294 557  64 595 479 519 120 806 652 881 805 445 110 705
 331 884 487 665 196 713 166 187 169 704 662 165 593 201 385 890 653 119
 551 344 649  87 601 666 817 207  63  15 626 656 522  47 728 824 475 575
 361 883 209 370 591 444 114 502 167 763 499 789  96 528 226 827 755 100
  28 426 177 263 395 613 190 745 651 392 508  50 711 521  44  97 423 823
 348 345 337 604 313 218  61  37  19 137 413 602 471 707  56 183  69 526
 764 888 168 874 670 570 418 672 809 873 828 761 480 264  57 372 709 309
 127 389 628 562 214 227 484 586 219 770 279 240 476 276 669  98 257 829
 686 133 720 891 882 258 880 898 191 238 590 792 382 663 565 618 549 552
 658 780 465 132 624 398  33 812 461 320  25 330 193 863 730 457 448 150
 518 853 876 245 905 130 878 589 378 102 867 837   7 800 681 178 244 774
 778 564 566 275  72  39 865 571  82 862 851 293 364 645 668 432 661  95
 470 447 738 810 765 671 203 859 213 478 775 229 729 438  13  78  66 576
 163  29 490 654 179 627 822 115 634 241 272 722  84 289 692 329 396 871
 482 648 814  51 870 550 152 500 636 367 838 326  43 496 216 280 542 749
 255 744 155 659 548 625 553 726 468 554 333 305 512 495 250 706 673 296
 173 317 807 660 572 646 391 633 839 694 759  41 277 813 903 225  88 887
 608 228 176 456 369 223 877   8 410 894 314 517 781  77 804 799 349 200
 295 138 545 386 768 752 507 158   1 266 341 174 600  52 268 788 767 635
 857 742 515   2 510 689 607 622 109 889 143  40 879 469 459 136 282 281
 544 441 246 900 357 141 435 118 501 750 682 406 677 338  71 525 185 212
 307 643 318 784 555 683 206 858 125  70 170 415 449 210 265 511 485 893
 520  16 473 346 747 383 843 142  27 679 617 402   6 902 532  68  73 271
 640 131 443 358 400  74 820 506 596 323 302 699 698 504 674 224 312  58
 236 181 408 700 321 237 156 779 363 217 409 539  14  20 192 818 336 582
 756 619 536  53 339 797 332 222 215 556 899  23 782 162 695 135 872 819
 497 815  76 773 503 300 771 766 584 112  35 283 256 427 611 647 816 691
 387 875 513 359 725 208 757 230 466 715 254 442 111 498  34 523 360 719
  91  59 845 129 355 399 736 446 340 790 148 892 675 631 811 642 239 416
 516 509 198 139 184 543  89 259 567 868 680 335 714 708 740  67 855 578
 342  93 786  42 430 269 411  49 609  46 104 488 404 866 140 379 772 463
 563 260 284 710 598 397 579 420 421 428 113 836  17 746 371 826 796 821
 327 262 334 205 154 375 581 615 560  80 374 538 541 108 462 524 301 343
 577 703 684 895 687   0 194 483 319 667 220 291 231 285  24 172 157 477
 434 568 251 233 175 733 464 678 597 454 147 188 160  75 412 901 785 380
  36  54 585 451 731 753 278   4 848 153 388 458 460 808 431 304 864 685
 424 616 171 531 324 267 794 486 474 825 697 189 840 384 377 540 204 718
 390 530 316 791 735 885 197  10 105 739 830 186  60 850 849 354 546 195
 754 297 362 455 783 290 741  45 798  81 394 303  92 144 422 716 860 439
 537 322 856 328  18 793 641 724 594 664 253 116 403 614 107 693 315 795
  21 904 146 529 243 252 149  26 117 325 639 569 373 535 308 103 737 676
 151 514 533 401 368 701 261 776 534 234 134 417 717 721 547  99 732 599
  30 606 352 637 638  85 453 310 440 847  12 433 842 841 751 106 688 854
 769 393 489  22  38  48 472   3  83 159 734 292 762 235 758 356 287 801
 274  31 376 612 286 381 242 696 273 592 832 603 365 405 288 353 436 180
 311 831 896 350   5 787 644 164 249 727 101 232 414 748 712 844 467 723
 452 573 425 429 846 587]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4907
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4093
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.357
INFO cross_voc_dataset_evaluator.py: 134: 0.624
INFO cross_voc_dataset_evaluator.py: 134: 0.227
INFO cross_voc_dataset_evaluator.py: 134: 0.354
INFO cross_voc_dataset_evaluator.py: 134: 0.552
INFO cross_voc_dataset_evaluator.py: 134: 0.571
INFO cross_voc_dataset_evaluator.py: 134: 0.469
INFO cross_voc_dataset_evaluator.py: 134: 0.008
INFO cross_voc_dataset_evaluator.py: 134: 0.526
INFO cross_voc_dataset_evaluator.py: 134: 0.201
INFO cross_voc_dataset_evaluator.py: 134: 0.384
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.390
INFO cross_voc_dataset_evaluator.py: 134: 0.714
INFO cross_voc_dataset_evaluator.py: 134: 0.624
INFO cross_voc_dataset_evaluator.py: 134: 0.591
INFO cross_voc_dataset_evaluator.py: 134: 0.299
INFO cross_voc_dataset_evaluator.py: 134: 0.366
INFO cross_voc_dataset_evaluator.py: 134: 0.380
INFO cross_voc_dataset_evaluator.py: 134: 0.491
INFO cross_voc_dataset_evaluator.py: 135: 0.409
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.612s + 0.001s (eta: 0:01:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.396s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.395s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.385s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.385s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.384s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.385s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.387s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.388s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.388s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.392s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.394s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.396s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.627s + 0.001s (eta: 0:01:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.413s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.400s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.387s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.390s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.390s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.399s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.398s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.396s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.395s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.396s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.397s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.394s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.434s + 0.002s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.376s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.376s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.388s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.385s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.384s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.383s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.386s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.384s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.388s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.387s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.389s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.391s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.436s + 0.001s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.366s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.386s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.377s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.380s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.382s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.379s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.380s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.383s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.383s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.380s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.383s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.383s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.310s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [492 147 779 619 131  18 357 134 129 584 263  46 351 122 494 467 416 475
 126 395 877 121 806 123 496 469  47 423 839 243 362 229 103 417 505 780
 149 852 390 198 420 697 470 871  75 124 235 430 422 393 540 736 507 493
 650 339 547 137 427 734 130 369  49 228   6 783 757 456 255  54 608 297
 310 440 751 721 648  25  85 498 159 428  50 653 127 713 375  11 683 832
 249 720 502  56 179 577  20 875 785 343 616 426 792 356 101 539 625 556
 383 191 302  97 808  70 572 758 617 563 867 311 660 551 100 136 218 495
 561 459  62 710 765 582  23 233 709  52 663 273 457 882 531 168  94 244
 701 352 849  40 545 623 740 397 154 153  16 661 771  42  12 196 388 199
 391 888 464 652 181 731 355 500 730 301 901 116   2 825 550  21 489 665
  22 613 874 845 744 164 828 812 857 521 614 313 788 452 768 128 231 891
 567 404 349 626 778 869 454 738 378 446 240 627 586 831 560 501 246 634
 549 220 114 327 319 112 155 429 354 745 274 415 334 292 132 836 477 647
  67 678 119 320 110 536 267 688 880 573 677 472  38 749 298 350  29  91
 860 670 148 138 835 143 239 532 611 418 248  63 712 637 843 466 139 859
 305 392 200 171 419 174 268 571 511 729 515 111 562 769  13 621 473 782
 421 346 400 280 435 789 851 125 848 776 166 451 151 588 600  48  27 814
  24  31 345 425 225 656 883 461 895 727 668 211 160  55 358 324 370 277
 607 726 639  15 439 911 118 842 156 695 363 434 558  57 381  19 497 766
 876 841 518 599 904 197 679 329 389  34 715 504 752  39 230 347 387 237
 587 692 195 157 503 340 341 739 182 570 396 479 517  35 361   9 242 283
 152 289 910 442 664 468 275 912 541 784  81 544 142 535 681 565 669 890
 750 528 666 207 315  33 486 807  90 533 645 773 508 108 786 809  68 169
 409 879 659 216 144 254 212 410 700  87 474  82 903 483 906 629 113 746
 514 817 718 902 795 624 690 610 755 175 844 133 402 299 530 360 282 760
 620 312 260 290 506 529 513 646 907 462 557 698 870 604 399 257 201 308
  86 680  96  93 909  32 534  72 185  26 167 682 846 295 364  88 261 478
 861 735 271  78 722   1 552 291 781  61 234 366 519  59 432 264 597  28
 655  65 437  84 574 840 460 165 330 805 644 702 742   0 447 485   8 335
 905 205 300  77 732 490 145 284 559 580 266 453 344 863 830 170 590 900
 761 525 190 176 307 865 893 908 443 163 382  36 725 336 304  92 107  43
 385  60 193 706 762 837 542 209 146 711 598 866 238 262 245  44 374 566
 158 403 662 796 135 450 109 413 592 394 463 615 884 104 448 367 140 406
 215 802 568 380 602 491 192 141 353 885 384 887  17 894 488 628 801 673
 601 187 864 564 465 898 717 723  73 671 270 401 150 213 569 482 217 827
 685 433 708 272 338 232 654 117 579 548 441 612 798 252 509 499 102 236
 219 596  64 321 763 638 719 258 741  83 554 606 303 753 177 286 414 631
  30 699 449 543 641 693 675 576 794 365 583 847 897 223 250 820 896 342
 872 445 799 716 822 221 520 797 581 247 743 594 546   4  14 667 850 657
  79 578 630 747 538 537 325 194 256 377 386 595 106 173 189 178 754 162
 733 259  69 322 889 371 609 276   5 704 288 804 811 748 899 436 886 853
 767 674 516 251 269 787 208 575 105  10 331 838 523 522 510 714 640 203
 278 241 823 800 253 813 306 632 398 172 372 412 348 858 555 527 642 728
 686 226 737  51 855 405 210 873 818 892  53 793 476  80 180 635 293 777
 591 651 214 703  89 296 618 524 854 285 206 411 444 770 222 816 526 316
  71 333 407 696 332 856  99 373 772 279 317 184  74 810 833 684 633 775
 359 764 408 161 294 878  76 868 512 204 455 643 480 585 323 815 481 824
 689 605 676 705 694 862 368  45 672  66 484 120   7 881  98 821 379 829
 826 593 636 471 791 224 188  58 622 834 724 759 774 186  95  37 328 589
 424 553 913 115 183 202 687 691 281 707 649 337 819 803 790 458 438 431
 265  41 318 326 314 287 227 756 376 309 658 487   3 603]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3695
INFO voc_eval.py: 171: [187  98 631 437 734 564 698 709 447 736 738 426  99 570 745 118 490 740
 565 733  84 708 667 339 484 509  58 432 216 721  56 244  46 613 431 747
 355 548 571 619 599 713 746 724 735 287 350 183 491 626 284 120 467 104
  44 683 567 390 270 147 462 436 289 684 687 485 445 150 102 483 670 553
 665 341 585 693 576 433 757 440 660 632 107 680 633 333 521 506 671 486
 750 446 617 595 176 574 387 189 190  59 622 380 428 311 250 651 732 627
 675 674 759 512  83 200 623 427 677 522 448 265 430 768 126 106 310 596
 760 172 434 328 703 538 100 765  52 435 158  57 722 643 654 566  14 636
 714 766 286 457 121 336 397  45 469 758 669 243 429 109 285 443 127 304
  16 471  27 105 550 280 717 718 365 597 730 199 658  47 112 460 308 132
 602 525 115 507  65 592 444 130 494 749 468 541 327 157 369 561 641 611
 424 258 111 616 214 637 505 146 756 269  94 351 134 719 589 513 384 238
 630 442 543 313 610 704 501 493 466 113 472 682  31 752  87 455 245 251
 340 324 315 737 635 767 191  36 707 458 473 242 609 422 527 701  74 572
 642 716 715 449 672 755 748 201 197 300   4 122 264 625 456 620  15  26
 379 741 196 184 356 138 278  42 140 383 516 606  60 763 185 640 524 260
 605 588 194 253 465 209 370 396  18 175 153 743  71  88 593 222 590  29
 604 557 152 225 178 139 344  49 419 401 655 103 556 653 174 221 694 594
 317 441 414 482 474 281 235 114 450 239 453 381 366  64 634 137 402  80
 279 182 464 273 728 262 754   5  61 579 608 618 108 762  92   6 712 628
 544 358 136 323 229 438 188 305 180 555 215 307  86 248 646  85 761 274
  55 325 206 154 668 454  72 256 378 409  40  48 391 170 526 160  17 346
 389 345 487 211 587 540 173 495 303 213 224  32  79 372 552   0  73 377
 208 476 277 692 686 764 582 773  77 171 347 400  38 167 403  50  11 770
 676  67 301 314 530 293 702 230 510 577  43 418 338 123  90 411 155 348
  21 710 159 644  69 332 463 249 695 169 135 488 361 110 252 656  10  22
 697 423   1 520 580 691 652   8 511 218 535 399 228 408 210 696 204  20
 309 591  66  97 295 452 451  37 645 559 681 607 162 624 349  13 412 492
 116 771 678 723 398 536 726 639 533 725 161  93 353  19  70 149 727 395
 331 334 480 193 711 330  75 546 514 220 425 352 417 198  68 405 261 705
 195 407 234 141 272 231 578 742 298 343 481 647 291 186 659 523 638 292
  76 335 415 145  95 321 688 268 679 359 219 202 165  23 489 288 223 319
 573 232 502 547 568 385 575 382 614 529 498 503 316 306 685 662 648 386
 563 720 470 282 360 500 271 586 144 148 373 664 267  51  28 475 584 302
 276 581 375 598 519 117 706 217 508 690  54 129  81 181  41 729 753 233
 376 275 528 226 497 477 515 357 364 439 246 329 496  62 257  91 700 542
 689 394 133 142 192 207 236 227 131 699  63 212  53 119 560 363 461 410
 532 156 504 459  39   7 164 125 299 388 101 479 539 772 393  34 318   3
 518  12 601 569 255 649 166  25 371   9 545 612 247 362 562  82   2 673
  96 205 731 554 128 558 621 769 744 661  78 499  35 168 237  89 629 241
 254 290 179 177 151 603 296 297 537 657 406 615 517 326 663 203 294 551
 404 478 163 320  24 240 337 266 583 413 367 650 739 143 600 322 416 534
 751 421 666 259 283 124 549 374 531 342 354 368  33  30 263 392 312 420]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6172
INFO voc_eval.py: 171: [ 98 245 293 190 192 288   4 273 583 191 584 539 585 443 542 480 367  92
 363 199   3 540 227 195 233 635 144 216 586 588 294 262 492 464 614 603
 230   9 285 487 597 274 476  30 154 589 311 622 405 598 257 653 365 228
 605 400 485 276 587 491 281 650 616 437  10 646  41 544 595 290 275 500
 315 518 655 360 167 564 196  79 314 374 590  45 351 174 507 217 604 108
 593 582  53 609 289 260 565  15 495 423 349 637 122 120  97 364 458 418
 606 600 625 135 117  31  68 569 457  28 218 431 197 126 624 100 234  54
  96 247  14 496 341 209 430 368 317 186  94 200 250 194 596 621  16  17
 541 248 394 432 269 643 118  70 466 258 221 529  81 645 535 254 207 169
 125 357 475 408 376 178 153 358  47 409  51 642 580 146 403 160 446 185
 337   7 225 413 477 406 121 320  55  90   8 348 318 415 412 497 331 148
 372 383 563 471 220 107 384 111 362 201  61  77 263 346 505 222 448 261
   6 165  74  50 138 568 550 296 301 343 511 302 304 291 410 438 127 110
 113 404 297 226 575 344 567 393 330 278  33 592  40 579 644 547 556 470
 456 188 557 106 570 392 395 300 280 420 385  69 324  95 469  59 129  13
 618 171 559 474 303 422 143 460 399 602 231 282 182 429 599 181 378 193
 339 633  11 283 386 536  32 472 502  42 243 416 552 398  75 632 434 513
 546 640 236 241 249 486 411 371 214 545 142 636 237 172 417  23 238  60
 591 638 345 156 647 427 433 613 309 270 407 566  57  20 155 161 611 112
 619 461 252  72 401 284 391 347 131 338 149 481 499 313 572 189  86 549
 179 627 424 435 255 321  83 450  49 623 578 136 152 381 140 659  29 489
 312  52 134  93  67  27 517 397 652 657  18 551  22 527 628 576 184 526
 229 232 558 515  34 631 402 277 451 141 326 390 508 629 267 649 440 133
 484 482 175 538 534 421 379 173 651  26 265 295 336 333   1 630 503  21
 128  66 414 574  46 608 244 520 607 447 620 212 287 388 555 116  56 594
   5 352 428  43 380 449  64 530 370 454   2 641 510 105 279 170 425 104
 463 223 350  39  73  12 145 163 325 101 202 455 132 235 525 523 654 660
 478 426 468 522 531 571 419 561 436 375 369 490 377  38 452 353 299 519
 387 356 271  37 198 180 130 648 442 150 488 319 639 340 109 327 532  84
 439  19 473 537 259 459 389  25 634  65 334 183 177 272 479  48  44 516
  35  85 162 187 512 205 208 656 253 483 528 445 504 316 506 366 581 494
  24 219 251 215 328 335 332 119 211  87 342 553 322 305 382 521 139 256
 158 137  76 658 204 203  63 102 610 123 124  91 617   0 310 462 242 354
 266 573 562 157 306 168  78 307  62 224  36 396 323 467 626 298 498 533
 286 465  80  89  82 554 444 548 615 147 292 239 240 246 264  58 164 308
 213 514 268 493 601  71 373 453 560 210 166 159 359 355  88 114 103 329
 206 115 612 577 176 509 543 524 501 361 441 151  99]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2310
INFO voc_eval.py: 171: [1087 1088 1269 ...  713 1682  182]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3544
INFO voc_eval.py: 171: [1237  776  726 ...  577 1449  621]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5514
INFO voc_eval.py: 171: [ 66  41 130 133  48  14 205 163 341  83   7 261  44 144 270 326  51 228
 324 255 243 254  28  59 191 280 339  38 332 132  50 180 306 278 127 168
 340 215 274 266 161 241 250  49 287 189 106 319 199 171  43  21 356 238
 281 209  52 202  54  71 346  95 154 152 244 139 350   5 293 136  55  11
 282 263  13 155 231 120 300  16  88  29 149 351 288 303  86 330 260 259
 108  89 314 121  31  22  82 114 223 187 196  93 141 184 313 342  33 273
 225 354 352 338 311  45 102 172 224  34 321  75 355 135 294 210  60  72
 315   4 256 192 220 217 193 128 148 262 289  79 214 190  53 195  58 137
 309 297 265 183 160 327 162 285  84 275 117  17  96  94  32 304 131 286
 158 292  62 283 100  47 271 124  15  99  18 276  56  46 317 150 219 109
 165  42  12 107 344 175 188   9 316  92 170 226 328  78   8 211 237 138
 251 347 232 101  76 230  24  90  20 122 104 222 331 269  40 208 123 325
 249 279 216 310 305 201 301 307  57 146 320  87 119  65 234 252 358 145
  23 277 337 197 239  68 113 284 335  10 105 308 267 240 343 253 218 112
 258 173 212 229 116  27  77 312 179 245  37 322 110 118 157 177 248 125
 143 147 164 181   0 264 213   6   2 207 336 111  61   3 272 129 268 167
 233 198 186 204  97  74 153 290 174 169  85 333 203 178  63  35 246 156
 318 159 242  80 194 126 291  73 206  81 295 103 329 200 353 349 151  26
  19 185 166 296  39  36   1 247 115 182 236 142 323  67  98 302 257  30
 357 176 299 345 134  69 140 348 227 298  64 221 334  70  91 235  25]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5456
INFO voc_eval.py: 171: [ 912  957  415 ... 1176  889   87]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4809
INFO voc_eval.py: 171: [ 10   5  22  98  23  88  40   7  95  92   1  53  69  36  70  72  50  91
  16  31  11  67  13  49  54  68  94  96  18  83  26   4  99  80  61  81
  51  42  19  30  37  43  39  60  73  65   6  55  17  34  48  59  89  93
 100  28  86  14  12   9  44  57  74  27  32  63  52  71   8  76  87   0
  24  29  41  77  47  64  79  75  15  33   3  97  82  90  58  46  85  38
  56  20  21   2 101  66  25  45  62  35  84  78]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0082
INFO voc_eval.py: 171: [5109 5111  554 ... 4512 1644 2870]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5289
INFO voc_eval.py: 171: [  5  64   6 115  46  93  38  35 103 129 125  41 132  63  86  66  56 133
  32  81  39  95 110 127   9 102 120  55  97 141  82  10  28 105 119 101
 126  53   7  71 107 109  44  83  37  27  89  68  91  67 131  57 139  34
  75  74   3  96  69  22  13 118  84  40  31 117  50  36  15  49  20  61
  59  73  94  43   8  30  70  78  12  77   4  21 123  45 137 128 116 124
   1  85  92 108  33  52  51 121 130 136  19  23  79  26  54 122 104  18
 111  25 100  87  65  48  80 135   2  88  98 106 114 134  90  62  29  16
   0  14  24 113 112  42  17  11  72  76  47 140  60  58 138  99]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1969
INFO voc_eval.py: 171: [ 726 1620  398 ... 1074  713  794]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3833
INFO voc_eval.py: 171: [ 90  98 169 139   3  18  95  91  51  99 190   6  35  57 187  38  53  86
  43 185  63  33   0 150  77 124  26  52  75 173 146  12  11 111 167  61
 132 127  85  36 141 197 198 178 183  32  84  45 140  68 116 191 157  55
  15 195   9 109 117  10 152 199  81  96 176  60 137 112  42 119  70 181
 133 123  25 165 120 168 145 172 158 170 125 106  58  92 121 110 134  59
  16 103 142  64 104  97 189 138 105 113 159  37 115 177  13   2 166 118
  22 154  66  71  82 144   5 180 136 128 114   7 135  47  80  79  89 153
  54 149  21  40  27 179  73 155 130 100   4  17 129 101  94  14 186  44
 200 182 188 148 156  88 107 160  67  23 151  78  20 162  30 143  19 102
 163   1  74 147  65 122 161 194 184  34 108 171  46  76 174 192  29  24
 193   8  49  62  87  48 164  56  93  41 175  69  83 126  31  50  28  39
  72 196 131]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0749
INFO voc_eval.py: 171: [125 109  20  25 149  80 112 159 108 165 126  53 132  22 103  64  21  89
  28  54 106 153  86  84 134 146   8 143 128 135  58 110  57  68  92   9
  39 129  95 147 111  72  40  47  52  37  94  27  30  49  74 130  81  61
   1  38  48 131  11  35   2 154   4  65 124 113 139  33 123  85 105  98
  55  63 144  91  26 142  16  66 150   0 161  43 115 156 145  59  75  56
  71  70 136 104  79  13 137 163  51 140  83  24  77 127  90  42 152 164
  10 148  31  45   3  32 155 116 114 121   7  18  69  50   6  41 133 107
 120  14 100  15  99 102 138  88 117  67 162 151 101  76 118  29  23  19
  96 141  82  62  78 160 157  36 122  17 119  73  60  46  93  44  34  12
 158  87   5  97 166]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3871
INFO voc_eval.py: 171: [ 91 520  72 339 264 554 353 405 472 247 396 417  51 502 104  32 277 375
  29 354 389 499 253 535 255 138 288  92 445 221 175 565 563 160 460 393
  64 128 157 177  66 424 368 197 454 316 340 463 167 330 248 129  83 315
  12 311 448 238 209  60 215 290 505 552 390 164 150  95 289 261 570 467
 465 528 199 329 553 305 107 380 475 557 166 422 108 401 425 501 457 269
 198  73 326 281 217 298   4 203 356  55 120 285  25 416 516 195 344 307
 562 442 523  35 498  69 546 192  46 522 262 213 409 564  82  86  43 470
 355 388 106 296 500 450 149  57 299 343 146 383 573  47 122 118 391 430
  33 109 511 369 185 123 241 173 550 148 545 100 462 260 362 372  65 503
 273 473 418 208 458 287 256  79 306 408 533 207 136 121 566 357 101 229
 335 529 114 484 351 486 245 142 246 230 477 504 568 447 302 518  68 331
 216 200 381 291 488 542  88 392 332 272 280 187 338 324 438 373   1 474
 515  16 300 282 400 412  48 371 179 429 489 378 507 410 219 131 115 508
 567 283 226 428 547 525 140 276 376 295  76 170 572  77  89 466 481 461
 301 512  87  23 419 124 310 478 321 314 555 569 172  56 366 292 252 135
 534 147 452  24 561  61 168 506 406 254 483 433 551 514 196 449  31 143
 395 309 560 234 308 451 318 377 440  75 364  45 133 432 455 385 379 543
 513 161 210 384 225  97 404  10 334 538  96 267 162 186  19 437 323 439
  67 519 317 193 113  59 402 181   9 249 304 349 154 286 228 517  42 132
 243 322 495 220 359  34 191 399 190  13 382 490 365 130 303  15 536  30
 110 103 539  93 212  78 163 544 509 521  14 459 494 126 530 237 182 119
 127 275 480 411 233 527 194 540 232 184 360  70 346 183  38  22 341 137
 125 431 421 398 497 444 397  63 313 426 271 414 427 325 274 574  49 134
 293 434 524  26 394  84 141 403 413 571 244 327 374  94 345 558 496 151
 367 165 491 441 333 239 206  52 152 435 236 537 250 156 257  37 214 224
   5 112  11 468 541 242 204 352   8 510  81 111 548  58 342 263 211 423
 153 176 174 251  98  36 116 531 358 361  71 268 443  50 265 159  53 493
 347 453 188 205 158 387 456 240 139   0 180  21  44 363   2  41 556   7
 279 278 532  62 320 223  80 178 259 328 144 370 471 386 415 485 420  18
 487  28  99  17 227 446 476 258  74 559 270 171 169 297 350   3 189 336
 337 266 284 105  85  54  39 222 482 469 492 526 102 464 218  27 319 145
 479   6  20 407 348 312 117 294 202  90 549 575 235 231 436 201 155  40]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7141
INFO voc_eval.py: 171: [ 8392  1527   643 ...  7012 10005  1475]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6217
INFO voc_eval.py: 171: [ 311  389  600 ...  232 2013 1248]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5929
INFO voc_eval.py: 171: [117   3 109   4  63  68 138 100  91  98  18 159 132 111 137  24  56  67
  64  43 124  65  19 153 110 120  61   6  86  26  62  41  31  66 112  46
 150 148  23  44 114 141 129 103  37  35  74 154  76  11 158  48  39 131
  82 125  22   7  28  59  14  55  52  69 160  99 119 122  81  33  34  29
  45  85  10  89 106  78  12  21 102 104 115  84  72   2 157 144  75  40
  83 151 156  54  32  51 121 123 155 146  80 113 140 143  49  94  20 134
  73  27  70 133  90 118  93  36  79  77  71   9   5  30  57  60 147   1
 101   0 126  97  50  15  87  96  58  92  95 105 107 130  38 139 108 136
 149  16  25 142  17  42 152  47  13 116 145  88 128  53 135   8 127]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2986
INFO voc_eval.py: 171: [179 176  59 248 265 387 194 144 125  14 266 196 299 286 199 280 167  62
 181 340 178 371 316  82 251  15 224 372 152  68 285 177  99 231 240 220
  74 386 226 180  57 361 124 135  42 358 119 153 120 322 149 229 123 186
  79 394 311  69 104 164 108 288 281 141 395  34 148 107  50 264  80 241
 323 339 269 138 214 287 317 102 301 230 203 118 297 198 324  61  35 100
 147 365 155  28 328  30 163 327  18 206 282 259 379 174 257 200 127  94
 115 154  72 252  48 274 376 294  20 161  16 207 300 283 313 191 381 212
  97 235 112 353 326  90 169 275 332 184 183 246 388 296  29  43 348 236
 367  65 331 396  58 106 117 110 369   4 360  56  70 364 385  71 225 338
 389  32 291 232 247 341 384 329 303 310 375  91  39 109 215 344 352 263
  46   5 355  36 204 351 193 262 304  10 133 185 391   3 192 302   6 201
 244  67 277 268 128 254 290 320 222 293 173  17 213 221 242 137 255  95
  38  37 114 270  12 354 189 202 261 279 373  51 159 370 368 305 234  31
 390 223 126 122 314  92 150 383 330 227 103 382  22 237 116 157 306  45
  44   7  83  19  33 156 239  78 197 356   8  27  75 140  55 233 136 258
 298 162 343  52  40 245 142 362 216 113  53 295 165 392  23  60 336 219
 378 158  89 393 333  87 195 132  26 315 131  77 334 278 172 359 256 130
   2 377 342  11  76 308   0 121  63 166 273 318 175 380 312 350  66 111
 146 271 325 319 366 374 272 205 321 210  47 208  25  88 345 209 363 105
 151  84  21 267 250 190 171 139 228   1 145 276  73  54 168 143 187  41
 346 347 260 182 129  86 134 253 349  81  93 357  49 289 170 335 309 188
 292  24 238  96 211   9 218  13  64 249 307  98 101  85 337 243 284 160
 217]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3697
INFO voc_eval.py: 171: [353 220  58  63 118 305 191 117  90 121 254 258 310 192  59 200  95 487
 225 284 460 447  40  62 145 120  52 394 402 290 245 304 308 224 396  87
  93 219 205 278 259  94 172 500 498  61 222  75 123 184 181 473  82 242
 416 330 241 453 332 216 193 517 413  56 496 183 512 306 472 337 243 169
 207 124 301   7 333 385 100 103 133  96 272 252 514 138 297 404 427 143
  51 215 167  45 152  11 350 116 435 177 260 376 170 372 480 321  99 287
 346 506  81 495 176 289  17 507 421   1 400 368 476 374 516 201 521 307
  36 223 403  41  76 285 365 265 237 196 303 467 326   4  91  47 513 142
  16 150  29 336 358 379 178  55 267 509 357 171 212 389 470 417 443 466
 354 186 206 292 146 492 388 458 137 275 107 147 315 302 456 296 240 407
 185 425 347 360 465 280 401  50 445  71 217 246 294 399 462 508 312 234
 165 397  69 213 375 463 233 244 251  35 343 148 151  54 106 202 128 135
 494 130  25  83 442 433  24  60 348 249 248 255  73 149 101 327 461 144
 322 398 114 415 410 180  37 426 484 387  49 320 318 405 378 519 182 483
 153 355 203 173 271 475  19 373  57   2 119  34 158 311 226 209 105 329
 485 454   3 256 408  85 491 392 440 273 471  33  12 161  20 437 208 179
 364 136 518 390 188  79 431  53  27 371 235 204 316 510 363 481 125 328
 501 393 268 239 317 168 175 115 369 488 253  67 511 349  42 187 469 446
 281  88  86 270 279 102 503  78  74 194 338  14 339  44  30 367 218 262
 264 468 154 522 432 300 164 441 232  46  22 482  66 160   5 380  26 195
 436 457 324 211 227 314 429  84 276 122 486 474  38 434 428 412 140 406
 499 126 352 439 155   9 108 342  80 263 313 247 331 323  92 464 490 419
  97 250 257  89  65  70 489 382  39 479 189 341  15  77 291 386 493 283
  18 515 381  72 361 384  98 109 277 502 266   6  13 340  32 334 459  31
 414 452   0 261 286  43 344 455 366  23 139 438 423 104  68 269 409 325
 351 298 449 236 430 157 282 112 163 199 129 477 221 293  64 444 229 309
 166 450 362 156 504 448 274 159 451 231 162 110 197 370 411 359 356 391
 497 395 377  48 319 288  28 214   8 418 132 505 520 424 422 335 478 131
 174  21 210 383 228 420  10 345 141 134 230 111 198 190 127 238 299 113
 295]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3715
INFO voc_eval.py: 171: [227 437 625 918 669  11 518 602 838 394  66 799 529  94 670  59 215 135
 898 679  70 865 756 937 628 322 104 566 656 198 177 618 530 132 601 372
  35 134 218 290 266 705 634 927 321 681 133 452 141 160 954 672 543 678
 378 698 604 267 100 330 817 531 237 532 899 966 703  84 139 744 486 866
 900 472   9 317 651  68 131 558 516 600 641 701 868 759 869 768 952 121
 185 482 356 524 715 181 950 758 712 212 183 414 202 639 700 217 958 593
 130 949 369  67  95 716 882  16 675 889 223 561 647 619 697 512  51 388
 707 704 225 785 124 637 398 540 537 479 106 182 820 567 852 242 193 801
 109 283 892 812 460  31 205 374 424 107 547 362 456 766 370 661  54 337
  48 650 234  21 152 560  40 421 699 508  60 648 761 888 613 565  65 446
 184 941 956  74 821 199 721 942 140 418 818 723 243 230  61 451 764 872
 960 605 521 256 517 400 284 677 893 301 948 739 298 333 147 631 235 951
 276 636 108 278 513 894 206 713 608 720 829 967 483 706 146 855 930 787
 777 498 876 667 254 946  28 673 594 411 144 164 502  36 841 944 428 719
 407 635   7 919 493 609 607 209 591 355 863 975 111 194 736 935 264 929
 557 345 835 901 263 297 693 839  42 718 711 722  14 219  77 874 105 620
 465 932  71 917 786 528 822 179  89 245 794 316  83 507 515 391 614 473
 229 925 195 259 125  32 702 683 676 292 779 312 878 938  92 538 836  55
 168 592 685 302 354 939 319 358 519 887 760 806 902 329 708 534 597 746
 274 682 800 783 232 351 710 725 505 674 696 189 551 426 595 299 870 654
 583 481 342 485 269 877 420 192 748 945 395 239  47 842 171 955 973 497
  97 533 615 241 694 153 904 816 867 590 244 375 963  44 366 646 862 492
 190 684 415 338 174 318   8 549 827 286 440 923 216  56 671 397 824 809
 363   2 506  43 957 851 546 151 476 653 265 120 807 947   1  82 331 587
 288 734 303 484 691 556 384 586 129 598 226 970 495 222 798  17 186 743
 845 539 559 158  30 304 343 201 156 554 145 564 709 371 412 666 285 804
 522 469 510 138  75 550  73 924 253 752 885 544 432 642 228  22 908 447
 732 361 436 240  78 730 727 735 430 438 972 157 208 962 571  76 348 813
 336   6 753  62  79 390 627 149 439 580 364 252 323 879 238 178 385 599
 357 688  15 860 535 326 830 749 233 291 172 478 843 668 840 629 231 771
 197 275 823 754 541 968  57 461 477 224 273 416 657 346 306  26 542 884
 745 246 881 961 940 776 536  38  63 162  37 943 123 610  81 143 577 503
 883 911 695 814 214 875 101 154 429 386 622 122 562 690 449 832 552 255
 728 279 387 200 782  98 655 793 480 155  72 365 280 360 584  46 606 936
 382 769 853 367 765 555  50 621 763 434 921 289 441 796 680 964  19 462
 113 802 368  87 103 359 221 282 664 454 891 626 399 717 644 307 408  27
 352 831 464 427 119 520 526 249 757 236 324 548 188 314 582 931 247 603
 579 500 934 611 499 859 176 191 169 847 849 630 737 453 733 563 270 886
  53 173 643 489 788 501 417 624 203 790 514 738 740 487 344  80 443 170
 403 402 187 409 953 774 890 210 161 971 844 494 871 511 570 308 468   4
 496 569   0 523 751 490 300 857 731 665 413 419 328 589  58 895  64 457
 854 220 581  10 114 211 795 341 349 466 213 861 914 714 423 327 474 772
 159 204 905 455  39 797 810 287 320 974 389  20 612 916  23 102 117 313
 128 261 381 271 406 163 491 792  29 347  49 332 126 789 433 272 148 811
 781 166 568 747 401 652 334 450 755  41  88 281 207 431 773 858 915 640
 926  33 112  25 922 353 689 645 659 575 295  52 638 574 573 488 196 475
 467 856   3 578 311 791 250 251 920 435 340 335 687 588 376 808 686 350
 116 310   5 392 379 906  34 422 260 585  12 819 767 142 180 663 896 828
 784 315 383 463 248 907 527 909 837 729 377 633 805 404 293 692 750 509
 525 380 396 913 778  45  69 545 850 296 471 309 880  24 770 632  90  93
  96  99 442 262 553 912 258 257 410 815 741 167 742 969 294 649 775 458
 444 175 762 268 136 425 910 803 864 660 903 959 277 848 834 572  85 897
 150 933 110 127 118 115 325 373  13 965 470 780 873 165 833 623 305 617
 459 616 724  86 504 846 405  91 825 576 826 393 662  18 726 658 137 339
 448 928 445 596]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4897
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4094
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.370
INFO cross_voc_dataset_evaluator.py: 134: 0.617
INFO cross_voc_dataset_evaluator.py: 134: 0.231
INFO cross_voc_dataset_evaluator.py: 134: 0.354
INFO cross_voc_dataset_evaluator.py: 134: 0.551
INFO cross_voc_dataset_evaluator.py: 134: 0.546
INFO cross_voc_dataset_evaluator.py: 134: 0.481
INFO cross_voc_dataset_evaluator.py: 134: 0.008
INFO cross_voc_dataset_evaluator.py: 134: 0.529
INFO cross_voc_dataset_evaluator.py: 134: 0.197
INFO cross_voc_dataset_evaluator.py: 134: 0.383
INFO cross_voc_dataset_evaluator.py: 134: 0.075
INFO cross_voc_dataset_evaluator.py: 134: 0.387
INFO cross_voc_dataset_evaluator.py: 134: 0.714
INFO cross_voc_dataset_evaluator.py: 134: 0.622
INFO cross_voc_dataset_evaluator.py: 134: 0.593
INFO cross_voc_dataset_evaluator.py: 134: 0.299
INFO cross_voc_dataset_evaluator.py: 134: 0.370
INFO cross_voc_dataset_evaluator.py: 134: 0.372
INFO cross_voc_dataset_evaluator.py: 134: 0.490
INFO cross_voc_dataset_evaluator.py: 135: 0.409
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.446s + 0.002s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.380s + 0.004s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.373s + 0.004s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.378s + 0.003s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.383s + 0.003s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.387s + 0.003s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.389s + 0.003s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.389s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.390s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.389s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.390s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.391s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.390s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.536s + 0.001s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.367s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.388s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.371s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.379s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.381s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.393s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.395s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.393s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.392s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.390s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.391s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.391s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.420s + 0.002s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.396s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.395s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.398s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.389s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.388s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.387s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.386s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.385s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.384s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.385s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.387s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.388s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.519s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.392s + 0.004s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.390s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.381s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.379s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.377s + 0.003s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.377s + 0.003s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.376s + 0.003s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.381s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.383s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.381s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.379s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.380s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 56.564s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [518 156 811 140 649  20 372 143 138 614 271  48 366 520 492 131 501 438
 135 916 415 130 132 839 522 444  49 494 873 251 377 237 532 110 812 437
 158 441 408 206 887 495 729 133  81 908 452 243 567 443 534 413 146 449
 768 139 519 684 385 573 767 354  51 815 789 236 463 638   7 481 525  55
 263 168 783 745  91 324 686 754  52 305  27 450 136 687 530 753 717 257
  60 866 393 604  13 913 817  22 824 188 655 371 922 448 357 566 582 108
 200 402 646 311 841 521 599 577 107 103 447 145 790 612 693 647 484 590
 226  75  25 904  53 588 241 742  18 163 100 653 572 367  66 695 559 177
 417 797 252 920 482 162 733 282 409 207 741 772  42 528 190 763  14 322
 125 685 884 370 803 310 489 209 411 576  44 697  24   1 880 926 764  23
 845 859 940 173 820 912 643 656 800 644 777 810 515 364 657 137 616 529
 548 862 469 477 893 248 228 594 239 930 329 665 119 770 865 164 397 254
 587 778 906 425 479 503  72 870 451 575 347 141 369 117 600 330 712 275
 918 122 679 338 869 152  97 147 498 306 436 300  40 283 157 365 668 247
 560  31 744 439 680 564 148 538 118  15 128 410 208 814 722 710 183 618
 180 256 651 781 641 598 316 896 442 499 440 821  67 160 457 703  59 682
 630 895 848 421  50 883 878 276 169 446 542 589 165 491  61 289  17 762
 134 801 175 333 886 378 700 808 127 232 386 373 360 219 462 456 637  32
  26 359 759  29 876 760 476 166 747  21 617 524 914 875 486 915 286 669
 798  35 406 934 204 943 531 205 950 343 597 407 728 191  41 245 400 161
 238 784 696 297  36 151 544 505 584 713 362 629 568 250 545 512 292 771
  87 928 115 535 376 952 356 816  96 523 416  93 561 921 284 654 325 465
 692 917 726 500  88 715 949 592 818 840 355 121 493  73 509 178 751 153
 262 220 375 842 779 782 828  11 142 571 698 563 184 650 805  34 423  78
 942 941 224 307 533 541 291 431 702 724 730 215 340  99 321 556 430 945
 504 583 298 732 881 879 676 265 659 268 319 907 562 269 813 280 487 948
 851 946 792 540 379  94  92 640 897 787 716  33 511 459 634  63 210 299
 176 557 677 558 272 102  28   2 302 454 154 309 578 381 470 773 874 420
 944 601 274 714 838  64 194 342 214 775 765 293  90 621 348  37 793 392
 242 358 734 485 939 755 609  84 690 167 114 246  98 569  65 155  10 350
 546 675 627 900  69 829 871 516 401  46 554 179 478 758 217 185  45  83
 174 318  30 253 382 749 864 923 149 150 223 947 595 585 202 834 144 427
 932 196 404 471 466 116  79 632 925 172 368 743 738 414  19 628 159 199
 903 434 719 902 315 403 201 831 488 622 596 341 455 536 126 681 645 508
 422 706 694 225 933 740 111 517 591 794 593 270 424 221 658 835 473 688
 331  68 490 827 752 399 608 861 281 380 353 704 514 527 570 636 611 260
 642 748 756 785 631 795 901 937 244 774 580 255 472 186 832 830 295 231
 885 671  16  89 909 468 240 266 780 464 227 613 264 574 708 258 603 663
  74 278 109 565 727 689 334   4 435 388 936 927 888 229 605 882 661   5
 547 626 212 458 182 313 259 113 660 699 171 731 216 720 624 872 203 819
 405 786 296  56 776 826  54 938 833 639 847 935 390 267 672 287 112 233
 249 261 666 924 746 856 181 854 337 395 837 844  85 761 303 502 581 852
 426 602 857 707  95 889 910 625 187 301 662 222 809  12 736 799 894 850
 285 890 931 769 543 218 537  80 363 735 766 467 620 198 506 345 670 551
 327 344 346 635 230 193  77 549 419 615 433 802 294 553  86 510 326 507
 496 277 718 555 919 391 317 867 737  62 432 723 807 648 374 104  71 899
 189 332 843 891 664 480 823 804 428 429 951 195 445 709 674  47 384 853
 124 739   0 170  82 105 652 619 211 213 705 234 791 288 806 339 836 552
 683 328 460 667 905 701 721 863 868  38 822 860 757 453 323 539 796 858
 290  43 336 383 320 335   3 846 691 273   9 513 579 197 387 129 633  58
 483 235 349 351 120 849 953 855 610 418 623  76 101 475 304 750 877 898
   6 398 394 725  39 526 192 673 497 314 474 586 788 312 389 461 352 550
 607   8  57 825  70 396 678 106 412 911 123 929 606 279 308 892 711 361]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3517
INFO voc_eval.py: 171: [191  99 662 455 770 733 596 744 465 772 774 444 100 602 781 120 512 597
 776 769 743  85 699 351 505 533 450  59 223  57 756 249 644 783 603  46
 449 650 576 368 630 782 513 748 293 771 759 187 105 656 363 290 488 122
 599 715 406  44 454 483 275 151 716 719 103 463 506 154 608 295 663 108
 353 451 697 504 702 584 664 692 458 794 616 727 712 345 507 703 530 545
 606 648  60 193 179 464 786 652 192 402 446 254 626 395 796 657 707  84
 706 536 768 683 321 546 445 205 270 709 448 627 107 738 802 805 128 653
 452 319 466 797 175 453 598  58 667 101 162 803  14 753  53 340 674 476
 564 292  45 686 490 414 110 701 248 749 579 447 348 795 106 123 492 311
 129 628 752 751  26 461 765 291  16 285 517 113 134 204 317 338 633  66
 623 377  47 132 479 690 489 642 567 531 161 551 462 117 150 274 364 442
 785 136  88 641 593 262 515 323 460 537 668 570 754 112 672 647 666 525
 661 620 477 250 399 493 219 244 487 804 739 529 793 335  95 382 736 325
 247 714 494 194 788 474  31 440 604 640 773 557 742 206 750 115   4 255
 777 269  75 655 556 704 352 651 394 307 673 201  25 142  35 540 792 784
 548 124  15 200  61 264 257 198 467 486 413 369 671 779  42 383 636 637
 800 214 178 144 475 619 283 188 156 459 157 229 398 104 143 189  28  18
  72  89 495 621 357 625  62 635 228 569 665  49 286 624 327 587 433 177
 182 468 116 139 266 589 245 232 418 747 685 611   6 419 109 379 485 437
 284 687 241 456 659 313 519 728 336  65 503 799 396 405 586   5 791 278
 138 371 158  81 164 211 472  93 186  56 334 798 184 315 408  87 572  86
 236  80 216 763 700 639 222 677 195 497  73  37 649 252 392 173  78 718
 358 552 473 176 426 218 213 359 566  17 807 762 745 393 279 725 737 708
 324 231 386 310 508  32 350 801 174 618 299 509 417 614  48 581 534 555
   0 260 163  91  74  68  50  39 730 360 810 171 159 361 125 137 237 441
 484  21 609 282 732 308 420  11 343 724 731 429 253 111 544   1   8  10
 612 225  70  67 318 209 688  22  43 684 436 638 675 622 374 514 808 256
  36 415  20  13 235 535 746 713 301 172 362 740 758 654 469 430 215 501
  98 591  94 470 761 760 342 670 425 562 412 166 778 416 165 197 757 196
 710 574 676 767 500 265 346 344 153 365  71 510 422 366 118  19 190 547
  76 600 696 538 169 400 435 669 554  69 605 720 607 273 294 238 149 711
  77 305 695 326 755 560 207 240 314 329 645 226 227 202 373 372 575 629
 680 443 491 276 691 298 152 424 610 331  96 309 145 356 277 741 717 615
  82 119  23 239 251 471 434 524 523 568 281 678 404 297 617 230 391 347
 527 496 224 722 131 539 148 526 553 632 723  64 272  55 457 411  27 288
 261 212 121 595 397 135 559 601 160 185 532 789 594 376  41  92 809 613
 370 780 481 233   7 306 427 766 217  90 387 401  52 341 734 102 498 806
  38 764 681 410  83 521 280 210 643 658 328 592 522 389  12 705 375 168
 735 528  63 259 660  79 234 170 543 384   3 565  33 242 478 409 590 127
  34  24 133 208 180 542  54 585 167 541 155 502 300 693   9 337 634 322
   2 646 580 243  97 423 388 775 246  30 631 499 296 349 726 511 439 183
 573 333 130 403 268 126 304 421 698 578 289 287 432 787 407 563 221 258
 146 520 267 271 367 689 147 378 181 302 721  29 790 199 354 381 682 480
 550 390 330 339 694 332 679 203 516 518  51 577 380  40 582 729 303 312
 438 571 320 428 316 561 558 263 141 220 385 355 431 114 482 588 140 549
 583]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6176
INFO voc_eval.py: 171: [104 263 315 204 206 310   4 646 293 647 205 597 648 489 600 533 406 214
 402 107   3 598 209 244 250 704 232 153 651 649 281 317 545 516 666 679
   8 540 247 307 660 529 652 294 163  31 341 661 276 688 445 725 404 650
 538 668 544 296 245 722 440 301 718 479   9 602 681 658 399 345  43 180
 622 211 344 312 653 413 295 556 576 727 388 189  82 667 114  47 656  55
 233 672 563 279  14 465 706 128 643 311 103 386 510 403 669 625 663 551
 126 692 143  70 234 459 123 133 212 471 509  32 106 407  13 629 251 225
 659 691  29 215 269 102 265  16 289 208  56 552 100 717 277 273 347 715
 201 687 124 237 266 182 519 223  17 473 599 470 593  84 376 434 587 415
 396  72 132 155 528  49 714  53 397 449 162 530 452 194 241 348 492   6
 157 641   7  96 443  57 127 200 372 454 351 173 401 422 117 446 524 363
 423 553 456 113 385 453 411 282 216 623 236 147 280  52   5  80 319 178
 327 116 238  76 325 378 496 608 329  63 135 320 655 383 628 298 433 313
 202 361 523 561 112 480 616 615 522 380 444 508 447 716 605 300 324 119
  34 630 618  71 152  12 566 243 636 435 683 328 462 424 464  61 662 184
 137 355 101 627 527  10 207 640 248  42 701 197 431 302 198 654 450 665
 512 700 261 610 439 374  78 457 417 258 410 604 594 712 158 438 268 467
 558 539 425 469 166  44 161  20 719 186 381 474 571 253 151 290 458 230
 118 254  33 705  62 303  59 525 475 174 164 271 603 710 352 274 607 255
  69 674 139 513 633 373 684 338 626  24 555 203 609  74 430 499 677  92
  87  51 144 384 441 448 534 639 733 343 476 584 617 286  18  23  30 149
 596 305 142 199 695 500 696 699 535 195 188 451  35 637 482 564  99  54
 585 690 357  68 724 542 442 340  21 150 284 697 297  28 463 730 420 537
 437 136 367 246 575 318 249 418 409 429 671 721  66 573 670 262 723 559
 493 614 191 578  27 698 391 635 592  58 497 309   2 455 685 110 140 427
 154 217 371 515 505 468 657  15 122  48 299 111 239  11 252 108 726 408
 588 159  75 176   1 323  45 387 580 213 624 228 521 183 414 392 291 541
 713 461 583 466 356 485 568 419 395 543 115 620 472  40  19 503  67  38
 589 711 632 531 734  39 196 278 478 506 511 224 221 567 702 526 292 270
 579 375 536 460 227 138  50  89 577 350 532 219 586 175 426 405 590 358
 235 428 346  36 148 548 481 729 416 272 642  46 491 218 330 368 240 364
 370 131 285  64 359  25 168 275 231 574 156 731  79  81 720  90 560 259
 514 611 393 141  95 129 354 193  86 353 377 125 595  65  93 591 322 562
 490 682 394 606 308 520 109 171 339 287 412  26 369 673  83 222 314 634
 335 518 283 436 145 400 398  98 226 612 360 554 336 160 256 177 694 664
   0 547 333 621 181  37 179 172 484 192  85 572 229 134 264 565 421 601
 167 120 185  73  60 546 676 581 105 260 121  94 686 349 483 680 557 502
 337 304 613 689 732 550 504 366 187 675 190 703 326 569 549 486 728 619
 288 342  77 644 332 645 494  88 390 477 495  91 487 631 389 267 638 169
 165 334 678 570 242 507 709 501 130 306 220 382 693 498 316 257 365 331
 170 582 432 146 708 707 362 210 488  97  41 517 379  22 321]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2288
INFO voc_eval.py: 171: [1176 1375 1177 ... 1197  150 1103]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3553
INFO voc_eval.py: 171: [1326  777  830 ...   59  823  671]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5503
INFO voc_eval.py: 171: [ 67  40 130 133  14  45 210 166   6 350  82  42 266 145 275  49 334 332
  27 232 260 248 259  60 195 287 348 340  37 183  47 132 314 127 284 170
 349 280 271  46 221 193 245 255 164 294 106 327 174  41  50 204  21 242
 207  72  52 366 154 288 214 249   4 355  95 153 359 300  11 140  54 137
  13 289 308  16 268 120  87 150 235 311 360  28  85 265 295 108 338  81
 264 114 322 121  88  30 142 201  22 191 227  93  32 188 362 229 102 347
  33 351 361 321 279 215 196 175 319 197 363  76  43  61 228 135 323 329
 301 224 128 222  51 267 261  73 296  80 194 219   3 200  59 149 155 304
 270 186 281 163 317 312 138 292 335  83  96 299 160  17  63 117  94 165
 290 276 124  15  44  31 282  12 293  48 131 192  55  99   9 100 151 325
 107 223  18 169   8 173 324  53 109 353 230  92 216 256 178 274 336 236
 241 234 356 339 101  79 316 333  77 226  20 309  39 139  24 213 122  89
 254 313 146 147 104 157 343 206 285  66 345 123  69 318 328 257 272 238
 368  26 243  56  10 159 291  36 113  86 105 283 244 217 320 119  23 315
 202 352 258 182 118 330 167  78 116 184 250   5 110 263 112 269 125 212
 144   2 218 176 233 129 277 341 180 111  62 148  34 273 237 190  64 158
   0 246 344  84 208 203  75 298 177 198 251 253 181 302 337 297  35 126
 209  74 172 205 161 103  97 331 211 143 115 326 168  19 189   1 310 303
 252  98 358  70  71  65 262 152 240  29 354 306 199 367 141  90 357  38
  68  25 136 342 185 305 171 179 156 187 239 231 365 134   7  91 286 278
 307 220  57  58 162 346 364 247 225]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5727
INFO voc_eval.py: 171: [ 963 1010  437 ...  515  379  860]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4751
INFO voc_eval.py: 171: [ 10   5  23 103  24  93   7 100  41  97   1  55  73  37  74  76  52  96
  17  32  99  71  11  13  72  56  51  19 101  85   4  27 104  43  77  64
  20  86  53  38  31  45  40  63   6  50  69  78  98  35  94  57  18   9
 105  62  14  91  29  12  28  79  60  46  67  54   8  33  81  82  42   0
  30  92  49  25  80  84  68 102  95  16  34   3  88  61  58  21  70  48
   2  90  22 106  39  83  26  47  65  75  44  66  36  89  15  87  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0055
INFO voc_eval.py: 171: [ 588 5377 5378 ... 6004 4326 5254]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5317
INFO voc_eval.py: 171: [  5  66   6 118  48  96  40  37 106 133 128  43 135  65  89  59  68  41
 136  84  98  34 113   9 105 123 100  57 146  30  85  10 122 108 104  55
  46   7 129  86 112  73  29 110  39  70  60  92  69 144  94 134  36  71
 121  76  77   3 132  14  99  23  42  87 120  38  52  16  33  45  75  13
  62  72  51   8  64  97  21  47 126  80  32 130 141   4  22  95 142  81
 131  54 124  20 119 111 127  53  88   1  35 125  82  28 114  24  56 107
 117  50  26  19 103  90  67 139  83 101 109  91  93   2 137   0  12 115
  44  78  15  17  18  25  49 143  31 116  58  63 140  74  11 145 138  79
  27 102  61]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1938
INFO voc_eval.py: 171: [ 782 1728  419 ... 1469   54  218]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3748
INFO voc_eval.py: 171: [ 96  88 168   3 137  93  18  89  50  97 188   6  56  34  37 186  52  42
  84  61 184  31   0 149  75  26 172  74  51  11 145 122  12 166  60 109
 131 126  35  83 139 197  32 198 182 177 138  82  44 114  67 190 156  54
 195   9  14 107 115  79  10 199 140 151  94 110  59 175 180 143  41 167
 169  69 117 164 132 118 121  90  25 108 171 119 133  57 124 157  62 104
  15  95 113 101  58 136 111 187  36 102 158 103   2 142 179 176   5  22
  80  13  65 112 135 165 153 116  70  78 134 127  77  46  39   7  87  21
  53  99 152  27 129 148  98  72   4 178 154 181  16  43 185 128  92 155
 147 200 150  76 159 120  63 170  86 146 141 105  73  19  20  30  66 173
   1  23  29 162 160  24 161 194 183 100  64 193  45  40  38  33 191  71
  81 125 106  85  55  91  48  68  28  47  49   8 189 174 196 163 130 144
  17 123 192]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1164
INFO voc_eval.py: 171: [131 114  20  27 156  84 117 166 113 172 132  56 138  22  67 108  21  93
  31  57 160 111  90  89 140 152 134   8 153  61 141 151  96 115  60  71
   9 116  99  42  75  49  43  98  55  40  30  33  78  51 136  85 161   1
  50  64  38  11   4  41 118  68 137   2 145  36 110  87 130 129  58 149
 157 102  95  66 135  15  29  69   0 148 163 150 168  79 109  46 120  73
  62  74  59  88 142  25 170 143 146 133  83  94 159  12 171  45  54  81
  35  34  10 119 155 162  47 154   3 121  17 139 127   7   6 112  13  53
  44  72  92  14 169 144 103 122 125  32 104  70  18 107 123 164  24  97
 100 106 158  26  80  86  77 173  23 165  65  82 147 128 101  39  37 124
 167  16   5  91  52  63 105  19  48  28 126  76]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3789
INFO voc_eval.py: 171: [ 96 537  76 352 277 575 366 420 489 259 411 433 520  54 109 389  34 290
  31 367 404 517 265 553 463 267  97 302 147 584 184 232 582 168 136 477
  67  69 408 186 166 440 480 175 206 382 331  13  88 472 226 260 343 353
  63 466 330 137 325 219 573 523 172 249 100 304 405 303 589 274 159 482
 484 546 342 574 577 394 208 114 113 491 319  77 438 519 174 474 441 282
 207 416 295 340  58 212 228 127 540 533   4 312 357 370  73  26 432 581
 204 299 516 321 460  48 539 223  37 275 368 583 565 468 201  45 397 356
 424 518  92  87 158 313 310 402 155 487  60 130 112 125  49 406 592 182
 105 528 383 194  35 445 273 376 564 479  68 521 157 490 115 585 131 570
 252  84 475 423 268 128 386 106 217 286 240 145 218 371 434 301 320 551
 501 587 241 316 348 547 122 494 344 522 151 364 255 395 535  71 407 504
 227 258 465 345 209  94 196 543 492 230 305 285 506 188 561 387 532 415
 586 338 392  50 294  17 351 454 525   1 446 385 566 427 149 526 139 296
 297 314 123 178 390 529 444 507 289 588 315 426 237 483 478 435  81  59
 181  82  95 552 498 309  25 264 591 391  64 324  33 329 576 143 531 156
  93 336 580 380 132 579 571 500 176 470 266 495 421  29 205 524 378 306
 101 530 103 410 152  80 195  47 469 236 322  70 562 449 556 399 398 393
 333 170 220 473 419  20 323 458 467 245 417 448  11 190 239 455 141 169
 337  14 261 362 453  36 108 119 202 317 231 347 280 513 534  44  16 165
  10  32  98 379  62  15 536 332 373 545 222 138 191 476 248  83 116 554
 126 558 414 563 300 527  40 171 134 140 318 200  24  74 437 508 199 135
 374 396 354 412 193 288  66 559 548 244 512 462 447 538 243 413 133 150
 542 497 203 515 327 192 359 307 418 224 146  51 341 160 161 443  27  99
 439 451 235 284 409 287  55 509 173 459 593  39 442 250 216 221 485 142
 590   5 450 388 568 578 102 213 429 355 339 375 163 278 428  89  75 514
 346 263 358 381 372 247  86 183 555  38  46  12 262  61   9 215 117 365
 360 162 560 148 281 185 270 254 549 461 118 189 251 187  85  53 292 400
 167 121 401 104 291  56 464 488 384 272 493 436 253   0 271 377 107  65
 153 430  19  18  43 471 511 197 177  41 110   3 550 198 503 283 363 350
 349   8 238 229 335 422  23   2 234 544 242  52 179 369 486 505  42 326
 233  28  30 481 210 499 594 129 124 279  79 154  57  78 567 496 298 361
 541 452 311 276 569  91  21 308 120 425 456 211 502 510 111 457 164 557
 225 144 180   6   7 256 431  22 334 328 293 214  72 257 403 246  90 572
 269]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7134
INFO voc_eval.py: 171: [9391 1715  727 ... 9549 4807 3515]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6220
INFO voc_eval.py: 171: [ 322  406  627 ... 1133  655 2111]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5922
INFO voc_eval.py: 171: [125   3 117   4  69  74 146 108  98 106  20 169 140 119 145  27  62  73
 132  70  71  49 161 118  22 128  67  91   6  29  68  46 120  72 158  35
  51 156 122  50  26 149 111  39  42 163 137  11  80 168  82  54  44  87
 139  25 133  16   7  31  65  61 171 130  75 107  58 127  10  86  38  90
  94 110  37 114 123 152 167 112  32  89  14 165   2  24  78  83  88 159
 154 131  81 164  45  57 151  60 121  85  55 129  36 148 101 142 141 126
  79 100  41  84  23  76  30  77   9  96 155  63   5   1   0  33  53  92
 113 134  64  66 109  56  17 138 147 115 105 103 116  99 150 157 160 104
 136  28 124 144  43 153  19  18   8  52  15  48 143  97  13  59  40  93
 170 135  95  34 166  21 162  12  47 102]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2963
INFO voc_eval.py: 171: [187 184  63 255 275 399 202 130 150  15 204 274 307 294 289 208  67 189
 173 348 186 381 323  85 259 158  16 185 382 231 293  71 103 398 239 247
 227 188 233  78  59 129 159 371 194 140 124  44 170 368 237 155 329 125
 113  82 128 406 318 296 147 109  72 407  36  52 290 330 154 273 112 107
 295  83 206 347 143 278 248 324 331 210 161  37  65 238 104 220 305 123
  19 375  31 309 153 335 169 267 180  29 334 207 308 212 391 132 120 260
  97 291 191 242 265  17 192 283 386 160  21  76 302  50 320 292 213 199
 218 400 167 116 363 175 333  58 339 101 393   4  93  30 115 253 377 284
  60 304 240 379 358 408 122  45  68 338 243 349 111 232 401  74 193 397
 370  73 254   5 299  41 221  33 346  38 336 396 317 353   3 211 138  94
 362 385 311 270 271 402 201 312 365 251 262 310 200  48 114 298 361  10
 277  18  70 249 286 263 228   6 327 179 301  32 165 229 219  40 133  99
 118 131 288 142 197 403 230 269 209  13 364  95 383 380 337 234 321 279
 313 156 394   7  53  39  47 127  86 162 395 205  23 121 163  28 108 164
 146 366 171  81   8  57  20 244  46 241  34  54 340 373 404  42  55 148
 266 306 246 117  64 252 352 222  27 136 303  24 344 203 405 369 137 390
 168 141 322  80 287 315  66  90 264 226 351 178 126 172  92  12   2  11
 282 341 135 214 319 389 360 332 328 217 181 355   0 257  79  22  69 326
 325 392  91 152  49 110 281 280 374  26 376 149 297 285  87 157 236 151
 261 198 384 215 356 276 144 177 357 196 134 250 190  77   9  51  56  89
 174 139   1  84 367 268 224 176 343  25  88 300 195 359 316 166  96 106
  43 342 145 119  14 272 245 256 223  75 100 105 314  35  62 183 388  61
 345 350 102  98 387 216 235 258 225 372 182 354 378]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3714
INFO voc_eval.py: 171: [376 238  62  68 124 323 203 123  97 127 273 277 328 204  63 102 214 520
 244 303 151 126  67  43 491 477  56 309 419 427 326 242 322 264 421  94
 100  65 237 297 101 532 179 240 220 278 129 534 195 443  82 192 261  88
 351 504 483 260 205 353 553  60 530 232 194 438 547 503 324 358 130 262
 223 176 110 354 107   9 103 316 429 319 290 410 149 271 139  55  12 550
 184 456 173 144 231 122  48 373 279 158 528 465 306 177 308 106 401 391
 215 183 395  89 513 325   1 552 241  18  83 341 368 557 542 424 449 304
 543 398  38 256 507 210 283 388 428  44 498  17 321  98   4 549 178  59
 185 546 284 404 152 156 380 148 501 377  30 348 228 196 198 381  50 357
 488 143 311 444 497 454 153 222 334 432 525 473 414 294 486  53 259 299
 426 320 496 413 315 369 114 329 425  74 383 253 545 154 234 492 265 475
 422  58  77  64 313 229  37  25 252 171 263 463 274 113 527 134 268 400
 267 157 472 365 141 270 343 440  24 490 494 455 136  79 370 216 349 108
  90  39 435 423 555 150 180  52 193 119 430 451 245 155 218 125  61 190
 403 412 159   2 339 275  35 337 516  13 225 484 292 433 112 167  34 202
  19   3 517  92 415 288 506 554 378   7 502 518  57 397 186 164 330 417
 182 219 467 387  28 470 258  86 536 548  20 386 460 131 524 418 224 254
 142 350 206 514 120 197 109 298 500 392 174 476 394 335 390  95  93 539
 371  81 282 499 300  45 280 360 208 521  72 336 405 333 272 285 287 462
  15 487 160 359 251 318 166 128 170  27  47 431 533  85 345 246  49  22
  31 295 471 132 146 235 332 558   5 161 269 344 519 515  96  40 466 464
 457 375  71 115 104 266 505  87 437 310  99  41 364 352 458 227 495 411
 281 493 526 469  75 447  14 551  91 537 489 439 199 105 523 522 302  84
  70 116 296  78  33 407 355 276  16 512 384 363   0 145  32 409 389  23
 362 485 366 374 406 434 385 327  46 452 286  73  69 346 468 211 482 172
 165 163 239 255   8 305   6 301 168 481 540  51 479 162 312 118 356 307
 213 556 420 379 135 111 541 508 480 382 138 169 248 200 478  21 338 293
 446 531 459 393 137  10 181 140 402 257 250 474  29 188 510 448 442  80
 243  26 538 436 347 212 226  36 230 209 509 117 396 201 453 416 450  76
 247 133  42 289 249  66 445 236 233 361 367 441 511 175 147 408  54 544
 217 342 221 189 535 191 461  11 121 399 314 317 331 529 187 340 207 372
 291]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3593
INFO voc_eval.py: 171: [244 465 666 ... 843  68 503]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4837
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4096
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.618
INFO cross_voc_dataset_evaluator.py: 134: 0.229
INFO cross_voc_dataset_evaluator.py: 134: 0.355
INFO cross_voc_dataset_evaluator.py: 134: 0.550
INFO cross_voc_dataset_evaluator.py: 134: 0.573
INFO cross_voc_dataset_evaluator.py: 134: 0.475
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.532
INFO cross_voc_dataset_evaluator.py: 134: 0.194
INFO cross_voc_dataset_evaluator.py: 134: 0.375
INFO cross_voc_dataset_evaluator.py: 134: 0.116
INFO cross_voc_dataset_evaluator.py: 134: 0.379
INFO cross_voc_dataset_evaluator.py: 134: 0.713
INFO cross_voc_dataset_evaluator.py: 134: 0.622
INFO cross_voc_dataset_evaluator.py: 134: 0.592
INFO cross_voc_dataset_evaluator.py: 134: 0.296
INFO cross_voc_dataset_evaluator.py: 134: 0.371
INFO cross_voc_dataset_evaluator.py: 134: 0.359
INFO cross_voc_dataset_evaluator.py: 134: 0.484
INFO cross_voc_dataset_evaluator.py: 135: 0.410
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.534s + 0.001s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.385s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.396s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.396s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.399s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.399s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.396s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.397s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.395s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.396s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.395s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.401s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.399s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.593s + 0.002s (eta: 0:01:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.422s + 0.003s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.410s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.405s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.411s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.410s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.421s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.415s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.408s + 0.002s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.406s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.405s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.407s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.400s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.414s + 0.002s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.391s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.386s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.383s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.383s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.384s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.385s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.384s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.385s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.383s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.381s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.383s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.382s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.515s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.377s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.371s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.365s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.368s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.369s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.372s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.369s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.368s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.370s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.366s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.368s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.367s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.792s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [545 167 856 ... 350 474 975]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3565
INFO voc_eval.py: 171: [194  97 692 471 801 763 622 774 482 803 805 460  98 628 812 531 119 623
 807 773 728 800  83 362 524 552 466 227  58  56 814 672 252 629 786  45
 465 678 532 600 379 657 813 297 103 802 778 683 789 191 374 294 507 625
 417 744 121  43 470 745 502 749 155 279 101 635 693 107 480 525 694 467
 158 364 299 720 474 725 827 523 731 609 741 354 526 643 757 632  59 676
 732 196 550 182 565 680 481 413 195 462 257 818 829 406  82 736 684 653
 567 735 555 799 461 654 274 835 106 712 768 464 329 738 209 624 697 468
 128 469 839  57 325 836  14 178 830 166 681  99 483 783 494 296  52 603
 704  44 109 509 251 730 426 715 463 655 779 104 349 782 587 511 781 357
 828 536 796 317  25 129 122 478 135 112  16 208 323 295 347 290 670 508
 133 660  86 375 154 278 165 590 696 650 668  65 534 138 457 479 551 497
 328  46 495 477 719 593 817 556 572 784 116 388 253 838 618 265 250 544
 698 766 111 675 512 343 691 702 331 506 513 577 647 410 247 769 210 197
 630 743 455 667 808 223  93   4 682 549  30 826 273 780 804 820 491 393
 772 576 679 405 733  73 146 258 559 312 569 114  24 260  60 201 425 205
 268 505 363 810 476 701 204 394 703 123  15 181 218 380 160 815  41 825
  61 663 147  35 514 664 833 695 646 102 148 484 161 233 777 270 652 288
  27  70 232   6 592 141 639  87 291 192 493 485 368 108  18 538 202 648
 409 662  48 472 333 115 431 248 504 180 612 447 687 416 651 319 289 714
 185 390 344 236 168 614 430 244 162 611 215 420 516 489 452 716 140 832
  77 831  37 220 382   5 321  55 824 282 775 407  64 187  85 729 342  84
 747 758 528  75 189 198  91 403 767 841 522  71 330  78 793 217 240 361
 755 707 595 370 226 369 222 176 737 167 255  17  89 179 760 573 439 589
 490 304 794 553 235  31 666  67 404 397 575 316 677 429 834 163 641 177
 527 139 456 372  49   0 283 762 124  72 503 761  47 645  20 371 606  66
 754 843 174 110 352 636   1 241  39 213 533 842 324 640 256 263 564 427
 665 770 520 776 443 229   8 649 287 717 314 713  13  21  11  68  36  10
 432 788 373 305  19 239 809  92 529 742 386 791 689 351 486  42 444 724
 598 200 554 259 616 705 792 574 700 424 626 739 787 568 631 353 798 519
 169 173 175 451 376 190  96 193 199 434 487 298 219 633 269 411 750 157
 170 723 377 656 355 785 699 438  69 332 740 153 557 277 310 585 211 449
 710 428 706 385 673 242  74 599 415 510 320 117 230 581 336 156 280 591
 383 659 771 315  79 254 243 118 627 620 437 752  88 402 543 722 642 285
 303 837  54 206 338 149 423  63 542 811 231 120 459  22 797 448 644 473
  94 579 746 558 164 136 840 132 216 753 264 188 500 638 281 228 214 421
 293 547 488 356 367 671 237 234 685  90 221 100 311 515 276 711  81 441
 301 422  26 152  38   7 621 545 381 734 822 708  40 387 790 688  12 398
 350 212 334 540  76 764 517 395 172  34 408 262 327 617 756 284  51 412
  33 171 183 530 306 615   3 795 399 765 580 345 548 435 414 561 806 159
 661 272  29  62 541 588 358 496 658 341  23 400 238 674 610 562 563 142
 604 249 454 225 292   2 125   9 418 539 727 245 721 127 521 518 186 184
 150  53 246 535 537 433  95 389 819 748 602 751 271 759 307  28 498 131
 446 300 596 823 309 286 348 144 203 318 275 378 601 686 207 401 340 501
 607 605 151 571 145  50 475 566 445 261 586 308 365 392 709 718 442 360
 130 816 326 322 366 339 450 613 634 419 669 224 105 113 384 337 391 396
 346 440 458 126 335 134 137 690 266 560 608 594  80 578 583 453 570 582
 143 726  32 584 492 499 637 546 619 597 267 302 436 359 313 821]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6173
INFO voc_eval.py: 171: [107 281 332 212 214 700 328   4 311 701 213 639 702 642 521 569 430 222
 426 110   3 217 640 261 255 762 705 242 703 298 156 334 583 720 550  13
 577 735 714 258 706 325 565 715 167 312 293  31 360 428 704 744 785 472
 722 574 582 782 777 313 256 644 317 423 467   8 712 510   9 365 737 186
 363 707 440 668 219  43 196 419 330 318 721 117 710  55 595  83 615 787
 296 726  47 243 496 106 131 601 544 427 411 717 329 723 695 244  70 748
 673 590 147 136 431 713 220 234 129 502 109 487 543 262 223 307  32 127
 284 409 289 776  16 216 294 747 105 279 677 247 774 232 189 103  29 128
 591 554 367  56 743 442 280 209 479 635  85 420  17 641 158 460 629 135
  72 501 399 368 564 566  49 160 773 251   7  53  99 421   6 524 478  57
 161 120 484 202 693 130 559 449 450 371 384 116 151 299 224 471  14 481
 297 208 394 592 246  52 473 119 433 480 346 336  81   5 410 179 348 337
 709 184 343 401  76 138 651 670 248 529 314 459  63 115 557 210 558 661
 381 676 660 155 338 407 316  12 716 663 542  71 344 347 511 739 647 495
 486 403 775 678 462 600 493  34  61  10 191 140 375 474 215 122 451 104
 708 254 162 166 688 758 259 605 205 756 563 319 435 275 653 696  20 269
 646 206 675  78 498 483 771 719 636 170 283 121 779  69 397 465 504 546
  42 308 404 444 457 576 291 476 193 466 649 372 287  44 485 597 239 264
  59 652 180 610 265 168 452 534 683 395 143 303 340 763  62 500 547 769
 638 594  89 740 728 211 645 456 266 148  51  33 674 698  74  18 662 794
  95 425 626 505 691  68  24 195 320 357 560 469 571 408  23 731 207  21
 570 475 362 513 153  30 301 764 602 755 146 139 535 506 389 494 752  35
 689 753 725 377 724  66 573 470 692 322  54 102 627 276 164 525 658 784
 415 113 751 477 580 790 618 257 203 783 445 598 530 157   2 746 341  28
 225 359  58 549 781 687 260 614 464 455 754 741 447 539 432 199 221 489
  27 453 786 327 342 578 309  11 263 612 315 249 482 114 416 499 182  15
 111 517  75 441 126 711 634 118 412 393 630  67 622 418 295 556 503 144
 671  19  38 666 236 233 230 625 190  48 770 497 538 545 606 572 581 227
 286   1 376  45  39 204 492  50 488 759 772 562 310 568 237 181 607 628
 245 163 509 680 446 490 567 429 366 491 587 795  64 288 398 152 142  40
 468  91 159 250 226  82 694 540  98 454 370  36 392 134 172 789 523 631
 112 302 616  88 378 512 385 621 231 732  79 374 241 390 417 292 548 271
 145  46 132 436 792 633 304 522 379 424  65 648 300 165 339 655 235 738
 331 326 422 599 355 613 555 380 373 443  84 400 553  93 718  96 391 516
  86 780 585 358 137 171 727  25  26 176 656 643 604 200 354 686 183 637
 463 593 515 750 742 201 185 192 369 611 345 584 272 238 178 101 306 352
 278 124 387  73 414 537 149 108 125 188 730   0 356 361  90 623 657  60
 413 198  37 588  97 667 518 788  77 697 793 282 760 596 526 589 353 194
 745 173 218 228 448 528 699 100 321 768 679 608 536 519 382 150 736 229
  22 586 333 252 349 458 729 439 133 351 253 767 533 169 324 733  94 175
 274 508 350 551  41 664 624 541 617 654 659 402 383 609 778 290 734 386
 765 531  80 323 749 174 335 579 514 141 690 396 405 603 575 461 507 520
 388 532 685 681 270 672 273 277 197 665 684 650 766 364 438  87 305 240
 619 177 187 267 632 434 552 268 791 527 761 682 669  92 437 406 757 123
 285 154 561 620]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2298
INFO voc_eval.py: 171: [1254 1464 1255 ...  245 1844  473]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3552
INFO voc_eval.py: 171: [1363  789  842 ... 1231 1200 1093]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5475
INFO voc_eval.py: 171: [ 70  43 134 137  16  48 218 173   7  45  85 366 276 151 285  52 348 346
  29 241 269 257  62 268 202 298 356 364 190  50 326  40 136 178 130 295
 365  49 290 281 200 254 264 341 230 305 171 108  53 182  44  75 215  55
 212  23 160 251 258   5 383  13 299 311 375 159 222  97  56 371  15 146
 300 320 142  18 323 156 376 123  89 244  87 275 278  30 110 306  84 148
 208 117  35 354 274 198 334  33  95 124  90 236  24 104 378  36 195 238
 363 203 377 204 223 367 380 333 289  64 139 233  79 183  54 335 131 231
 331 277 312  46 207 343 201  83 270 237  61 228 307  77   3 161 155 324
 291 280 316 310 193 167 170 303  98  86 350  66  19 329 120 143 199  96
  14 127 286 292  10  17  51  57 301  47 172 157 109 337 101  34 135 304
   9 232 176 181 102 336 284  20 239 369 224 245 111 265  94 243 355 103
 372 328 250 347 351 321 185  82 359 235 152 164  42  22 153  80 325  71
 263 166  39 282  28  26 361 221  91  69  12 144 125 296 266 247 106 116
 332 214 225 107 253 302 252 385 174   6 267 342 330 126 189 191  58 368
 327 344 121 119  88 122 209  81 293   2  25 259 112 279 220  37 357 227
 133 150  67 165 128  65 287 246 113 255 283 309 216  38 115 197 184 273
 205 242 313 345 154 187 188 149 353 260 211 360   0 129 213 105 168 308
  78 219 262  73 322  21 118 217 175 206 196 261  68 180 100  99  72 384
 370 271   1 315 338  31 147 373 138 318  92 179 158 162  76 141 249 194
 186 358  41 374 317   8 248 382 192 240  27 381 288  93  59 362  60 145
 256 229 297 163 339 114 319 210 169 177  74 132 349 379  32 352 140  11
 340 294  63 226 314   4 234 272]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5687
INFO voc_eval.py: 171: [1024 1072  461 ... 1032    8  546]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4729
INFO voc_eval.py: 171: [ 10   5  22 106  23  95 103   7  41  99  55   1  37  73  74  76  52  98
  17 101  71  72  31  11  13  56  86  51  19  43  77 104   4  64 107  26
  38  87  53  30  45   6  63  50 100  40  69   9  34  78  96  18  57  27
 108  14  62  93  12  28  79   8  54  60  67  46  32  42  83  81   0  29
  82  49  94  97  85  68  24 105  33  16  58  20  70  61   3  89  84   2
  44  25 109  48  91  47  75  66  65  39  21  15  35  36  59 102  90  92
  80  88]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0056
INFO voc_eval.py: 171: [5583  615 5584 ... 6067 5770 2976]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5232
INFO voc_eval.py: 171: [  5  68   6 123  42  98  41  38 108 138 133  45 140  67  90  60  70  43
 141 100  35  85 115 107   9 128 102  30  58 151  86 127  10 106 110  56
  29  48  87   7  61 134 114  75 112  72  40  93  71 149  73  36 126 139
  95  78   3 137  44  15  79 125 101  23  88  53  39  16  14  47  34  77
  74   8  64  49  52 136 146  99 131  82  20  97  66 129  21 147  33  55
   4 135 130  22 116  83  54 122 124 113  84 132  89  28  37   1  24  51
 109  57  26 105  19  69 111 144  12  91 103   0  94  92  80 142  59 119
   2  46 148  50 145  18  65  25  17  11  27  32 121  81 143  76  31 104
  62 150 120 117  13 118  63  96]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1918
INFO voc_eval.py: 171: [ 821 1816  437 ...  280 1532 1856]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3712
INFO voc_eval.py: 171: [100  92 178   3 144  97  93  19  51 101 197   7  57  38  35 195  53  43
  87  62  32 194   0 182  27  77  76 157  12 176  52  13 153  61 127 137
 114 146  36 131  86  33 145 208 191 209 189  85 119  68  45 164 188 199
  55  10 204 120  15 111  81 147  11 115 210  98 159 150 179  60 177 185
  94  70  42 112 174 122 123 138 126 181 139  63  26 124 118  58 129 165
 108  99 116 196 143 105  37  16 149  59   2 106   5 166 117  83  23 107
 141  80 187  14  79  66  40 161 140 175 121 103  47  22  71 133  91   8
  28  54 102 192 160 135 190 156   4  73  44 158 180 162  64 125  78 183
 134  17 163 154  30 155 148  96 211  74  31  20 167  21  25   1  90 109
  24  65 169  39  41 206 172  67 203 193  72 171 104 130  84  46  56  34
  69 152 198 200  89  95  29  50 207  49 184 110   9  48 136 202 173 151
  18 113   6 128 201 142 168 132  88  82 170  75 205 186]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1173
INFO voc_eval.py: 171: [139 120  21  28 165  90 123 174 119 180 140  60 147  23  73 113  22  32
  98 169  61 117  94  95 149 142 162 163   8  66 101 161 150   9 121  65
  77 122  52 104 103  45  81  46  59  43  31  34 145  84  55 170   4  91
  41   1 124  53  11  39  74 155  70 115  44  93   2 146 138 137 159 166
 143  62 100  15  72 107 172 160   0 114  75  85  30 158 176  79  26 126
  80  67  49  64 141 156 151 178 153 168  99  38  48  89 179  36 125 171
 164  12  58  50  87  10 148 118  17 127  13 177   6   3  97  92 135  57
   7 154  47  33 128  18  14 173  27 129 108  78 102  76  25 132  24 109
 105 112  86  83 167  20 181 111  56 110 144  88  71 106 136 157  16  40
 116  96  19   5 175  42  29 130  35  82  37  63  68  51  69 131 134 152
 133  54]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3776
INFO voc_eval.py: 171: [ 96 547  75 360 283 587 374 429 498 265 420 442 397 530  53 110  33 296
  29 375 413 527 471 273 271  97 565 310 146 596 185 236 594 168 136  69
 486 187  67 417 230 175 489  13 166 208 449  88 309  63 391 340 266 474
 481 100 350 339 533 222 585 361 172 137 333 414 601 311 254 312 280 158
 589 491 557 493 586 349 403  76 115 500 114 211 327 529 483 447 550 174
 450 288 210 301  72  58 425 347 543 365 215 127 593 232   4 320 205 441
 378  24 526  47 376 329 227 406 305 476 549 281 364 468 595  37  44 576
 528 321 125 157 130 202 154 182  60 415 433  48 106  92  87 318 411 496
 113 538 604 392 195 386 597 279  68 499  34 488 531 575 453 107 128 156
 244 432  83 484 274 220 131 582 116 258 143 395 510 599 324 292 380 245
 416 553 404 328 351 356 563 559 221 307 443 122 545 532  71 503 231 150
 197 372 353 261 234  94 212 473 513 264 501 189 598 542 291 401 313 424
 396  49 148 577 600 536 535  17 539 454 572 515 345 139 394 303 178 399
 300 359 436 295   1  59 462 400 323 181 302 492 123 444 487 452 322 564
  32 517 241  64 270 435  80 541 591 592  81  23 155 507  95 317 389 583
 338 332 272 588 176 479  27 101  70 603 509 387 196 344 207 103 430  93
 534 132 540 568 504  46  79 240 478 407 408 573 419 170 223 151 402  20
 426 330 428 243 109 482  14 342 314 457 325 191 466 331  11 456 267  31
 370  36 249  16 235 463  15  98 556  43 388 544 164 203 120 141 475 523
 169 192 126 461 253  10 566 485 226  82 138  39 382  62  22 574 446 355
 286 117 421 570 546 134 537  66 171  73 516 341 194 383 362 423 306 470
 552 140 135 149 518 326 571 248 201 448 294 560 200 455 405 422 133 525
 522 427 160  99 335 315 239 247  50 348 385 159  35 204  25 506 459 225
 548   5 579 284 451 467 102  55 255 144 363 367 269  74 494 519 216 418
 193 145 173 293 219  45 398 290 381 590 605 184 104 147 162 458  86 602
 218 108 409 190 142  38 354  85 524 256 502 438 567 297 472 188 287 251
 298 346 469 497 561 368 437  12  61 118 445 277 278   9 393 390  40 186
  89 161 177  18 167 152 121 111  19 268 439 366 431 410 233 357 358 276
 371  65 377 373 246 119  52   0  42  41   3 289 199 554 260 562 129 495
  51 480 224  56  77   8 512 259 521  26 334 198 343 242   2 578 124 551
 213 238 179 282 237 505 514 153 490 460 229  28 285 262 508 434 369 464
 316 511  78 112 214 465 304  57  21 337 581 584 569 477 319  54 275  91
 217 520 352 228 336 384 183 252   6 206 163 555 558  90 440 180 257 165
 299 308 580 263 412  30 105   7  84 379 250 209]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7130
INFO voc_eval.py: 171: [10310  1880   787 ...  3844 11954  4670]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6185
INFO voc_eval.py: 171: [ 337  429  659 ...  446 2522  713]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5927
INFO voc_eval.py: 171: [126   3 120   4  70  75 148 108  99 106 176  20 141 119 147  26  63  74
 133  72  71  49 155 118  21 129  91  68  46  29  69   6 121 164  73  51
  35 123 112  50 152 149  25 168  38  41  11 172 175 138  54  80  82  44
  87  16  24 140 131  31 134  66  62 178   7  10 107  76  94 111  40 128
  59  90 157 115  86 174 132 160  37  89   2  14 113  88 162 170  23  78
  83 156  45  58  32  55  81 122 151 127 102 142  61 143  85 101  84 130
  36  79 153  30  77  22   0   9 161   1  64 114  42  92  53   5  65 109
 135 139  96  33 116 104  17 165  67 150  57 154 163 117 137 125 105 100
 158  27   8  19 169  13  52 145  43  93 159  48  18  15  97  39 177  12
 173 167  60  47  95 103 171 110 136  34  98  56  28 144 166 146 124]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2952
INFO voc_eval.py: 171: [195 192  65 266 285 414 210 137 212  14 157 319 284 306 300 216  69 197
 194 362 181 395 336  90 165 193 270 396  15 305 239 412 110 247 166  75
 255 202 196 136 235 241  61 383  83 178 131 120 147  43 245 384 163  87
 342 132 135 420 308 154 331 114 214 116 421  76  54 343 307  35 301 361
 283 119 162 150  88 168 257 288 344 337 218  67  36  18 111 246  30 278
 320 389 130 317 177 228 189 199 348 161 215 200 321 250  28 347 220 271
  16 127 139 405 105 294  20 276 303 413 400  60 333 314 207 167 226 304
  80 123 221  50 183 122   4 248 378 353 175 346  29 201 109  99 391  62
 364 407 393 415 316 263   5 240 129 422 118  71  78 295 351  45 373  40
 411 251  37 229   3 265 219 416 311 145 368  77 273 349 410 281  32 360
 330 377 282 261 208 100 325 323 399 209 287 258 324 310  49  17  31 380
 274  10 236 121 138 173 376 340 297  74 188 299 313  38   6 125 238 107
 148 205 227 237 242 280 350 103 213 172  48  51 379 140  12   7 334 169
 217 408 394  91 397 134 164  27 128 409 170 289  39  22 153 179  55  59
 381   8 115  57 249 354  86 418  42 387  19 385 143 124  56  68 155 277
  26  66  44  33 328 318 262 298 252 144 149 358 419 404 315 211  85  23
 275 367 230 335 366 254 268 133 176 180  95 292 332 222 234 370   2 341
 151 355 186 375  11 309  98 345  96  21 225 156 339 403 142 117  72 160
 171   0 272 244 291 406 158 388 338 204 371 259  25 296  84 372 290   9
  92 198 390 141 206 398 185  53 223 357  82  94 256  79  58 184  24  89
 232 146 182 126  93 113 382 374 312 191 174   1 152 279 402 112  63  64
 365 104 386 106 269 356  34 231 329 108 267  13 233 190 392 224 253  41
 203 322 243 187 327 359 326 159 352  73 264 260 401 302 293 363 417 102
 369  97  81  46  47 101 286  52  70]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3709
INFO voc_eval.py: 171: [403 253  67  76 133 345 216 132 106 136 290 294 350 217  69 111 260 227
 556 321 160 135  75  47 526 510  61 329 348 449 456 257 344  71 103 281
 109 450 255 568 252 110 189 315 138 471 295 234 207  91 570 516 278  97
 204 218 376 277 538 566 378 589  65 206 583 466 537 247 346 139 384 119
 279 237 379 185  11 116 112 458 336 158 194  14 308  60 440 339 564 131
 288 586 182 485 148 524 153 246  52 400 296 328 324 186 115 419 228 167
 347 495 193 256  92  98 588 423   3 322 429 548  20  41 273 593 453  19
 301 365 579 223 187 394 531 578 477 107 426 416 585 457  48 343  64 196
   6 582 161 541 208 302 406 432 534 408 210 523 165 482 461 242 157  33
 152 331 162 409 472 530 357  57 372 317 382 236  54 519  83 312 351 455
 561 276 163 270 454 529 504 395 444 581 335  63  70 411 341 451 443 124
 291 492 249 282  28 507  86  40 284 285 469 563 123 243 367 484 143 280
 268 503 190 333 166 591 150 464 180 373 428  27 391  42 261 452 205 144
 396 287  88 117 292  56 459 134 479 159 229  66 527 231 129  15  99 168
 214   4 431 442  38 362 202  37 517 462 310 176 192 445 239 164 122  62
 552 590 360 535   5 233 572 101 275 197  21  31 352   9 554 306 584  95
 540 140 447 415 553 407 497 173 500 219 425 414 448 489 316 418 209 118
 433  22 221 130 102 300 356 533 508 532 271 104 550 420 575 151 183 298
 560 238 374  90 521 422 397 525 569 460 318  49  30 386 358 137 491 355
 175  81 557 169 369 179 286 267 313 262 305  17  53 155 141 368 338 105
 359  25 170 289 303 385 402 330 113   7 283  94 377 125 555  51 486  45
  44 494  96 562 573  34  16 108 528 441 468 390 502 496 250 465 551  84
 587 539 299 241  80 594 475 297  36  93 114 211 314 320 558 380 499 559
  87 126 154 487   0  79 399 412  35  26 100 417 413 547 518  18 224 349
 388 392 435 293  10 389 437 463 480 174  77  82  50  55 254 181 370 381
 514 212 498 172 177 576 272 326 304 592 434 319 199 146 145 332 191 171
 149 515 258 512 577   8 274  12 410 398 323 128 361 226  23 470 511 178
 264 574 476 522 567  29  32 213 513  89 421 545 542  72 147 474 222 311
 430 120 488 424 371  85  39 544  46 473 266 387 571 225 478 248 127 467
 307 505 198  74 580 244 251 240 481 184  68 220  13 546 201 446 565 265
 263 342 366 230  43  58 142 393 509 309 235 501 520 325   2  59 506 543
 436 156 259 200 203 354 383 195 245 490 405 353 215 188  73   1 375 427
 337 363 439 364  24 404 334 327 269 232 493 401 536 121 438  78 483 340
 549]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3566
INFO voc_eval.py: 171: [256 484 701 ... 361 542 959]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4752
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4078
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.356
INFO cross_voc_dataset_evaluator.py: 134: 0.617
INFO cross_voc_dataset_evaluator.py: 134: 0.230
INFO cross_voc_dataset_evaluator.py: 134: 0.355
INFO cross_voc_dataset_evaluator.py: 134: 0.548
INFO cross_voc_dataset_evaluator.py: 134: 0.569
INFO cross_voc_dataset_evaluator.py: 134: 0.473
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.523
INFO cross_voc_dataset_evaluator.py: 134: 0.192
INFO cross_voc_dataset_evaluator.py: 134: 0.371
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.378
INFO cross_voc_dataset_evaluator.py: 134: 0.713
INFO cross_voc_dataset_evaluator.py: 134: 0.619
INFO cross_voc_dataset_evaluator.py: 134: 0.593
INFO cross_voc_dataset_evaluator.py: 134: 0.295
INFO cross_voc_dataset_evaluator.py: 134: 0.371
INFO cross_voc_dataset_evaluator.py: 134: 0.357
INFO cross_voc_dataset_evaluator.py: 134: 0.475
INFO cross_voc_dataset_evaluator.py: 135: 0.408
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.508s + 0.002s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.386s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.365s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.372s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.381s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.386s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.386s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.384s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.382s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.382s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.384s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.386s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.389s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.467s + 0.001s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.393s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.398s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.391s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.395s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.391s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.404s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.403s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.401s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.403s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.401s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.401s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.398s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.363s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.393s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.388s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.403s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.399s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.398s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.395s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.397s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.396s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.397s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.395s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.396s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.395s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.524s + 0.002s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.412s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.388s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.388s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.382s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.383s + 0.003s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.387s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.381s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.383s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.385s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.383s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.385s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.386s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.409s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [569 177 885 ... 121 311 453]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3565
INFO voc_eval.py: 171: [203  97 705 482 817 779 635 790 493 819 470 821  98 828 641 544 122 636
 823 744 789 370 816 537  83 565 476 830 685 642 235 261  55  58 802  44
 475 545 691 670 105 612 386 829 306 696 818 794 200 805 425 520 303 638
 382 760 124 480  42 761 515 765 706 648 109 163 288 102 707 491 538 477
 166 372 735 485 757 844 740 308 539 747 536 621 363 645  59 689 656 771
 205 748 191 563 472 693 421 846 492 204 266 577  82 752 697 835 579 568
 414 751 667 852 108 283 471 784 815 666 637 474 710 754 334 727 853  56
 478 218 479 335  14 133 856 174 506 187 847 615  99 799 305 111 260  43
 717 746 668 522 494 434  51 798 550 694 106 473 797 730 524 795 812 324
 365 845 358  24 599 134 140 115 489 356 125  86  16 682 217 330 709 521
 680 162 548 383 287 507 138 304 173 299 602 143 333 467 673 605 663 488
  66 855 782 490 262 569 259 564 558 510 352 800  45 834 630 589 525 340
 526 734 273 711 119 114 584 704 824 519 219 688 394 643   4 660 715 679
 465 418 785 695 255 206 282 759 796 588 788 692 413  29 232  93 837 503
 153 820 571 562 843 581 749 433 400  73 487 268 209  60 276 518 826 320
  23 267  62 401 214 708 190 714 227 168 527 117 213 154 676  15 793 552
 371  40 387 126 716   6 278 104 850 677 831 842 665 659 146 652 110 604
 241 483 240 169 155 496  26 424 300  34  70  87 376 297 211 440 256 118
  18 495 326 700 517 342 353 176  47 661 457 417 675 170 624 189 298  36
 396 529 201 505 224 428 623 791  77 194 729 664 501 541 252 626 145 848
 244 229 763 439  54 849 389  75 328 783 858 336 411  85 745 369 462   5
 841 226 196 351  84 731 809 753 290 770  89  65 207 415 776 585 378 175
 198  71 231 312  17 248 185  91 722 773 377 234 188 339 566 535  78 607
 601 448 587 171 466 243  68 144 502 264 778 405  67 380 654 684 777 438
 546  30 851 127 186 516  48 412 323 786 792 769 533   0 435 540 859 810
 222 690   1  20 113 361  72 331 379  46 618 292 678 653 658 576 739 649
 542 662 860 265   8 183 249 237  13 728 732 825 381  38 453  35  92  21
 804  69 360 313 586 639  10  19 610 321 271 758 247 296  11 807 182 702
 580 498 208 644 454 441 362 713 814 432 307 443 628 210 419 808 202 177
 384 669 392 738 801 567 423 532 766 646 755  41 341 803 712 756 725 391
 277 499 161 286  96 523 672 718 603 570 165 385 686 220 364 184 640 263
  88 164 179 289  79 461 318 250  74 787 199 447 228 322 611 327 459 632
 827 593 238 121 345 429 854 410 720 737 813  64 597 120 767 437 431 857
 251 123 557  53 172 655 591 223 294 484 446 698 225 157 768 137 726 197
 513 272 762 311 347 141 556 657  81 683 236 430  94 319  22 772 215 806
 100 245 302 239   7 458 230 450 528 701 750 221 560  37 469  90 291 160
 375  12  25 285 242 839 367  39  76 388 651 543 393 500 633 343 407 780
 359 180  33 310 723 402 281 192 629 354 270 181 822 422 314 406  28 293
 554 551 549 167 464 627   3 444 233 674  32 573 193 553 257 671 426 350
 775 366 416 301 147 129 781 420 600 616 622  50 811 561 687  27 295 617
 158 743 592 150  63 530 840 395 764 574 555 509 195   2 279 614 699 408
 736   9 325 246 254 514 836 107 534 442 511 132 212 575 315 357 253 317
 338 309 456  52 136 486  95 578 216 152 409 368 613 619 349 135 455 284
 159 427 647  49 583 348 832 608 468 274 595 355 625 390 681 399 703 598
 460 148 139 451 116 269 449 724 512 329 344 373 609 316 631 572 559 374
 112 280  31 719 733 504 101 103 142 596 403 404 436 547 130 131 741  80
 650 620 606 332 398 151 156 742 452 178 721 258 445 275 838 590 463 634
 346 833 397 149 481 128 774 582  61 531 337  57 508 497 594]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6145
INFO voc_eval.py: 171: [118 309 364 230 232 765 360   3 341 766 231 699 767 703 563 616 466 242
 462 121   6 235 700 287 281 770 832 328 768 264 366 171 631 788 595  15
 625 779 771 803 284 612 780 356 322 181 342 769 464  34 857 397 813 514
 790 622 852 847 630 458 705 343 777 282 347 402   8 552 204 400 772 477
 509 805 732 238 456 214 129 789 775  48 348  61 362 326 794  52  91 645
 117 673 859 144 539 589 265 463 783 791 267 652 449 361 760  76 467 778
 149 817 255 160 239 337 317 544 120 846 243 312 640 446 737 288  18 324
 588 234 207  35 273 253 142 844 530 116 479 140 307 816 600 114 141 741
 404 641 811 308 172 521 457 694  32  62  93 227 405 702  19 173 613 148
 688   7 498 109 277  54 611  78 843  59   4   5 133 566 543 461 437  63
 174 520 487 605 486 329 128 143 164 526 327 244 421 132  16 408  56 758
 774 378 367 220 271 642 368 381 469 374 523  89 226 432 522 439 151 202
 513 344 515  83 126 603 448 713 497 274 170 571 228 724 604 782 418 196
  69 375 734 722 726  13  77 346 380 180 807 740 369 175 587 538  10 773
 708 444  67 845 502 742 553 233 441 536 209 529 471 153  37 412 488  22
 115 826 650 828 285 135 223 516 302 715  75 761 541 349 280 707 295 753
 610 134  85 184 525 849 546 338 841 224 311 319 442 714 409 657 695 711
 315 333 506 435 787 445 624 211  65 578 698 591 481 197 747 739 528 182
 156  49 433 371 261 290 508 291 495 518 663  46 647 460 592  98  74  57
 229 213 808  20 644 725 796 494 756 490  68 833 161 839 685  23 867 618
 152  81 763 292 706 527 738 105 426 331  26 555 178  72 453 653 225 511
 579 825 537  36 793 792  33 159 393 166 617  27 547 821 399 351  38 823
 303 799 414 372 124 517 834 606 567 548 621 452 720 754 512 245   2 169
 572 240 594 676 757 468 648 686 112 626 482 855  60 858 856  64 339 583
 353 373 752 809 863 131 283 851 454 824  11 628 559  73 820 519 289 359
 275 325 478 532 671 345 286 491 455 122 395 200 258 125 815 247  21 221
  31 254 681 217 545 250 524 493  41  30 314 730 505 619  82 542 735 776
 484 658 447 602 590 689 139 666 582 531 684 176 840 629 222 829 270 540
  55 430 609 208 687  42 199 340  70 350 165 157 403 615  53  58 413 276
 535 123 465 108 637 252 533 246 316  90  50 534  97 759 188 551 147 842
 510 429   1 332 659 744 155 492 436 473 459 411 100 868 259 614 407 330
 862  39 693 179 565 334 263 483  86 691  43 297 422 321 593 158 674 564
 865 145 415 417 710 363 554 584 257 427 416  51 558 806 391 370  71 358
 785 680 150 186   9 633 717 557  12 601 800  92 649 410 599 810 377 428
 655 451 704 218 406 336 668 210 106 424 201 438 394 398 795 718 632 298
 103 750 664  99 390 203 195  29 192 450 503 119 643 480 819 581 177 568
 137 216 110 387  28 638 260 560 249 798 682 392 236 310 719 138 850 306
  79 386 762  84 860 111 163 639 838 419 248 696 162  14 743 814 570 189
 219 830 107 278 764 866   0 382 580 646 212 206 635  66 301  25 669 731
 352 596 476 383  40 365 576 420 496 561 660 318 675 716  88 279 146 154
 837 801  94 848 556 736 797 385 191 194 355  45 501 550 104 573 183 485
 683 586 721 627 802  95 835 190 425 423 712 804 354 623 335 440 574 470
 251 654 101 300 475 296 745 729 434 749 751 728 304 598 569 215 784 262
 472 323 562 662 241 692 678 401 389 549 294  87 818 167 755 205 827 733
 575 376 474 709 113 746 864 661 136 836 489 690 396 500 313 620 388 701
 831  96 585 320  47 667 293 679 443 499 607 634 636 168  24 187 597 431
 268 198 127 651 504 861 379 697 854 102  80  17 672 299 727 748 384 272
 357 269 812 786 256 237 305 266  44 677 130 853 193 577 781 665 608 507
 656 670 185 723 822]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2248
INFO voc_eval.py: 171: [1331 1332 1547 ...  997 1475  326]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3530
INFO voc_eval.py: 171: [1418  821  874 ... 1101 1241  322]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5437
INFO voc_eval.py: 171: [ 71  44 137 140  17  48 222   7  46  88 177 374 154 281 290  52 351  30
 353 245 273  63 261 272 304 206 363 194  50 372 330 182 133 139  41 300
  49 373 286 295 204 347 258 268 111 311  53 175 235  77 186  55 219  45
 163 216  24 255 262  14   5  16 316  56 162 383 391 101 379 305 226 324
 306 149 327  19 159 384 126 145  93  90 280 248 151  31 113 283  87 312
 212  36 119 202  99 108  37 359 339 386  34 127 279  94 240 199  25 371
 208 209 242 227 385 388 142  54 375  65 238 134 187 340  82 236 338 211
 205 282  86 294  61 233 317 313 346 274 328  47 335 164 241 296 315 285
 171  79   3 158 320 197 174 102  89 309 355  67 203  15  20  10  51 297
 123 130  18 100  57 291 333 160 146 307 112   9 343 105 180 289 176 237
 185 310 138  35 341 249 229 269 243  21 247 362 332 377 106 380 107 122
 114  98 352 254 325 155 356 367 168 170  29  40 287 189  72 239 156  85
  23 369  43 329  83 337  13  70 267 178  27 118 230 270 225   6 251 195
 110 257 301 308  95 128 271 349 376 193 109 256 393 147 218 121  58 331
   2  38 224 348 364 129  68 284  84 263 334 136 298 115  39 314 169 213
  91 124 232  26 153 259 152 350  66 207 250 131 292 220 116 201 288 318
 192 358 264  75 188 210 278 157 326 191 217  69 132 215 246 368 223 172
  80 179  22 120 265  73   0 200 104 150 275 378 165 392   1 344 381 221
  32 183  96 322   8  78 266 144 184 103 141 198 253 161 190 365 321 390
  97 252 345 293 389 382  60 244  42 167 117 370  76 196 260 354 148 214
 181  28 303 234  59 135  11  33 323 173 143 387 228 231  92 319   4  64
 342 277 299 360 357 125  62 302  74 336  81 366 276 166 361  12]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5737
INFO voc_eval.py: 171: [1068 1117  487 ...  418  989  163]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4734
INFO voc_eval.py: 171: [ 11   6  24 114  25 102 111   8  45 106  40  60   1  79  80  82  57 105
 108  19  77  78  92  12  34  47  83  14  61  56  21  70   5 112 115  29
  41  49  93  33  54  58   7 107  10  69  44  75  37  84 103  30  20   9
  62  16 116  85  59  31 100  13  68  73  66  46  50  87  89  35   0 104
  32  53  88  91 101 113  74  27  76  36  64  48  90  18  22  67   2   4
  95 117  28  51  81  72  52  17  23  71  39  98  65  42 109  38  96  86
  99  63  55   3  43  94 110  97  15  26]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0057
INFO voc_eval.py: 171: [5756  632 5757 ... 1724 1351 6753]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5232
INFO voc_eval.py: 171: [  4  74   5 127  44 102  43  40 112 141 137  47  73 143  95  63  75  45
 144 104  37  90 111 119   8 132  32 106  60 157 131  91  31   9  58 110
 114  66  92  50   6 138 118  80 116  42 155  98  77  79 130  38 142 140
  83   3  99  46 129  15  84  55 105  14  93  41  16  25  49  82  36  51
   7 139  70 150  20 135  87  78 103 101 133  54  57 126 151 134  72 120
  23  35  56  24  89  88 128 117  53  94  30 136  39  26   1 113  59  11
  28  61 115  19 109 148  76   0  85 107 154 145  96 123  97  52  48 149
   2  71  29  18  27  10  86 146 125  33  34  81 124  12  67 108 147  68
  65 122  69 121 100 156  21  22  64  62  17 152 153  13]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1835
INFO voc_eval.py: 171: [ 856 1882  463 ... 1978 1416  225]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3676
INFO voc_eval.py: 171: [104  96 188  97   3 152 101  19  54 105 208   7  60  40  36 206  46  56
  65  91  33 205   0  27 192 186  12  79  80  64  55 166  13 162 131 143
 154 118  37 136  90 153  34 202 200 219  89 220 123  71  48 173 199 210
  58 124  10 155  85 215 115  15 119 189 221 159  11 102 187 168  98  63
  26 116 195  73 184 122 146  45  66 127 126 191 144 128 130  61 133 158
 120 103 174  39 207 151 109 112   2 121   5  16  84  62  23 175 110 149
  83 107  43 111 147 198  14  69 170  50 125 185  22 203 138 106  29  74
  95  57 201   8 141 190 193 169 165  67   4 167 129  81  31  76 163  47
 157 172 171 164  32  41 140  77  17  68  44  25 222  20 100  21 179  75
 176   1 217  24 204 214 113  94 161 182 135  70  88  59 108 181  49 209
  72  82 218  30  53 160  35  99  86  93  28 211  52 213 194   6   9 212
 117  51  18 183 114  87 142 137 178 150 216  42  78  38 196  92 139 180
 132 156 148 177 134 145 197]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1169
INFO voc_eval.py: 171: [141 124  28  21 167  92 127 176 123 142 183  63 149  23  75  33  22 117
 101 171  64 121  97  98 144 151 164 165 104  69   8 163 152   9 125  68
  79 126  53 106 107  83  47  48  35  32  62 147  45  86  57 172   4 128
  41  43  54   1  76 119  93 157  96  11  73  46   2 148 168 145 139 161
 140  65 174 103 118  15 162 109  74  87   0  77  31 160  26  81 143 178
 158  82 130  67  70 180 155 153 170  40 129  50 181 102  49 166  91  37
 173  51 150  12  61  10  89 179 122  94  13   6  27  17 100  34 131  18
  60 156 175   3   7 137 133 132  24 105  14  20  95 111  85  88  25 108
 146  78 136  80 169  59 184 116 112 120 114  19 115 110 138  90  36  42
  99  38   5  84  66 159  16  71 177 154  44  29  39 134  72  55 135  52
  30  58 182  56 113]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3781
INFO voc_eval.py: 171: [ 96 546  74 358 282 587 373 264 396 498 420 428 441 529  51 110  30 295
  26 374 411 470 526  97 272 309 270 565 147 596 186 594 236 170 137  68
 187 229 486  65  12 176 489 416 207  61 448  88 168 100 308 339 473 390
 265 532 174 585 601 221 349 310 338 481 138 413 359 332 311 589 253 279
  76 491 160 401 557 586 348 493 115 500 549 114  71 528 210 326 483 446
 287 175 209 449 363 542  56 300 214 128 593 346 424 231 204 375 404   3
 525 319 362  44 475 440  21 377 226 280 328 595 548 467 304 129 183  41
 527 320 159 106 126 414 156  34  45  58 576 200 317 537 432  92  87 597
 604 391 385 194 409 496 278  66 530 107 488 575 244 130  31 552 431 158
 451  83 219 273 323 484 599 510 144 415 402 582 132 245 116 379 394 350
 257 291 233 559 354 122 544 196 327 563 352 152  70 531 230 306 220 503
  94 442 371 501 260 188 598 211 600 513 472 263 399 150 538 541 577 423
 535 290 398 534  46 452  16  57 395 312 179 302 397 294 140 182 344 572
  29 393 443 322 564 515 461 487 492 435 540  62 299   1 123 301 591 450
 269 357 592 499 517  69 241  81 157 101 321  82  20 583 103 195 434 507
  24 177 388 271 386  95 509 206 568 479 316 331 588 109 337 539 405 240
 429 172 533  43  80 343 425  18  13 603 243 573 478 406 427 418 400  93
 222 133 324 504 153  28 329 190 482 555  33 266  15 341  14 369  98  10
 234 455 465 313 454 387 543 191 166 127 330 249  36 252  40 523 201 120
 225 485 421  19 139 445 142 566  64   9 381 171 474 459 574 117 536 551
 135  72 570 193 382 173  60 469 353 360 447 285 545 516 340 571 248 136
 422 305 453 162 384 518  99 426 141  32 198 239 293 149 161 347 524 102
 134 560 224 325 314 199 334 457  47 283   4 403 579 145  22  73 247  42
 522 108 268 361 466 215 494 519 254 104 380  53 203 417 218 366 148 506
 185 407 151  85 502 547 292 296  37 192 146 189 217 297 497 276 471 430
 289 376 590 605 111 255 246 164 392  35 277  86 602 561 367 178 444 232
 456 143 286  38 468 438 355 356 118  11 567  17 169 370 154  63  59 437
 163 250   8 113 345  78   2 408 223 553  89 197  39 436 131 267  48   0
 364 372 389 495 288  49 275 550 124 333 562 578 212 119 125 512  23 228
 480 261  54 433   7 155 281 180 490 259 242 463 342 336 458 237 505 521
 511 508 284 368 476 258 584 514 460 238 315 464 351  25 112  52 558 274
 554 251 213  55 581 335 303  79 205 256 184 569 227 105 181 383 580 520
 216 412 439  90 167  67  27   5  91 318 410 262 298 208 165 307  77 121
 556  84 419 202  50 235 462  75 378 477 365   6]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7128
INFO voc_eval.py: 171: [ 2009 11159   584 ...  5490  9780  7349]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6206
INFO voc_eval.py: 171: [ 339  432  666 ... 1199 2468 1593]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5920
INFO voc_eval.py: 171: [127   3 121   4  68  73 150 107  98 105 179  19 143 120 148  25  72  61
 134  70 157 119  69  47  89  20 130  66  44  28  67   6 149 165 122  71
 112 124  49 154  48  33 151 169  24  36  39  11 173 176  52 141  78  80
  42  85  16  23 132 182  10  60  64 142  29 136 106   7  38 110  92  74
 158  88 129 175 161 133 115  57  84   2 171 163  86  87  35  14 113  53
  22 128  76  56  81 123 144 101 100 153  43  83  79 145  82  30  59 131
  77  34 155   0  75 114  21   9 162   1  40  62  51  63  90 139   5 166
 137 108 116 152 103  31 156 140 126 118  55 159  94 164  65  13   8 170
  99  91  18 104  50  26 146 181  12 168  15  45 174  93  37  41  46 160
  95  96 102 172 109  54  17 167  58 125  27  32 138 135 117 180 147 111
 178  97 177]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2830
INFO voc_eval.py: 171: [202 199  70 273 292 425 217 139 219  16 161 327 291 307 223 313  74 204
 201 371 188 406 200 170 346  94 277 407 312 244 423  17 209 171 112 253
 203 261 138  79 185  66 246 240 392  87 122 133 251 151  48  91 393 168
 158 352 134 221 315 137 431 116 314 353 432 341  59  39  80 118 173 370
 308 121 290 354 154  92 167 295  72 263  21 347 328  40 225 206 113 285
  34 207 252 196 184 256 399 222 132 358 325 234 166 278  18  31 129  65
 141 357 329 227 414 424 107 281  23 301 410 343 208 254 310   4 124 214
 232 311 322 190 125  55 172  84 387 363  32 373 356 402   5 182 404  67
 101 426 245 111 417   3 324 270  45 131  41 235 427  82 226 433 120 280
  76 422  50 361 149 272 382 377 302 359 319 257 288  35 421 268  81 331
 340 385 215  36 289 334 264 102  19 369 294 282 318 216 140 179 409 306
 241 333 389  54  11 177 350 304 243 321 386 195 123 248 220  43  53  78
 127 109 212 360   6 174 287 233 105 152   7 418  95 344 388 242 224 144
  56  30  13 186 169  44 136 157 175 405 365 130  73 419 390  63  25 255
  61   8 147 117  60 394  47 296 429  29 305 397  90 275 159 126  71 338
  22 153 284 148  89 283 430 269 218 413  37 345 326 323 368  49  26 187
 258 228 375 316 135 299 379 342 142 351 384  98 183 160  99 260  24   2
 279  12 239 231 355 193 364 176 119 155 265 211 349 380 412   9 303 164
 250 298 162 146 205 398 367  77   0 416  28 381 348 143  96  58 262 400
  88 297 213 192 229  83  86 408  97 191 108 150 237  93 128 115  27 411
 198 156 106 276  38  62 181 391 396 320 114 236 374  68 189 383  69 286
 197 339 103 238  14 230 330   1 366 259 403 267 274 110 271 100 194 104
  46 309 335 163 362 266 210  51 249 145 337 293 332  52 428 180 395  85
 376  57 401 300 420 372 378  20  42 178  10 165  75  64 247  15 317 415
 336  33]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3766
INFO voc_eval.py: 171: [419 267  73  82 139 359 226 138 111 420 143 307 310  75 227 364 116 275
 238 580 335 167 142  81  52 550 362 344 471 533  67 272 478  77 358 269
 592 114 108 198 115 266 329 472 298 144 494 217 311 247  98 594 539 228
 295 103 294 391 214 590 562  72 613 216 607 393 145 561 360  99 488 261
 399 124  13 394 296 121 250 480 117 351 193 204 165  16 588 322  66 137
 190 548 610 305 461 343 354 338 507 312 194 416 260 239  57 120 438 161
 155 361 270 203 336 612 104 421 175 518   5 442 290  21 449 195  45 572
 218  22 112 315 617 609 555 234 168  71 606  69 206 475 357 435   8 445
 603 425 316 479  53 380 500 558 409 505 220 602 547  70 452 483 427 565
 256 173 160 164  38  63 365 169 331  91 346 477 170 372 554 495 428 286
 542 249 293 476  76 397 326 585 553 605 387  60 410 308 473 527 350 514
  30 430 466 355 491 200 301 465 506 382 263 129 299  46 302 128 530 587
 526  92 615 150 297 257 276 309 486 284 215 388 141  47  17 157 502 474
 224 481  29 174  62 122 348  95 448 406 411 151 166 188 243 202 304   6
 134 467  42 324 184  68 484 176 241  43 451 540 614 105 377 551 127 464
 212 252 596 559 246 292 576   7 207  35 106 375 171 230 608 146 101 366
  11 578 453  23 232 437 370 123 330 434 469 520 470 320 219 433 564 314
 523 511 136 577 556 426 531 107 557 444 181  24 313  97 599 369 574 439
 109 545 482 389 593 191 140 288 549 412 303  34 159 277 251 183 384 401
 110 441 584 327 147  54 383 187 513 332 163 283 345 118 177 373 319 178
  58 581  27  87 597 300  18 353  19 418  50 490 392 463 130 102 586 552
 113   9 508 579 374 306  49 517 400 611  90  56 100 317 405 395 487 519
 328  41 119 575 334 525  39 415 221 162   1 255 498 264  28 235 563 363
 131 582  86  93  12  40 432 431 436 583 618 522  85 541  83 182 222 407
  61 509 571 396 485 403 503  20 209  88 268 189 185 273 537 152 385 455
  55 340 180 404 458 521 287 318 546  14 201 600 156 291 153 233 347 598
 601 492  25 429 414  78 179 423 376 333 223  96 499 454 535  32 237 133
 538 534 569  89 186 279  10  36 337 566 443 325 386 591  51 595  74 208
  44 497 489  80 440 154 536  15 231 321 496 568 450 356 402 211 493 125
  48 210 132 274 236 262 510 589 192 281 544 265 604 501 242 532 339 323
 524 570 381  64   3 528 254 205 567  65  79 196 258 367 213 248 504 259
 424 280 560 149 278 225 529 468  84 342 378 422  26 408 398 368 390   2
 245 512 459 457 199 460 447 456 417 446 126 371 413  37 285  59 573 158
 135 379 543 341 516 352  33 197  31 462  94 253 244 240 515 349 229 271
 148 172 282   4 289 616   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3454
INFO voc_eval.py: 171: [268 502 722 ... 892 844 636]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4790
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4062
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.356
INFO cross_voc_dataset_evaluator.py: 134: 0.615
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.353
INFO cross_voc_dataset_evaluator.py: 134: 0.544
INFO cross_voc_dataset_evaluator.py: 134: 0.574
INFO cross_voc_dataset_evaluator.py: 134: 0.473
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.523
INFO cross_voc_dataset_evaluator.py: 134: 0.184
INFO cross_voc_dataset_evaluator.py: 134: 0.368
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.378
INFO cross_voc_dataset_evaluator.py: 134: 0.713
INFO cross_voc_dataset_evaluator.py: 134: 0.621
INFO cross_voc_dataset_evaluator.py: 134: 0.592
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.377
INFO cross_voc_dataset_evaluator.py: 134: 0.345
INFO cross_voc_dataset_evaluator.py: 134: 0.479
INFO cross_voc_dataset_evaluator.py: 135: 0.406
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 5499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.472s + 0.001s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.395s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.394s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.387s + 0.003s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.396s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.393s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.391s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.390s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.389s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.391s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.390s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.393s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.396s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.572s + 0.003s (eta: 0:01:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.428s + 0.002s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.407s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.397s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.391s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.391s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.397s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.395s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.394s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.394s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.390s + 0.003s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.393s + 0.003s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.390s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.572s + 0.001s (eta: 0:01:11)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.394s + 0.003s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.392s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.399s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.388s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.386s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.385s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.386s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.387s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.389s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.389s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.391s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.391s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.599s + 0.002s (eta: 0:01:14)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.402s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.389s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.386s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.382s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.381s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.383s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.380s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.381s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.383s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.383s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.382s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.380s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.079s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [572 177 890 ... 313 742 221]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3568
INFO voc_eval.py: 171: [203  97 703 815 481 777 633 788 492 817 469 819  98 826 639 543 122 634
 821 742 787 369 536 814  83 564 475 828 683 640 261 235  55  58 800  44
 474 544 689 105 668 385 610 827 306 694 816 792 200 424 803 519 303 636
 381 758 124 479  42 759 514 704 763 646 109 163 288 102 705 490 537 476
 166 371 733 484 755 842 738 308 538 745 535 362 643 619  59 687 654 769
 205 746 191 562 471 691 844 491 420 266 204 576  82 750 695 578 833 567
 413 749 665 850 108 782 283 470 635 813 664 708 473 752 333 851 725  56
 477 478 218 334  14 133 854 174 505 187 613 845 797  99 305 111 260 666
  43 521 715 744 493 433 796 549  51 106 692 472 795 523 728 793 810 323
 843 364 357  24 598 140 134 115 355 488  16  86 680 125 217 707 329 520
 678 382 547 287 162 506 138 173 304 143 299 600 466 332 671 661 603  66
 487 853 780 262 259 568 489 563 351 557 798 509 832 628 588  45 339 525
 524 732 583 709 119 273 114 518 702 822 219 686 393 641 713   4 677 658
 693 464 206 255 783 417 282 757 587 794 786 690 412  29 232 502 818 835
  93 570 153 432 561 841 580 747 268 486 399  73 517 209  60 276 319  23
 824  62 267 400 214 190 706 168 712 227 526 117 154 213 386  40  15 791
 551 674 126 370 278   6 104 714 848 675 110 663 829 840 650 657 146 482
 602 169 300 423 240 241  26 495 155  34  70 375 211  87 439 297 256 698
 516  18 341 325 118 176 494  47 659 352 170 673 456 416 189 621 298 622
 395 427 528  36 504 224 201 789  77 727 540 194 662 500 145 252 761 624
 229 244 846 438 847  54 388  75 856 781 327 410 335 368  85  84 196 743
 226 839 461 350 807   5 290  65 774 729 768 207 584 751 175 414 198  89
  71 377 185 312  17 376 234 338  91 248 771 720 231 565 188 447 534 465
  78  68 605 171 599 586 243 144  67 264 501 776 404 775 379 437 652  30
 849 545 682 127 515 434 186 411 532  48 857 790 784 322 767 222   0 539
 330   1 808  20  72 113 676 688 360 378 616 292 651 737 575  46 237 656
 541 660 726 647 858 265  13 183   8 249 823 730  38  35 359 313 380 452
 585 802  92  69 756 608  10  21 271 637  19 182 320 700 805 296 208  11
 642 453 247 579 440 431 361 497 812 667 418 806 202 736 307 711 644 383
 177 391 799 566 626 210 442 422 531 764 753 710 340  41 754 277 390 801
 670 723 286 161 522 384 498 220 165  96 569 601 684 363 184 716  88 164
 179 326 609  74 638 263 289 317 460  79 630 785 250 344 825 446 199 458
 228 321 428 852 592 238 121 718  64 811 765 409 735 855 123 251 436 596
 556 120 430 172 590 653 294  53 483 445 223 512 157 272 555 225 766 346
 696 724 681 141 197 760 770 655  81 311 137 236 429  22 318  94 215 457
 239   7 449 748 230 804 100 302  37 245 699 221 527 468  39 559  90  12
 374 291 542 285 366 392 242  76  25 837 160 387 499 342  33 649 270 180
 358 406 401 778 631 627 421 353 310 281 550 553 181 192 721  28 405 820
 314 293 572 548 167 233 625 463 672 669  32 425   3 443 349 773 257 552
 365 301 193 415 685  50 560 419 615 614 620 809 295 779 147 129 150 158
 591  27 279 741 762 394 697 573   2 554 529 508 195   9  63 838 510 407
 734 612 246 254 315 212 324 337 253 316 485 441 834 513 132 356 107 533
 574 136 309 455  95 617 577 216  52 645 367 135 348 408 152 426 611 454
 159 284 830  49 467 354 582 347 606 679 139 459 450 398 389 594 597 623
 701 722 274 148 448 343 116 269 511 328 142 372 607 595 558 373 503  31
 112 571 731 629 717 101 103 648 280 130 546 739 131 402 618 403 435 156
  80 444 604 331 530 451 397 462 480 396 151 836 831 719 258 178 345 589
 632 740 275 336 772 128 507 149 581  61  57 496 593]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6037
INFO voc_eval.py: 171: [119 313 369 232 234 772 365   3 346 773 233 706 774 710 570 623 473 245
 469 122   6 237 707 290 284 777 839 333 775 267 371 173 638 795  15 602
 632 786 778 810 287 619 787 361 327 183 347 776 471  34 864 404 820 521
 629 797 859 854 637 465 712 348 784 285 352 409   8 559 206 407 779 484
 516 739 812 240 463 216 130 796 782  61  48 353 367 331 801  52  92 118
 652 146 680 866 546 596 268 470 790 798 270 456 659 366  77 474 767 785
 151 258 824 162 241 342 321 853 121 246 551 316 453 647 291 744  18 329
 595 236 209 256 276  35 851 144 486 142 117 537 311 823 115 607 143 411
 748 174 312 648 818 464 701 528  32  62  94 229 412 709  19 175 620 150
   7 695 505 110 280 618  54 850  79  59   5   4 134 573  63 468 550 444
 176 527 494 612 166 493 129 334 145 533 332 428 247 133  16 415  56 781
 765 383 372 222 274 373 649 386  90 379 530 476 228 439 153 446 529 520
 349 204 127 522  84 610 455 504 720 277 172 578 230 731 611 789 425 198
  69 380 733 729 741  13 182  78 351 385 814 177 594 374 747  10 545 780
 715 451 749  67 235 509 560 852 448 543 478 210 155  37 536 419 833  22
 495 116 835 288 225 657 523  75 306 136 722 548 714 768 354 299 283 760
 135  86 617 186 553 532 848 323 856 343 721 315 226 449 416 718 664 338
 702 319 513 794 452 442 631 213 585  65 705 199 598 535 754 488 746 184
 158 376 440 264 293 294  49 467 515 654 670  74  46  99 599 525 502 231
  57 215  20 651 815 803 763 732  68 501 497 163 846 874 840  23 625 692
 154 770 713 336 295 433  82 106 745 534  26 180  72 460 660 562 800 544
 586 832 518 227  36 799 400 161  27 168 554  33 406 828 624 377  38 830
 727 307 125 356 574 841 421 459 613 628 806 524 519 555 761 248 171   2
 579 242 601 475 683 633 693 655 862 764  60 344 113 865 489 863 590  64
 378 759 461 358 132 816 635 870  11 831 286 364 566 858 526 292  73 462
 485 123 827 498 250 261 202 678 350 278 330  21 253 402 539 126 289  31
 688  30 219 318 223 822 500 552  41 257 549 531 737 626  83 665 512 454
 742 609 597 491 696 783 141 178 673 847 538 691 589 224 636  55 836 273
 547  42 437 212 616 694 201 355 345  70  58 167  53 410 420 622 279 159
 542 472 124 109 255 320 644  91 249  98 540  50 541 766 190 149 436 337
   1 666 849 443 517 751 480 558 157 101 499 418 466 869 572 621 875  39
 339 262 335 181  87 700 429 414 301  43 600 698 571 147 490 325 160 266
 717 681 368 424 260 872 434 398 422 591 561 375 565  71   9 363 792  51
 188 813 423 152 687  12 564 640 608 807 724 382 606 417 341  93 458 711
 817 656 662 413 431 435 203 445 107 211 220 639 675 802 401 302 405 725
 100 205 757  29 671 193 197 457 397 104 588 120 179 510 650 567  28 139
 487 263 218 645 111 826 140 314 805 252 575  80 726 238 394 399 769 689
 165 310 867 857 391  85 281 221 750 426 251 577 845 191 646 112  14 821
 164 703 653 837 214 108 642   0  66 587 483 771 305  25 357 388 387 873
 208 738 603 427 583 676 667 370 282 723 503 568 148 682  40 360  45 156
  95 322  89 844 743 563 580 508 196 855 634 808 804 728 390  96 105 809
 185 557 340 593 690 630 492 432 359 719 300 304 842 482 581 430 192 102
 811 661 447 605 308 791 217 441 254 736 477 752 735 265 243 699 576 685
 328 479 669 758 756 381 169 569 556 408  88 582 207 297 834 668 496 507
 114 396 825 481 871 740 716 762 317 403 843 138 753 674 450 592 395 686
 697 627  24 838 708  97 868 324 296  47 384 511 128 658 643 438  81 604
 200 704 362 861 614 793  17 272 189 259 170 271 506 819 641 239  44 275
 755 672 103 389 131 734 679 303 584 860 269 514 309 663 195 677 684 788
 326 187 137 392 298 829 615 244 730 194  76 393]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2248
INFO voc_eval.py: 171: [1336 1337 1554 ...  193  431  326]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3527
INFO voc_eval.py: 171: [1422  827  880 ...  200  534  687]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5434
INFO voc_eval.py: 171: [ 71  44 136 139  17  48 221   7  46  87 176 373 153 280 289  52 350  30
 352 244  63 272 260 303 271 205 362 193  50 371 329 181 132 138  41 299
  49 372 285 294 203 346 257 267 110 310  53 174 234  77 185  55 218  45
 162 215  24 254 261  14   5  16 161 315  56 382 390 100 378 304 225 323
 305 326 148 158  19 383  92 144 125  89 279 247 150 112  31 282  86 311
 211  36 118 201  98  37 107 358 385 338  34  93 278 126 239 198  25 370
 207 241 208 226 384 387 141  54 237  65 374 133  81 339 186 235 210 204
 281 337  85  61 293 232 312 316 345 273 327 163  47 334 295 240 314 284
 170   3  79 173 157 196 319 101  88 308 354  67 202  15  20  10  51 296
  18 122 129  99  57 290 159 332 145 306 111   9 342 104 288 179 236 184
 175 340 137 309 248  35 246 242 268 331 228  21 376 361 105 379 121 106
 351 113  97 324 253 355 366 167 154 169  40  29 286  72 368 155 238 188
  23  43  84  13  82 336 328 266 117 177  70 194 229 269  27   6 250 256
 224 109 127 300 307 192  94 348 270 255 375 392 108 146 217  58 223   2
 330 120 363 347  83  68  38 128 135 283 262 333 212 297 114 168  90  39
 123 231 313 152 258  26 206 151 219 249 130 349  66 291 287 115 317 200
 357 191 263 187  75 209 325 216 156 190 277 131 214  69 245 367 264 171
 119 222 178  73  22 199 343 103   8   1 321 377   0 391 274 164 149  78
  95 182  32 220 380 265 143 140 183 102 197 160 252 364 251 189 320 389
  96 344 381 292  42 388 166 369  60 243  76 116 353 195 233  11 147 259
 180 213  59 386 302  28 134 172 322 230 142  33 341 227 356 298  64 124
  91 318 359   4  62 276 360 301 275  74 335  80 365  12 165]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5736
INFO voc_eval.py: 171: [1120 1070  489 ...  991 1130  110]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4735
INFO voc_eval.py: 171: [ 11   6  24 114  25 102 111   8  45 106  40  60   1  80  79  82  57 105
 108  19  77  78  92  12  47  34  83  61  14  21  56   5  70 112 115  29
  41  49  33  93  54  58 107   7  10  69  75  44  37  84  30 103  20   9
  16  62 116  85  59  31 100  13  73  68  66  46  50  87  35  89 104   0
  32  53  88 101  91  74 113  27  36  76  48  22  64  67  90  18   2   4
  95  28 117  72  81  51  39  17  52  71 109  23  42  98  65  38  99  96
  86  63  55  43   3  97  94  26 110  15]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0058
INFO voc_eval.py: 171: [5779  633 5780 ... 3777 4622 2225]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5234
INFO voc_eval.py: 171: [  4  74   5 127  44 102  43  40 112 141  73  47 137 143  95  63  75  45
 144 104  37  90 111 119   8 132  32 106  60 157 131  91  31   9  58 110
 114  66  92  50   6 138 118  80 116  42 155  98  77  79 130  38 140  83
 142   3  46  99 129  15  84  55  14 105  41  93  16  25  49  82   7  36
  51 139 150  70  20 135  87 101  78 133 103  54 126  57 134 151  72 120
  23  35  56  24  89 128 117  88  53  94  30  39 136 113  11  59   1  26
  61  28  19 109 115 148   0  76 107  85  96 145 154 123  48  52  97 149
  71   2  18  29  10  27  86 146 125  34  33  81 124 108  12  67  65 122
 147  68  69 100 121 156  21  64  62 153  22 152  13  17]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1828
INFO voc_eval.py: 171: [ 861 1891  466 ... 2114  183 1263]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3670
INFO voc_eval.py: 171: [105  97 189  98   3 153 102  19  55 106 209   7  61  40  36 207  46  57
  66  92  33 206   0  27 187 193  12  80  81  65  56 167  13 163 132 144
 155 119  37 137  91 154  34 203 201 220  90 221 124  72  48 174 200 211
  59 125 156  10  86 216  15 190 116 120 160 222  11 188 103 169  99  64
 117  26 196  74 185 123  45 147  67 128 127 192 145 129 131  62 134 159
 121 104 208  39 175 152 113 110 122   2   5  16  85  63  23 176 150 111
  84 108  43 112 148 199  14  70 171  51  22 126 186 204 107 139  29  96
  58  75 202   8 142 191 194 170  68 168 166   4  31 130  77 164  82  47
 158 173 172  41 165  78  32  69  25  17 141  44 223  20 180 101  21   1
  76 218 177 114 205 215  24 162  95 136 183  60  71  89 210  49 182 109
  73  54  83  30 161 219 100  94  35 212  28  87 214   6  53 195   9 118
 184  52  18 213 138  88 143 179 115  42  79 217 151  93 197  38 157 133
 140 181 178 146 149 135 198  50]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1168
INFO voc_eval.py: 171: [141 124  21  28 167  92 127 177 123 142 184 149  63  23  75  33  22 117
 101 172  64 121  97 144  98 151 164 165 104  69   8 163 152   9 125  68
  79 126  53 106 107  83  47  48  35  32  62 147  45  86  57 173   4 128
  41  43   1  54 119 157  76  93  96  11  73  46   2 148 145 169 161 139
 140  65 175 162 118  15 103 109  87  74   0  77  31 160  26  81 143 158
 179  82 130 153  70  67 181 155  40 171  50 129 182 102  49 166 174  91
  37  51 150  12  61  10  89 122  94 180  27   6  13 100  34 131  17  18
  60 176   3 156   7 132 137  24 133  95 105  20  14 111  85  25 146  88
 108  78  80 136 170 185  59 116 112 168 120 114  19 115 110  36 138  38
  90  71  99  42  16  84 159  66   5 154  44 178 134  29  39 135  55  52
  72 183  30  58  56 113]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3781
INFO voc_eval.py: 171: [ 95 546  73 358 282 587 373 264 396 420 498 428 441 529  50 109  29 295
  25 374 411 470 526  96 272 309 270 565 146 595 185 593 236 169 135  67
 186 228 486  12  64 175 489 416  60 206  87 448  99 167 308 473 339 390
 265 532 173 585 600 220 310 349 338 481 413 136 359 332 311 589 253 279
  75 491 159 401 557 586 348 493 114 500 549 113  70 528 209 326 483 446
 287 208 174 363 542 449  55 300 213 126 592 346 424 230 375 404 203 525
   3 319 362 475  43 440 377  20 225 280 328 594 548 467 304 127 182 527
 105  40 158 320 155 414 124  33  44  57 576 199 537 317 596 432  91 603
 391 385  86 193 409 278 496  65 106 488 530 575 244 128  30 552 431 157
  82 451 218 323 273 598 484 510 415 143 402 582 130 245 115 394 379 350
 291 257 232 559 354 544 195 121 352 563 327 151 531  69 229 306 219 503
  93 371 442 501 260 597 187 210 599 263 472 513 399 149 541 538 577 398
 535 423 534  45 290  16 452  56 395 178 312 294 397 302 139 181  28 572
 344 322 564 443 461 393 515 492 487 435 540  61 269 299 590   1 450 122
 301 357 499 591  68 517 156  80 241 100  19 321 583  81 194 434 102 386
 507 271  23 388 176 568 509  94 205 108 337 479 171 533 429 316 331 539
 588  79  42 240 405  17  13 243 343 425 573 602 478 400 406 418 221 324
  92 427 504 131  27 152 555 190 329 482  32  15 341 369  14 266  97 234
  10 455 387 454 465 313 543 165 191 330 125 523  35  39 421 224 119 249
 252 200 485  18 566 141 445  63 137   9 170 474 116 459 381 574 382 551
 172 192 536 133  71 570  59 360 447 469 353 545 285 516 340 134 248 571
 384 422 518 426 161 453 305  98 197  31 239 347 140 560 293 148 314 524
 132 160 223 101 325 283 334   4 144 198 579  72  46 457 268  21  41 361
 403 247 494 522 466 107 519 214 103 254 202 380 417  52 217 147 366 506
 184 150 407 292 547  84 502 188 189  36 296 216 497 145 376 430 392 255
 246 276 110 471 297 289 604 163  34 367 277  85 444 231 456 177 356 142
 561 286 601 438  37 468 355  11 370 153 117  58 567  62 437 162 168 408
   2  77 553 112   8 250 129 222 345 495  38 436  88 196 372  47 389 364
 267   0 288 275  48 562 550 333 578 118 211 123  53 512 261 480  22 227
 154 179   7 281 433 490 342 511 463 505 242 336 259 521 476 458 237 284
 584 111 238 514 508 368 258 460 251  24 315 351 464 569  51 274 335 558
 212 554 581 303  54  78 183 256 204 226 104 412 180 383 166 215 520   5
 439 580  26  89 318 410 164  90 262  66 556  76 207 298 307 201 462  74
 120  83 419 365  49 235 477 378 138   6 233]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7128
INFO voc_eval.py: 171: [11230  2021   591 ...  6019  8689  6925]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6181
INFO voc_eval.py: 171: [ 339  432  664 ...  991 2160 1035]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5919
INFO voc_eval.py: 171: [128   3 122   4  69  74 151 108  99 106 180  19 144 121 149  25  73  62
 135  71 158 120  70  47  90  20 131  67  44  28  68   6 150 166 123  72
 113 125  49 155  48  33 152 170  24  36 174  39  11 177  52 142  79  42
  81  86  16  23 133 184  10  61  65  29 143 137  38 111 107   7  93 159
  75  89 130 162 134 176 116  58  85 172 164   2  87  88  35  14 114 129
  53  22  56  77  82 102 124 154 145  43  84 101 146  80  83  30  60 132
  78  34 156 115   0  76 163  21   9   1  40  51  63  64  91 140 167 109
 117   5 138 141 104 157 153  31 127  55 119 160 165  13  95  66 100   8
 171  92  18  50 105  26 147  12 182 169  94  45  15  37 175  97 161  96
 173  41  54 103  46 168 110  17  59 126  27  32 139 136 181 118 148 112
 179 178  98  57 183]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2830
INFO voc_eval.py: 171: [203 200  70 275 295 218 430 140 220  16 162 332 294 310 224 316  74 205
 202 376 189 411 201 171 351  94 280 315 412 428 210 246 172  17 113 255
 204 263 139 186  79  66 242 248 397  87 123 134 253  48 152  91 398 169
 159 357 222 135 318 117 138 436 317 358 437 346  59  39 119 174  80 375
 311 359 293 155 122 168 298  21  92 333  72 265 352  40 226 207 114 288
 208  34 254 197 185 258 404 223 363 133 329 235 167 281  18  31  65 142
 228 130 362 419 334 429 108 284  23 304 415 348 256 209 313   4 125 215
 233 325 314 126 191 173  55  84 392 378  32 368 407 361   5  67 431 409
 183 101 247 112   3 328 272 422  45 432 236  41 132 227 438 121  76 283
  82  50 427 150 274 366 382 387 364 305 259 291 322  35 270 426 390  81
 339 216 336 345  36 266 292 374 103 141 321 180 285  19 297 217 414 243
 309 394 338  54  11 178 245 355 324 250 391 221 124 196 307  43 128  78
 110  53 365 213 106   6 290   7 234 175 423 153 244 349  95 393  30 145
 225 170  56 410  13 187 158 176 131 424 137  44  73 370 395  63   8  25
  61 399 257  60 434  47 118 148  29 308 402  90 299 277 127 160 343  89
 154  22  71 287 219 149  37 271 326  49 286 418 435 350 380 373 229 260
 188  26 331 384 136 319 302 347 143 389 356 161  98  99 184 262  24   2
 282 360  12 232 177 240 194 212 369 267 120 156 385 163 252 306 354 417
   9 372 301 206 165 147  77 353   0 403 421 386  28  96  58 144 264 405
 214 300  88  83 230  86 193  97 413 129 192 416 238  93  27 151 116 109
 157 182 199 396  38 279  62 401  68 107 323 115 237 190  69 379 198 371
 289 335 388 276   1  14 231 344 239 104 100 367 105 111 273 195 261 408
 269 164 340 312  46 146 211  85 337 268 400  51 251 296 342  52 433 377
 181  10 303  20 425 381 249  57 406 383  42 420  75  64  33 320 166 179
  15 241 330 341 278 327 102]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3774
INFO voc_eval.py: 171: [420 268  73  82 139 360 227 138 111 421 143 308 311  75 228 365 116 276
 239 581 336 167 142  81  52 551 363 345 472 534  67 273 479  77 359 270
 593 114 198 108 115 267 473 330 299 144 495 217 312 248  98 595 540 229
 296 103 295 392 214 591  72 563 216 614 608 394 145 562 361  99 489 400
 262 124  13 395 121 297 481 251 117 352 204 193 165  16 589 323  66 137
 190 549 611 344 306 462 355 339 313 508 194 261 417 240  57 120 439 161
 155 362 271 203 422 337 104 613 175 519   5 443 291 450  21 195  45 218
 112  22 573 618 610 556 316 168 235  71  69 206 607 358 436 426 476 604
  53 317 480 446   8 501 559 381 506 410 548 220  70 603 453 484 428 566
 257 160 173  38 169  63  91 164 366 332 347 170 478 373 287 496 555 543
 429 294 327 250 477  76 398 586 554 388 606  60 309 411 474 528  30 515
 351 431 467 356 492 302 200 507 383 466 300 303  46 129 264 128 588 531
 616 527 150  92 277 258 298 310 487 389 215 285  47 141  17 475 225 503
 482  29 157 174 449  62 122  95 349 151 407 166 412 244 188 202   6 184
 305 325 468 176  68 134  42 485 615  43 452 541 242 378 127 465 253 552
 105 293 597 212 247 560 577   7 207 376  35 106 171 231 609 146 101 367
 454 579 233  23 435  11 371 438 331 470 521 123 471 219 434 321 315 512
 565 136 524 558 107 557 427 532 578 575 109 181 314 546 370 445 440 594
 483 600  24  97 140 390 191 550 304 289 183  34 413 159 278 252 110 385
 442 147 402 585 384 328 514 163  54 333 284 118 177 187 178 374  58 346
 419 320 301  18  27 598  87 582 354  19 491  50 393 102 587 130 464 518
 553 113   9 580 509 612 375  90 307 401  49 100  41 406 318  56 488 162
 520 329 335 119 396 576   1 416  39 526 221 583 265 236 131 499 256  12
 364  40  28  86 564  93 433 432 542 584 437 619  85 222 510  61 408 523
  83 486 182 209 404 269 397 504 572 274 538  20 189 185  88 456 152 386
  55 341 180 153 459 201 405  14 292 522 288 156 547 599 601 602 415 319
  78 493  25 234 424 430 348 377 224 334  96 179 500 570  89 186 455 536
  32 387 535 592 596 238 133 539 280  10 326 208 567  80 338  74 444  51
 498  44  36 441 537 154 497 490  15 232  48 569 451 357 322 494 125 211
 403 545 340 243 590 511 192 132 266 237 275 571 210 525 263 533 605 324
 282 502 529 205   3 568 255  64 382  79 249  65 561 196 425 368 213 279
 259 260 530 281 505 226 149 379 343 399  84 423 409 391 369 469 460 199
  26 458 513 461   2 418 246 126 457 372 135 447 448 158  33 574 414  37
 342  59 544 286 517 245 241 353 380  94 463  31 617 197 172 230 516 254
 350 272 223   4 283 290 148   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3453
INFO voc_eval.py: 171: [268 501 722 ...  71 992 675]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4790
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4055
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.357
INFO cross_voc_dataset_evaluator.py: 134: 0.604
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.353
INFO cross_voc_dataset_evaluator.py: 134: 0.543
INFO cross_voc_dataset_evaluator.py: 134: 0.574
INFO cross_voc_dataset_evaluator.py: 134: 0.474
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.523
INFO cross_voc_dataset_evaluator.py: 134: 0.183
INFO cross_voc_dataset_evaluator.py: 134: 0.367
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.378
INFO cross_voc_dataset_evaluator.py: 134: 0.713
INFO cross_voc_dataset_evaluator.py: 134: 0.618
INFO cross_voc_dataset_evaluator.py: 134: 0.592
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.377
INFO cross_voc_dataset_evaluator.py: 134: 0.345
INFO cross_voc_dataset_evaluator.py: 134: 0.479
INFO cross_voc_dataset_evaluator.py: 135: 0.406
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 5999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.488s + 0.001s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.387s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.385s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.391s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.403s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.400s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.399s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.398s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.398s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.397s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.396s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.396s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.396s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.628s + 0.002s (eta: 0:01:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.388s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.389s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.385s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.389s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.387s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.396s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.397s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.398s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.397s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.394s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.400s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.397s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.570s + 0.002s (eta: 0:01:10)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.400s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.385s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.393s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.385s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.383s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.380s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.379s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.381s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.385s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.386s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.385s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.385s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.423s + 0.002s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.374s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.390s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.387s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.381s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.381s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.379s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.378s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.379s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.379s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.376s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.378s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.380s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.665s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [571 177 889 ... 764 822 217]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3578
INFO voc_eval.py: 171: [203  97 705 818 481 780 634 791 492 820 469 822  98 829 640 544 121 635
 824 745 790 369 537 817  83 565 475 831 685 641 261 235  55  58 803  44
 474 545 691 105 669 385 611 830 306 696 819 200 795 424 520 806 303 637
 761 381 479 123  42 762 515 706 766 647 109 162 102 288 707 490 538 476
 165 371 736 484 758 845 741 539 308 362 644 748 536 620  59 689 655 772
 205 749 190 563 471 693 847 420 266 491 204 577 753  82 697 579 836 568
 413 752 666 853 108 785 283 636 470 816 711 665 473 755 854 333 728  56
 477 478 334 218  14 132 857 173 505 186 614 848 800  99 305 111  43 260
 667 522 718 747 799 550 433 493 106  51 472 694 798 524 731 796 813 323
 364 846  24 357 599 133 139 115 355 488  86 124 682  16 709 329 217 521
 680 382 548 506 161 287 137 172 304 142 332 299 466 601 672 604 487 662
 856  66 783 569 262 259 489 801 351 558 564 510 589 835 629  45 339 525
 526 825 735 273 712 704 118 584 114 219 519   4 688 642 393 716 695 464
 679 659 417 786 255 206 282 760 588 797 692 412 789  29 502 232 821 571
 152 838  93 432 562 581 844 750 486 268  60 209 399 518 827  73 276 319
  23  62 267 400 708 715 189 227 167 214 527 116 153 552 675 794 213  40
 278   6 386  15 370 125 104 717 664 677 651 851 110 832 145 843 482 658
 603 423 240  26 168 495 241 300 154  70  34 439  87 211 375 517 256 297
 700 325 117 341  18 175 660 494 352  47 456 169 529 674  36 416 427 298
 188 224 792 623 395 622 504 201  77 541 730 500 191 252 764 849 663 144
 229 625 438 244 850  75 327 784  54 388 335 368 859  85 410 746 226   5
  84 195 810 350 461 777 771 842  89 290  65 174 754 207 732 585 377 414
 197  71 312  91 723 376 184  17 338 231 774 187 248 234 566 465 587 535
 170 447  78 501 143 779 243 606 600  68 264 778 379  67 126 653  30 404
 437 546 793 185 533 852 434  48 516 860 787 684 770 322 411   0 222 540
   1  20 360 330 811 113  72 740 678 542  46 690 292 652 617 576 378 661
 657 648  13   8 265 237 729 826 861 182 249 733 586  92 313 380 452 359
 805  35  21  38 609  19  10  69 638 759 808 271 643 580 320 247 181  11
 702 208 361 453 296 307 497 440 418 714 431 815 668 422 202 383 645 802
 442 210 739 809 391 532 176 756 767 567 627 713 340 804 671 390 277  41
 726 757 160  96 523 286 498 686 363 164 384 220 602  88 317 289 570  79
 719 263 163 183 639  74 631 326 458 610 460 321 788 250 178 446 855 344
 198 228 828 428 120 814 238 409 593 738 768 721  64 430 251 122 171 557
 858 597 119 436 294 483 223 654 591  53 445 698 225 556 272 196 346 156
 513 769 727 140 763 656 773 311 136 318 429 683  81 807 457 100 230 236
  22  94 245 449  37 239 701 221 302   7 751 215 560  25 528 468  39  12
  90 291 242 392  76 285 374 543 159 840 366 781 342 499 387 406  33 179
 650 192 551 358 270 632 401 281 180  28 549 421 554 310 724 353 628 314
 293 233  32 626 823 443 405 573 166 673 257 301   3 193 463 425 776 670
 349 365 553 615 616 415 744 812 782 419 157 146 128  27  50 621 561 687
 295 394 279 149 841 765 530 737 574 555 592 324  63   2   9 509 699 194
 356 514 337 613 316 837 107 315 212 407 511 246 254 253 441 575 534 578
 485 131 612 134 135  95  52 151 618 216 455 309 426 408 348 367 646 454
 158 284 583 347 398 354 833 607 459 147 389 681  49 703 467 450 725 512
 138 624 274 598 595 448 269 720  31 328 101 343 372  80 373 649 572 559
 280 630 608 435 503 734 103 141 112 596 177 155 834 605 743 547 839 130
 451 331 402 722 444 742 619 403 129 633 775 345 480 258 462  61 396 397
 336 531 150 590 127 275  57 508 148 582 676 496 199 507 594 710]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6036
INFO voc_eval.py: 171: [120 317 374 236 238 784 370   3 350 785 237 717 786 721 577 631 480 249
 476 123   6 241 718 294 288 789 852 337 787 271 376 175 646 807  15 609
 640 798 790 823 291 627 799 365 331 186 351 788 478  34 877 411 833 528
 637 809 872 867 645 472 723 352 796 289 356 416   8 566 414 210 791 491
 523 751 825 244 470 220 131 808 794  61 357  48 372 335 813  52 119  92
 661 148 603 553 879 691 272 477 802 810 274 463 668 371 481  77 797 779
 153 262 837 164 346 245 325 866 122 250 558 320 460 295  18 655 756 333
 602 213 240 260 280  35 864 146 493 118 544 144 315 836 145 615 116 418
 176 831 316 471 656 760 712 535  94  32  62 419 233 720 177  19 628 152
   7 706 111 284 512 626  54 863  79  59 135   4   5 580  63 475 557 451
 501 178 534 168 620 130 500 338 147 336 540 134 251  16 435 793  56 422
 777 388 377 278 378 226 391 657 483 384 537  90 155 232 353 208 446 453
 536 527 128 618 529  84 511 731 281 462 174 619 234 742 585 801 432  69
 385 202 744 185  13 740 753  78 827 179 355 390 379  10 552 601 759 792
 726 516 458 761 239  67 865 485 567 550 455 214 157 426 543  36 846  22
 502 848 117 229  75 666 292 733 530 137 310 555 725 303 780 358 287 136
 189 625  86 772 732 560 327 539 869 347 861 456 319 230 423 729 342 323
 673 713 520 459 639 806 449 217  65 716 592 766 203 495 605 542 187 447
 268 758 160 381 474  49 297 298 522 680  74 663  57 606 100 219  20 509
  46 235 532 743 816 775 828 660 508  68 165  23 703 504 887 859 633 156
 853 440 782 724 340 299 107 182  82 757 541  72 569  26 669 467 593 812
 525 845 551 231 811  37 163  33 382 407 170 632 561  27  38 841 126 581
 738 311 843 360 413 636 854 466 531 252 428 819 773 173 621 562   2 246
 526 586 641 608 482 694 664 875 776 348 114 704  60 597 133  64 878 496
 383 468 362 876 771 296 290 844 829  11 883  73 871 643 265 369 573 354
 492 533 124 689 206 334 469 505 254 282  21 257 261 127 546 322 840 835
 699  41 293 409 559  31 507  30 749 223  83 227 538 634 617 556 754 461
 604 674 519 498 795 683 707 180 545 142 860 596 277 702 849  55 644 228
 554  42 624 444 216 705 349 205 169 359  70 283 110 125 417 427 161  58
 479  53 630 259 253 549 652 324  91 548 547  99  50 778 443 193 341 151
   1 487 565 450 524 862 183 506 675 763 425 159 473 266 102  39 339 343
 579 711 882 629 888  87 421 607 329  43 709 497 305 162 578 270 436 692
 264 149 728 885 373 431 441 380 405 429 572 568 826   9  71 430  51 154
 804 598 368 191 571  12 648 698 387 616 735 671 424 665 614  93 830 820
 345 465 438 207 224 215 442 722 420 306 647 108 412 452 736 408 101 815
 686 201 121 209 681 105 464  29 769 404 595 196 181 517 222 839 112 658
 818 318 653 400 256 267  28 737 140 574 494  80 582 242 141 406 700 880
 314 285 255 870 584 654 396  85 858 781 167  14 834 433 166 194 850 762
 225 109 714 650 113 783 309 218   0  25 886  66 662 594 392 393 750 361
 610 212 687 490 434 590 676 510 575  40 375  89 734 150 693 286 755 326
 158 857 868  45 587 570 515 642 739 197 395 364 200  95 821 106 344 701
  97 439 600 638 817 103 195 188 670 499 730 822 564 308 437 363 588 764
 484 489 855 824 258 454 448 612 748 803 221 304 312 710 768 770 583 696
 386 679 847 486 247 415 747 269 332 838 171  88 563 589 727 211 774 503
 488 514 403 576 752 301 139 677 410 884 765 115 321 457 697 328 599  47
 129 684 708  98 635 389 401 856 611 832 667 851 445 719 649 805 651 204
 192  24 518 300 172 275 263 513 881 767  81 243 745 715 874 622 276 279
  17 690 591 307 367  44 873 199 104 394 800 132 313 682 399 190 248 273
 741 521 330 695 688 198 397 402  76 672 842 302 623 678 138 613 143 366
  96 746 659 398 184 685 814]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2247
INFO voc_eval.py: 171: [1344 1563 1345 ...  257 1305 1129]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3525
INFO voc_eval.py: 171: [1425  830  883 ...  531 1720 1686]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5434
INFO voc_eval.py: 171: [ 72  45 137 140  18  49 222   8  47  88 177 375 154 281 291  53 352  31
 354 245  64 273 261 305 272 206 364 194  51 373 331 182 133 139  42 301
  50 374 287 296 204 348 258 268 111 312  54 175 235  78 186  56 219  46
 163 216  25 255 262  15   6  17 317  57 162 384 101 392 380 226 306 325
 307 328 149  20 159 385  93 126 145  90 280 248 151 113  32 284  87 313
 212  37 119 202  99 108  38 360 387 340  35  94 279 240 127 199 372 208
  26 209 242 227 386 389 142  55 238  66 376 134  82 341 187 211 236 205
  86 283 339  62 295 233 314 274 347 318 329  48 164 336 297 241 316 171
 286   3  80 158 174 197 321  89 310 102  68 356 203  16  11  21  52 298
 123 130 100  19  58 292 334 160 112 146 308  10 344 290 180 105 185 237
 176 311 138 342 249  36 229 243 333 247 269 363  22 378 106 107 381 122
  98 353 114 326 254 155 368 357  41 168 170  30 288  73 239 370 156 189
 282  24  44  85 338  14  83 330 267 178  71 195 118 230   7 270  28 110
 309 251 225 257 193  95 271 377 128 302 350 256 394 147 109 218  59 332
 121 365   2  39 224  84  69 285 263 349 136 129 335 232  40 124 213  91
 169 299 315 259 115  27 152 351 207 153 250 220 293 131 116  67 289 201
 319 192 359  76 264 210 188 157 132 217  70 191 327 278 215 369 223 246
 120  74 265 179 172  23 200 104 150 379 345 165 275   1  96   9  79 382
 393   0 323 183 221  33 141 198 266 144 253 184 103 366 190 161 252 322
 346 391  97 390 383 117 294 167  43 355  61 371 244  77 148 214 196 260
  12 181  60 135 234 304 388  29 324 143 231 173 228  34 277 300 320 343
 358  92  65   4 361 125 303  63 362  75  13 367  81 276 337 166   5]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5737
INFO voc_eval.py: 171: [1128 1078  491 ...  112  855  637]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4735
INFO voc_eval.py: 171: [ 11   6  24 115  25 103 112   8  45 107  40  61   1  81  80  83  58 106
 109  19  78  79  93  47  12  84  34  62  14  21  57  71   5 113 116  29
  41  49  94  33  54 108  10   7  59  70  76  44  37  85  30 104  20   9
  16  63 117  86  60  31  13 101  74  67  69  46  50  88  35  90 105   0
  32  53  89  92 102  75 114  27  65  77  36  48  91  22  18   2  68  96
  28   4 118  82  73  51  17  52  39  23 110  72  66  99  42  38 100  97
  87  64  56   3  43  95  98 111  26  15  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0057
INFO voc_eval.py: 171: [ 638 5794 5795 ... 1188 3856  951]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5238
INFO voc_eval.py: 171: [  4  75   5 127  45 102  44  41 112 141  74  48 137 143  95  64  76  46
 144 104 111  38  90 119   8 132  33 106  61 157 131  91  32   9  59 110
  67 114  92  51   6 138 118  81 116  43 155  98  78  80 130  39 142  83
 140   3  47  99 129  16  14  56  84  93 105  17  42  26  50  52  37   7
 139 150  71  21 135  87  79 101 126 133 103 134  58  55 151 120  73  24
  36  57  89  25 128  88 117  54  94  31 136 113  11  40  60  62  27  29
   1 115 109  20 148  77   0 107  85 154 145 123  96  49  53 149  97   2
  72  30  19  28  86  10 146 125  35  34  68 124  82 108  66 147 122  12
  69  70 100 121  22 156  23  13  18  65  63 152 153  15]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1822
INFO voc_eval.py: 171: [ 864 1897  466 ...  183 2129 1468]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3666
INFO voc_eval.py: 171: [103  95 188  96   3 151 100  19  55 104 208   7  61  40  36 206  46  57
  66  90  33 205   0  27 186 192  12  80  79  65  56  13 165 161 130 142
 153 117  37 152 135  89  34 202 200 219  88 122 220  72  48 172 199  59
 210 123 154  10  85 215 189 114 118  15 158 221  11 101 187 167  97  64
 115  26 195  74 121 184 145  67  45 126 125 191 143 127 129  62 132 157
 119 102 207 173  39 150 108 111 120   2   5  16  84  23  63 174  83 109
 106 148  43 110 146 198  14  70 169  51 124  22 203 185 137 105  29  58
  75  94 201   8 140 193 190 168  68 164  31 166   4 128  81 162  77  47
 156  41 171 163 170  32  69  25 139  44  17  20 222  21  99 179   1  76
 176 217 214  24 204 160 112  93 182 134  71  60 107  49 209 181  73  54
  82  98 218  30 159  92  35  86  28 211 213   6  53 194 212  18   9 183
 116  52 136 141 178  87 216 113  38 149  42  78 196  91 155 131 138 180
 197 133 144 147 177 175  50]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1167
INFO voc_eval.py: 171: [141 124  28  21 167  92 127 177 123 142 184  63 149  23  75  33  22 117
 101 172  64 121  97 144  98 151 164 165 104  69   8 163 152   9 125  68
  79 126  53 106 107  83  47  48  35  32  62 147  45  86  57 173   4 128
  41  43  54   1 119 157  76  96  93  11  73  46   2 148 145 169 161 139
 140  65 175 162 118  15 103 109  87  74   0  77  31 160  26  81 143 158
 179  82 130 153  70  67 155 181 171 129  40  50  49 102 182 166  37  91
 174  51 150  12  61  10 180 122  94  89  27  13   6  17  34 100 176 131
  18   3 156  60 133 137  24 132   7 105  95  14  20 111  25  88  85 108
 146  78 185  80  59 136 170 168 116 120 112 114  19 110 115  36 138  42
  90  38  71  99   5  66  84 159  16  44  39 154 178  29 135 134  55  52
  72 183  30  58  56 113]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3781
INFO voc_eval.py: 171: [ 96 548  74 360 283 589 375 265 398 500 422 430 443 531  51  29 110 296
  25 376 472 413 528  97 273 310 271 567 147 597 186 595 237 170 136  68
 229 187 488  12 176  65 491 418  61 207  88 100 450 168 309 475 341 266
 392 174 534 587 602 221 311 351 340 483 415 137 363 334 312 591 280 254
  76 493 160 403 588 559 350 495 115 502 551 114  71 530 210 328 485 448
 288 209 175 544  56 451 301 594 127 214 348 426 231 406 377 527 204   3
 365 477 320  44 442 226 379  20 281 330 550 596 469 128 305 183  41 529
 106 159 321 156 125 416  33  58  45 578 200 539 318 598 434 393 387  92
 605 194  87 279 411  66 498 107 532 490 577 245 129  30 554 433 158  83
 219 453 600 324 274 486 144 512 417 404 246 131 584 381 116 352 396 258
 292 233 356 196 561 122 546 152 354 565 329  70 230 533 307 505  94 220
 503 444 373 261 599 211 188 601 474 264 401 150 515 540 543 579 400 537
 425 536 291  46  57 454  16 397 179 313 295 399 303 182 140  28 566 574
 445 346 323 395 463 489 517 494  62 437 542 270 592 361   1 123 300 452
 501 593 302 359  69 519 101 157  81 242 585 322  19  82 103 195 436 509
 388 177 272 390  23 570 206 511  95 109 481 590 541 333 339 241 535 431
 407 172 317  43  13 427  80 345 244  17 575 480 408 604 429 420 402 222
 325  93 557 132  27 506 153 331 191  32 484 267  98  15 371  14 235 343
  10 389 456 457 314 126 545 166 192 467 332 525  40 120  35 253 201 423
 487 225  18 250 138 447 568 142  64 383   9 117 461 576 476 171  72 553
 134 193 538 572 173 384 471  60 449 362 355 547 286 518 135 386 342 573
 162 249 424 455  31 306 240  99 428 520 224 562 102 294 141 149 349 161
 315 198 133 326 284 459   4 526 199  47 145  73  42 336 581  21 468 108
 405 269 364 524 496 521 248 382 104 255 215 203 419  53 148 218 368 151
 409 185 504 508 297  85 217 549 189 293 378  37 190 499 277 432 146 473
 247 394 256 290 111 298 278 470 606 446 232 458 178 164 369  86 287  38
  34 440 143 563 357 358 603  78  63  59 569 154 118  11 372 163 223   2
 169 439 113 410 555   8 251 130 347  89 197  48 497  39 438 268 552   0
 289 374 366 391 580 276 335 212 564  49 119  22 262 124 228  54 155 282
 514   7 435 482 180 492 243 465 513 344 338 507 478 370 260 460 238 523
 112 586 466 462 285 239 259 516 510 316  52 252 275 353 560  24 213 571
 184 337 556 304 205  55 583 227 257 105 385  79 167 582 216 441 181 414
 522  26 263   5  67  90 165 319  91 412  77 299 208 308 558 202 464 121
  75 421  50  84 236 479 234 367 380 139   6 327  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7083
INFO voc_eval.py: 171: [11305  2030   596 ... 11142 13521 11503]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6182
INFO voc_eval.py: 171: [ 339  432  663 ...  225  685 2220]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5920
INFO voc_eval.py: 171: [128   3 122   4  69  74 151 108  99 106 180  19 144 121 149  25  73  62
 135  71 158 120  70  47  90  20 131  67  44  28  68   6 150 166 123  72
 113 125  49 155  48  33 152 170  24  36  11  39 174 177  52 142  79  81
  42  86  16 133  23  10 184  61  65  29 143 137 107  38 111   7  93  75
 159  89 130 134 176 162 116  58  85 172 164  87   2  88  35 129 114  14
  53  22  56  77 101  84  82 124 145 102 154  43 146  83  80  30  60 132
  34  78 115 156   0  76   9  21 163   1  40  51  63  64  91 167 140   5
 109 117 138 141 104 157  31 153 127 165 119  55 160  95  13 171  66   8
  92 100  18  26  50 105 147 182  12 169  45 175  94  15  37  17 173 110
  41 103  46  54  97  96 161 168  59 126  27 181 148 139  32 136 118  98
 179 112 178  57 183]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2830
INFO voc_eval.py: 171: [203 200  70 275 295 218 429 141 220  16 163 332 294 310 224 316  74 205
 202 376 189 410 201 172 351  95 280 411 315 210 246 427 173  17 114 255
 204 263 140 186  79  66 242 248 396  88 124 135 253  48 153 397  92 170
 160 222 357 318 136 118 435 139 317 358  59 436 346  39 175 120 375  80
 359 123 156 311 293  21  93 298 169 333  72 207 265 352  40 226 288 115
 208  34 254 197 258 185 403 223 134 363 329 168 235 281  18  65  31 131
 362 143 228 428 334 418 284 109  23 304 414 256 348 209   4 126 313 215
 233 314 325 127 191 174  55  84 391 368 378  32   5 361 406 408  67 430
 183 102 247 113   3 328 272 133 421 236  45 431 227  41 283 437  82  76
 122 151  50 426 274 382 366 387 364 305  35 291 322 259 425 270  81 336
 216 292 339 345 266  36 180  19 374 285 142 104 297 321 217 243 309 413
 393 338 178  11  54 355 390 221 324 245 125  43 250 196 307 111  78  53
 129 365 213 107 290 176   6   7 154 234 422 349  96 392  30 244 146 187
 171 225  56 159 177  13 409 138  44  73 132 370 423 394  25  63   8 257
  61 149  60 398 119  47 433 299 401  91  29 308 277 161 343 128 287  71
 155 150  22  90 286  37 271 219 417 326  49 188 434 373 260 331  26 380
 350 229 384 137 319 302 144 347 356 162 389  24  99 100   2 184 262 282
 232 212 360 369  12 194 121 267 240 354 157 385   9 252 164 416 306 372
 301 206 166 353 148 420  28 402  77 386   0 145 264  58  97 404 214  83
 193  89 300  87 230  98 412 130 110 117 192 152  94 415 199 238  27 158
 279 182  68 108  62 400  38 379 116 395  69 237 190 323 289 198 388 335
  14 371 105 273 407 231 344   1 239 269 276 106 101 112 195 367 165 261
 312  51 147 340  46 251 211 181  85 342 337 399 268  52 296 405 432 377
 167 320 424  57  20 381  10 383 249  15  42 303  75 419 341 179  64  33
 327 278 330  86 103 241]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3774
INFO voc_eval.py: 171: [424 271  73  83 140 363 230 139 112 425 144 311 314  75 231 368 117 279
 242 586 339 169 143  82  52 556 366 348 476 538  67 276 483  77 362 273
 598 115 201 109 116 270 333 477 302 145 498 220 315 251  99 600 545 232
 299 104 298 396 217 596  72 568 219 619 613 398 146 567 364 100 493 404
 265 125  13 399 122 300 254 485 118 355 207 195 167  16 594 326 138  66
 554 192 616 347 309 466 342 358 316 511 196 243 421 443 121 264  57 156
 162 365 274 206 426 340 618 105 177 522   5 447 294 197  21  45 221 454
 113 578  22 561 170 615 623 319 238  71 209 612  69 361 440 430 480 320
   8 609 484  53 450 509 564 385 504 223 553 414  70 488 608 457 432 571
 175 161 260  92  38 166 369 335  63 171 172 350 482 376 560 290 499 548
 297 433  76 253 481 330 402 591 559 611 392 312  60 478 415  30 518 354
 532 435 471 203 496 305 359 510 387 470 306 267  46 130 303 129 593 530
 621 535 151  93 280 301 313 491 261 218 393 142  47 288  17 506 228 479
 486 176 158  29  62 123  96 453 352 152 416 205 168 247 411 190   6 186
 472  42 489  68 328 308 135 620 178 245 546 456  43 128 382 250 106 602
 215 256 469 557 296 565 582 210   7  35 107 379 614 147 173 234 102 370
 236  11 584 334 442  23 374 458 124 439 474 524 222 318 438 475 324 515
 137 570 527 563 108 562 536 373 317 431 583 605 599 551 580 449 487 444
 183  24 110  98 141 394 193 307 292 555  34 281 160 185 417 406 255 389
 331 111 388 165 590 446  54 148 189 517 119 336 287 349 179 377 180 304
 323 603  58  18  27  50 423 587 397  88  19 357 495 103 468 131 592 558
 114   9 585 512 521 378 310 405  49  91 617 332  56 492 101 400  41 410
 321 420 523 120 163 338  39 224 239 581   1 529 502 268 259 132 588 367
  94  28 437  40  12 569  87 436 441 526 547 589 624 225  86  61 513 412
  84 184 490 401 507 212 408 277 272 577 344 187 191  20  89 153 542  55
 295 525 390  14 460 204 552 182 157 463 154 606 409 497 607  25  78 291
 322 434 419 237 428 351 604 503 337 380 181 227  97  32 575 540 459 188
  90 501 391 211  10 601 539 329 544 597  44 134 448  81 283  74 241 445
 341  36  51 572 494  15 155 541 574 360 235 500 214 407  48 455 325 595
 126 278 213 514 343 327 194 550 133 240 269 537 266 246 610 576 528  79
 208   3 505 285 386 429 566  64 371 258 199 573 284 263 262 533  65 216
 508 282 252 229 150 534  85 427 403 383 346 202 372 473 464 249   2 413
 395  26 462 422 461 465 375 516 289  59 451 418 136 452 127 159 345 579
  33 549  37 356 353 275 248 244  95 520 233 200 467 384 293 257 519 226
   4 622  31 174 164 286  80 531 543 198 149 381   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3456
INFO voc_eval.py: 171: [269 502 723 ... 940 676 845]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4790
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4053
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.358
INFO cross_voc_dataset_evaluator.py: 134: 0.604
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.353
INFO cross_voc_dataset_evaluator.py: 134: 0.543
INFO cross_voc_dataset_evaluator.py: 134: 0.574
INFO cross_voc_dataset_evaluator.py: 134: 0.474
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.524
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.367
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.378
INFO cross_voc_dataset_evaluator.py: 134: 0.708
INFO cross_voc_dataset_evaluator.py: 134: 0.618
INFO cross_voc_dataset_evaluator.py: 134: 0.592
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.377
INFO cross_voc_dataset_evaluator.py: 134: 0.346
INFO cross_voc_dataset_evaluator.py: 134: 0.479
INFO cross_voc_dataset_evaluator.py: 135: 0.405
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 6499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.474s + 0.002s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.382s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.384s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.389s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.392s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.393s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.394s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.390s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.391s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.394s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.396s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.397s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.397s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.432s + 0.002s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.404s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.410s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.400s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.402s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.400s + 0.003s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.402s + 0.003s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.398s + 0.003s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.397s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.399s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.396s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.396s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.393s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.408s + 0.002s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.418s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.404s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.400s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.397s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.396s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.395s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.399s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.392s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.393s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.392s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.388s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.387s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.474s + 0.002s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.388s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.384s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.381s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.389s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.385s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.388s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.387s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.391s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.390s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.385s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.382s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.382s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.075s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [572 179 890 ... 219  87 715]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3579
INFO voc_eval.py: 171: [203  97 705 481 818 780 634 791 492 820 469 822  98 829 640 544 121 635
 824 745 790 537 368 817  83 565 475 831 685 641 260 235  55  58 803  44
 474 545 691 105 666 384 611 830 305 696 819 664 200 795 423 520 806 302
 637 761 380 479 123  42 762 515 706 766 647 109 102 162 287 707 490 538
 476 165 370 736 484 758 845 539 741 307 644 361 748 536 620  59 689 655
 772 205 190 749 563 471 693 847 419 265 491 204 577  82 753 697 579 836
 568 412 752 853 108 282 785 636 470 816 711 473 755 854 332  56 728 477
 478 333 218  14 132 857 505 173 186 614 848 800  99 111 304 667 259  43
 747 522 718 550 799 432  51 106 493 472 694 798 524 731 796 813 322 363
  24 846 356 139 599 115 133 354  86 488 682 709  16 124 328 217 521 548
 680 506 381 286 161 137 172 142 303 331 601 466 672 298 604 662 487  66
 856 783 261 258 569 489 350 558 801 564 510 835 589 629  45 526 525 338
 825 118 735 272 114 712 219 704 519 584   4 642 688 464 392 679 695 659
 716 254 206 786 281 416 588 760 797 411 789 692  29 838  93 152 232 502
 571 821 431 581 486 562 844 750 267  60 209 275 827 398 518  73 668 318
  23  62 399 708 266 167 214 715 189 227 527 552 116 153 794 213  15  40
   6 277 675 125 385 369 104 110 851 717 651 832 145 677 843 482 658 603
 422 239 495 299 240 168  26  70 154 374  34  87 438 255 211 340 517 324
 700 175 296 117  18 351 494  47 660 529 456 169  36 224 674 426 415 297
 394 622 623 188 792 541  77 504 201 730 251 229 144 764 500 191 849 663
  54 625 850 784  75 437 243 326 387 859 334 367 409 195  84  85 746 810
 349 226   5 842 777  89 754 461 585 771 174 732  65 289 207  71 413 197
 376 311  17 247 231 566  91 184 723 375 337 234 774 187 465 170 606 447
 600  78 587 779 143  68 535  67 378 242 501 263 778 403 546  30 126 787
 860 433 653 436 793 533 852 185 516  48 684 321 770   0 410 222 540   1
 329  20 678  72 359 811 113 740 542 652 576 236 617 291 377  46 690 661
 264 657  13 248   8 648 729 826 861 182 733 586 358 312 452 805  92 379
  35  69  38  21 638 609 759 181 643  19  10 319 246 270 702 208 580 808
  11 295 360 306 669 430 497 453 439 815 714 202 739 421 417 627 645 441
 809 802 210 176 382 567 390 756 532 665 767 671 389 804 276 713 339 757
  41 726  96 602 285 220 523 160 262 498 570  88 639 383 362 686 164 631
 288 316 183  79 788 610 325 719 163 458 320 828 178 343 427  74 460 249
 198 228 855 237 120 593 445 814  64 768 408 429 557 721 738 122 858 591
  53 119 597 435 171 250 654 223 483 444 293 698 513 225 769 727 156 683
  81 196 140 271 763 773 556 345 136 310 428 656 317 100 221  94 457 807
 215  37  22   7 230 751 244 301  90 701 449 238 543  12  76 528 468  39
 560  25 241 840 284 159 290 391 365 373  33 781 386 405 179 632 650 341
 499 420 551 549 269 352 192 400 357 180 280 628 313 724 554  28 309 823
 233 776 404 166 626  32 673 463 442 424 573 292 553   3 364 670 348 256
 300 193 414 294 128 621 615 616 782  50 418 157 744 146 812 765 687  27
 149 561 530 278 699 393 841 737   2 592 194 613 323 107  63 574 509 511
 336 555   9 253 514 314 406 315 355 837 245 485 212 534 252 575 455 578
 440 131 612  52 134 151 646 308 216 407 135 366  95 425 618 454 347 833
 467 283 583 158 353 346 624 397 681 607 388 273 598 595 147 138 459  49
 703 450 725 327 512 448 608 268 279 103 342 112 720  31 101 559 596 503
 372  80 547 572 129 734 130 630 141 434 402 371 649 839 401 722 743 605
 531 330 443 451 742 155 834 619 177 480 396 462 395 508 775 335 257 590
 344 150  61 633 127 148 710 582 274  57 496 446 507 199 594 676]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6137
INFO voc_eval.py: 171: [120 318 375 237 239 793 371   3 351 794 238 795 726 730 582 639 484 250
 480 123   6 242 727 295 289 798 863 338 796 272 377 176 654 816  15 616
 648 807 799 833 292 635 808 366 332 187 352 797 482  34 888 414 843 645
 818 532 883 878 475 653 732 353 805 290 357 419   8 211 417 571 800 495
 527 760 835 245 473 221 131 817 803  61 358  48 373 336 822  52 119  92
 148 669 610 557 699 890 273 811 481 819 275 466 676 372 485  77 806 153
 788 263 847 347 246 326 164 877 122 251 321 563 463 296  18 663 334 765
 214 609 241 261 281 497  35 875 146 118 316 144 548 846 116 622 145 177
 421 474 841 317 664 769 539 721  94 422  62  32 234 178 729 636  19 152
   7 715 111 285 516 634  54 874 135  79  59   4   5  63 585 478 562 454
 179 505 538 627 168 504 339 130 147 337 134 252 438  16 544 802 425  56
 389 786 378 379 279 227 392 665 385  90 487 541 155 354 456 233 449 209
 540 128 625 531 533  84 740 515 175 282 465 751 235 590 626 810 435 386
  69 203 186 753  13 180 749  78 356 762 837 391  10 380 801 768 608 556
 735 240  67 461 489 770 520 876 572 458 554 215 157 547 429  36  22 857
 506 117 859  75 293 230 742 674 560 311 534 137 136 304 789 734 359 288
 190 781  86 633 741 543 565 880 348 328 459 320 872 231 738 426 324 343
 722 524 681 647 462 452 597  65 815 725 218 204 775 546 499 450 188 382
 612 269 160 767  49 299 477 298  74  57 220 526 688 100 671 613 236  20
 513 536 752  46 165 784 825 838 668 512  68 712 156  23 864 641 870 898
 508 341 791 443 183 766  82 300 733 107 545  72 574  26 677 470 856 598
 555 820 821 529 232  37  33 383 409 163 852 640 126 170  38 586 747 566
  27 416 854 312 253 469 865 361 644 431 174   2 829 535 567 782 628 247
 530 591 649 486 702 615 672 349 886 713 384 114 603 500  60  64 133 785
 889 887 471 780 578 297  11 839  73 363 255 472 266 894 291 855 355 207
 882 335 283  21 370 651 496 697 550 127 258 262 537 509 124 323 294 564
  41 412 850  30  31 758 511 224 845 642 708 682 542  83 624 763 561 228
 611 523 464 804 502 716 549 181 691 142 602 871 278 860 229 652 711  55
 632 217 714  42 558 360 350 169  70 206 447 284 420 110 125 430  53  58
 483 638 660 553 260  91 161 254 325  99 552 551 194  50 151 446 787 342
 476 491 528 453   1 159 428 873 570 184 510 772 340 584 102 683 344 893
 637 720 899 424  87  39 267 330 162 306 559 614 439 271 583 434 149  43
 896 718 737 501 265 374 577 700 381 433 836   9 432 192 573 444 407 813
  71 154 604 369  12  51 576 744 623 388 707 656 427 840 621 468 346  93
 423 216 225 441 445 731 679 673 830 307 415 455 208 655 108 101 410 745
 824 694 778  29 210 600 467 689 182 202 666 197 223 112 406 121 105 521
 243 579 849 661 587 268 402 257 140  28 498 746 141 319 828 891 408  80
 662 869 256 167 589  85 315 790 709 286 397 881 771 411 844 658 195 436
  14 113 166 109 670 393 310 723 861 226 394 219 599 213 792  66   0 494
 617  25 897 695 514 376 362 595 759 437  40 158 764 287 684 580 150 701
 743 327  89 201 868 879  95 650 365 198 575 519  45 396  97 345 831 748
 592 442 607 646 189 710 827 106 103 569 678 309 196 440 305 364 866 259
 493 503 593 488 739 832 619 588 222 773 834 757 812 457 313 451 704 490
 779 777 270 719 333 248 387  88 581 858 594 171 212 418 302 687 756 405
 507 492 685 895 848 761 736 568 322 115 783 518 329 129 139 413 774 717
 605 643 867 706 692  98 403 301  24 728 892 522 675  47 448 657 862 517
 814 390 460 842 308 724  81 205 193 277 276  17 173 368 244 618 659 264
 698 690 885 809  44 596 132 629 280 776 754 331 200 680 191 314 104 884
 395  76 696 274 525 853 620 630 404 401 400 399 398 667 367  96 703 686
 750 143 185 199 138 755 249 303 172 693 705 823 631 826 851 606 601 479]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2247
INFO voc_eval.py: 171: [1355 1356 1575 ...  108 1084 1379]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3525
INFO voc_eval.py: 171: [1431  831  884 ...  184  701 1077]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5431
INFO voc_eval.py: 171: [ 72  45 137 140  18  49 222   8  47  88 177 376 154 281 291  53 353  31
 355 245  64 273 261 305 272 206 365 194  51 374 331 182 133 139  42 301
  50 375 287 296 204 349 258 268 111 312  54 175 235  78 186  56 219  46
 163 216  25 255 262  15   6  17 317  57 162 385 101 393 381 226 306 325
 307 328 149  20 159 386  93 126  90 145 280 151 248 113  32 284  87 313
 212  37 119 202  99 108  38 361 388 340  35 240  94 279 127 199 208 373
  26 209 242 227 390 387  55 142 238  66 377 134 341  82 187 236 211 283
 205  86  62 339 233 295 318 348 314 329 274 164  48 336 297 316 241 171
 286   3  80 174 158 197 102 321  89 310 357  68 203  16  11  21  52 298
  19 130 123  58 100 292 160 334 112 308 146  10 345 290 180 237 105 185
 176 311 138 249 342 247 243  36 333 364 229 269  22 379 122 106 107 382
 354  98 326 114 254 369 155  41 168 358  30 170 288  73 371 239  24 156
 189 282  85  44 178 338  14  83 267 330  71   7 118 270 230 195  28 257
 251 110 309 225  95 271 378 128 302 193 351 256 395 147 109 332 121 366
  59 224 218  39   2  69 263 285 350 136  84  40 335 129  91 115 169 315
 299 124 232 213 352 152 153 259  27 250 207  67 131 220 293 319 116 289
 201 192 360 264  76 210 327 157 188 132 278 191  70 217 265 246 370 223
 172 215 179 120  74  23 200 346 150 380   9 275   1 165 394 104  79  96
   0 323  33 383 183 221 144 141 266 198 253 103 184 347 367 161 190  97
 322 252 392 294 167 391 384  43 117  77  12 372 356  61 244 214 196 148
 260 181 234 304 135  60  29 143 389 324  34 228 231 173   4 362 277 359
 300 344 320 125 303  92  63  65  13  75 276 368 337 363 166  81   5 343]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5737
INFO voc_eval.py: 171: [1133 1083  493 ... 1003  509  408]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4736
INFO voc_eval.py: 171: [ 11   6  24 117  25 105 114   8  45 109  40  62  83   1  82  85  58 111
 108  19  80  81  95  47  86  12  34  63  14  21  57  73   5 115 118  29
  41  49  33  96 110  54  10  60   7  72  78  44  37  87  30 106  20   9
  16  64 119  88  61 103  13  31  76  70  68  46  50  90  92  35 107   0
  32  71  53  91  94 104 116  77  36  79  48  27  66  22  93  18   2  69
  98  28   4 120  84  75  51  39  17  52 112  67  74 101  23  42 102  38
  89  65  99  56   3 100  43 113  15  97  55  26  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0057
INFO voc_eval.py: 171: [5794  640 5795 ...  595 4637 1731]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5215
INFO voc_eval.py: 171: [  4  75   5 127  45 102  44  41 112 141  74  48 137 143  95  64  76  46
 144 104 111  38  90   8 119 132  33 106  61 157 131  91  32   9  59 110
  67 114  92  51   6 138 118  81 116  43 155  98  80  78 130  39 142 140
  83  47   3  99 129  16  56  14  84  42  93 105  17  26  50   7  52  37
 139 150  71  21 135  79 101  87 126 133 103 134  58  55 120 151  73  24
  36  57  25  89 128  54 117  88  31  94 136  11 113  40  62  29  60   1
  27 115  20 109  85 148  77   0 107 154 145 123  96  49  53 149  97  72
   2  30  19  28  10  86 146 125  35  34 124  68  12  82  66 147 108 122
  70  22  69 100  63  65 121 156  23  18  13 152 153  15]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1825
INFO voc_eval.py: 171: [ 866 1899  467 ... 1649  265  893]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3664
INFO voc_eval.py: 171: [103  95 188  96   3 151 100  19  55 104 208   7  40  61  36  46 206  57
  66  90  33 205   0  27 186 192  12  79  80  65  56  13 165 161 130 142
 153 117  37 152 135  89  34 202 200 219  88 220 122  72  48 172 199  59
 210 123 154  10  85 215 189 118 114  15 158 221  11 101 187 167  97  64
 115  26 195  74 121 184 145  67  45 126 125 191 143 129 127  62 132 157
 119 207 102  39 173 150 108 120 111   2   5  16  84  23  63 174 106  83
 109 148  43 146 110 198  70  14 169  51 203  22 124 185  29 105 137  58
  75  94 201   8 190 193 140 168  68  31   4 128 166 164  81 162  77  47
 156  41 171  32 163 170  25  69 139  44  17 179  20 222  21  99   1 217
  76 160 214 204 176 112  24  93 134 182  60  71 107 209  49 181  73  54
 218  82 159  98  30  28  35  86  92 211 213   6  53  18   9 212 116 194
 183  87  52 113  42 216 141 178 136  78 149  38 196 155 138  91 131 180
 177 133 144 147  50 197 175]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1168
INFO voc_eval.py: 171: [141 124  28  21 167  92 127 177 123 142 184 149  63  23  75  33  22 117
 101 172  64 121  97 144  98 151 164 165 104  69   8 163 152   9 125  68
  79 126  53 106 107  83  47  48  35  32 147  62  45  86  57 173   4 128
  41  43   1 119 157  54  76  96  93  11  73  46   2 148 145 169 161 139
 140  65 175 118 162  15 103  87  74 109   0  77  31 160  26  81 143 158
 179  82 130  70 153 181 155  67  40 171 129 182  50 102  49 166 174  37
  91 150  51  12  61  10 180 122  94  89  27  34  13 100   6  17 176  18
  60 131   3 156  24 132 133  20   7 137  14  95 105 111  25  88  85 146
 108  78 185  80  59 136 120 170 168 116 112 114  19 110 115  36 138  38
  42  90  71  99   5  66  84 159 154  16  44  39 178  29 135 134  55  52
  72 183  30  58  56 113]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3774
INFO voc_eval.py: 171: [ 96 546  74 359 282 587 374 264 397 499 421 429 442 529  51 110  29 295
  25 375 471 412 526  97 272 309 270 565 595 147 185 593 236 170 136  68
 228 186 487  12 176  65 490 417  61 206 100  88 449 168 308 474 340 265
 532 391 174 585 600 220 310 350 339 482 414 362 137 333 589 311  76 279
 253 492 160 402 586 557 349 494 115 549 501 114  71 528 209 327 484 447
 287 542 208 175 450  56 300 592 213 127 347 425 230 405 376 203 525 364
 476   3  44 319 441 225  20 378 280 329 594 548 128 468 304 182 106 527
  41 320 159 415 125 156  33  45  58 576 537 199 596 317 386 392 433 603
 193  87  92 410 278  66 497 107 530 489 575 129 244 552  30 432  83 158
 218 452 598 323 273 416 485 510 144 403 245 131 582 380 116 395 351 291
 257 232 195 559 355 544 122 152 353 563 328  70 229 531  94 504 219 306
 502 372 597 443 187 260 210 599 263 473 400 513 150 541 538 577 399 535
 534 424 290  57  46  16 453 396 179 312 294 398 302  28 181 140 564 322
 444 462 572 345 394 515 540 493  62 488 436 590 360 269 451 500   1 123
 299 301 591  69 358 157 101 517  81 241 583 103  82 194 321  19 387 271
 435 508  23 389 177 568 205  95 109 509 539 480 588 172 338 332 240  13
 316 533 406 430  80 243 426  43  17 573 479 344 602 407 324 401 428 221
 419  93 555  27 330 190 132 153 505 483  15  32 370 266  14  98 342 234
  10 388 166 126 456 543 455 191 466  35 313 523 252 331 422 200 224 120
  40 249 446 486  18  64 566 138 142 382 574   9 475 460 117 171 134 551
 570  72 192 536 448 383 173  60 470 361 354 285 545 516 385 571 135 423
  99 162 454 248 341 293 102 239 305 149 518  31 427 560 197 348 161 141
 223 314  47 524   4 283 458 133  73 145 579 198 335 325  42 268 108 495
 467  21 363 247 519 522 254 214 404 104 381  53 418 202 184 217 148 408
 367 151 377 503  37  85 189 507 296 216 431 292 498 188 547 297 393 111
 472 146 276 246 255  34 277 289 561 469 604 368 357 164  38 286 178 231
 457 445  86  11 439 356 601 371 143 567 118  63 154  59   2  78 169 163
 222 130 113 409 438 553 250   8 496 196 437  48 346  89  39 288 550 267
   0 373 365 390 562 275 334 211 578  49 119 124 261 227  22  54 434 512
 281 481   7 155 180 242 491 464 337 343 511 477 506 521 584 459 259 237
 369 352 461 238 251 258 465 514 284 112 558  24  52 554 274 315 569 256
 212 336 183 204 303 581 105 226 384 413  55  79 580 440 215 167  26  67
  90   5 520 262  77  91 411 165 318 556 420 121 298  75 207 307 201 463
 235  84  50 233 326 379 139 366  36 478   6]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7060
INFO voc_eval.py: 171: [ 2046 11383   601 ...  6280 11101  8975]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6214
INFO voc_eval.py: 171: [ 340  433  664 ...  811 1197 1899]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5918
INFO voc_eval.py: 171: [127   3 121   4  69  74 151 107  98 105 180  19 144 120 149  25  73  62
 135  71 158 119  70  47  89  20 131  44  67  28  68   6 166 150 122  72
 112 124  49 155  48  33 152 170  24  36 174  11  39 177  52 142  42  78
  80  85  16 133  23  10 184  61  65  29 143 137 110  38 106  92   7 159
  75  88 130 134 162 176 115  84  58 172 164   2  87  86  35  14 113 128
  22  53  56  76 123  83 100 101  81 154 145  43 146  82  79  30  60  77
 132  34   0 156 114   9 163  21   1  40  64  51  63  90 140 167 108 138
   5 116 103 141 157 153  31 126 118  13 160  55 165  94  66  99 171   8
  91  18 104  26  50 147 182  12 169  45 175  93  37 173 102  96  15  46
 161  17 109 168  54  95  41  59 125 136 181  32 148 139  27 117 111 178
 179  97  57 183 129]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2830
INFO voc_eval.py: 171: [204 201  70 275 295 219 428 141 221  16 163 332 225 310 294 316  74 206
 203 376 189 409 202 172 351  95 280 315 410 211 173 426 246  17 114 255
 205 263 186 140  79  66 248 242 396  88 124 135 253  48  92 153 397 170
 160 223 357 136 318 118 139 434 317 358 435  59 346  39 175 120  80 375
 156 311 123 359 293  21 298 333  72 208  93 169 265  40 352 227 288 115
 209  34 254 198 258 185 403 224 134 363 329 168 236  18 281  31  65 131
 143 362 427 229 417 284 334 109  23 304 413 348 256 210   4 126 216 234
 313 314 325 127 191  55 174 391  84  32 378 368   5 405 361  67 407 429
 183 102 247 113   3 328 430 272  45 228 420  41 133 436 283  82  76 151
 122 425  50 366 274 382  35 364 387 322 305 291 270 259  81 424 339 336
 217 292 266 345  36 142 285 180 321 297  19 104 374 309 218 243 412 178
 393 338  54  11 245 355 250 197 390 222 324  53  43 307 125 111  78 365
 214 129   6   7 176 235 107 290 154 421 392 349  30  96 146 187 244 226
  56 171 159  44  13 177  73 138 408 394 132 370 422  63   8  61  25 398
 257 149  60 119 432 277  47  29 299 308  91 401 343 161  90 155 128  71
 287 150  22 220 286  37 271 433 416 188 373 350 326 230  26 380  49 331
 260 319 384 137 347 302 356 144 162 389 100  24  99 184   2 262 282 233
 360 369 213  12 240 267 195 121 157 306 385 354 164   9 252 415 372 207
 402 166 301  28 353 419 386   0  77 148 145  58 264  97  83 215 193  89
 300  87  98 231 192 411 130 117 110 414 238 152  94 200  27 279 108 182
 158 400 395  62 323  68 379 116  38 237 190 199  69 105 388 289  14 335
 371 101 269 273 276 239 406 344 106   1 232 165 312 147 112 196 367 261
 268  46  51 251 337 340 342 296 212 399 181  52  85 431  10 404  20 377
 381 383  42  15 423 179 303 249 167 194  57 320 418 330  75  33  64 341
 278 241 103 327  86]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3853
INFO voc_eval.py: 171: [424 271  73  83 140 363 230 139 112 425 144 311 314  75 231 368 117 279
 242 586 339 169 143  82  52 556 366 348 476  67 538 276 483  77 362 273
 598 115 201 109 116 270 333 477 302 145 498 220 315  99 251 545 600 232
 299 104 298 396 217 596  72 568 219 620 614 398 146 567 364 100 493 404
 265 125  13 399 122 485 300 118 254 355 207 195 167  16 594 326  66 138
 554 192 617 347 342 309 466 358 196 316 243 511 121 421 443 264  57 365
 162 156 274 206 426 340 105 619 177 522   5 447 294 197  21  45 221 454
 113 578  22 561 616 170 319 624  71 238 613 209  69 361 430 440 320  53
 480   8 610 509 484 450 564 504  70 223 553 385 414 488 609 457 432 571
 260 175 161 369  92 335  38  63 166 171 172 482 350 376 290 499 548 560
 433  76 297 481 330 253 402 591 559 612 312 392  60 415 478 518  30 354
 532 435 203 496 305 471 510 359 387 306 130 470  46 267 129 303 593 530
 622 535 151  93 280 491 301 313 218 261 393  47 288 142  17 228 506 486
 479  29  62 158 176 152 453  96 123 205 168 416 352 247 411 186   6 472
 190  42 328  68 489 308 178 135 546 621  43 456 245 382 128 250 603 469
 256 215 565 557 106 296 210 582 107   7  35 379 234 147 615 236 173 102
 370 584 442 458 374 334  23 439  11 124 524 475 474 222 438 318 324 137
 108 527 570 563 515 373 562 536 551 599 431 580 110 487 583 317 449 141
  98 444  24 183 606 555 394 307 193 292  34 417 185 111 160 255 281 389
 331 446 388 148 406  54 165 517 590 189 336 287 349 119  18  58 179 180
 377 323 604 423 304  27 357 397  50  19 495 587  88 103 468 592 131 114
   9 558 521  49 585 512 618  91 405 378 310 410  41 420 321 101 338  56
 400 120 492 332 163 523 224  39   1 502 259 529 588 581 239 367  28 132
 569 437  12  40  94 268  87 547 436 589 441 526 625 225  86  61  84 513
 401 412 184 507 212 272 490 277 408  89 191 577 153 542 187 460  55  20
 390 344 204 295 154 463 157 182  14 552 525 607 409 605 419 291 237 322
 608  78 497 434  25 351 428 181 227 337 380  97 503  90  32 188 540 391
  81 602 459 539 134  74 575 241  36 211 501  10 448 544 597 329  51 283
  44 341 235 445  15 572 494 541  48 500 360 155 325 574 213 455 407 214
 126 528 194 550 343 266 595 278 514 611 240 246 327 133 576 537 269 386
   3 505 285 208 371  79 573  64  65 284 216 263 262 258 252 533 429 199
 566  85 534 229 346 508 282 383 150  26 427 464 403 202 473 465 462 372
 249 395 413   2 452 422 461 516 375 451 289  37 418  59  33 136 549 127
 356 159 244 345 579  95 384 200 248 353 226 257 520 275 467 233 519 623
 164   4  31 293  80 198 286 174 381 149 531 601 543   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3452
INFO voc_eval.py: 171: [269 501 721 ...  57  72 100]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4791
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4061
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.358
INFO cross_voc_dataset_evaluator.py: 134: 0.614
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.543
INFO cross_voc_dataset_evaluator.py: 134: 0.574
INFO cross_voc_dataset_evaluator.py: 134: 0.474
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.522
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.366
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.377
INFO cross_voc_dataset_evaluator.py: 134: 0.706
INFO cross_voc_dataset_evaluator.py: 134: 0.621
INFO cross_voc_dataset_evaluator.py: 134: 0.592
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.385
INFO cross_voc_dataset_evaluator.py: 134: 0.345
INFO cross_voc_dataset_evaluator.py: 134: 0.479
INFO cross_voc_dataset_evaluator.py: 135: 0.406
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 6999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.540s + 0.002s (eta: 0:01:07)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.382s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.393s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.391s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.397s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.390s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.392s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.389s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.389s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.390s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.390s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.390s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.393s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.427s + 0.002s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.386s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.399s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.397s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.396s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.394s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.403s + 0.003s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.401s + 0.003s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.401s + 0.003s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.399s + 0.003s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.397s + 0.003s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.396s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.393s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.417s + 0.002s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.402s + 0.003s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.394s + 0.003s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.395s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.390s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.392s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.392s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.396s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.398s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.394s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.391s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.394s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.396s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.525s + 0.002s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.396s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.385s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.380s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.389s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.387s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.390s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.388s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.385s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.383s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.380s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.381s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.380s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.178s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [575 179 893 ... 224 429 209]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3570
INFO voc_eval.py: 171: [204  97 707 820 483 782 636 793 494 822 471 824  98 831 642 546 121 637
 826 747 792 539 369 819  83 567 477 833 643 687 261 236  55  58 805  44
 476 547 693 105 668 386 613 832 306 698 821 201 666 797 425 522 808 303
 639 763 382 481 123 764  42 517 708 768 649 109 102 163 288 709 492 478
 540 166 371 738 486 760 847 541 743 308 646 362 750 538  59 622 691 657
 206 774 191 751 565 473 695 849 421 266 493 205 579 755  82 699 581 570
 838 414 754 855 787 108 283 638 472 713 818 475 757 856  56 333 730 479
 480 334  14 219 133 859 507 174 616 187 850  99 305 802 111 669 260 524
  43 749 720 552 801 434 106  51 474 495 800 696 526 815 733 798 323 364
  24 848 140 357 115 601 134 355  86 490 684 711  16 124 329 218 523 682
 383 508 550 287 162 138 173 143 332 603 304 674 299 468 606 664 489 858
 785  66 259 571 262 491 351 591 566 803 560 512 837 631 528  45 527 339
 827 714 737 273 220 118 114 521 706 586   4 644 690 697 466 681 661 718
 394 207 788 418 255 282 590 762 799 413 694 791  29 573 153 840 823  93
 504 233 433 583 488 268 752 564 210 846 829 276 520  60 400  62  23  73
 319 670 267 710 401 190 168 529 228 215 717 554 796 154 116 214 677 278
   6  15 387  40 125 104 653 110 370 719 146 679 853 484 834 605 660 845
 424 240 300 497 169  26 241 155  70 376 440 325 519 702 212 176  87  34
 256 352 297 341 117  47  18 496 170  36 662 794 225 624 531 298 458 428
 417 676 625 396 189 543  77 506 202 732 192 502 230 766 145 252 851 244
 786  75 665 327 861 627  54 335 852 439 411 368 389 748  85 227 779  84
 196 350 175  89 844 773   5 812 587 463 208 756  65 734 290 198 378  71
 415 312 377  17 338 185 248  91 725 232 568 235 776 467 188 449 144 589
 171 243 781  68 608  78  67 602 537 548 503 780 380 655 438 535 862 126
 264 789 405 518 795 435 854  30 772 686  48 186 322 223   0 412 330 113
 542 742   1  20 360 680  72 813 654 544 578  13  46 237 379 692 619 663
 292   8 659 249 731 828 265 650 735 863 183  92 588 611 381 359 313 807
 454  35  69 761  38  21 182 640  19 645 582 810  10  11 271 320 247 455
 209 704 361 307 296 671 499 419 817 741 716 423 432 804 811 384 443 647
 441 211 203 758 177 629 769 392 667 340 569 715 534 391 277 673 759 806
 728 161  41 286 525  88 604 165  96 221 688 572  79 385 633 363 289 164
 641 263 500 721 184 830 790  74 612 317 429 326 460 462 321 120 344 250
 857 179 199 816 410 229 770 595 238 447 740  64 431 860 122 559 723 172
 437 485 224 599 656 119 593  53 251 294 700 446 515 272 729 226 775 141
 157 771 430 137  81 318 765 558 197 346 685 658 100 311 809 216 231   7
 222  37 451  94 459 703 245 753  22 562  12 302  90 239 842  39 545 366
 530 470  76  25 180 407 160  33 242 388 285 291 393 375 281 783 270 342
 652 501 553 634 181 193 353 422 358 314 551 402 444 234 167 726  28 575
 556 310 630 825 672 293 426 194 675 301 406 628 465 778  32 295   3 555
 365 257 618 349  27 623 689 617 128 147 563 814  50 746 767 150 784 416
 158 420 279 843 395 576 701 532 324 511 615 316 107   9   2 739 337 213
 254 513 195 557 516  63 594 487 580 442 408 246 356 315 839 217 536 620
 457 253 577 131  95 409 152 309 614 648 136 348 135 456 367 427  52 835
  49 469 585 354 159 284 148 347 683 705 274 609 597 600 390 626 139 461
 399 727 328 514 450 452 101 632 610 280 103 112 574 722 561  31 269 343
 373 598 374 549 404 436  80 505 607 129 724 651 142 736 130 397 621 403
 744 336 178 156 445 151 453 777 533 745 841 331 258 635 836 149 127 464
 398 482 345 592 510  61 275 498 200  57 678 584 712 448 132 372 596 509]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6035
INFO voc_eval.py: 171: [125 324 381 243 245 803 377   3 804 357 244 736 805 740 590 648 491 256
 487 128   6 248 737 301 295 808 874 344 806 278 383 181 663 826  15 624
 657 817 809 843 298 818 644 372 338 192 358 807 489  35 899 420 853 654
 828 540 894 889 482 662 742 815 359 363 296 425   8 423 217 810 579 502
 535 770 251 845 480 227 136 827 813  62 364  49 342 379 832 124  53  94
 153 618 565 680 710 901 821 488 279 829 281 473 687 492 816 378  78 158
 798 269 858 353 332 252 169 888 127 257 327 571 470 302  19 340 673 775
 220 267 247 287 617 504 886  36 123 151 322 556 857 149 121 630 150 182
 427 481 323 851 674 779 547 731  96 428  63  33 240 183 739 645  20   7
 157 116 725 291 524  55 643 885 140   5   4  60  80  64 593 485 570 461
 513 184 635 173 546 345 135 512 152 343 139 258 444  16 812  57 552 431
 395 384 385 796 285 233 398 675 391 494  92 360 549 160 463 215 456 133
 239 633 548 539  85 523 541 750 180 288 472 761 820 241 634 598 441 392
 191  70  13 185 763 209 759  79 847 362 397 772 811  10 616 564 386 778
 246 745 496  68 528 780 887 580 468 465 562 221  23 162 868 435  37 555
 514  76 870 122 299 236 317 752 568 542 685 142 744 365 141 310 799 294
 196  87 642 573 334 751 551 354 891 791 883 326 432 466 237 349 748 330
 732 532 656 692 459 469 735  66 605 825 224 210 785 554 457 388 620 506
 193 275  75 484 165 305 304 777  58  50 226 103 699 621 534 682  21 242
 762 170 679 794 835 848 521 544  24 722 161  47 650 520 909  69 188 347
 881 875 516 449 801 112  73 743  83 306  27 553 582 776 477 831 688 867
 606 563 830 238 537  38 389  34 168  39 415 863 175 131 757 476 594  28
 318 649 422 437 179 259 574 865 253 876 367 653   2 543 792 839 538 636
 575 599 493 658 623 713 683 355 897 138  65 611 390 119 723 507 795 900
  61 478 898 586  11  74 849 272 790 303 369 297 261 866 905 341 479 503
  22 660 329 361 893 264 289 213 376 545 268 708 572 129 517 558  42 861
 651 132 768 418 300  32 718 693  84  31 632 856 230 519 569 619 550 234
 773 471 186 814 702 726 557 531 610 882 284 510 147 724  56 661 235 871
 721  43 640 566 453 212 174 290  71 223 426 356 115 130 366 266 647  59
 490 436  93 669  54 260 561 102 331 166 200 797 156 560 559 452  51 498
 348 483 578 350 189 434 460 536 164 346 884 782 694   1 518  88 904  40
 592 730 105 336 622 273 646 167 312 591 910 430 445 567 747 728  44 271
 380 277 440 154 198 585 907 509 711 413 159 581 387 846 450  12   9 612
 375  72 584  52 439 438 823 631 665 352 394 754 629 475 850 840 447 690
 684 741 429  95 231 433 222 214 421 451 113 664 104 313 462 416 834 705
 755 700 208 788 187 412  30 608 203 474 216 126 229 117 677 670 145 110
 529 595 860 408 263 587 249 274  81 756 325 838 719 172  29 505 800 414
 902 292 321 146 880 667 417 201  14 403 597 442 855 671  86 262 781 114
 872 118 892 171  67 607 316 733 802  26 225 400 399 232 681 908 501 706
   0 219 382 443 368 603 625  90 695 588 712 774 163 769 155 522 333 753
 879 527 207 293  97  46  41 758 600 583 204 402 371 659 890 655 720 837
 100 841 448 106 351 689 577 111 446 195 370 615 315 500 265 842 749 202
 311 822 767 511 877 601 464 783 844 228 627 458 495 497 339 789 319 393
 596 729 254 698  89 715 869 787 766 276 746 411 308 424 526 499 602 120
 576 589 515 218 176 328 906 859 703 696 771 784 419 793 652 101 613 717
  48 467 530 335 134 727 878 409 396 144 852 270 666 873 738 686 626 211
 199 734 250 178 455 282  25 307 903 525 824  17 668 137 374  45 637 786
 283 896 604 286 314  82 764 709 401 701 320 854 206 337 819  77  98 691
 707 895 109 714 205 190 760 280 406 638 765 255 309 410 197 486 373 697
 833 407 704 836 404 533 609 405 716 614 864 194 628 678 148 639 862 177
 143  18 641 672 508 676 108  91 107 454  99]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2245
INFO voc_eval.py: 171: [1359 1360 1578 ... 1089 1777  659]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3523
INFO voc_eval.py: 171: [1432  830  883 ...  885 1174 1638]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5431
INFO voc_eval.py: 171: [ 72  45 137 140  18  49 222   8  47  88 177 377 154 281 291  53  31 354
 356 245  64 273 261 306 272 366 206 194  51 375 332 182 133 139  42 302
  50 376 287 296 204 350 258 268 111 313  54 175  78 235 186  56 219  46
 163 216  25 262 255  15   6  17  57 318 162 386 101 394 382 307 226 326
 308 329 149  20 159 387  93  90 126 280 145 151 248 113  32 284  87 212
 314  37 119 202  99  38 108 389 362 341  35 240 279 127  94 208 199 374
  26 209 242 227 391 388  55 142 238  66 134 378 342  82 187 205 236 211
 283  86  62 340 330 233 295 315 319 274 349 164 297 317 337  48 171 241
 286   3  80 322 102 158 174 197 311  89 203 358  68  16  11  21  52 298
 123 130  58  19 100 292 160 112 335 309  10 146 346 290 105 180 237 185
 176 138 343 312 249 365 247 334  36 243 269 229  22 380 383 107 122 106
 114 327 355  98 254 370 155 359 170 168  41  30 288  73 239 282 156 372
  85  24 189 339  44  14 178  83 195 230  71 267 118 331 270   7  28 257
 110 251 310 352 271 193  95 225 128 379 303 256 396 147 109   2  59  39
 367 121 333 218 224  84  69 136 263  40 285 351 129 316 336 169 115 124
  91 232 152 213 353 300 259 153 207 220 250  27 293 131 289  67 116 320
 201 361 192 264  76 210 328  70 157 188 217 191 278 132 371 172  74 120
 223 246 265 215  23 179 200 347   9 165 150 104  79 275 381 324 183 384
   1 395   0  33  96 144 221 141 266 198 103 253 161 184 368 190 348 323
  97 393 252 117 294 167 392  77  43 385  61 244  12 148 357 373 181 196
 214 260 234 305  60 390 135 228 325 231  29  34 143 173 345 321   4  63
 277 301  92 125 360 363  65  13 304 369  75  81 338 276 364 166   5 344
 299]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5734
INFO voc_eval.py: 171: [1086 1136  495 ...  511   15 1605]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4741
INFO voc_eval.py: 171: [ 11   6  24 117  25 105 114   8  45 109  40  62  83   1  82  85  58 111
 108  19  80  81  47  95  86  12  63  34  14  21  57  73   5 115 118  41
  29  49  33 110  54  10  96  60   7  72  78  44  37  87  30 106   9  20
  16  64 119  61  88 103  13  76  31  70  68  46  90  50  92  35 107   0
  32  53  91  71  94 116 104  77  36  79  48  27  66  22  93  18   2  69
   4  98  28 120  84  17  75  51  39  52 112  67  74 101  23  42  65 102
  38  89  56  99   3  43 100 113  15  97  26  55  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0056
INFO voc_eval.py: 171: [ 643 5796 5797 ... 6146 6507 4660]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5211
INFO voc_eval.py: 171: [  4  76   5 128  46 103  45  42 113 142  75  49 138 144  96  65  77  47
 145 105 112  39  91   8 120 133  34 107  62 157 132  92  33   9  60  68
 111 115  93  52   6 139 119  82 117  44 155  99  79  81 131  40 141 143
  84  48   3 130 100  17  14  57  85  94  18  43 106  51  27  53   7 140
  38 151  72  22 136  88  80 127 102 134  59 104 135  56 152 121  74  37
  25  58  90  26 129  55 118  89  32  63  11  95 137 114  41   1  28  61
 116  30 110  21 149   0  78  86 108 154  97 146  54 124 150  50  98  73
  31   2  20  10  29  87 147 126  35  36  67  69 125  12  83 148 123 109
  70  23  71 101 153 122 156  64  66  24  19  13  15  16]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1832
INFO voc_eval.py: 171: [ 873 1909  469 ... 2002 1657   96]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3642
INFO voc_eval.py: 171: [104  96 189  97   3 152 101  19  55 105 208   7  61  40  36  46 206  57
  66  91  33 205   0  27 187 193  12  79  80  65  56  13 166 162 131 143
 154  37 118 153  90 136  34 202 200 219  89 123 220  72  48 173 199  59
 210 124 155  10  85 215 190 119 159  15 115 221  11 188 102  98 168  64
 116  26 196 122  74 185  67 146 127  45 192 126 144 128 130  62 133 158
 120 207  39 103 174 151 109 121 112   5   2  84  16  23  63  83 107 175
 149  43 110 147 111 198  70  14 170  51  22 203 125 186 106  29  58 138
  95  75 201   8 191 194 141  68 169 129  31 167 165   4  81 163  77  47
 157 172  41  69 164 171  32  44  25 140  17 180  21  20 222  76   1 100
 217 161 214 204 177  24 113  94 183  60 135  71  88 108 209  73  49 182
 160  54  82 218  30  86  99  93  35  28 211 213  53   6   9  18 117 212
 195  87  52 142 137 184 179  42  38 114 216 197  78 150 156 132  92 139
 181 148 178 176 145 134  50]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1166
INFO voc_eval.py: 171: [142 125  28  21 169  93 128 179 124 143 186 150  64  23  76  33  22 118
 102 174  65 122  98 145  99 152 166 167 105  70   8 165 153   9 126  69
  80 127  54 107 108  84  48  49  35  32 148  63  87  46  58 175   4 129
  41  43 120 159   1  55  97  77  94  11  74  47   2 149 146 171 163 140
 141  66 177 119 164  15 104  88  75 110   0  78  26  31 162  82 144 160
 181  83 131 156 154 183  71 130 173  68  40 103 184  50  51 168 176  37
  92 151  52  12  62  10 182  95 123  27  90   6  34  13 101  18 178  17
  61 132   3 158  24 134 133 138   7  96 106  20  14 112 147  86 109  89
  25  79  60 170 172 187 137  81 121 117 113 115  19 116  36 111 139  38
  72  91  42 100   5  16  67  85 161  39 155 180  45 135  56  29 136  53
  73  30 185  59  57 114  44 157]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3774
INFO voc_eval.py: 171: [ 96 545  74 358 281 586 373 263 396 420 498 428 441 528  51  29 110 294
  25 374 470 411 525  97 271 308 269 564 594 147 185 593 170 235 136  68
 227 186 486  12 176 489  65 416  61 100 205  88 448 168 307 473 339 264
 174 531 584 600 390 219 309 338 349 481 413 361 137 332 588 310  76 278
 252 491 160 401 585 556 348 493 115 548 500 114  71 527 208 326 483 446
 286 541 207 175  56 449 299 592 127 212 346 424 229 404 375 202 524 363
 475   3  44 318 440 224  20 377 279 328 595 547 128 467 182 303 106 526
  41 159 319 414 156 125  45  33  58 575 536 596 198 385 316 391 192 603
 432  87  92 277  66 409 496 107 529 488 574 129 243 551  30 431 158  83
 322 598 217 451 272 484 415 509 144 402 244 131 581 379 350 394 116 231
 290 256 558 194 543 354 152 122 352 562 327 530 228  70  94 503 501 305
 218 597 187 371 209 259 442 599 150 399 398 262 472 537 540 512 576 534
 423 533  57  46 289 452  16 179 395 311 181 397 293 301  28 563 140 443
 321 344 571 461 539 393 492 487  62 514 435 589 268 359 499 298 591   1
 123 450  69 300 357 157 101 516  81 582  19 193 240 103  82 320 386 590
 434 270 567 388 507 204 109 177  23 508  95 479 172 538 532 587 331 429
 405 239  13 337 425  80 242  43 315  17 572 343 323 406 478 400 602 220
 427 418  27 554  93 504 132 153 329 189  32 482  14  15  98 265 369 233
 341  10 387 190 455 542 166 454 126 465 522 421 251 312  35 223  18 120
  40 199 330 485  64 445 248 565 138 142 459 573 381   9 474 550 171 117
  72 382 134 569 191 535 173 469 447  60 360 353 544 284 515 384 570 247
 135 162 422  31 426 340  99 453 149 238 304 313 347 222 102 517 161 292
 282 141 196 457 559   4 133 334 523  47 145 578  42  73 108 324 362  21
 267 197 466 494 104 246 253 403 213 521 518 380 201 417  53 148 216 407
 184 502  85 151 295  37 366 188 506 376 471 497 291 430 215 275 245 392
 254 111 146 546 296 276 230 288  38  34 178 438 468 164  86 444 560 604
 285 456 355 356 367 601  11  78 154 143 370   2  59  63 221 118 169 566
 130 163 408 437 552 113  39 195   8 249 495 549   0 436  48 345 287  89
 389 210 372 266 364 333 577 260  49 561 226 119 124 274  22 280 480 155
 433 511  54   7 241 510 463 336 490 180 236 505 458 476 342 258 368 520
 112 583 460 283  52 557 314 250 513 237 351 464 553 257 273 335  24 105
 203 211 568 225 255 183  79  55 580 302 412 383 167  26 439 579 214 261
   5  67 519  90 317  77 555  91 410 165 306 206 121  75 200 297 419  50
 365 232 378 234  84 139 462  36 477 325   6]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7083
INFO voc_eval.py: 171: [ 2062 11455   863 ...  4972  5050 10356]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6212
INFO voc_eval.py: 171: [ 342  435  667 ... 1966  744 1902]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5920
INFO voc_eval.py: 171: [128   3 122   4  69  74 152 107  98 105 181  19 145 121 150  25  73  62
 136  71 159 120  70  47  89  20 132  44  67  28  68   6 167 151 123  72
 113 125  49 156  48 153  33 171  24  36 175  11  39 178  52 143  42  78
  80  85  16 134  23  10 185  61  65  29 144 138  38 109 106  92   7 160
  75  88 135 177 163 131 116  58  84 165 173   2  87  86  35 129  53 114
  14  22  76  56 124  83 100 101 146  81 155  43 147  82  79  30  60 133
  77 115   0 157  34   9  21 164   1  40  64  51  63  90 141 168 108 139
 117   5 142 103 158 154  31 127 161 166  55  13 119   8 172  91  99  94
  66  50  18 104  26 148 183  12 176 170  37  93  15  45 174 111  96  54
 102  46  17  41 169  95 162  59 126 182 137  32 118 149  27 140 112 180
 110  97 179 184  57 130]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2825
INFO voc_eval.py: 171: [204 201  70 275 295 219 428 141 221  16 163 332 225 310 294 316  74 206
 203 376 189 409 202 172 351  95 315 280 410 211 173 426 246  17 114 255
 205 186 263 140  79  66 248 242 396 124  88 135 253  92  48 153 397 170
 160 223 357 136 318 118 139 434 317 358  59 435 346 175  39 120 375  80
 156 359 311 123 293 333  21 169  72 208 298  93  40 352 265 227 288 209
 115  34 254 258 198 185 403 224 363 134 329 168  18 236 281  65  31 131
 229 143 362 284 417 427 109 334 304  23 413 210 256   4 348 126 313 216
 234 314 191 127 325  55 174 391 378  84  32   5 368 361 405  67 407 429
 102 183 247 113   3 430  45  41 272 328 228 133 420 283  82 436 151 122
 425  50  76 274 382 366  35 364 387 305 291 322 424 259 270  81 345 339
 217 292 336  36 142 266 180 297  19 285 321 374 104 243 309 218 412 178
  54 393 338  11 245 250 222 355  43 197 324 390  53 307 125 129 365 111
  78 214 176 107 290   6   7 235 154 421  96  30 349 392 244 146 187 171
 159  73  56 177  13 226 132  44 408 370 422 138 394  63 257   8  25  61
 149 398  60 277 119  47 432 299 308 343  91  29 401 161 155 128  71  90
  22 220 150 287 286 271 188  37 416 326 433 319  26 350 384 373  49 230
 380 260 302 137 331 347 144 389 162 356 100  24  99 282   2 184 233 262
  12 360 213 267 121 369 240 195 385 157 164 354 306   9 252 415 372 207
 301 166 402 386  28 353   0  77 148 419 264 145  58  97  83 215 300  89
 193 231  98  87 130 411 110 192  27 238 152 200 414 117  94 158 182  38
 279 108 395  68 400 323 116 237 379  69  62 190 199 105 335 289 269 371
  14 232 106 388   1 406 196 273 276 367 101 239 344 165 112 312 261 340
 147 337 399 268 251  51  46 342 212 181  52  85 296 377 423  42  20  10
 404 431 383 381 249 179 194 167  15 303 320  57  33 418 278 103 241  75
 330  64 341  86 327]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3843
INFO voc_eval.py: 171: [425 271  73  83 140 364 230 139 113 426 144 311 314  75 231 369 120 279
 242 586 340 169 143  82  52 556 367 349 477 276  67 538 484  77 363 273
 598 201 117 109 118 270 334 478 302 145 499 220 315  99 251 545 600 232
 299 104 298 397 217 596  72 568 219 614 620 399 146 567 365 100 494 405
 265 114  13 400 123 300 486 121 254 356 207 195 167  16 594 327 138  66
 192 554 348 617 343 309 467 196 243 316 444 359 122 512 422 366 264  57
 162 156 274 206 427 341 619 105 177 522   5 448 294 197  21 221  45 455
 115  22 616 578 170 561 320 238 624  71 613 209  69 431 362 321 441  53
   8 510 481 610 485 564 223 451 553 505  70 489 386 458 415 609 433 260
 571 161 175  92 370  63 336 166  38 171 172 483 377 351 290 500 548 560
  76 482 434 297 331 253 403 591 559 612 312 393 479  60 416  30 519 532
 355 436 203 497 511 305 360 472 388 306  46 130 129 267 303 471 593 622
 530 535 280 492 313 151  93 394 261 301 218 142 288  17  47 228 507 480
 487 158 176  29  62  96 454 124 205 417 152 247 353 412 168   6 186 473
 190  42 329  68 490 178 135 621  43 308 546 457 245 128 603 383 111 215
 106 296 250 470 256 557 565 582  35 210 107   7 234 380 102 173 147 615
 236 375 371 459 443  11 335 584 125 440  23 475 524 222 476 439 319 325
 516 137 563 108 374 562 570 527 536 318 551 488 110 606  98 580 432 583
 599 445 450  24 141 183 307 555 193 395 292  34 112 418 185 281 332 389
 160 255 148 390 407 447 165  54 287 518 590 180 179  18 189 337 350 378
  58 604 304 424 496 324  50  27 398 587 358  19 592  88 131 103 469 558
 116 521 585   9 618  91 317  49 513 310  41 406 379 119 493 411 333 401
 421 339  56 101 322 523 163 239 224 529  39   1 588 503 368  12  28 132
 581 259 268 569  40 438  94 442 437  87 589  61 225 547 526 625  86 184
 413  84 491 508 212 514 402 272 409 277  89 577  20 345 187 191 153 542
 461 295 391  14  55 204 605 154 464 607 237 182 525 157 552 608  25 498
 323 291 429  78 410 420 435 352  97 338 381 181 227 504  32  74 575  90
 540 241 539 211 188  44 134 392  81 597 449 460 283 602  10 544 495 330
  36 502  51 342 361  15 572 501 235 446 408 574 155 456  48 541 326 278
 213 214 550 537 194 528 240 246 126 133 269 266 576 328 595 344 285  79
 208 515 611  64 199 566 573 430 372 506   3 387 284 258 252 262  65 216
 533 263 347 150 202  85 384 534 282 509 229 428 404 249  26 465 376 463
 466 373 396 474 414   2 453 419 423 517 462 127 136 549  33  59 159 289
 346  37 452 579  95 275 468 244 357  31 623 226 520 385 174 354 200 233
 248 257 382   4 293 286  80 531 543 198 164 149 601   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3444
INFO voc_eval.py: 171: [272 506 725 ... 929 498  17]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4790
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4054
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.357
INFO cross_voc_dataset_evaluator.py: 134: 0.604
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.543
INFO cross_voc_dataset_evaluator.py: 134: 0.573
INFO cross_voc_dataset_evaluator.py: 134: 0.474
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.521
INFO cross_voc_dataset_evaluator.py: 134: 0.183
INFO cross_voc_dataset_evaluator.py: 134: 0.364
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.377
INFO cross_voc_dataset_evaluator.py: 134: 0.708
INFO cross_voc_dataset_evaluator.py: 134: 0.621
INFO cross_voc_dataset_evaluator.py: 134: 0.592
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.384
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.479
INFO cross_voc_dataset_evaluator.py: 135: 0.405
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 7499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.676s + 0.001s (eta: 0:01:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.393s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.405s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.396s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.404s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.401s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.396s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.397s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.396s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.401s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.398s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.402s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.400s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.518s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.404s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.406s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.390s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.386s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.389s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.396s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.401s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.397s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.399s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.401s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.399s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.396s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.517s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.410s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.396s + 0.003s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.396s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.390s + 0.003s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.388s + 0.003s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.389s + 0.003s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.387s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.385s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.383s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.382s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.384s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.386s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.440s + 0.003s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.403s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.391s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.389s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.391s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.386s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.385s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.386s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.385s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.386s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.383s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.382s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.381s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.761s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [579 181 900 ... 437 622 701]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3578
INFO voc_eval.py: 171: [205  98 710 824 485 785 639 796 497 826 473 828  99 835 645 549 122 640
 830 750 795 542 370 823  84 570 479 837 646 690 262 237  55  58 810  44
 478 550 696 106 671 387 616 836 307 701 825 202 669 426 800 525 304 801
 642 766 383 483 124 767  42 520 711 771 652 110 103 289 712 164 495 480
 543 167 372 741 488 763 811 851 544 746 649 309 363  59 753 541 625 694
 207 660 777 192 754 568 475 698 853 267 422 496 206 582  83 758 702 584
 573 842 757 859 415 109 790 641 284 474 716 822 477 760 860  56 334 482
 481 733 335  14 220 134 863 510 175 619 188 306 100 854 112 672 261 752
  43 527 555 723 806 436 107  51 476 498 805 529 699 819 736 802 324 852
 365  24 141 358 116 604 135 356  87 714 687 493 125 526 330  16 685 219
 553 511 384 288 163 139 174 144 333 606 305 470 300 677 609 492 862 667
 788  67 260 263 574 494 352 807 563 594 569 531 515 841 530 634 340 831
  45 221 115 717 274 524 740 119 709   4 647 589 700 693 468 684 208 664
 721 283 791 256 395 419 803 765 593 414 697 794 576  29 154 507 844  94
 234 435 827 586 491 755 211 269  60 523 277 567 850  63  74 833 401  23
 320 673 713 402 268 191 169 532 720 229 216 557 799 155 279   6 117 680
  40 215 105 388  15 126 656 371 147 111 486 857 722 682 663 804 425 838
 608 241 849 170 500 301 242  26 156  71 377 177 257 326 442 705 213  88
 522  34 353 298 342 118  18  47  36 171 665 627 226 460 534 429 797 499
 190 299 397 679 628 418 546  78 735 203 509 193 253 855 505 146 231 769
 668 789 336 630  54 245 865  76 441 369 390 856 328 412 228  86  85 751
 782 590 197 848 351 759   5 776 176  90 291 465 209 416 199  72 379 737
  66  17 313 571 186 233 339 378 728 249 189 469  92 779 236 172 784 451
  68 145 592  69 611  79 551 605 244 381 783 540 792 538 506 866 440 437
 406 798 658 127 265 521 187 858 775  48 689  30 323 224 808   1   0 745
 545 413  20 331 114 683 361 657  73 816 547 581 238 832  13 695  46 266
 622 293   8 380 666 662 653 734 867 184 250 591 738 314 360  93 382  35
 643 614 456 183  10 648 585  70 764  21  19  38 707 210 248 814 362 433
 424 272 674 308 457 321  11 204 821 502 744 420 719 297 809 650 385 815
 178 212 445 443 761 632 341 537 772 676 718 393 572 392 731 812 762 670
 278 607  89 222 287  41 528 162 386 166  80 636 644 264 691  97 327 503
 290 793 364 575 165 834 430 185 318 251 322  75 861 615 724 180 820 464
 462 121 345 598 411  65 449 200 239 773 230 864 123 432 743  53 252 173
 562 596 726 703 225 487 602 295 120 659 439 778 732 448 518  82 158 227
 273 142 688 198 774 768 319 138 661 431 561 347 101 756 232 813 223 312
  37 246  22 453 461   7  95 217 706 533 303  12 240  77  91  39 565 548
 161 472 286 846  25 367 389 408 394 655 292 376 243  33 181 556 343 282
 359 271 403 554 354 786 637 423 194 504 315  28 182 829 559 235 729 168
 781 633 467 446 578 311 427 678   3 407 558 621 258  32 631 195 294 302
 675 350 366 296 566 770 129 692 148 151 159 620 626  27 749 787 818  50
 847 280 417 704 421 396   2 579 338 108 325 535 196 317  64 316 560 583
 214 618 742 597 490 519   9 516 514 255 444 357 409 247 843 580 623 254
 218 459 132 136 651 539 137 349 153 428 617  96 410 310 458 368  52 355
 391 348 839 588  49 471 285 686 149 160 708 629 454 517 275 612 603 463
 140 400 600 344 329 730 725 104 102 452 113 270 613 438 577  31 281 564
 508 552 635 654 405  81 739 130 375 374 157 610 131 727 601 143 845 840
 455 259 447 780  61 152 337 332 624 398 404 536 638 748 747 179 513 466
 595 399 484 150 346 128 450 715 133 201 373 587 276 501  57 681 489 434
 512 817  62 599]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6035
INFO voc_eval.py: 171: [125 324 381 243 245 804 377   3 805 357 244 737 806 741 591 649 492 256
 488 128   6 248 738 301 295 809 875 344 807 278 383 181 664 827  15 658
 625 818 810 844 298 819 645 338 372 192 808 358 490  35 900 421 854 655
 829 541 895 890 483 663 743 816 359 363 426 296   8 424 811 217 503 580
 536 771 251 846 481 227 136 828 814  62 364 342  49 379 833 124  53  94
 153 619 566 681 711 902 822 489 279 830 281 473 688 493 817 378  78 158
 799 269 859 353 332 889 252 169 257 127 327 471 572 302  19 340 674 776
 220 267 247 287 618 505 887  36 123 151 322 149 858 557 631 121 150 182
 482 428 852 323 675 732 548 429 780  96 183  63  33 240 646 740  20   7
 157 116 726 291 525 644  55 140 886   5   4  60  80  64 594 486 514 571
 462 184 173 547 345 636 135 513 343 152 139 258 445  16  57 813 553 395
 432 384 394 797 285 398 676 390  93 233 360 160 495 550 464 133 215 549
 239 457 634 540  85 524 180 542 751 821 762 635 241 288 472 599 442 191
 391 185  13 764  70 760  79 362 397 848 209 812  10 773 385 617 565 779
 246 746 497  68 529 781 888 581 469 466 563  23 221 162 436 869  37 556
 122 515  76 871 299 236 753 317 569 686 142 141 543 745 800 365 310 196
  87 752 334 892 574 552 294 467 643 433 354 792 749 884 326 349 330 237
 733 533 736 470 657 606 460 224 826 693  66 210 786 555 387 458 193 507
 165 485  75 275 621 305 103 226 304  58  50 778 700  21 622 683 795 535
 763 242  24 680 161 170 849 836 545 651 522 521 723 188  47  69 910 876
 517 450 347 882 802  73 112 306 744 777  83 583 689  27 554 868 478 832
 564 831 607 538 238 168 131  38  34 416  39 175 318 477 650 758 259 595
 864 438 423 866 575 179  28 877   2 654 253 539 840 793 367 544 600 624
 637 576 494 659 714 684 355 138 612 724 898 901  65 389 508  74 119 479
 796  61 272 899 791 261 587  11 850 303 341 369 504 480 867  22 376 361
 268 329 906 297 264 289 213 129 573 894 661 518  42 546 709 132 559 652
 719 300 694 862 769 570  32  31 419 230 620  84 551 633 520 857 474 234
 186 774 703 815 558 611 532 284 147 883 727  56 722 725 511 662 872 641
 399 235  43  71 290 567 174 223 130 212 356 366 454 427 115 648 491 266
 437  92 670 260  59 331 102 562 166  54 798 561 560 453 156 200 484  51
 348 499 350 537 189 461 435 519 346 164 579 783 731 593 105 568  40 885
 695   1 911 905  88 431 623 446 312 277 748 336 273 592 380 647 154 271
 167 441 198 729 908 586  44 159 386 414 712 510 847 451  72 439 824   9
 375 582  12 440 585 613 352 666  52 393 476 851 632 755 742 691 630 434
 430 448  95 214 231 222 685 841 452 422 104 665 463 313 113 756 706 835
 417 187 701 117 126  30 216 789 609 208 475 263 110 588 530 249 596 409
 413 671 203 229 678 757 145 274 325 861 388 801 415  81 172 903 292 506
 839 146  29  14 418 443 321 598 672 720  86 881 782 262 201 893 668 404
 856 118 114 502 401 400 225 873 171 608 316 682  26 734 707 803 909   0
 626 444  67 232 382 604 219 523 163 368 775 770 696 293  90 333 528 754
 155 713 207 589  97 601 880 891 584  41 204  46 660 371 100 403 759 842
 656 106 449 351 578 750 265 721 838 111 616 315 195 823 690 501 878 311
 628 496 370 202 843 602 447 465 459 319 784 512 339 392 768 254 597 845
 228 730 716 790 498 788 308 412  89 870 276 425 516 603 767 176 527 218
 699 747 772 500 120 577 420 328 907 590 697 704 144 335 860 396 134 653
 794 614 785 101 728 468 667 739  48 825 531 526 904 687 456 853 410 879
 718 270 211 178 669  25 250  17 374 627 735 282 283 307 314 199 765 874
 137 787 855 710 337 638 605 286  82 897 820  45 896  77 702 205 197 411
 320 692 708 402 206 255 534 761  98 766 698 109 407 406 408 639 373 143
 190 309 715 280 679 405 629  18 717 673 642 640 705 487 610  99 509 107
 108 148 834 837 177 865 863 194 615 455  91 677]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2243
INFO voc_eval.py: 171: [1366 1367 1585 ...  250   13   75]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3520
INFO voc_eval.py: 171: [1428  826  879 ... 1590  640  685]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5431
INFO voc_eval.py: 171: [ 73  46 138 141  18  50 222   8  48  89 178 378 155 282 292  54  32 355
 357 246  65 274 262 307 273 367 206 194  52 376 333 182 134 140 303  43
  51 377 288 297 204 351 259 112 269  55 314 176  79 235  57 186 219  47
 164 216  26 263  15 256   6  17  58 319 163 387 102 395 383 308 226 327
 309 330  21 150 160 388 127  94  91 281 146 152 249 114  33 285  88 212
 315  38 120 202 100  39 109 390 363 342  95 241 128  36 280 208 199 375
 209  27 243 227 389 392  56 143 239  67 135 379 343 237 211 205  83 284
 187  87  63 341 331 233 296 316 350 320 275 165 298 318  49 338 172 242
 287   3  81 175 197 103 323 312 159  90 359  69 203  16  11  22  53 299
  20 124 131  59 101 293 161 113 336 147  10 310 347 291 181 238 106 185
 177 250 313 139 344 335 248 366 270 229 244  37 381  23 108 384 107 123
 356 115 328 255  99 371 156 171 169  42  31 360 289  74 283 157 240 373
  45 189  25 340  86 179  14 332 268  72 195  84 230   7 119 271  29 111
 258 252 311 129 193 353 225 272  96 380 304 257 397 368 110  40   2 148
  60 122  70 334 224 218  85 286 137 264 317  41 352 130 170 232 337 125
 354 213 260 153 116 301  92 207 154 294 132  28 251 220  68 321 290 117
 201 362 192  77 265 210 329 217  71 188 158 133 279 191 180 247 215  75
 223 372 266 121 173 348 200  24 105   9 382 151 166 385  80 396   1 325
 183  97 276   0  34 145 142 221 198 267 349 162 369 104 184 254 324 190
 394 253  98 393 118 295 168 374 386  78  62  44 358 245 214  12 261 149
 136 196  61 234 144 391 306 174  30 231 326 228  35 302 346 364 361  93
   4 322  66  64 278 126  13 365 370 305  76  82 167 339 277 345   5  19
 236 300]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5734
INFO voc_eval.py: 171: [1138 1088  497 ...  975  463   27]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4737
INFO voc_eval.py: 171: [ 11   6  24 117  25 105 114   8  45 109  40  62  83  82   1  85 111  58
 108  19  80  81  95  47  86  12  34  63  14  21  57  73   5 115 118  41
  29  49  54  33 110  10   7  96  60  72  78  44  37  87  30 106   9  20
  16  64 119  88  61 103  13  76  31  70  68  46  90  50  92  35 107   0
  32  53  91  71  94 116 104  77  48  36  66  79  27  22  93   2  18  69
  28   4  98  75  84 120  51  17  52  39  67 112  74  23 101  42  38  89
  65 102   3  99  56  43 100 113  15  26  97  55  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0058
INFO voc_eval.py: 171: [5812  646 5813 ... 2033  506 5503]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5208
INFO voc_eval.py: 171: [  4  77   5 129  47 104  46  43 114 143  76  50 139 145  97  66  78  48
 146 106 113  40  92   8 121 134  34 108  63 133 158  93  33   9  69  61
 112 116  94  53   6 140 120  83 118  45 156 100  82  80 132  41 142 144
  85  49   3 131 101  17  14  58  86  44  95 107  18  27  52  54 141   7
 152  39  22  73  81 137 103  89 128 135 105  57 136  60 122 153  75  25
  38  59  91  26 130  56 119  90  32  11  96  64 138  42 115  62  30  28
   1 117  87 150 111  21   0 109  79 125 147 155  51  55  98 151  99  74
  20  31   2  10  88  29 148 127  35  37  68  70 126  84  12 124 149 110
  23 123  72  71 102  65  13  24  67 154 157  19  16  15  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1822
INFO voc_eval.py: 171: [ 874 1917  471 ... 1073 1299 1655]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3638
INFO voc_eval.py: 171: [105  97 191  98   3 153 102  19  56 106 210   7  62  41  37  47 208  58
  67  92  34 207   1 189  28 195  12  80  81  66  57  13 168 164 132 144
 155  38 119 154  91 137  35 204 202 221  90 124 222  73  49 175 201  60
 212 156 125  10  86 192 217 120 160  15 116 223  11 190 103  99 170  65
 117  27 198 123  75  68 187 147 128  46 194 127 129 145 131  63 159 134
 121 104  40 209 152 176 122 110   5 113   2  85  16  24  84 108  64 150
 177  44 111 148 112 200  71  14 172  52 205  23 126 188 107 139 203  30
  59  76  96 193 196   8 142  69 171 130 169 167  32  82   4 165  78  48
 158  42 174  33  70 166 173  26  45  21  17 141 182 224  22   0  77 101
 163 219 179 114 216 206  25 136  95  89 185  61 211  72  50 109  74 184
  55 161  83 220  31 100  29  87  36  94 213 215   6  54   9 214  18 197
 118  88 186 181 143  53 138  43 218  79 115  39  93 151 199 157 133 140
 183 135 149  51 180 178 146  20 162]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1166
INFO voc_eval.py: 171: [145 128  28  21 172  95 131 182 127 146 189 153  64  23  77  33  22 121
 104 177  65 125 100 148 101 155 170 169 107  70   8 168 156   9 129  69
  81 130  54 110 111  86  48  49  35  32 151  63  46  89  58 178   4 132
  41  43 123  99 162   1  78  55  96  11  75  47   2 149 152 174 166 143
 144  66 180 122 167  15 106  90  76 113   0  79  31  26 165  83 147 163
 184  84 134 186 157 159  68  71  40 133 176 187  50 105  51 171  37 179
  94 154  52 185  97  62  12 126  10  27  92  13   6  34  18 181 103  17
 161  61 135  24 137   3  20 141 136  98   7 109  14 115 150  88  91  25
 112 190  80 124  60 173 175 140  82 120 116  19 118  36 119  38 114  93
 142  42  72 102 158  87   5  16  67  39 164 139 183  45  29 138  56  59
  53  74 188  30  57 117 160  44  73 108  85]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3773
INFO voc_eval.py: 171: [ 96 544  74 356 279 586 371 261 395 419 497 427 526 440  51 110  29 292
  25 372 469 410 524  97 269 306 267 564 594 146 184 593 169 233 135  68
 225 185 485  12 175 488  65 415  61 100 203  88 447 167 305 472 337 173
 262 529 600 584 389 217 307 336 347 480 359 412 136 330 588  76 308 276
 250 490 159 400 585 346 555 492 115 547 499 114  71 525 206 324 482 445
 540 284 205  56 174 448 297 592 126 210 344 423 403 227 373 361 200 523
 474   3  44 316 439 222  20 375 277 595 326 546 127 466 181 301 106  41
 158 317 413 155 124  58  45  33 575 535 596 196 383 314 390 190 603 431
  92  87 275 107  66 408 495 527 487 550 241 128 574  30 430  83 157 215
 598 320 450 270 414 483 508 143 401 242 581 130 377 348 393 116 229 288
 531 254 192 352 558 542 151 350 122  70 325 226 562 528  94 502 500 303
 216 597 186 257 369 207 441 599 536 398 149 397 471 576 539 260 511 533
 532 422 287  57  46 451 178  16 394 180  28 309 291 396 563 299 139 319
 442 460 571 342 392 538 486  62 491 589 513 434 266 591 498 357 449   1
  69 123 296 298 156 101 355 191 515  81 582  19 103 238 318  82 268 590
 384 567 109 433 176 506  23 386 507 202 537  95 171 478 329 335  13 530
 404 237 240 587 424 428  80  17 313  43 572 477 405 341 321 417 399  27
 218 426 602 553 503 131 187  93 152 327  15 481  32  14 367  98 263 231
 339  10 385 125 188 541 454 453 420 464 165 249  35 328 221 310  18  40
 521 484 120 444 197 137 246 565  64 141   9 458 549 573 379 117  72 473
 133 170 446 189 172 569 534 380 468 358  60 282 543 351 382 514 161 134
 421  99 570 245 236 338  31 452 425 102 148 160 302 345 311 290 280 516
 220 194 456 132 559 522 108 144 578  73   4 140  47 265  42 332 322 195
 465 360  21 493 104 517 378 520 402 211 244 251 147  53 416 199 150 183
 406 214  37 364 374 501 293  85 213 496 505 429 289 294 391 470 243 111
 273 252 145 545 274  38  34 443 228 177 354 286 560 467 604 365  86  78
 163 437 283 455 353 118 368 153  63 601 142  11 168 129 219 113 566  59
   2 551 162 436 407  48  39   8 494 193 247  89 548 285   0 343 435 331
 387 362 370 577 264 208  49 258 561  22 224 278 272 119 432 154  54 510
   7 489 479 239 462 334 509 504 179 475 340 457 234 583 519 256 366 248
 112  52 271 235 512 349 556 463 312 459 281  24 552 255 333 182 105 253
 411 209 568 201 223 166 381 580 579  55 300  79 212 438  90  67  26 554
  77 259 518   5 295 315 164  75  91 409 204 232 418  50 461 198 121 304
 230 363  84  36 138 376 323 476   6 557 388]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7061
INFO voc_eval.py: 171: [ 2067 11517   602 ...  2905  3572  7344]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6206
INFO voc_eval.py: 171: [ 343  436  668 ... 2247  551 1443]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5912
INFO voc_eval.py: 171: [126   3 120   4  67  72 150 105  96 103 179  19 143 119 148  25  71  60
 134  69 157 118  68  45  87  20 130  43  65  27  66   6 165 149 121  70
 111 123  47 154  46 151  32 169  24  35  11 173  38 176  50 141  41  76
  83  78  16 132  23  10 183  59  63  28 142  37 107 136 104  90 158   7
  86  73 133 161 129 175 114  82 171 163  56  84  85   2 127  34  51 112
  14  22 144 122  98  81  53  74  99 153  42  79 145  80  77  29  58 131
 113   0  33  75 155 162   9  21   1  39  49  62  61  88 166 139 106 115
 137   5 140 156 101 152 125  30 159 117 164  13  54   8 170  89  97  64
  92  18  48 102 146  12 181 168  44 174 172  15  91  36 109  52 100 167
  94  40 160  17  93  57 124  26 180 116 135 110  31 138 147  95 177 178
 108 182  55 128]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2825
INFO voc_eval.py: 171: [204 201  70 276 296 220 429 141 222  16 163 333 226 311 295 317  74 206
 203 377 189 202 410 172 352  95 316 411 281 211 173 427 247  17 114 205
 256 186 140 264  79  66 249 243 397 124  88 135 254  92  48 153 398 224
 170 160 358 319 118 136 139 435 318 359 436  59 347 175  39 376 120  80
 360 156 334 294 123 312 208  21 299  72 169  93 228  40 353 266 209 289
 115  34 255 259 198 185 404 225 134 364 330  18 168 282 237  65  31 131
 230 418 428 143 363 285 109 335  23 305 210 414 257   4 349 126 235 217
 315 314 127 326 191  55 174 392 379 369  84  32 406   5  67 362 430 408
 248 183 102   3 113 431 329  45  41 133 229 273 284 421 151  82 122 437
  76 426  50  35 383 275 367 365 388 306 271 323 292 425 260  81 340 337
 267 293 346 218 286 180 142  36 298  19 322 375 104 310 244 219 178 413
  11 339 394  54 356 223 246 325 251 197  43 391 308  53 366 129 125 214
 111  78 176   7 291 107   6 422 236 154 350  96  30 393 187 159 146 171
 245 227  56  44 409  13 177  73 138 132 371 395 423  63  25  61 258 399
   8 149 278  29 433  60  47 119 300 402 344 309  91 161 128  90 155 150
  22 288  71 287 417 221 188 434 320 327 374 272 231  37  26 351  49 381
 385 303 348 261 332 137 162 357 144 390 100  99  24   2 184 283 213 268
 263  12 361 234 121 241 370 195 355 307 386 416 164 157   9 253 373 166
 207 302 403 387  28  77 148 354 420 145   0 265  97  58  83 216  89 301
 193  98 232  87 130 110 192 412 117  27  94 200 415 239 152 280 158 401
 182 116 108  62  38 396  68 324 105  69 238 380 190 336 199 101 389  14
 233 290 106 274 270 240 372 196 407 165 147 345 368   1 277 341 313 262
 112  52 269 338 400  51  46 252 181 343 212 405 297  85 382 384 424  57
  42  20  10 432 194 378 321 167  15 179 250  33 419 304 103  86 331  64
  75 242 342 279 215 328]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3841
INFO voc_eval.py: 171: [425 270  73  83 140 364 229 139 113 426 144 311 314  75 230 369 120 278
 241 586 340 169 143  82  52 556 367 349 477 275  67 538 484  77 363 272
 598 117 200 109 118 269 334 478 302 145 499 219 315  99 250 545 600 231
 299 104 298 397 216  72 596 568 218 614 620 399 146 567 100 365 494 405
 114 264  13 400 123 486 121 300 253 206 356 205 201 167  16 594  66 138
 327 554 192 617 348 343 309 316 242 195 467 444 512 359 122 422 366 263
  57 273 162 156 427 341 619 105   5 177 522 448 294 196 220  21  45 455
 115 616 170 561 320  71 237 578 624  22 613 362  69 431 321 441 510 481
  53   8 564 553 610 222 485  70 451 505 386 489 458 415 433 609 161 571
 175 259  92 370  63 336  38 171 172 166 483 351 290 377 548  76 560 500
 297 482 434 252 331 403 559 591 612 312 416 393 479 519  30  60 355 532
 497 202 436 305 511 388 472 360  46 306 266 303 129 130 471 622 593 530
 535 313 279 151  93 217 492 301 394 260 288 142  17 227  47 507 480 487
 176  29  62 158 124 204  96 454 417 246 353 168 152 412 186 473 329  42
   6 190 490  68 308 135 621 178  43 546 457 128 244 603 111 383 249 296
 255 470 214 106 565 557  35 582   7 233 107 235 147 380 102 615 173 375
 371 459 443 335 584  11 125 440  23 319 439 524 476 221 475 325 137 527
 108 516 563 562 536 551 570 374 318 599 110 488 583 445 432 580 141  98
 606 555 183 450 307 395  24 292  34 193 112 185 280 418 390 160 389 407
 254 148 332  54 165 447 518 286 189 590 337 350 179 304 604  18 180 424
  58 378 496 324  27  50 398 469  19 587  88 103 358 592 131 116 558 618
 521  91   9 585 317 513 379  49  41 310 119 406 333 339 401 493 421 163
 411   1 238  56 101 523 322 529 223 209  39 368  12 258 503 581  94  40
 438 588  28 132 267 569 437  87 224 547  61 442 589 526 625 184 402  86
 211  84 413 271 409 514 508 491 276 153  89 187 577 345 542  20 191 295
 203 154 157  14 182  55 391 461 552 429 607 605 236 525 464  78 420  25
 608 291 435 410 498 323  97 352 226 504 181 381 338  90  32 575 540 392
  81 602  74 210 539  44 240 188  51 134 597 495 460 282 449  10 330 544
  15 502 234  36 572 208 361  48 342 574 446 501 456 212 213 408 155 326
 541 277 595 550 344 528 207 126 194 133 537 576  79 268 245 611 265 239
 328 284 387 372 506 515   3 430 198 573 262  64  65 566 257 251 509 283
 534  85 533 215 347 228 261 281  26 384 150 428 404 396   2 248 414 465
 423 466 474 463 376 462 373  37 549 419 136 517  33 453 127 159  59 289
 579 346 243 452 225  95 385 357 468 247 199 274 623 232 520 174 256  80
  31 197 164 354 382   4 601 543 149 531 293 285 287   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3442
INFO voc_eval.py: 171: [272 505 724 ... 497 920 926]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4790
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4051
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.358
INFO cross_voc_dataset_evaluator.py: 134: 0.604
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.543
INFO cross_voc_dataset_evaluator.py: 134: 0.573
INFO cross_voc_dataset_evaluator.py: 134: 0.474
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.521
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.364
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.377
INFO cross_voc_dataset_evaluator.py: 134: 0.706
INFO cross_voc_dataset_evaluator.py: 134: 0.621
INFO cross_voc_dataset_evaluator.py: 134: 0.591
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.384
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.479
INFO cross_voc_dataset_evaluator.py: 135: 0.405
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 7999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.406s + 0.002s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.371s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.382s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.381s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.385s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.387s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.388s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.384s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.384s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.389s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.391s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.396s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.396s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.483s + 0.001s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.405s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.394s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.391s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.394s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.401s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.409s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.409s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.405s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.402s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.400s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.402s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.398s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.432s + 0.002s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.375s + 0.003s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.362s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.384s + 0.003s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.382s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.378s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.379s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.382s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.383s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.382s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.383s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.383s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.384s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.519s + 0.008s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.400s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.397s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.386s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.378s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.381s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.383s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.383s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.382s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.383s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.384s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.382s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.383s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.582s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [580 181 901 ... 261 671 136]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3570
INFO voc_eval.py: 171: [206  98 711 825 487 786 641 797 499 827 475 829  99 645 836 644 551 122
 648 831 751 544 796 371 824  84 573 653 481 647 838 691 263  55 238  58
  44 811 552 480 696 106 672 619 388 837 308 701 826 203 427 670 801 527
 305 802 767 384 485 124 768  42 712 522 772 110 103 713 290 164 482 497
 545 167 373 742 490 764 812 852 546 747 310 364  59 754 543 628 695 208
 661 778 193 755 571 477 698 854 268 423 498 207 585  83 759 587 702 650
 576 860 843 758 416 791 109 642 285 717 476 823 479 761 861  56 335 483
 646 484 734 336  14 221 134 864 622 175 512 189 855 307 112 673 100  43
 262 753 529 557 807 724 437 107 478  51 806 500 531 820 699 737 803 325
 366  24 853 141 116 359 135 607 357  87 688 715 495 686 528 331  16 513
 555 220 125 289 163 385 139 174 144 334 609 472 306 612 678 301 494 863
 789 668 261  67 264 577 353 496 808 597 565 572 533 341 637 532 517 842
 832  45 718 222 710 275 115 526 119   4 741 592 700 470 685 694 665 722
 209 284 792 257 396 420 596 766 804 415 697 795 154 579  29 509 436 845
 828 235  94 493 589 756 270 525  63  60 212 834 278 570 851  74 674  23
 402 321 714 403 192 269 169 230 721 534 217 559 155 800   6 280 216 681
 117  40 105  15 389 111 657 126 488 147 858 372 426 723 683 611 664 839
 805 502 242 850 302 243 170  26  71 156 706 378 443 177  88 524 214 258
 327 118  34 354 343  36  18 299 171 666 430 798  47 536 501 227 300 630
 462 191 548 398 680 631 419  78 232 507 736 194 770 511 204 790 146 856
 254 669  76  54 246 857 370 337 866 633 329 442 413 391 752 783  85 229
  86 198 593  90 176 777   5 760 352 849 210 467 380 292 738 417 200  66
  72 314 379 340 187  17 729 574 234 190 471 250 237  92 172 145 785 595
 780 608  69 453  68 508 245 540 382 614 553  79 784 793 542 438 867 127
 799  30 659 523 266 407 441 225  48 188 690 776 859 746 809 324   1 114
 362   0 332  20 684 547 414 549 584 817  73 658 239 625 667 381  13 251
 833 703 735 294   8 267  46 654 594 663 868 185  93 739 617 315 361  35
 383 184  21 765 649 458  70  19 815  10 588  38 675 249 708 363 211 822
 421 309 273 434 504 322  11 459 745 425 205 810 720 298 386 651 635 178
 773 816 213 444 446 762 677 342 539 393 763 671 575 394 719 732 279 813
  89 162 530 288 692 643 223  41 610 166 265  97 365  80 291 639 165 505
 387 319 578 794 618 328 431 835 464 323  75 121 180 186 346 252 725 862
 240 466 412 601 821 774 201 433  65 865 231 451 744 173 123 564 226 599
  53 253 727 120 296 605 779 440 704 489 660 733 228 274 775 520 450 158
 199 348 689  82 142 563 432 320 814 662 769 138  95 313 101 224 707  22
 757  37   7 247 233 455 550 463 304 218 241  91  39 535  12 568  77  25
 287 409 181 847  33 395 161 474 368 283 293 390 656 344 244 377 558 787
 556 404 506 355 272 640 360 183 424 195  28 236 316 636 447 196 469 782
 581 168 830 634 730 312 428 561 679   3 297 259 676 408 295 303 560 367
  32 624 351  27 129 623 771 788 148 750 151 159 281 705 569 693 629 418
 108 397 819 422  50 848 339 582 326   2 197 521 537 586 256 358 215 743
 492 518 318 317 600 844 621 516   9 562  64 248 445 410 369 132 136 137
 583 153 219 626 541 429 652 461 255 620  96 411 311 460 350 356  52 149
 473 349 286 160 840 591 392 465 687 603 140 709  49 456 632 401 615 519
 606 276 345 731  31 330 726 113 104 102 454 510 406 728 638 439 566 580
 271 616 604 282 131 655 554 740 841 448 748 538 157 143 613 338 376 130
 375  81 749 152 515 781 260  61 333 179 399 405 627 457 128 846 486 468
 716 598 400 150 347 602 590 374 202 491 682 133  62 452  57 277 503 435
 182 567 449 818 514]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6034
INFO voc_eval.py: 171: [126 327 384 245 247 810 380   4 811 360 246 812 743 747 597 655 496 258
 492 129   7 250 744 304 815 298 347 882 813 281 386 182 670 833  16 664
 631 824 816 850 301 825 651 341 375 193 814 361 494  36 907 424 861 835
 661 486 897 902 547 669 749 822 362 429 366 299   9 427 817 218 508 586
 777 542 253 852 484 229 137 834 820  63 367 345  50 382 839 125  54 625
 154  95 572 687 717 909 828 493 282 836 284 476 694 823 497  79 159 381
 805 271 356 335 896 866 254 170 259 330 128 474 578 305  20 343 221 680
 269 782 249 290 624 510 894  37 124 325 152 637 865 150 151 122 563 183
 485 431 326 858 432 554 681 738 786  97 184  64 242  34 652 746  21   8
 158 117 294 732 531  56 141 893 650   6   5  61 600  81  65 490 520 577
 185 174 465 348 642 136 519 553 346 260 153 140 448 819  17 435 559 398
 387 397 288 401 803 393  94 682 161 363 499 235 467 216 556 134 640 555
 241 460 530 181  86 548 757 546 827 291 243 768 475 641 605 192 445 394
 186  14 770  71 766  80 365 400 854 818 210  11 571 388 779 623 785 501
 248 752  69  60 535 787 895 569 472  24 587 222 469 163 876 439  77 562
  38 878 123 521 238 302 759 320 575 549 142 751 692 313 368 806 143 758
 197 580 337  88 558 899 357 297 891 352 470 649 436 755 798 329 333 239
 742 739 539 612 463 663 473 228 832  67 699 211 792 561 461 514  76 390
 489 166 194 627 227 278 308 307 104  51 784 628 706  22 801 769 244  25
 689 171 541 686 855 162 729 842 551 189 657  59  70 917 527 528 350 883
  48 453 889  74 523 808  28 695 309 113  84 589 750 783 837 570 481 875
 838 560 613 240 544  39 169 132 601 176 764  35 419  40 480 261 321 871
 441 255 656 873  29 180   2 426 581 660 545 370 606 884 799 665 630 498
 550 846 582 720 643 690 358 618 139 905 908 730 392 482  66 120 275 513
 802  75  62 306 906 856 593 263  12 344 509 483 874 797 332 372 270 379
 913  23 292 266 300 364 214 524 667 579 901 130 133 715  43 658 565 775
 725 552 700 303 557 187 422 626 869  85 639 780  32  33 576 232 864 526
 821 564 709 477 236 617 733 538 287 879 890 148 402  57 731 668 517 647
 237 728  72 131 430 573  44 116 293 224 175 213 226 369 359 268 495 654
 457 676 262  93  55 103 334 440 567 503 566 804 157 568 167 201 351 487
 456 438 190  52 353 543 349 585 525 464 574 737 165 106 789   1 892 599
  89 629  41 912 315 701 449 444 653 383 274 276 280 339 598 434 918 168
 155 592  58 915 735 199 754 417  45 389 516 160 718 378 442  10 853 830
  73 591 454 588 443 479 355 672 396  13  53 619 857 638 433 697  96 761
 451 636 691 233 223 425 437 748 215 316 847 105 671 455 466 114 762 615
 420 217 478 188 841 209 712 118 707 251  31 265 127 412 231 795 677 204
 416 146 111 536 684 328 602 594 391 845 763 277 147 173 295 418 910 868
  82  87 726 446 604 888  15 324  30 512 807 264 678 863 788 407 202 674
 421 880  27 900 403 404 119 614 115 225 319 447 507 632 234 688 809 916
 172 713 740 781   0 164 610  68 385 371 776 529 156  91 719 760 336 208
 702 220 595  98 887 534 666 590 296 205  42  47 607 898 101 107 452 374
 354 765 230 848 406 267 727 662 844 584 506 756 203 500 318 696 373 112
 622 196 849 829 774 462 468 885 790 450 314 518 395 603 256 634 608 851
 736 342 502 796 322 877 279  90 794 415 177 609 722 311 705 428 522 596
 219 505 121 773 533 753 914 710 583 423 778 331 703 135 831 272  49 338
 620 724 734 537 659 532 886 673 145 399 791 459 102 800 413 867 693 745
 741  18 633 200 252 285 471 911 860 317 212  26 179  46 138 881  83 904
 377 675 611 286 310 826 340 771 793 207 414 644 862 716  78 323 698 289
 708 772 903 411 257 405 110 410 767 198 635 714 206 283 191  99 843 621
 616 723 645 646 721 685 711 100 108 704 540 312 376 408 409 195 872 178
 870 144 149 491 840  19 515 458 648 109 679  92 683 504 859 488 273   3
 511]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2242
INFO voc_eval.py: 171: [1372 1591 1373 ... 1867  214  782]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3518
INFO voc_eval.py: 171: [1432  830  883 ... 1238 1704  447]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5430
INFO voc_eval.py: 171: [ 74  47 139 142  19  51   9 224  49  90 179 380 156 284 294  55  33 357
 359  66 248 276 264 309 275 369 207 195  53 378 335 183 135 141 305  44
  52 379 290 299 205 353 261 113 271  56 316 177  80 237 187  58 221  48
 165 218  27  16 258 265   7  18  59 321 164 390 103 385 398 228 310 329
 311 332  22 161 151 391 128  92  95 283 153 147 251 115  34  89 287 214
  39 317 121  40 203 110 101 393 365 344 243 129  96  37 282 209 200 377
 210  28 245 229 392 395  57 144 241  68 136 381 345  84 206 212 188 286
 239  88  64 235 333 343 298 352 277 318 322 300 166 320 340  50 173 244
 289   4  82 198 176 104 325 204 160  70 314  91 361  17  12  54  23 301
 132 125  21  60 102 295 162 114  11 338 148 312 349 293 182 240 186 107
 346 178 252 337 315 140 368 272 246 250  38 231 383  24 109 386 124 358
 108 330 116 100 373 257 170 172  43 157 362  32 291  75 375 242 285 158
 190  46  87 342  15 180  26   8 270 232 196 334 120  73 273  85  30 260
 112 254 313 355 194 274  97 227 382 130 306 259 400  71 123 370   3 226
  41 149 336 111 220  61  86  42 138 266 288 171 319 354 356 234 154 262
 131 339 215  93 126 303 117 208 253 222 155 296 133  29 292 118 323  69
 202 364 193 267  78 211 331 159 219 189 134  72 192 225 268  76 281 217
 122 174 374 181 249 152  10 350 167  25 201 384 399 106 387  98  81 184
   2 278 327  35   0 146 143 223 199 269 185 191 163 351 105 256 371 326
 397 255  99 169 297 396  79 119  45  63 360  13 389 376 216 247 150 197
 263 137  62 236 230 308 394 145 328  31 175 233  36 348 280 304 324   5
 127  94 366  67  65 363  77 307 367 168  14 372 341  83 279 347   6  20
 302 388 238   1 213]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5734
INFO voc_eval.py: 171: [1092 1142  500 ...   15 1381  420]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4733
INFO voc_eval.py: 171: [ 11   6  24 117  25 105 114   8  45 109  40  62  83   1  82  85 111  58
 108  19  80  81  47  95  86  12  63  14  34  21  57  73   5 115 118  41
  29  49  54 110  10  33  96   7  60  72  78  44  37  30  87 106   9  20
  16  64 119  61  88 103  13  76  70  31  68  46  90  50  92  35 107   0
  32  53  91  71  94 116 104  77  79  48  36  66  27  93  22  18   2  69
  28   4  98 120  84  75  39  17  51  52 112  67  74  23 101  65  42  89
 102  38  56  99   3 100  43  26  55 113  15  97  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0057
INFO voc_eval.py: 171: [ 643 5819 2526 ... 5442 6313 2599]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5213
INFO voc_eval.py: 171: [  4  77   5 129  47 104  46  43 114 143  76  50 139 145  97  66  78  48
 146 106 113  40  92   8 121 134  34 108  63 158 133  93  33   9  69  61
 112 116  94  53   6 140 120  83 118  45 156 100  82  80 132  41 142 144
  85  49   3 131 101  14  17  58  86  95  44  18 107  27  52  54   7 141
 152  39  22  73  89  81 137 128 103 135 105 136  60  57 122 153  75  25
  38  59  26  91 119 130  56  90  11  64  32 138  96  42 115  30  62  28
 117   1 111 150  79  21  87 109   0 155 125 147 151  98  51  55  99  74
  31   2  20  10  88  29 148  35 127  37  68 126  70  12 149  84 124 110
  23  72 123  67 102  24  71  65  19  13 154 157  15  16  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1818
INFO voc_eval.py: 171: [ 877 1923  472 ... 2202  823 1366]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3634
INFO voc_eval.py: 171: [106  98 191  99   3 153 103  19  57 107 211   7  63  41  37  47 209  59
  68  93  34 208   1 189  28 195  12  81  82  67  13  58 168 164 133 145
 155  38 120 154  92 138  35 203 205 222  91 125 223  74  49 175 201  61
 213 126 156  10  87 192 218 160 121 117  15 224  11 190 104 100 170 118
  66 198  27 124  76  69 147 187 129  46 128 194 130 146 132  64 159 135
 122 105 210  40 152 176 123   5 111  86 114   2  16 109  24  85 150  65
 177  44 112 148 200 113  14  72 172  52 206  23 127 188 204 108  30 140
  60  97  77 193 196   8 143  70  32 171 131 169 167   4 165  83  79  48
  42 158 174  71 166  33  26  45 173 182  17 142  21 225  78  22 220 102
 163   0 207 217  25 179 115  62 137  96  90 185 212  73 110  50  75 184
 161 221  56  31  84  88 101  29  36  95   6 214 216  54 215   9  18  53
 197 119  89 139 144 219 181  43 186 151 116  39  80 141 199  94 157 134
 183 149  51 136 180 178  20 162  55 202]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1167
INFO voc_eval.py: 171: [146 129  29  22 173  96 132 183 128 147 190 154  65  24  34  78  23 122
 105 178  66 126 101 149 102 156 171 170 108  71   9 169 157  10 130  70
 131  82  55 111 112  87  50  49  36  33 152  64  90  47  59 179   5 133
  42  44 124 163 100   1  79  56  97  12  76  48   2 150 175 153 167 144
 145  67 181 123 168 107  16  91  77   0  80 114  27  32  84 148 166 164
 185  85 135 187 177 134 158  41  69  72 160 188  51 106 172  52 180  38
  95 155  53  63  28 186 127  13  98  11  93   7 104  35  14  18  19 182
 138 136 162  62  25 137  21  99   3 110 142   8  15 151  89 116  26  92
 113 174 191  81 125  61 176 141  83 117  20 121 119  37 115 120  39  94
 143  73  43 103  68  17 159   6  40  88 165  30  46 140 184 139  57 189
  54  60  75  31  58 118 161  45  86  74   4 109]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3773
INFO voc_eval.py: 171: [ 95 542  73 354 277 584 369 259 393 417 495 425 524 438  50 109  28 290
  24 370 467 408 522  96 267 304 265 562 592 145 183 591 168 231 134  67
 223 184 483  11 174 486  64  60  99 413 201  87 445 166 303 470 335 527
 172 598 260 582 387 305 215 345 334 357 478 410 135 328 586  75 306 274
 248 488 158 398 583 344 553 490 114 545 497 113  70 523 204 322 480 443
 538 282 203  55 173 295 446 590 208 125 342 421 401 225 371 359 521 198
 472  43   3 314 220 437  19 373 275 593 324 544 126 464 180 105 299 157
 315  40 123 411 154  44  57  32 594 533 573 195 312 381 388 601 189 429
  86 273  91 106  65 406 493 525 485 548 572 127 239  29 428  82 596 318
 213 156 412 448 268 481 506 142 399 240 579 129 346 375 227 391 116 529
 286 191 556 252 150 350 540 121 348 224 526 560 323  69  93 498 500 301
 595 214 185 255 367 205 439 597 395 148 534 574 396 537 258 469 531 509
 420 530  45  56 285 449  15 177 392 179  27 561 289 394 307 297 138 440
 317 340 458 390 489 536 569  61 484 511 587 432 264 496 589  68 447 122
   1 294 355 100 296 155 353 580 513  80 102 190  18 236  81 316 382 588
 108 565 266 384  21 200 431 175 504 505 476 170  94 535 528 235  12 402
  79 585 426 238  42 422 327 333  16 311 570 319 475 403 424 339 397 551
 415  26 600 216  92 501 130 186  14  31 151 479 325  13  97 365 261 229
 337 383 187   9  34 539 418 164 451 124 452 247 462 519 442 482 196 219
 120 326  39  17  63 308 563 244 136 140 377 571   8 456 547 117 378  71
 132 471 169 188 444 532 567 171 466  59 356 380 349 280 541 160 512 450
  98 419 568  30 243 133 147 218 101 336 343 423 234 193 159 278 288 107
 300   4 309 514 520 576  72 139 557  41 454 143 131  46 330 358 263 491
 320 463 103 194  22 515 209 518 249 400 376 242  52 414 197 212 146 182
 404 499 372 291  84  36 149 427 362 271 241 494 468 211 503 287 110 389
 292 250 226 144 272  37 176 543 363  33 465 441 352 351 558 602 284 281
 162 366 453 435  85  77 152 141 118 217  10  62 599 564  58   2 405 112
 128 167 549 434 161 492 245 192   7  38  47 546 283  88 575   0 341 433
 329 222 360 256 368 262 385 206  20  48 559 153 276 270 430 119 487 477
 508  53   6 460 473 178 502 507 237 332 232 455 581 338 517 554 364 347
 246 111 461  51 457 233 310 279 550 254 510 269 181 104 566 199 253 251
 207 331  23 379 221 409  54 577 165 578 298  78 436  25  66  89 210   5
 552 257 516  76 407  74  90 313 163 202  49 293 230 416 302 459 228 361
  83  35 137 374 321 555 474 115 386]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7061
INFO voc_eval.py: 171: [11600  2078   605 ... 11619  7176 10823]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6193
INFO voc_eval.py: 171: [ 342  435  667 ... 1440 1127 1445]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5899
INFO voc_eval.py: 171: [126   3 120   4  67  72 151 105  96 103 180  19 144 119 149  25  71  60
 135  69 158 118  68  45  87  20 130  43  65  27  66   6 166 150 121  70
 111 123  47 155  46 152  32 170  35  24 174  11  38 177  50 142  41  78
  76  83  16 133  23  10 184  59  63  28  37 143 107 137 104  90 159   7
  86  73 162 134 129 176 114  82 172 164  56  84   2  85 127  51  34 112
  14  98  81  22  53  74  99 145 122  79 154  42 146  80  77  29  58  75
 132 113 156   0  33   9 163  21   1  39  62  49  61  88 140 167 106 115
 138   5 141 101 157 153  30 125 117 160  13 165  54 171   8  89  64  97
  92  18  48 102 147  12 182 169  44  91 175  15 173 109  36  52 100  94
 161 168  40  17 124  93  57 136  26 181 116 148  31 139 110  95 178 179
 108  55 183 128 131]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2825
INFO voc_eval.py: 171: [206 203  71 278 298 222 432 143 224  16 165 336 228 313 297 319  75 208
 205 380 191 204 413 174 355  96 318 283 213 414 175 430 249  17 115 207
 258 188 266 142  80  67 251 245 400 126  89 137 256  49  93 155 226 401
 162 172 120 321 361 138 320 141 438 362 439  60 177 350  40 379  81 122
 158 363 210 337 125 314  21 296  73  94 301 171  41 356 268 211 230 291
 116  35 257 200 261 187 227 407 367 136 333  18 284 170  66 239 133  31
 145 431 287 366 232 421 110 338 307  23 212 259 417 128 352   4 219 237
 129 316 317 193 328  56 176 395 382  32  85   5 372 409 365 433  68 411
 250 185   3 103 434 114  46 332  42 231 286 135 275 424 153 440  83 124
 429  77 386  36  51 368 370 277 391 294 325 273 308 428 262 343 340 269
 220  82 295 349 144 182  19  37 288 300 324 312 378 105 221 246 180 416
 397 342  11  55 225 359 253 248 199 327 310  54 369  44 394 131 216 127
 112  79 178 293 108   7   6 238 425  30 156 396 353  97 189 173 148 229
 161  74 247  13 374  45 140 412  57 179 426 134  64 398  25   8  62 260
 402 280 151  48 436 347  29  61 311 121  92 405 302 163  72 130 157  91
  22 290 223 152 437 274 289 420 190 329 354 322 377 233  38 384 388 139
  26  50 335 351 164 305 263 146 393 360  24 101 100 285   2 215 186 236
 270 265 364 373  12 243 197 123   9 389 255 166 309 358 376 419 159 209
 168 304 390 150  78 357 406  28 423  59   0 147 267  98  84 218 303 195
  90  88 194 111  99 132 234 415 418 118  95 202 241  27 154 160 184 282
 404 383 109 117 399  39  63  69  70 192 201 326 106 339 240 107 102 235
 375 276 392 272 242  14 410   1 292 348 198 167 371 279 149 315 341  52
 271 344 264 113  53 403  86 299 183  47 254 214 346 435 408  10 422 196
 427 387  20  58 323  15 381 385  43 252 181  34 169 306  65 244 345  76
  87 281 104 334 217  33 330 331 119]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3849
INFO voc_eval.py: 171: [425 269  73  83 139 363 228 138 112 426 143 310 313  75 229 368 119 277
 240 586 339 168 142  82  52 556 366 348 477 274 538  67 484  77 362 271
 598 199 116 108 117 268 333 478 144 301 499 218 314  98 249 545 600 230
 298 103 297 396 215 596  72 217 568 614 620 398 145 567 364  99 494 404
 113 263  13 399 122 486 120 299 355 205 252 204 200 166  16 594 137  66
 326 554 347 191 617 342 241 308 194 467 315 444 121 421 358 512 365  57
 262 272 161 155 427 340 104 619   5 176 522 448 195 293  21 219  45 114
 455 169 616 319  71 561  22 578 236 624 613 431 361  69 320 510 441 564
   8  53 221 553 485 481 489 610 451  70 505 385 458 414 433 609 160 258
 369 571  92 174  63 335 171 170  38 483 165 350 289 376  76 548 500 560
 296 482 434 330 251 402 612 559 311 591 479 392  30 519 415  60 201 497
 436 532 354 304 511 387 472 359 305  46 622 302 265 128 471 530 593 129
 278 312 535  93 150 492 216 393 259 300  17  47 226 141 287 507 487 480
  29 157 175  62 203 123  96 151 245 454 473 185 416 167 352 328 411   6
  42  68 189 490 177 621  43 134 307 546 457 603 110 127 382 248 243 295
 254 470 565 213 105 557  35 232 582 234   7 106 615 379 146 101 374 459
 172 370 443 334 584 124  23 440  11 220 524 318 439 475 476 516 324 563
 107 136 527 373 562 570 536 551 488 317 599 580 109 606 140  24 182 445
 583 306 432 192 450 394 555 184 291  34 111 279 417 331 147 389 388 253
 159 164 406 349  54 447 518 590 285 179 188  18 336 604 303  58 178 377
 496 424 397  50 323  27 469 102 357 592  88  19 130 587 115 558  91 316
 521   9 513  49 618 585  41 378 400 405 309 162 410 118 332 420 237 493
 338   1 100 208 222 367  56 523 321 581 503 131  12  28 529 438 588  39
 257  40  94  87 569 266 223 437 547  61 442 589 625 183 401  86 412  84
 210 526 275 491 270 508 408 514 542 186  89 152 190 344 577 390  55 552
  14  20 294 156 461 235 202 153 525 605 429 181 290 608 498 409 607 419
 464  78  25  97 435 322 351 225 380 337 180 504  32  90  74  81  44 575
 540 187 449 209 602 391 539  51 597 233 239 502 495 460 329 281 133  10
 544  36 207 360  15 572 574 501  48 325 341 446 154 212 456 541 407 276
 211 595 264 537 343 550 528 125 206 193 132 244  79 238 576 611 267 327
 430 386 573 283 515 371 566   3 506 197 261  85  64  65 214 256 282 346
 534 383 227 260 250 509 533 403  26 465 280 428 149 466 247 395 413   2
 372 375 422 517 418 453 462 463 474  59 135  37 345 549 242 452 126  33
 468 288 579 273 158  95 246 623 384 224 356 198 231   4 353 163  31 173
 196  80 292 601 255 520 543 531 284 381 148 286 423   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3440
INFO voc_eval.py: 171: [273 506 725 ... 498 167 932]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4803
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4050
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.357
INFO cross_voc_dataset_evaluator.py: 134: 0.603
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.543
INFO cross_voc_dataset_evaluator.py: 134: 0.573
INFO cross_voc_dataset_evaluator.py: 134: 0.473
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.521
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.363
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.377
INFO cross_voc_dataset_evaluator.py: 134: 0.706
INFO cross_voc_dataset_evaluator.py: 134: 0.619
INFO cross_voc_dataset_evaluator.py: 134: 0.590
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.385
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 135: 0.405
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 8499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.561s + 0.002s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.378s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.402s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.403s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.403s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.398s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.397s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.395s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.391s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.393s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.392s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.392s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.394s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.426s + 0.002s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.390s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.395s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.393s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.391s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.394s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.406s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.402s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.401s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.399s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.396s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.395s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.393s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.566s + 0.002s (eta: 0:01:10)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.414s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.402s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.401s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.400s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.400s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.393s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.395s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.392s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.390s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.386s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.386s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.385s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.517s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.360s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.365s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.361s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.364s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.368s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.367s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.366s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.372s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.374s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.373s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.375s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.375s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.145s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [580 181 902 ... 136 663 261]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3570
INFO voc_eval.py: 171: [206  98 710 824 487 785 640 796 499 826 475 828  99 644 835 643 551 122
 647 830 750 544 795 371 823  84 573 652 481 646 837 690 263  55 238  58
  44 810 552 480 695 106 671 618 388 836 308 700 825 203 427 669 800 527
 305 801 766 384 485 124 767  42 711 522 771 110 103 712 290 164 482 497
 545 167 373 741 490 763 811 851 546 746 310 364  59 753 543 627 694 208
 660 777 193 754 571 477 697 853 268 423 498 207 584  83 758 701 586 649
 576 842 757 859 416 641 790 109 285 716 476 822 479 760 860  56 335 645
 483 484 733 336  14 221 134 863 512 621 175 189 307 854 112 672 100  43
 262 752 529 557 806 723 437 107 478 805 500  51 531 819 698 736 802 325
  24 366 852 141 116 359 135 606 357  87 714 687 495 528  16 685 331 513
 555 125 220 289 385 163 139 174 144 334 608 472 677 306 611 494 862 301
 667 788 261 264  67 577 353 496 807 596 565 533 572 517 341 841 532 636
 831  45 222 717 709 275 115 526 119   4 740 591 470 699 684 693 664 284
 721 209 257 396 791 420 765 595 803 415 696 794 154 579  29 436 509 844
 588  94 493 827 235 755 270 525  63  60 212 278 850 570 833  23  74 673
 321 402 713 403 192 169 230 269 534 720 217 559 799 155 280   6 216 680
 117  40 105 656  15 389 147 111 126 372 426 857 488 682 722 804 610 663
 838 302 849 242 502 243 170  26 156  71 177 443 705 378 327  88 214 354
 258 524  34 118 343  18 171  36 299 797  47 430 536 665 227 501 300 629
 462 548 398 630 679 191  78 419 507 735 204 232 769 194 511 146 855 789
 254 668  76 337  54 246 856 370 865 632 329 442 413 391 751  86 782 592
 229  85 198 759 776 176 352   5  90 848 210 292 467 380 200 737  66 314
  72 417 187 340  17 379 574 728 234 172 471 250  92 190 237 594 145 779
 784  69  68 553 783 453 607 542 382 540 613 245 508 792  79 798 438 127
 441 866 407  30 266 658 523 188 775  48 225 689 858 324 808 745 114   1
 362   0 332  20 683 547 414 549 583 816  73 657 239 624 666 381  13 251
 832 702 734 294   8 267  46 653 593 662 867 185 738  93 616 383 315  35
 361  21 184 764 648 458 587  10 814  19  70  38 363 707 249 425 459 674
 309 211  11 322 821 744 504 273 434 421 205 650 815 386 719 446 298 809
 213 634 178 772 444 761 393 676 342 539 394 670 575 718 279 762 731 812
 288 162 609  89 530 223 642 691  41 505 291 387  80  97 638 265 165 166
 365 793 319 617 578 834 431 328 323  75 346 724 464 121 252 180 186 861
 466 820 773 240 600 412  65 864 231 743 201 433 451 173 123 564 598  53
 726 226 253 659 296 604 489 120 440 778 703 228 450 732 520 274 774  82
 199 348 320 688 563 142 158 138 432 661 768 813 233 313   7  95 224 101
 247 706  22 756  37 455 550 304 218 463  91  12 568 535 241  39  77  25
 287 395 474 181  33 368 409 161 846 390 283 558 786 293 244 424 655 377
 556 344 272 183 195 355 404 506 360 639 635 236  28 561 316 312 168 469
 447 678 428 580 633 829 729 196 781 408   3 623 351 675  32 259 303 297
 367 295 560 622  27 129 787 770 281 628 749 148 159 847 692 151 704 569
 422 108 418  50 818 397 339 581 197 521 537   2 326 358 742 620 256 516
 562  64 518 215 317   9 599 585 843 318 492 445 248 410 153 461 541 369
 625 132 255 137 136 219 429 582 619 651  96 460 411 311 350  52 839 149
 473 349 286 356 160 392 602 590 140 686  49 708 465 519 456 631 401 614
 276 605 102 104 454 113 730 282 345 330 725  31 271 603 654 566 439 637
 615 554 727 406 131 510 747 338 157 840 375 376 143 448 739 130 612 538
  81 128 515 780 399 457 845 333 626 179 748 405 152  61 260 150 597 400
 347 715 486 468 589 601 452 491  57  62 277 681 503 202 133 435 374 567
 817 449 182 514]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6034
INFO voc_eval.py: 171: [126 328 385 246 248 811 381   4 361 812 247 813 744 748 598 656 497 259
 493 129   7 251 745 305 816 299 348 883 814 282 387 183 671 834  16 665
 632 825 817 851 302 826 652 342 376 194 815 495 362  36 908 425 862 662
 836 487 898 903 548 670 750 823 363 430 367 300   9 428 818 219 509 587
 543 778 254 853 485 230 137 835 821  63 368 346  50 383 840 125  54 626
  95 155 573 688 718 910 829 494 283 837 285 477 695 824 498  79 382 160
 806 272 336 357 867 897 255 171 260 128 331 475 579  20 306 344 222 681
 270 291 783 250 625 511 895  37 124 326 866 153 638 151 152 122 564 184
 486 432 327 859 433 739 555 682  97 787 185  64  34 243 653 747  21   8
 159 117 733 295 532 141  56 651 894   5   6  61  81  65 601 491 521 578
 175 466 186 643 349 554 136 520 347 140 154 261 820 449  17 436 560 399
 398 388 289 804 402 394  94 683 500 236 162 364 468 641 557 134 217 461
 242 556 531  86 182 547 758 549 828 769 244 292 476 642 606 193 446 395
 187 771  14  71 767  80 855 366 401 211 819  11 389 780 572 624 786 502
 249  60  69 753 788 896 536 570 473 588 223  24 470 164 877 440 563  77
  38 879 522 123 303 239 760 321 576 142 752 550 693 314 369 143 807 198
 759 581  88 338 298 559 358 900 471 892 437 650 353 756 330 799 334 240
 743 740 540 613  67 464 474 664 229 833 212 793 700 562  76 462 391 167
 515 195 490 279 628 228  51 309 308 104 707 785 629  22 770  25 690 163
 245 802 172  59 190 542 687 856 552 730 658 528 843 918 529  70 351 884
  48 454 890  74 524  28 809 784 310 590 751 113  84 696 561 876 838 482
 571 839 614 241 545  39 170 132 765  40  35 602 177 322 872 420 262 481
 657 874 442 256 181 427  29 582   2 661 546 885 666 371 607 499 800 847
 583 551 631 721 644 359 691 139 619 483 393  66 731 909 906  75 803 264
 120 514 594 276 798  12 857 307  62 907 267 875 510 484 333 345 914 373
 215 380 301 271  23 293 365 902 668 130 525 580 716  43 659 133 566 701
 776 726 553 627 423 640 558 188 870  85 304  33 577 781 478 233 865 527
  32 822 710 565 237 539 734 288 618 669 729 403 880 148 732  57 891  72
 518 238 648 225 214 131 116 574 431  44 294 176 269 227 360 370 655 458
  93 496 263 441 677  55 335 103 202 567 568 805 504 158 168 569 457 352
 488  52 544 354 439 191 526 586 350 465 106 893 600 166 790 575   1 738
  89 913 316 702 630  41 654 384 156 755 277 340 599 281 275 169 919 445
 450 435 736 593 200 916 161  58 390 719 418 517 592  45  13  10 455 854
 831  73 589 379 443 444 356 673 480 397  53 620 858 639 749 434 637 698
  96 452 762 426 234 692 216 224 438 105 672 848 317 456 763 114 467 708
 210 616 189 218 842 118 479 713 421 603 127 413 252 205  31 266 232 678
 796 417 392 685 146 111 329 595 869 537 147 764 174 419 846  82 296 911
 278 447 727 605 325 679  30 265  15 889 513  87 808 789 422 864 675 408
 203 881 115 901 615 119 320 405  27 404 226 917 235 508 810 689 633 448
 173 782 741 714   0  68 611 209 530 165 386 372 337 221 596 157 761 720
 777  91 535 703 899  42 297 206 591 888 667  47  98 608 453 375 107 101
 766 355 268 407 849 231 663 728 623 501 697 112 451 197 319 204 845 585
 757 830 507 635 850 374 775 886 519 791 396 257 463 469 609 315 604 323
 343 503 797 737 852 795  90 416 280 534 878 610 312 178 723 121 429 774
 506 706 754 523 220 597 868 332 135 704 584 711 424 915 779 801 832 621
 102 735 660 538 674 725 887 145  49 533 273 792 400 414 339 460  26 201
 912 861 694 213 253  18 742 472 286 634 180 746 318 882 341 772  46 676
 827 905 287 378 794 612  83 138 311 699  78 717 324 863 290 415 208 645
 709 411 715  99 412 110 904 199 406 207 646 636 258 284 773 313 768 192
 108 100 705 377 149 541 647 144 492 724 722 410 409 617 680 844 196 686
 871 179 873 712 622  19 649 684 505  92 841 109 516 459 512 489 274 860
 150   3]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2243
INFO voc_eval.py: 171: [1374 1594 1375 ...   71 1125  500]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3518
INFO voc_eval.py: 171: [1433  830  883 ...  433 1239 1705]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5422
INFO voc_eval.py: 171: [ 74  47 140 143  19  51   9 225  49  90 180 381 157 285 295  55  33 358
 360  66 249 277 265 310 276 370 208 196  53 379 336 184 136 142 306  44
  52 380 291 300 206 354 262 113 272  56 317 178  80 238  58 188 222  48
 166 219  27 266  16 259   7  18  59 322 165 391 103 386 399 229 311 330
 312 333  22 162 152 392 128  95  92 154 284 148 252 115  34  89 215 288
  39 318 121 204  40 110 101 394 366 345  37  96 129 244 210 283 201 378
 211  28 246 230 393 396 145  57 242  68 137 382 240 207  84 213 346 189
 287  88  64 334 236 344 299 278 353 319 323 301 167 321  50 341 174 245
 290   4  82 104 199 177 161 315 205 326  70  91 362  17  12  23  54 302
  21 133  60 125 102 296 163 114  11 339 149 313 350 294 183 241 187 107
 347 179 253 141 338 316 251 273 369 247  38 232 384 109 387  24 124 108
 331 359 116 100 374 258 173  43 171 158 363  32 292  75 159 286 243 376
  46  87  26  15 181 343 191  73 335 233 120   8 271 197  85 274  30 112
 261 255 314 356 195  97 130 228 275 383 307 260 401 371  41 221  71 111
   3 150 227 337 123  61 139 289  42  86 267 172 320 355 357 304 263 235
 340 155 132 117  93 216 126 209 223 254 156  29 297 134 118  69 324 293
 203 365 194 212 268  78  72 332 220 135 160 190 269 226  76 282 193 175
 182 122 218 375 202  10 250 351 168 153  25 106 185 279 385 400  98 328
 388  81   2   0 147  35 200 144 224 270 164 186 192 352 372 105 257 256
  99 327 398 119 170 298 397  79 377 361 151  63 217  13  45 248 390 198
  62 264 237 138 234 146 309 231 395 329  31  36 176  65 325 305 281 349
 127   5 367 364  94  67  77 169 308 368 373  14  83 342 280   6 303 348
  20 389 239 131 214   1]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5735
INFO voc_eval.py: 171: [1092 1142  500 ...  978 1381  420]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4738
INFO voc_eval.py: 171: [ 11   6  24 117  25 105 114   8  45 109  40  62  83   1  82  85 111  58
 108  19  80  81  47  95  86  12  63  14  34  21  57  73   5 115 118  41
  29  49  54 110  10  33  96   7  60  72  78  44  37  30  87 106   9  20
  16  64 119  61  88 103  13  76  70  31  68  46  90  50  92  35 107   0
  32  53  91  71  94 116 104  77  79  48  36  66  27  93  22  18   2  69
  28   4  98 120  84  75  39  17  51  52 112  67  74  23 101  65  42  89
 102  38  56  99   3 100  43  26  55 113  15  97  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0057
INFO voc_eval.py: 171: [ 644 5827 5828 ... 4554 6854 2878]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5208
INFO voc_eval.py: 171: [  4  77   5 129  47 104  46  43 114 143  76  50 139 145  97  66  78  48
 146 106 113  40  92   8 121 134  34 108 133  63 158  93  33   9  61  69
 112 116  94  53   6 140 120  83 118  45 156 100  82  80  41 132 142  85
 144  49   3 131 101  17  14  58  86  95  44  18 107  52  27  54 141   7
  39 152  22  73  81 137  89 128 103 135 105  60 136  57 153 122  75  38
  25  59  91  26 119  56 130  90  11  32  64  96 138  42 115  28  62  30
 117   1 111  21 150   0  79  87 109 155 125 147  51 151  98  55  99  74
  31   2  20  10  88  29 148 127  35  37 126  68  70  12 149  84 124 110
  23  65  24 123  71  67 102  72  19  13 157 154  15  16  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1818
INFO voc_eval.py: 171: [ 878 1925  472 ... 2204  916 1246]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3645
INFO voc_eval.py: 171: [106  98 191  99   3 153 103  19  57 107 211   7  63  41  37  47 209  59
  68  93  34 208   1 189  28 195  12  81  82  67  13  58 168 164 133 145
 155  38 120 154  92 138  35 203 205 222  91 125 223  74  49 175 201  61
 213 126 156  10  87 192 218 121 160 117  15 224  11 190 104 100 170 118
  66 198  27 124  76  69 147 187 129  46 128 194 130 146 132  64 159 135
 122 105 210  40 152 176 123   2   5 111  86 114  16 109  24  85 150  65
 177  44 112 148 200 113  14  72 172  23  52 206 127 188 204 108  30 140
  60  97  77 193 196   8  70 143  32 171 131 169 167   4 165  83  79  48
  42 158 174  71 166  33  26  45 173 182  17 142  21 225  78  22 220 102
 163   0 207 217  25 179 115  96  62 137 185  90 212  73 110  50 161  75
 184  56 221  31  88  84 101  29  95  36 216 214   6  54   9 215  18  53
 119  89 197  43  39 186 219 181 139 144 151 116  80 141 199 157  94 134
 183 136 149  51  20 178 180 202 162  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1167
INFO voc_eval.py: 171: [146 129  29  22 173  96 132 183 128 147 190 154  65  24  34  78  23 122
 105 178  66 126 101 149 102 156 171 170 108  71   9 169 157  10 130  70
 131  82  55 111 112  87  50  49  36  33 152  64  90  47  59 179   5 133
  42  44 124 163 100   1  79  56  97  12  76  48   2 150 175 153 167 144
 145  67 181 123 168 107  16  91  77   0  80 114  27  32  84 148 166 164
 185  85 135 187 177 134 158  41  69  72 160 188  51 106 172  52 180  38
  95 155  53  63  28 186 127  13  98  11  93  14 104  35   7 182  19  18
 162 138 136  25  62  99 137  21   3 110 142   8  15 151  89 116  26  92
 113 125 174 191  61  81 176  83 141  20 117 121 119  37 115  39 120 143
  94  73 103  43  68  17   6 159  40  88 165  30  46 140 184 139  57 189
  54  60  75  31  58 118 161  45  86  74   4 109]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3773
INFO voc_eval.py: 171: [ 95 542  73 354 277 584 369 393 259 417 495 425 438 524  50  28 109 290
  24 370 467 408 522  96 267 304 265 562 592 145 183 591 168 231 134  67
 223 184 483  11 174 486  64  60  99 413 201  87 445 166 303 470 335 598
 527 172 260 582 387 305 215 345 334 357 478 410 135 586 328  75 306 274
 248 488 158 398 583 344 553 490 114 545 497 113  70 523 204 322 480 443
 538 282 203  55 173 295 446 590 125 208 342 421 401 225 371 359 521 198
 472  43   3 314 220 437  19 373 275 593 324 544 126 464 180 105 299 411
  40 157 315 123 154  44  57  32 594 533 573 195 381 388 312 189 601 429
  86  91 273 106  65 406 525 485 493 548 127 572 239  29 428  82 156 213
 596 318 412 268 448 481 506 142 399 240 129 579 346 375 227 391 116 191
 529 286 556 252 350 540 150 121 348 560 224  69 526 323  93 498 500 301
 595 185 214 255 367 205 439 597 395 148 534 396 574 469 537 258 531 509
 420 530  45  56 285 449  15 177 392 179  27 307 297 394 561 289 138 440
 317 340 458  61 489 536 390 484 569 587 511 264 432 496  68 589 447 355
 122 294   1 100 155 296 353 580 190  80 102 513  18 236  81 382 316 108
 266 565 588 504  21 175 200 431 384 505 476  94 535 170 528 402  12 235
 585 333 422 327  79 426  16 238  42 570 311 319 403 475 424 339 397  26
 600 551 415 216 501 130  92 186 151 479  31  14 325  13  97 229 365 261
 337 187   9 383 418 164 451  34 452 539 124 519 247 462 482  39 219 326
 120  17 196 442  63 563 308 244 136 140   8 571 377  71 117 456 547 378
 471 132 188 169 532 444 171 466 567 356  59 380 280 541 349 568 160 512
  30 243 450 133  98 419 336 343 234 147 423 218 101 309 107 159 278 300
 514 288   4 193 520 454  72 557 139 576 131 143  41 358 263 330  46 320
 103 194 463  22 491 209 518 249 515 400 376 242 212 197 404 182  52 146
 414  84 149 499 372  36 291 362 427 211 271 287 503 241 494 468 110 389
 292 176 272 226 144  37 250 543 441 352 351 465  33 363 558 602 284 281
 162 366 453 435  85  77 152 141 128 118 217  62   2 599  10 167 161 549
 564 112 405  58 434 492  38 546 192 245  47   7 575 283  88   0 341 433
 329 222 360 256 368 262 385 206  20  48 559 153 270 276 430 119 487 477
 508  53   6 178 332 460 473 502 237 507 581 232 455 338 554 461 364 111
 246 517 347 254 550  51 279 269 510 233 457 310 104 566 331 253 251 181
 199  23 207 379 221 409 577 298 165  54  78 578  25 436  89 210  66   5
 552 516  76 257  74 407  90 202 163 313  49 293 416 459 228 302 230  35
 137 374 361 321  83 555 474 115 386]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7085
INFO voc_eval.py: 171: [11605  2078   866 ...  7602  1660  1268]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6191
INFO voc_eval.py: 171: [ 342  435  667 ... 1445  159 1956]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5907
INFO voc_eval.py: 171: [126   3 120   4  67  72 151 105  96 103 180  19 144 119 149  25  71  60
 135  69 158 118  68  45  87  20 130  43  65  27  66   6 166 150 121  70
 111 123  47 155  46 152  32 170  24  35 174  11  38 177  50  41 142  78
  76  16  83 133  10  23 184  59  63  28  37 143 107 137 104  90 159   7
  73  86 162 134 129 176 114  82  56 164 172   2  84 127  85  34  51 112
  14  22  81  98  99  53  74 145 122  79 154  42 146  80  77  29  58 113
 132   0  75  33 156   9 163  21   1  39  62  49  61  88 140 167 106 138
 115 141   5 157 101 153 125  30 165  13 160 117  54 171   8  89  92  97
  64  18 102  48 147 182  12 169  44 175  91  15 109  36  52 100 173  94
 161 168  93  40  17 124  57 136 148 181 116  26  31 139 110  95 178 179
 108 183  55 128 131]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2825
INFO voc_eval.py: 171: [206 203  71 278 298 222 432 143 224  16 165 336 313 228 297 319  75 208
 205 380 191 204 174 413 355  96 318 213 283 414 175 430 249  17 115 188
 207 258 266 142  80  67 251 245 400 126  89 137 256  49  93 155 226 401
 162 172 120 321 361 138 320 141 438 362 439  60 177 350  40 379  81 122
 158 363 337 210 125 314  21 296  73  94 301 171  41 356 268 211 230 291
 116  35 257 200 261 187 227 407 367 136 333  18 284 170  66 239 133  31
 145 431 287 366 232 421 110 338 307  23 212 259 417 128 352   4 219 237
 129 316 328 317 193  56 176 395 382  32  85   5 372 409 365 433  68 411
 250 185   3 103 434 114  46 332  42 231 286 135 275 424 153 440  83 124
 429  77 386  36  51 368 370 277 391 294 325 273 308 343 428 262 340 269
 220  82 295 349 144 182  19  37 288 300 324 312 378 105 221 180 246 416
 397 342  11  55 225 359 253 248 199 327 310  54 369  44 394 131 216 127
 112  79 178 293 108   7   6 238 425  30 156 396 353  97 189 173 148 229
 161  74 247  13 374  45 412 140  57 179 426 134  64 398  25   8  62 402
 280 260 151  48 436  29 347  61 311 121  92 405 163 302 130  91 157  72
 290 223  22 152 190 274  38 289 420 437 329 354 322 377 233 388 384  50
 139  26 335 263 305 164 351 393 146 360 101  24   2 100 285 186 215 236
 270  12 265 373 364 197 389 123 243   9 255 166 309 358 376 419 209 159
 168 304 150  28 406 390 357  78  59 423   0 147 267  98  84 218 303 195
  90  88 194 111  99 132 234 415 418 118  95 202 241  27 154 160 184 282
 404 383 109 117 399  39  63  69  70 192 201 326 106 339 240 107 292 102
 235 375 276 392 272 242  14 410   1 348 198 167 371 279 149 315 341  52
 271 344 264 113  53 403  86 299 183  47 254 214 346 435 408  10 422 196
 427 387  20  58 323 306  15 381  43 252 385 181  34 169  65 244 345 334
 281 104  87  76 217 119  33 330 331]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3847
INFO voc_eval.py: 171: [425 269  73  83 139 363 228 138 112 426 143 310 313  75 229 368 119 277
 240 586 339 168 142  82  52 556 366 348 477 274  67 538 484  77 362 271
 598 199 116 117 108 268 333 478 144 301 499 218 314  98 249 545 600 230
 298 103 297 396 215 596  72 217 568 620 614 145 398 567 364  99 494 404
 113 263  13 399 122 486 120 299 205 355 252 204 200 166  16 594 137 326
  66 554 347 191 342 617 308 241 315 467 194 444 121 421 365 512 358 262
  57 272 161 155 427 340 619 104   5 522 176 195 448 219 293  21  45 114
 455 169 616 319  71 561 578  22 236 624 613 431  69 361 320 441 510 564
   8  53 221 553 481 485 610 451 489  70 505 385 458 433 414 609 160 258
 369 174 571  92  63 335 170 171  38 165 483 350 289 376  76 560 500 548
 296 482 434 330 251 612 311 402 559 591 479  30 415 519 392  60 201 354
 497 532 436 511 304 387 359 472 305  46 128 593 302 622 265 471 530 129
 312 278 535 492  93 150 216 393 300 259 141  17 226  47 287 507 480 487
  62  29 157 203 175 123  96 151 454 416 245 167 185 352 473  42   6 411
 328 490  68 189 621 177 546  43 134 307 457 603 110 127 295 248 382 243
 254 470 565 105 213 557  35 232 582   7 106 615 234 146 379 101 459 374
 172 370 334 443 584 124  11 440  23 524 220 476 475 318 439 324 516 107
 563 136 536 570 562 373 527 551 488 599 317 140 580 109 606  24 432 306
 445 583 182 555  34 450 394 192 184 291 111 279 417 331 389 147 388 164
 406 159 253 518  54 447 349 604 285 188 336  18 590 179  58 303 178 397
 323 377 496  50 424 469  27 357  19 592  88 102 587 130 115 558   9  91
 521 316  49 513 618 585 378  41 309 405 118 410 400 162 420 332 338 493
 237 222 208   1 367 100 321 523  56  39 131 503 588  12 438 581  28 529
 257  40  94  87 223 266 569 437 547  61 442 589 625 183 401  86 412  84
 210 526 275 508 491 270 408 514 542 186  89 152 190 344 577  55 390  14
 552 294  20 181 235 202 605 429 156 153 461 525 409 464 419 290  78 607
 498 608 322 225 435  97 351  25 337 180 380  32 504 602  90  81  74 391
 575 187 209 540 449  44 239 539 233 597  51 133 495 329 281 502 460 360
  36  10 501 207  15 544 572 574 154  48 407 325 446 341 456 541 212 276
 211 528 550 537 244 595 264 343 267 238  79 193 132 125 206 611 576 327
 430 283 386 197 566 515 371 573 506   3  65  64 261  85 214 256 533 534
 346 383 260 282 227 250 509  26 280 465 403 428 149 372 375 413 395   2
 247 466 422 517 418 453 462 463 474  59 135  37 345 549 242 452 126  33
 468 288 579 273 158  95 196 246 623 384 224 356 198   4 353 163  31 173
 231  80 292 601 255 520 543 531 284 381 148 286 423   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3439
INFO voc_eval.py: 171: [273 506 725 ... 498 167 932]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4803
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4051
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.357
INFO cross_voc_dataset_evaluator.py: 134: 0.603
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.542
INFO cross_voc_dataset_evaluator.py: 134: 0.573
INFO cross_voc_dataset_evaluator.py: 134: 0.474
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.521
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.364
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.377
INFO cross_voc_dataset_evaluator.py: 134: 0.708
INFO cross_voc_dataset_evaluator.py: 134: 0.619
INFO cross_voc_dataset_evaluator.py: 134: 0.591
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.385
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 135: 0.405
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 8999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.416s + 0.001s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.359s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.361s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.353s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.364s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.371s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.372s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.376s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.378s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.378s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.382s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.383s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.386s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.515s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.407s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.414s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.407s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.396s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.397s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.403s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.400s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.398s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.400s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.398s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.398s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.394s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.515s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.400s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.402s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.400s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.403s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.405s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.404s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.404s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.403s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.398s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.395s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.393s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.395s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.467s + 0.003s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.412s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.405s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.394s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.392s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.387s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.389s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.383s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.384s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.383s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.381s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.384s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.382s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.297s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [579 181 901 ... 135 137 260]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3569
INFO voc_eval.py: 171: [206  98 709 823 486 784 639 795 498 825 474 827  99 643 834 642 550 122
 646 829 749 543 794 370 822  84 572 651 480 645 836 689 263 238  55  58
  44 809 551 479 694 106 670 387 617 835 307 699 824 203 426 668 799 526
 304 800 765 383 484 124 766  42 710 521 770 110 103 711 290 164 481 496
 544 167 372 740 489 762 810 850 545 745 309 363  59 752 542 626 693 208
 776 659 193 753 476 570 696 852 268 422 497 207 583 757  83 700 585 648
 575 756 858 841 415 109 789 640 285 715 475 821 478 759 859  56 334 644
 482 483 335 732  14 221 134 862 511 175 620 189 112 306 853 100 671  43
 262 528 556 751 805 722 436 107 477 804 499  51 530 697 818 735 801 324
 851 365  24 141 116 358 135 605 356  87 713 686 494  16 512 330 684 527
 220 125 554 384 289 163 139 174 144 333 607 471 610 305 676 787 493 861
 300 666 264 261 576  67 495 352 595 806 564 571 532 635 840 516 340 531
 830  45 716 222 708 115 275 525   4 119 739 590 469 698 683 692 663 209
 284 720 257 395 419 790 594 764 802 414 695 793 578 154  29 843 435 508
 826 587 235 492  94 212 270 754  63  60 524 278 569 849 832 672  23  74
 320 401 712 402 192 169 269 533 719 230 217 558 798 155   6 280 679 117
 216  40  15 111 105 655 388 147 126 856 371 487 425 681 721 803 837 609
 662 301 501 848 242 243  26 170 156  71 326 177 704 442 377 258 214 353
  88 523 118 342  34 298 171  36  18 796  47 429 535 664 461 227 628 299
 678 629 397 500 547 191  78 418 768 506 232 194 734 204 510 788 254 854
 146  76 336 864 667 246 369 855 631  54 328 390 412 441 781 750  86 591
 229  85 198   5  90 758 775 176 351 847 379 292 210 466 416  72 313  66
 200 736 378 573 187 339  17 234 727 190  92 237 250 470 172 593 783 145
 778 606 782  68  69 452 552  79 245 507 381 791 541 539 612 865 437 440
 266 797 127 406 657  30 522  48 688 188 225 857 774 744 323 807   1 682
   0 546  20 361 413 331 114 548 582 815  73 656 665  13 380 239 623 831
  46 251 267 652 294   8 733 701 592 661 866 185  93 737 615 314  35 360
 382 184  21 763 457 647 586  70  38  19  10 813 308 673 362 249 424 706
 211 458 433 503 743 420 820  11 321 273 649 385 445 718 213 814 205 808
 178 443 633 771 392 760 717 341 675 538 761 730 574 811 393 669 279  89
 529 288 608 162 641 223 690  41 166 165 265 430 386 637 364 291 504  97
  80 616 833 327 792 318 577 723 322  75 121 345 860 463 252 180 186  65
 465 411 772 240 819 599 231 450 432 742 863 201 173 563 123 253  53 226
 597 725 702 658 296 488 120 439 603 777 519 449 274 228 731 773 319  82
 347 199 158 431 142 687 562 138 660 767 812 101 454 312  22  95  37 705
 247 233 755   7 224 218 549 462 303 241  12  39  77  91 534 567 367 408
 287 845 181 161 473 394 557  25  33 785 423 343 283 244 555 293 376 654
 389 272 195 359 403 354 638 505 183 828 315 560 634 236  28 780 728 196
 674 311 579 677 446 632 302 468 427 168 366 350 297 295 407   3  32 259
 622 559  27 621 129 159 748 703 568 769 627 281 786 846 151 148 691 396
 417  50 338 817 421 108 536 520   2 517 580 197 325   9 561 741  64 357
 842 598 619 491 316 215 515 256 584 317 409 444 248 618 581 368 255 153
 219 460 137 132 136 650 624 428 540 310  96 410 459 349 286 348 838 355
  52 149 472 160 140 707 685 391 464  49 276 589 601 400 613 630 604 455
 518  31 453 282 113 329 104 102 724 344 729 405 614 553 636 602 653 271
 509 438 565 131 726 375 374 611 839 746 337 738 447 157 537  81 130 143
 179 404 398 152 844 514 779 260 456 625 332 128 747  61 596 399 467 346
 714 150 485 680 133 502 277 434 202 490 451 600 588  62 373  57 566 448
 816 182 513]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6034
INFO voc_eval.py: 171: [126 328 385 246 248 811 381   4 812 361 247 813 744 748 598 656 497 259
 493 129   7 251 745 305 816 299 348 883 814 282 387 183 671 834  16 665
 632 825 817 851 302 826 652 342 376 194 815 495 362  36 908 425 862 662
 836 898 903 487 548 670 750 823 363 430 367 300   9 428 818 219 509 587
 543 778 254 853 485 230 137 835 821  63 368 346  50 383 840 125  54 155
 626  95 573 688 718 910 829 494 283 837 285 477 695 824 498  79 382 160
 806 272 336 357 867 897 255 171 260 128 331 475 579  20 306 344 222 270
 681 291 250 783 625 511 895  37 124 326 638 153 866 151 152 122 564 184
 432 486 327 859 433 555 682 739  97 787 185  64  34 243 653 747  21   8
 159 117 295 733 141 532  56 651 894  61   5   6 601  65  81 491 521 578
 175 466 186 643 349 136 554 520 347 140 261 154  17 449 820 399 560 436
 398 388 289 402 804 394 683  94 500 162 236 364 468 641 557 134 217 556
 242 461  86 531 182 758 549 547 828 642 292 769 244 476 606 193 187 446
 395  14 771  71 401  80 767 366 855 819 211  11 572 389 780 624 786 502
  60 753 249  69 536 788 896 473 588 570  24 470 223 164 877 440 563  77
  38 123 522 879 303 239 760 321 576 142 752 550 693 314 807 198 369 759
 143 581  88 338 298 559 358 900 353 471 756 437 892 650 330 334 799 240
 743 740 540 474 613  67 464 664 229 212 700 833 793 562 462  76 391 167
 515 195 490 279 628 228 309  51 308 104 707 785 629  22 770  25 542 690
 245 802 163 172  59 190 687 843 856 552 730 658 528 918 529 351  70 454
 884  48 890  74 524  28 809 590  84 696 751 113 310 839 784 561 571 838
 876 482 614 241 545  39 132 170 765  40  35 602 177 322 657 872 481 420
 262 874 256 442 181 427  29 582   2 661 607 800 666 885 546 371 499 847
 583 551 631 721 644 359 139 691 909 619 483 393  66 731 906  75 803 264
 514 120 594 276 798  12 857 307  62 907 267 875 510 333 484 345 373 215
 271 380 130 301 293 914 365  23 902 580 668 659 525 716  43 133 566 701
 726 776 553 640  33 558 188 870 423  85 627 304 527 233  32 577 478 865
 565 781 237 822 710 539 734 288 618 891 669 729  57 403 732 880 238 148
 518 648  44  72 214 225 131 360 116 574 431 294 176 269 227 370 655 458
  93 496 441 263 103 677 158 335  55 202 567 569 805 504 168 568 457 352
 488 191  52 544 354 439 526 586 350 465 106 893 600 166 790 575   1 702
 738  89 913 316 599 630  41 654 384 156 755 277 340 281 275 169 919 445
 450 435 161 593  58 916 200 736 592 418 719  45 517 390 444 831  13  10
 854 455  73 589 379 443 858 356 673  53 397 480 620 639  96 698 637 438
 426 434 452 749 762 234 692 216 224 105 672 848 317 456 763 114 467 708
 210 616 421 189 842 118 479 127 713 218 603 413 417 252 205 595  31 266
 232 678 796 685 146 392 111 537 329 869 764 846 911 278 296 419  82 147
 174 889 265 447 679  15 727  30  87 513 808 325 605 675 864 408 422 789
 203 881  27 226 404 119 615 405 115 320 901 235 917 507 689 448 810 173
 633 741 782 714   0  68 530 386 209 165 372 611 221 777 703 596 535 337
 720 761 157  91 297  47 591 899 667 608  98 888  42 206 663 355 766 453
 101 107 375 407 268 849 231 728 585 319 374 501 757 451 506 697 830 623
 112 635 197 845 204 850 886 775 396 519 463 604 609 257 315 791 469 343
 737 503 323 797 852 795  90 610 534 754 312 280 723 178 878 416 706 220
 429 121 523 774 711 505 597 915 779 868 332 704 584 424 135  49 339 725
 674 538 533 832 102 735 660 694 621 400 887 145 792 414 801 273 460 253
 286 746 742 912  26 318 861 634 180  18 472 213 201 676 882  46 138 794
 827 772 905 341 612 311 378  83 287 290 415 709 208 207 699 324  78 863
 645 717 412 406 411 715 904 773 258 199 192 768 313 284  99 646 636 110
 377 108  19 100 705 149 622 647 144 492 649 724 410 409 541 722 844 196
 871 686 179 873 712 680 617 684 508  92 841 860 109 516 459 512 489 274
 150   3]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2242
INFO voc_eval.py: 171: [1374 1594 1375 ...   71 1838  500]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3518
INFO voc_eval.py: 171: [1433  830  883 ...  447 1595 1705]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5429
INFO voc_eval.py: 171: [ 74  47 140 143  19  51   9 225  49  90 180 381 157 285 295  55  33 358
 360  66 249 277 265 310 276 370 208 196  53 379 336 184 136 142 306  44
  52 380 291 300 206 354 262 113 272  56 317 178  80 238  58 188 222  48
 166 219  27 266  16 259   7  18  59 322 165 391 103 386 399 229 311 330
 312 333  22 162 152 392 128  95  92 154 284 148 252 115  34  89 215 288
  39 318 121 204 110  40 101 394 366 345  96 129  37 244 210 283 201 378
 211  28 246 230 393 396  57 145 242  68 137 382 240 207  84 213 346 189
 287  88  64 334 236 344 299 319 278 353 323 301 167 321  50 341 174 245
 290   4  82 177 104 199 161 315 205 326  70  91 362  17  12  23  54 302
  21 133  60 125 102 296 163 114  11 313 339 149 350 294 183 241 187 107
 347 179 253 251 141 338 316 273 369 247  38 232 384 109 387  24 124 108
 331 359 116 100 374 258 173  43 171 158 363  32 292  75 159 286 243 376
  46  87  26  15 181 343 191  73 335 233 120   8 271 197  85 274  30 112
 261 255 314 356 195  97 130 228 275 383 307 260 401 371  41 221  71 111
   3 150 227 337 123  61 139 289  42  86 267 172 320 355 357 304 263 340
 235 155 132 117  93 216 209 126 223 254 156 293  29 297 134 118  69 324
 203 365 194  78 268 212 332  72 135 190 160 220 269 226  76 282 193 182
 375 351 175 122 218 106 168  25 153 202 250  10 185 385 400 279  81  98
   2 388 328   0 147  35 200 144 224 270 164 186 192 352 372 105 257 256
  99 327 398 119 170 298 397  79 377 361 151  63 217  13  45 248 390 198
  62 264 237 138 234 146 309 231 395 329  31  36 176  65 325 305 281 349
 127   5 367 364  94  67  77 169 308 368 373  14  83 342 280   6 303 348
  20 389 239 131 214   1]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5735
INFO voc_eval.py: 171: [1092 1142  500 ... 1381 1345  420]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4743
INFO voc_eval.py: 171: [ 11   6  24 117  25 105 114   8  45 109  40  62  83   1  82  85 111  58
 108  19  80  81  47  95  86  12  63  14  34  21  57  73   5 115 118  41
  29  49  54 110  10  33  96   7  60  72  78  44  37  30  87 106   9  20
  16  64 119  61  88 103  13  76  70  31  68  46  90  50  92  35 107   0
  32  53  91  71  94 116 104  77  79  48  36  66  27  93  22  18   2  69
  28   4  98 120  84  17  75  39  51  52 112  67  74  23 101  65  42  89
 102  38  56  99   3 100  43  26  55 113  15  97  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0056
INFO voc_eval.py: 171: [5829  645 5830 ... 2605 6856 3742]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5199
INFO voc_eval.py: 171: [  4  77   5 129  47 104  46  43 114 143  76  50 139 145  97  66  78  48
 146 106 113  40  92   8 121 134  34 108 133  63 158  93  33   9  61  69
 112 116  94  53   6 140 120  83 118  45 156 100  82  80 132  41 142 144
  85  49   3 131 101  14  17  58  86  44  95  18 107  27  52 141  54   7
  39 152  22  73 137 128  89 103  81 135 136 105  60 122  57 153  75  38
  25  59  26  91  56 119 130  90  32  64  11  96  42 138 115  28  30  62
 117   1 111  21 150   0  79  87 109 155 125 147  51 151  98  55  99  74
  31   2  20  10  88  29 148 127  35  37 126  68  70  12 149  84 124 110
  23  71  65  24 123 154  67 102  72  19  13 157  15  16  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1818
INFO voc_eval.py: 171: [ 878 1925  472 ...  816 1246 1025]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3634
INFO voc_eval.py: 171: [106  98 191  99   3 153 103  19  57 107 211   7  63  41  37  47 209  59
  68  93  34 208   1 189  28 195  12  81  82  67  13  58 168 164 133 145
 155  38 120 154  92 138  35 203 205 222  91 125 223  74  49 175 201  61
 213 126 156  10  87 192 218 121 160 117  15 224  11 190 104 100 170 118
  66 198  27 124  76  69 147 187 129  46 128 194 130 146 132  64 159 135
 122  40 105 210 152 176 123   5   2 111  86 114  16 109  24  85 150  65
 177  44 112 148 200 113  14  72  52 172  23 206 127 188 204 108  30 140
  60  97  77 193 196   8  70 143  32 171 131 169 167   4 165  83  79  48
  42 158  71 174  26 166  33  45 173 182  17 142  21 225  78  22 220 102
 163   0 207 217  25 179 115  96  62 137 185  90 212  73 110  50 161  75
 184  56 221  31  88  84 101  29  95  36 216 214   6  54 215  18   9 186
  43  53 119 197  89 219 181  39 139 144 151 116  80 141 199 157  94 134
 183 136 149  51  20 178 180 202 162  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1167
INFO voc_eval.py: 171: [146 129  29  22 173  96 132 183 128 147 190 154  65  24  34  78  23 122
 105 178  66 126 101 149 102 156 171 170 108  71   9 169 157  10 130  70
 131  82  55 111 112  87  50  49  36  33 152  64  90  47  59 179   5 133
  42  44 124 100 163   1  79  56  97  12  76  48   2 150 175 153 167 144
 145  67 181 123 168 107  16  91  77  80   0 114  27  32  84 148 166 164
 185  85 135 187 177 134 158  41  69  72 160 188  51 106 172  52 180  38
  95 155  53  98  63  28 186 127  13  11  93   7  35 104  14 182  19  18
  25 138  62 162 136   3  99 137  21 142 110  15   8 151 116  89  26  92
 113 125  61 174 191  81 176 141  83 121  20 117 119  37 115  39 120 143
  94  73 103  43  68  17  88   6 159  40 165 184  46 140  30 139  57 189
  54  60  75  31  58 118 161  45  74  86   4 109]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3773
INFO voc_eval.py: 171: [ 95 543  73 355 278 585 370 394 259 418 496 426 439 525  50  28 109 291
  24 371 468 409 523  96 267 305 265 563 593 145 183 592 168 231 134  67
 223 184 484  11 174 487  64  60  99 414 201  87 446 166 304 471 336 528
 172 599 260 583 388 306 215 346 335 358 479 411 135 329 587  75 307 274
 248 489 399 158 584 554 345 491 114 546 498 113  70 524 204 323 481 444
 539 283 203  55 173 447 296 591 125 208 343 422 402 225 372 360 522 198
 473  43   3 315 220 438  19 374 276 594 325 545 126 465 180 105 300 412
  40 157 316 123 154  44  57  32 595 534 574 195 382 389 313 602 189 430
 273  91  86 106  65 526 407 494 486 549 127 573 239  29 429  82 319 156
 597 213 413 268 449 142 482 507 400 240 347 129 580 376 227 116 392 530
 191 287 252 557 150 351 541 121 349 324 527 561  69 224  93 499 501 596
 302 214 185 368 255 440 205 598 396 148 535 397 575 258 470 538 532 510
 421 531  56  45 450 286 177  15 393  27 179 298 308 562 395 290 138 441
 318 341 459 485 490 391 537  61 570 588 512 433 264 497 590  68 122 448
   1 295 356 297 100 155 354 581 190  80 102 514  18 236  81 383 317 266
 108 566 589 385 505 200 432  21 175 506 477 170  94 536 423 235 529 403
  12 586  16  79 334 328 238 427  42 312 571 476 404 320 425 340 398  26
 601 216 552 416 130 502 186  92 480 151  14  31 326  13  97 366 261 229
 338   9 384 187 540 164  34 452 453 124 419 520 247 463 483 443 120  39
 219  17 196 327 309 564  63 136 244 140 378 548 572   8 457  71 117 533
 379 169 445 132 188 472 568 171 467 357  59 381 281 350 542 513 569 160
  98 420 133 451 243  30 424 218 337 344 234 101 147 310 107 279 301 159
 193   4 515 289 521 143 558 139 131 455  41  72 577 359 331 263  46 194
  22 321 492 464 103 519 209 249 516 401 242 377 405 182 415  52 212 146
 197  84 373 292 149  36 500 428 363 211 469 504 495 241 271 288 293 390
 110 144 544 353 250 226 272  37 176 352  77  33 364 466 442 559 603 282
 367 285 162 454 436  85 217 118  62  10 152 600   2 141 128 167 565 112
 550  58 406 161 493 435 245   7 192 547  47  38 576 330 342  88 284 434
   0 206 222 361 262 369 386 256 560  48  20 119 277 431 270 153 178  53
   6 461 333 488 474 478 509 508 503 237 582 339 232 456 518 246 365 111
 348 555 462 458 311 551  51 254 511 269 233 280 199 104 332 207 567  23
 251 253 181 410 380 221  54 579 578 165  78 299  89 210 437  76 553  25
  66 517   5 257 408 294 314  49 163 202  90  74 460 303 137 228 230 417
  35 322 375  83 362 556 475 387 115 275]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7083
INFO voc_eval.py: 171: [11611  2080   866 ...  3928  9558  4338]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6195
INFO voc_eval.py: 171: [ 342  435  667 ... 1440 1351 1445]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5907
INFO voc_eval.py: 171: [126   3 120   4  67  72 151 105  96 103 180  19 144 119 149  25  71  60
 135  69 158 118  68  45  87  20 130  43  65  27  66   6 166 150 121  70
 111 123  47 155  46 152  32 170  24  35 174  11  38 177  50  41 142  78
  76  16  83 133  10  23 184  59  63  28  37 143 107 137 104  90 159   7
  73  86 162 134 129 176 114  82  56 164 172   2  84 127  85 112  34  51
  14  22  81  98  99  53  74 145 122  79 154  42 146  80  77  29  58 113
 132   0  75  33 156   9 163  21   1  39  62  49  61  88 140 167 106 138
 115 141   5 157 101 153 125  30 165  13 160 117  54 171   8  89  92  97
  64  18 102  48 147 182  12 169  44 175  91  15 109  36  52 100 173  94
 161 168  93  40  17 124  57 181 136 148 116  26  31 139 110  95 178 179
 108 183  55 128 131]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2825
INFO voc_eval.py: 171: [208 205  71 280 300 224 434 145 226  16 167 338 230 315 299 321  75 210
 207 382 193 206 415 176 357  96 320 416 215 285 177 432 251  17 116 260
 209 190 268 144  80  67 253 247 402 127  89 139 258  49  93 157 228 403
 164 174 121 323 140 363 143 440 322 364 441  60 179 352  40 381 123  81
 339 365 160 316 212 126 298  21  73 303  94 173  41 270 358 232 213 293
 117  35 259 202 263 189 409 229 369 138 335  18 172 286  66 241  31 134
 234 433 147 423 289 368 110 340  23 214 309 261   4 419 129 354 239 221
 195 130 330 319 318  56 178 397 384   5 374  85  32 367 411 435  68 413
 252 103   3 187 115 436  46  42 334 233 277 288 136 426 442 155  83 431
 125  51  77 388  36 372 279 370 393 296 327 310 275 430 345 264 222  82
 271 342 297 146 351 184 302  37 290  19 326 314 248 223 182 105 380 418
 399  11 344  55 227 250 255 361 329 201 396 371  44  54 312 128 132 113
 218  79 295 108 180   7   6 427 240  30 158  97 355 398 191 175  74 231
 249 150 163 142 135  45  13 181  57 414 376 400  64 428  25   8  62 262
 153 282 404 438  48  61 349 122 313  29 407  92 304 165  72 159  91 131
  22 225 154 292 291 439 276 422  38 192 235 390 331 324 379 386 356 141
  26  50 307 353 166 337 265 395 148 362 101  24 287   2 100 217 272 238
 188 267 375  12 366 391 245 124 199 257 378 168 311 360   9 421 161 211
 306 170 152  78  28 408 359 392 425   0 269 149  59  84  98 220 305 197
  88  90  99 236 133 111 196 119 417 420 204  27 156  95 243 162 109 186
 284 406 401 118 385  69  63  39 242 106 203 328 194  70 341 294 107 244
 237 102 412 394  14 274 377 151 278   1 200 281 373 169 317 350 343 346
 114 405 273  52 266  86  53 301 185 256  47 216 437 348  10 410  58 308
 429 325 424 254 198  43 383  15 387  20 389  34 171 183 104 336  87 347
 246 283  76  65 332 333 219 137 120  33 112]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3841
INFO voc_eval.py: 171: [425 269  73  83 139 363 228 138 112 426 143 310 313  75 229 368 119 277
 240 586 339 168 142  82  52 556 366 348 477 274 538  67 484  77 362 271
 598 199 116 108 117 268 333 478 144 301 499 218 314  98 249 545 600 230
 298 103 297 396 215  72 596 217 568 620 614 398 145 567 364  99 494 404
 113 263  13 399 122 486 120 205 299 355 252 204 200 166  16 594 137  66
 326 554 191 347 342 617 241 308 194 467 315 444 121 512 421 358 365  57
 262 272 161 155 427 340 619 104 176   5 522 448 293 195 219  21  45 455
 114 616 169 561  71 319 578  22 236 624 613 431  69 361 320 510 441  53
   8 564 221 481  70 485 553 451 610 489 505 385 458 414 433 609 160 258
 571 369  92 174 335  63  38 171 170 483 165 350 289 376 560 500 548  76
 482 296 434 330 251 311 559 612 402 591 519 479 392  30 415  60 201 497
 532 354 436 511 304 359 387 472 305 622  46 128 302 265 593 471 278 530
 129 312 535  93 150 492 216 259 393 141 226 300  17  47 507 287 480 487
  62  29 157 203 175  96 123 245 454 167 416 151 185 473 352 328 411  42
  68   6 490 189 177 621 134 307  43 546 457 110 127 603 243 295 382 248
 254 470 565 213 105 557  35 582 232   7 146 615 234 106 379 101 459 374
 172 370 443 334 124 584 440  11  23 220 524 318 476 439 475 373 107 324
 516 563 136 527 562 551 570 536 599 317 488 109 580 140 606 583 182 306
 555 432  24 445 192  34 450 394 291 111 184 279 417 406 331 147 389 388
 253 164 159 518 447  54 349 179 336 188 604 285  18 590 178 496  58 303
  27 424 323 377 397  50 469 357 102 130 592 587  88  19 115 558 521   9
  91 316 618 585 513  49 118 309  41 378 405 410 400 420 162 332 493 338
 237   1 100 321 367  56 523 208 222 581 529  12  28 503 131 438 588  39
  94  40 257 266 223  87 569 437 547  61 442 589 625 183 401  86 412  84
 210 526 275 508 491 270 408 514 542 186  89 152 190 344 577 294 390  14
  55  20 552 181 461 156 202 153 235 525 605 429 608 419 464 290 498  25
 607 409  78 351 435  97 225 322 337 380 180  32 504 602  81  74  90  44
 449 575 187 209 391 540 233 239 597 539 460 133  51 502 329 495 281 360
 501 544  10 572  36 207  15  48 341 407 154 574 325 446 276 211 541 212
 456 193 550 528 537 244 264 595 343 206 576 327 125 267 238  79 132 611
 515 566 197   3 371 506 283 573 430 386  65 214 261  64  85 256 534 533
 509 383 260 282 227 250 346 247 280 465 403 428 149  26 372 375 413   2
 395 466 422 517 418 453 462 463 474  59 135  37 345 549 242 452 126  33
 468 288 579 273 158  95 231 623 384 356 198 196 246 224 353 292  80 520
 173  31 163 255   4 601 148 531 284 543 381 423 286   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3437
INFO voc_eval.py: 171: [273 506 725 ... 932 498 167]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4803
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4050
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.357
INFO cross_voc_dataset_evaluator.py: 134: 0.603
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.543
INFO cross_voc_dataset_evaluator.py: 134: 0.573
INFO cross_voc_dataset_evaluator.py: 134: 0.474
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.520
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.363
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.377
INFO cross_voc_dataset_evaluator.py: 134: 0.708
INFO cross_voc_dataset_evaluator.py: 134: 0.619
INFO cross_voc_dataset_evaluator.py: 134: 0.591
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.384
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 135: 0.405
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 9499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.410s + 0.001s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.361s + 0.003s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.375s + 0.003s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.384s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.386s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.390s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.386s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.391s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.388s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.387s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.385s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.388s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.390s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.689s + 0.003s (eta: 0:01:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.420s + 0.002s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.403s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.400s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.403s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.403s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.415s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.411s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.409s + 0.002s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.405s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.402s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.402s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.399s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.581s + 0.003s (eta: 0:01:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.381s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.385s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.384s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.380s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.380s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.380s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.377s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.383s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.383s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.384s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.380s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.381s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.517s + 0.003s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.436s + 0.002s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.418s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.411s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.403s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.395s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.392s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.389s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.389s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.390s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.385s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.384s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.385s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.462s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [579 181 901 ... 135 137 260]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3569
INFO voc_eval.py: 171: [206  98 709 823 486 784 639 795 498 825 474 827  99 643 834 642 550 122
 646 829 749 543 794 370 822  84 572 651 480 645 836 689 263 238  55  58
  44 809 551 479 694 106 670 387 617 835 307 699 824 203 426 668 799 526
 304 800 765 383 484 124 766  42 710 521 770 110 103 711 290 164 481 496
 544 167 372 740 489 762 810 545 850 745 309 363  59 752 542 626 693 208
 776 659 193 753 476 570 696 852 268 422 497 207 583 757  83 700 585 648
 575 756 858 841 415 109 789 640 285 715 475 821 478 759 859  56 482 334
 483 644 335 732  14 221 134 862 511 175 620 189 112 100 306 853 671  43
 262 528 556 751 805 722 436 107 477 804 499  51 530 697 818 735 801 324
  24 365 851 141 116 358 135 605 356  87 713 686 494 330 512 684  16 527
 554 125 220 384 163 289 139 174 144 333 607 471 676 305 610 300 493 861
 787 666 261 264 576  67 495 352 595 806 564 571 532 340 840 516 531 635
 830  45 222 716 525 275 708 115 739 119   4 698 590 692 469 683 663 720
 209 284 790 257 419 395 594 764 802 414 695 793 578 154  29 843 435 508
 826 587 235 492  94 270 212 754  63  60 524 278 832 569 849 672  23  74
 320 401 712 402 192 169 269 533 719 230 217 558 798 155   6 280 679 117
 216  40  15 111 105 655 388 147 126 856 371 487 425 681 721 803 837 609
 662 242 301 501 848 243  26 170 156  71 326 177 258 442 704 377 214 353
  88 523 342  34 118 796 171  18 535 298  36  47 227 664 429 628 461 299
 191 629 397 500 547 678  78 418 768 506 232 854 194 734 204 510 788 254
 146  76 336 864 667 246 369 855 631  54 328 390 412 441 781 750  86 591
 229  85 198   5  90 758 775 176 847 351 210 379 292 466 416  72  66 736
 313 200 339 187 378  17 573 234 727 470 237 172 190 250  92  68 783 145
 778 593 452  69 552 245 606 782 381 507 612  79 539 541 791 266 127 657
 797 440 865 437 522 406  30  48 225 857 688 774 188 744   0 331 323 807
 361 413 546 682  20   1 114 548 656 815  73 582 380 831 239 665 623  13
   8 251 733 652 294  46 701 267 592 661 866 185 360  93 737 615 314  35
 382 184  21 763 457 647 586  70  38  19  10 813 308 673 362 249 424 706
 211 458 433 503 743 420 820  11 321 273 649 385 445 718 213 814 205 808
 178 443 633 771 392 574 717 675 341 760 538 761 730 811 393 669 279 162
 529 288 608  89 690 223 641  41 166 833 386 430 364 577 637 291  97  80
 504 165 265 616 327 318 792 180 252 463 860  75 723 186 345 121 322 599
 240 772 819 465 411  65 231 450 432 742 863 201 173 563 123 253  53 226
 597 702 725 488 658 296 120 603 777 439 228 519 449 274 773 731 687 142
 347 319 158 431 562 199  82 812 767 660 138 454  37 233 705 247 312  95
   7 224  22 101 755 549 462 303 218  77 534  12 241  39 567  91 161 473
  25 408 287 557 181  33 845 367 394 785 244 283 654 555 389 376 343 423
 293 638 403 272 505 195 354 183 359 828 236 634 315 560  28 622 311 677
 728 632 302 168 780 196 579 468 427 674 446   3 559  32 259 297 295 366
 350 407 129 151 621  27 159 748 703 568 769 627 281 846 148 691 786 338
 421 108 417 396  50 817 197 215   2 580 536 520 517 325 561   9 741  64
 357 842 598 619 491 316 515 256 584 317 409 444 248 618 581 368 255 153
 219 460 137 132 136 650 624 428 540 310  96 410 459 349 286 348 838 355
  52 149 472 160 140 707 685 391  49 276 464 601 589 400 613 630 102 604
 455 518  31 282 453 614 113 104 329 724 344 729 405 553 636 565 271 653
 509 131 438 602 726 375 374 611 839 746 337 130  81 537 157 447 152 738
 747 143 260 398 844 404 179 514 779 456 625 332 128  61 150 485 399 467
 346 714 596 502 490 202 451 600 434 373 513 588  57 277 133 680  62 448
 816 182 566]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6034
INFO voc_eval.py: 171: [127 330 387 248 250 813 383   4 814 363 249 815 746 750 600 658 499 261
 495 130   7 253 747 307 818 301 350 885 816 284 389 184 673 836  16 667
 634 827 819 853 304 828 654 344 378 195 817 497 364  37 910 427 864 664
 489 838 905 900 550 672 752 825 365 432 369 302   9 430 820 221 511 589
 780 545 256 855 487 232 138 837 823  64 370 348  51 385 842 126  55 628
 156  96 575 690 720 912 831 496 285 287 839 479 697 826 500  80 161 384
 808 274 359 338 899 869 257 172 333 262 129 477 581 308  20 346 224 272
 683 785 293 252 627 513 897  38 125 328 640 154 868 153 152 566 123 185
 488 434 329 861 435 557 741 684  98 789 186  65 245  35 655 749   8  22
 160 118 297 735 534 142  57 653 896   5  62   6  66 603  82 493 523 468
 176 580 645 187 556 137 351 522 141 349 155 263  17 822 451 562 401 438
 400 390 291 404 806 396 685  95 366 502 238 163 470 559 219 135 643 463
 244 558 533  87 183 549 760 551 830 294 771 644 246 478 608 194 448 188
 397  14 773 769  72 857  81 368 403 821  11 213 391 574 626 782 788 504
 755 251  61  70 790 538 898 590 475 572 472 225  25 165 879  78  39 442
 565 881 124 524 305 241 578 323 762 143 754 695 316 371 552 809 199 144
 761 340 583  89 300 360 902 439 355 561 473 758 332 652 894 336 801 242
 742 745 615 476 542 466  68 666 231 214 835 702 795 564  77 168 196 492
 393 464 517 281 630 311 230 105  52 310 709 787  23 772  26 631 804 247
 692 544 164 689 173 660 191 732 845 858  60 530 554 456 353  71 920 531
  49 526 886 892  75 114 592 811  29 753 698 841 312  85 786 878 484 563
 573 840 616 243 547  40 133  36 171 324  41 604 767 264 874 178 422 876
 182 659 483 663 444 429   2 258  30 887 584 553 501 548 373 609 668 802
 585 633 849 723 361 646 140 693 911 621 908 733 485 395  67 805 516 596
 121  76 266 278  63 800  12 859 909 309 486 877 335 269 512 347  24 217
 303 273 916 131 367 375 382 295 582 661 670 527  44 904 568 718 703 134
 778 555 728 306  34 629 189 560 872 642 425  86  33 579 529 567 235 867
 480 783 239 712 290 824  58 893 620 736 541 731 671 240 149 882 405 734
 650 520 296  73  45 576 227 216 362 177 132 433 117 271 229 372 265  94
 460 657 498 679 443 104 337 159  56 169 569 459 571 570 490 807 203 354
 506 192 546  53 441 356 792 167 352 588 528 467 602  42   1 895 577 107
 704 740  90 632 318 915 452 601 656 757 921 277 386 342 157 437 283 170
 279 447 595  59 738 918 201 162 594 519 420 392  46 721 446 445 457  74
 833 591 856  13  10 381  54 358 482 399 860 675 641 436 764 622 428 639
 700  97 454 751 440 694 218 226 236 458 850 674 319 106 115 469 765 190
 481 423 710 618 220 128 212 119 844 715 415 680 419 268 207 597 234 798
  32 605 254 871 687 112 394 147 766 331 539 175 280  83 848 913 421 148
 298 810 729 681  15 267 515 327 449 891  31 607  88 866 424 677 204 791
 410 617 322 228 883 407 406 903 116  28 120 919 812 509 691 237 450 635
 174 716 211 166 743 784   0 374  69 388 613 532 722  92 763 598 158 339
 705 537 223 779 610 669  99 593  43 208 890  48 901 299 377 102 665 357
 768 108 455 409 270 730 851 233 453 503 832 587 625 508 205 637 198 759
 376 113 852 847 699 398 321 465 611 317 521 259 606 793 345 471 888 777
 799 797 325 505 739 854 612 536 725 418 431 282 179 880  91 756 314 525
 507 713 706 334 599 222 776 122 708 103 870 917 781 136 426 586 146 889
  50 834 540 696 737 275 727 341 676 402 416 794 662 623 462 803 474 535
 288  27 678 863  18 320 914 744 636 748 202 255 181 215 343 380 139  84
 884 829 774 796 614 313 907 289  47 647 210  79 209 865 701 417 292 719
 326 711 648 775 315 770 906 286 100 379 260 413 414 200 111 193 638 408
 717 707 846 619 180  19 543 494 197 624 461 651 873 109 110 688 101 649
 875 682 145 726 150 714 724 411 412  93 518 510 862 686 843 206  21   3
 514 151 276 491]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2242
INFO voc_eval.py: 171: [1374 1594 1375 ... 1125  910 1838]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3514
INFO voc_eval.py: 171: [1433  830  883 ...  447  680  433]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5422
INFO voc_eval.py: 171: [ 74  47 140 143  19  51   9 224  49  90 180 380 157 284 294  55  33 357
 359  66 248 276 264 309 275 369 208 196  53 378 335 184 136 142 305  44
  52 379 290 299 206 353 261 113 271  56 316 178  80 237  58 188 222  48
 166 219  27  16 265 258   7  18  59 321 165 390 103 385 398 228 310 329
 311 332  22 152 162 391  92  95 128 283 154 148 251 115  34  89 287 215
  39 317 121 204 110  40 101 393 365 344 243  96  37 129 210 282 201 377
 211  28 245 229 392 395  57 145 241 137  68 381 213 207  84 345 239 189
  88 286  64 235 333 343 318 298 352 277 322 167 300 320  50 340 174 244
 289   4  82 177 199 104  70 205 161 314  91 325 361  17  12  23  54 301
 125  21 133  60 102 295 163 114 338 312  11 149 349 293 183 187 107 240
 252 346 179 250 337 141 315 272 246 368  38 231  24 386 109 383 124 358
 108 330 100 116 373 257  43 171 173 158  32 362 291  75 242 159 285 375
  26 191  46 342 181  15  87 197 270   8 232  73 120 334 273  85 112  30
 260 195 313 254 355 306 382 227 130 274  97 259 400  41 370 226   3 221
  71 111 123 150 336  61  42  86 288 266 139 319 354 172 132 155 262 303
 339 117 234 356 156 126 209  93 216 253 223 292  69 134 296  29 323 118
 203 364  78 194 212 267 331  72 190 220 160 135 193  76 225 281 268 122
 374 218 182 175 350  25  10 106 202 168 249 153 399 384  81 278 185 327
  98   2 387   0  35 147 144 200 269 192 371 164 186 105 256 351 326  99
 397 255 396 119 297 170  79 376  63  45 360 151 247  13 217 389 263 198
 236 138  62 394 308 146 233 230 176  31  36 328   5 324  65 304 280 348
 366 127  14  67  94 363 169 372 367  77 307 341 279  83 347  20   6 302
 214 131 238   1 388]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5734
INFO voc_eval.py: 171: [1143 1093  500 ...  459 1382 1051]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4738
INFO voc_eval.py: 171: [ 11   6  24 117  25 105 114   8  45 109  40  62  83   1  82  85 111  58
 108  19  80  81  47  95  86  12  63  14  34  21  57  73   5 115 118  41
  29  49  54 110  10  33  96   7  60  72  78  44  37  30  87 106   9  20
  16  64 119  88  61 103  13  76  70  31  68  46  90  50  92  35 107   0
  32  53  91  71  94 116 104  77  48  79  36  66  27  93  22  18   2  69
  28   4  98 120  84  17  75  39  51  52 112  67  74  23 101  65  42  89
 102  38  56  99   3 100  43  26  55 113  15  97  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0056
INFO voc_eval.py: 171: [ 645 5828 2532 ... 3859 6613 5450]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5209
INFO voc_eval.py: 171: [  4  77   5 129  47 104  46  43 114 143  76  50 139 145  97  66  78  48
 146 113 106  40  92   8 121 134  34 108 133  63 158  93  33   9  69  61
 112 116  94  53   6 140 120  83 118 156  45 100  82  80 132  41 142 144
  85  49   3 131 101  14  58  17  86  95  44  18 107  27  52  54 141   7
 152  39  22  73  81 137 103  89 128 135 105 136  60 122  57 153  75  25
  38  59  91  26 119 130  56  90  32  11  64  96  42 138 115  62  30  28
   1 117  87 111  21 150   0  79 109 155 125 147  51 151  55  98  99  74
  31   2  20  10  88  29 148 127  35  37  68  70 126  12 149  84 124 110
 102  23  71 154 123  72  24  67  65  13  19 157  15  16  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1822
INFO voc_eval.py: 171: [ 876 1923  470 ... 1365 2225 1295]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3650
INFO voc_eval.py: 171: [106  98 191  99   3 153 103  19  57 107 211   7  63  41  37  47 209  59
  68  93  34 208   1 189  28 195  12  81  82  67  13  58 168 164 133 145
 155  38 120 154  92 138  35 203 205 222  91 125 223  74  49 175 201  61
 213 126 156  10  87 192 218 121 160 117  15 224  11 190 104 100 170  66
 118 198  27 124  76  69 147 187 129  46 128 194 130 146 132  64 159 135
 122  40 105 210 123 152 176   5   2 111  86 114  16 109  24  85 150  65
 177  44 112 148 200 113  14  72  52 172  23 206 127 188 108  30 204 140
  60  77  97 196 193   8  70 143  32 171 169 131 167   4 165  83  79  48
  42 158 174  71  33  26 166  45 173 142  17 182  21 225  78  22 220 102
 163   0 207 217  25 179 115  96  62 137 185  90 212  73 110  50  75 161
 184  56 221  31  88  84 101  29  95  36 216 214   6  54 215  18   9 186
  43  53 119 197  89 219 181  39 139 144 151 116  80 141 199 157  94 134
 183 136 149  51 180  20 162 178 202  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1167
INFO voc_eval.py: 171: [146 129  29  22 173  96 132 183 128 147 190 154  65  24  34  78  23 122
 105 178  66 126 101 149 102 156 171 170 108  71   9 169 157  10 130  70
 131  82  55 111 112  87  50  49  36  33 152  64  90  47  59 179   5 133
  42  44 124 100 163   1  79  56  97  12  76  48   2 150 175 153 167 144
 145  67 181 123 168 107  16  91  77  80   0 114  27  32  84 148 166 164
 185  85 135 187 177 134 158  41  69  72 160 188  51 106 172  52 180  38
  95 155  53 186  98  63  28 127  13  11  93  35   7 104  14 182  19  18
  25 138  62 162 136   3  99 137  21 142 110  15   8 151 116  89  26  92
 113 125  61 174 191  81 176 141  83 121  20 117 119  37 115  39 120 143
  94  73 103  43  68  17  88   6 159  40 165 184  46 140  30 139  57 189
  54  60  75  31  58 118 161  45  74  86   4 109]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3773
INFO voc_eval.py: 171: [ 95 543  73 355 278 585 370 259 394 418 496 426 439 525  50  28 109 291
  24 371 468 409 523  96 267 305 265 563 593 145 183 592 168 231 134  67
 223 184 484  11 174 487  64  60  99 414 201  87 446 166 304 471 336 528
 599 172 260 583 388 306 215 335 346 358 479 411 135 329 587  75 307 274
 248 489 399 158 584 554 345 491 546 114 498 113  70 524 204 481 323 444
 539 283  55 203 173 447 296 591 125 208 343 402 422 225 372 360 522 473
 198  43   3 315 220 438  19 374 276 594 325 545 126 180 465 105 300 157
 412  40 316 123 154  44  57  32 595 534 574 195 382 389 313 189 602 430
  91  86 273 106  65 526 407 486 494 549 573 239 127  29 429  82 597 156
 213 319 413 268 449 507 482 142 400 240 347 580 129 376 227 392 116 191
 530 287 252 557 150 541 351 121 349 561  69 224 527 324  93 499 501 302
 596 214 185 368 255 440 205 598 396 535 148 575 397 470 258 538 510 532
 421  56 531  45 450 286 177  15 393  27 179 562 395 298 308 290 138 441
 318 459 341 570 490 485  61 391 537 588 512 264 433 590 497  68 448   1
 122 356 295 297 100 155 354 190 581 102 514  80  18 236 108  81 383 317
 589 566 266 432 200 505 175 385  21 506  94 477 170 536 529 235 403 238
 423  12 427  42 328  16 334  79 586 312 571 476 404 320 340 398 425  26
 216 601 552 416 186 502 130  92  31 151  14 326 480  13  97 229 261 366
 338   9 187 384 540 419 164  34 124 452 453 247  17 463 520 327 196 443
 219 483 120  39 309  63 136 564 244 140 378   8 572 548 117  71 457 445
 533 132 188 472 379 169 568 171 467 357  59 381 350 281 542 160 569 513
 451 243 133  98 101 420  30 424 344 218 147 234 337 455 301 289 515 279
 107 310  72 193 159   4 131  41 577 521 139 558 143 359 331 263  22  46
 464 194 103 492 321 249 377 516 519 209 146 242 401 182 197  52 212 415
 405 149 373  36  84 292 500 428 363 504 288 211 271 469 241 495 390 110
 293  37 226 250 144 353 272 176 544 364  33 466 442  77 282 352 559 603
 285 367 454 162 436  85  62 112 217 118 128 141 152 600   2  10 565 167
 550 161  58 406 493 435 192 245   7  47  38 547 576 330 342 434  88 284
   0 222 206 361 262 369 386 256 560  48  20 119 277 431 270 153 178  53
   6 461 509 333 488 474 478 503 237 508 582 339 232 456 518 246 365 111
 348 555 462 458 311 551  51 254 511 269 233 280 199 567 332 207 104 181
  23 251 253 410 380 221  54  66 579 578 165  25  78 299  89 210  76 437
 553   5 517 257 294 314  49  90 408 163 202  74 230 228 137 417 460 303
  35 322 375  83 362 387 475 556 275 115]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7083
INFO voc_eval.py: 171: [ 2081 11617   866 ...  3945  7234  6975]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6210
INFO voc_eval.py: 171: [ 342  435  667 ... 1352 1446 1441]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5900
INFO voc_eval.py: 171: [126   3 120   4  67  72 151 105  96 103 180  19 144 119 149  25  71  60
 135  69 158 118  68  45  87  20 130  43  65  27  66   6 166 150 121  70
 111 123  47 155  46 152  32 170  24  35 174  11  38 177  50  41 142  78
  76  83  16 133  23  10 184  59  63  28 143  37 107 137 104  90 159   7
  86  73 134 162 176 129 114  82 164  56 172   2  84  85 127 112  34  51
  14  98  99  53  81  22  74 145 122  79 154  42 146  80  77  29  58 113
 132   0  75  33 156   9 163  21   1  39  62  49  61  88 140 167 106 138
 115 141   5 157 101 153 125  30 165  13 160 117  54 171   8  89  92  97
  64  48  18 102 147 182  12 169  44 175  91  15 109  36  52 100 173  94
 161 168  93  40  17 124  57 181 136 148 116  26  31 139 110  95 178 179
 108 183  55 128 131]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2825
INFO voc_eval.py: 171: [208 205  71 280 300 224 434 145 226  16 167 338 230 315 299 321  75 210
 207 382 193 206 415 176 357  96 320 416 215 177 285 432 251  17 116 260
 209 190 268 144  80  67 253 247 402 127  89 139 258  49  93 228 157 164
 403 174 121 323 140 363 143 322 440 364 441  60 179 352  40 381 123  81
 160 339 365 212 316 126 298  21  73 303  94 173  41 270 358 232 213 293
 117  35 263 259 202 189 409 229 369 138 335  18 172 286  66 241  31 134
 433 368 234 147 289 423 110 340  23 214 309 261   4 419 129 354 239 221
 195 130 330 319 318  56 178 384 397   5 374  85  32 367 411 435  68 413
 252 103   3 187 115 436  46  42 334 233 277 288 136 426 442 155  83 431
 125  51  77 388  36 372 279 370 393 296 327 310 275 430 345 264 222 146
  82 271 342 297 351 184 302  37 290  19 105 326 314 182 223 248 380 418
 399  11 344  55 227 250 255 361  54 201 329 396 371  44 312 128 132 113
 218  79 295 108 180   7   6 427 240  30 158  97 355 398 191 175  74 231
 249 150 163 142 135  45  13 181  57 414 376 400  64 428  25 262   8  62
 404 153 282  48 438 349 407 122 313  29  61  92 304 165  72 159  91 131
  22 291 225 154 292 439 276 324  38 422 192 235 390 331 379 386 356 141
  26 307  50 353 166 337 265 395 148 362 101  24 287   2 100 217 272 238
 188 267 375  12 366 391 245 124 199 257 378 168 311 360   9 421 161 211
 306 170 152  78  28 408 359 392 425   0 269 149  59  84  98 220 305 197
  88  90  99 236 133 111 196 119 204 417 420  27 156  95 243 162 109 186
 284 406 401 118 385  69  63  39 242 106 203 328 194  70 341 294 107 244
 237 102 412 394  14 274 377 151 278   1 200 281 373 169 317 350 343 346
 114 405 273  52 266  86  53 301 185 256  47 216 437 348  10 410  58 308
 429 325 424 254 198  43 383  15 387  20 389  34 171 183 104 336  87 347
 246 283  76  65 332 333 219 137 120  33 112]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3844
INFO voc_eval.py: 171: [425 269  73  83 139 363 228 138 112 426 143 310 313  75 229 368 119 277
 240 586 339 168 142  82  52 556 366 348 477 274 538  67 484  77 362 271
 598 199 116 108 117 268 333 478 144 301 499 218 314  98 249 545 600 230
 298 103 297 396 215  72 596 217 568 620 614 398 145 567 364  99 494 404
 113 263  13 399 122 486 120 205 299 355 252 204 200 166  16 594 137  66
 326 554 191 347 342 617 241 308 194 444 467 315 121 512 421 358 365  57
 262 272 161 155 427 340 619 104   5 176 522 448 293 195  21 219  45 455
 114 616 169 561  71 319 578  22 236 624 613 431  69 361 441 320 510  53
   8 564 221 481  70 485 553 451 610 489 505 385 458 414 433 609 160 258
 571 369  92 174 335  63  38 170 171 165 483 350 376 289  76 560 500 548
 482 296 434 330 251 612 559 402 311 591 479 519 415 392  30  60 201 354
 532 497 436 304 511 359 387 472 305  46 622 128 593 265 302 471 278 530
 129 312 535 150  93 492 216 259 393 141 226 300  17  47 507 287 480 487
  62  29 157 203 175  96 123 245 454 167 416 151 185 473 352 328 411  42
  68   6 490 189 177 621 134 307 457  43 546 110 127 603 243 295 382 248
 254 470 565 213 105 557  35 582 232   7 146 615 234 106 379 101 459 374
 172 370 443 334 124 584 440  11  23 220 524 318 476 439 475 373 107 324
 516 563 136 536 527 562 551 570 488 599 317 606 140 306 109 580 445 432
 182 583 555  24 450 394  34 192 291 279 111 184 417 406 331 147 389 388
 253 164 159 518 285 447  54 349 336 590 188 604  18 179 178 496  58 303
  27 424 323 377 397  50 469 357 102 130 592 587  88  19 115 558 521   9
  91 316 618 585 513  49 118 309  41 378 405 410 400 420 162 332 493 338
 237   1 100 321 367  56 523 208 222 581 529  12  28 503 131 438 588  39
  94  40 257 266 223  87 569 437 547  61 442 589 625 183  86 412  84 210
 401 526 275 508 491 152 270 408 514 542 186  89 190 344 577 294 390  14
  55  20 552 202 181 461 156 153 235 525 605 429 608 419 464 290 498  25
 607 409  78 351 435  97 225 322 337 380 180  32 504 602  81  74  90  44
 449 575 187 209 391 540 233 239 597 539 460 133  51 502 329 281 495  10
  15 207  36 446 360 572 501 544 574  48 341 211 407 325 154 541 212 456
 276 537 193 550 528 244 264 595 327 343 206 576 125 267 238  79 132 611
 515 566 197   3 371 506 283 573 430 386  65 214 261  64  85 256 534 533
 509 383 260 282 227 250 346 247 280 465 403 428 149  26 372 375 413   2
 395 466 422 517 418 453 462 463 474  59 135  37 345 549 242 452 126  33
 468 288 579 273 158  95 224 231 623 384 356 198 196 246 292  80 520 173
  31 163 353 255   4 601 148 284 531 543 381 423 286   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3437
INFO voc_eval.py: 171: [273 506 725 ... 932 498 167]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4802
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4052
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.357
INFO cross_voc_dataset_evaluator.py: 134: 0.603
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.351
INFO cross_voc_dataset_evaluator.py: 134: 0.542
INFO cross_voc_dataset_evaluator.py: 134: 0.573
INFO cross_voc_dataset_evaluator.py: 134: 0.474
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.521
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.365
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.377
INFO cross_voc_dataset_evaluator.py: 134: 0.708
INFO cross_voc_dataset_evaluator.py: 134: 0.621
INFO cross_voc_dataset_evaluator.py: 134: 0.590
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.384
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 135: 0.405
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 9999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.389s + 0.002s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.364s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.372s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.380s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.389s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.389s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.388s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.387s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.385s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.387s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.387s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.393s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.392s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.535s + 0.001s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.401s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.410s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.405s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.399s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.399s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.404s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.404s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.400s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.399s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.397s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.394s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.391s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.468s + 0.001s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.401s + 0.003s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.392s + 0.003s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.393s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.396s + 0.003s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.397s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.392s + 0.003s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.400s + 0.003s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.402s + 0.003s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.402s + 0.003s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.402s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.400s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.397s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 2,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': True,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.531s + 0.002s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.405s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.391s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.384s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.377s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.375s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.380s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.374s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.375s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.375s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.374s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.377s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.377s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_2stream_noreg_copy_freeze-boxhead/Nov12-02-16-14_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.505s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [580 181 902 ... 135 137 663]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3569
INFO voc_eval.py: 171: [205  98 708 485 822 783 638 794 497 824 473 826  99 642 833 641 549 122
 645 828 748 542 793 369 821  84 571 650 479 644 835 688 262 237  55  58
  44 808 550 478 693 106 669 386 616 834 306 698 823 202 425 667 798 525
 303 799 764 382 483 124 765  42 709 520 769 110 103 710 163 289 480 495
 543 166 371 739 488 761 809 544 849 744 308  59 362 541 751 625 692 207
 775 658 192 752 475 569 695 851 267 421 496 206 582  83 756 699 584 647
 574 840 755 857 414 109 788 639 284 714 474 820 477 758 858  56 481 482
 333 643 731 334  14 220 134 861 510 174 619 188 100 305 852 112 670  43
 261 555 527 750 804 721 435 107  51 803 476 498 529 696 817 734 800 323
 364 850  24 140 116 135 357 604 355  87 712 685 493 511  16 553 329 526
 683 219 125 383 288 162 138 173 143 332 470 606 675 860 609 304 492 786
 299 260 665 263 575  67 494 351 594 563 805 570 839 531 339 634 515 530
 829  45 524 715 221 707 115 274   4 119 738 697 589 691 468 682 662 208
 719 283 418 256 789 394 593 763 801 413 694 792 577 153  29 842 434 507
 586  94 234 491 825 269 211 753 523 831 277  60  63 848 568  74 671  23
 319 400 711 401 168 191 229 532 718 268 557 216 154 797   6 279 117 215
 678  40  15 387 105 111 654 126 146 424 855 486 370 608 720 802 680 836
 241 661 300 500 847 169 242  26 155  71 376 441 325 176 703 257 213 352
  88 522 118  34 341 534 170  18 297  36 795  47 226 663 428 298 627 460
 546 396 628 677 499 190 417  78 505 853 509 145 733 767 203 231 193 787
 253 863 335  76 666  54 245 368 630 327 854 440 389 411 780  86 749 228
 197  85 590 350 757 175  90   5 846 774 209 465 378 291 199  72 415 735
  66 312 338  17 572 377 186 726 233 249 469 189 236  92 171 592 144 782
  68 777  69 451 781 244 551 538 605 506 611 790 380 540  79 439 436 864
 405 656 265 796 127 521  30 224 856 687 773  48 187   0 322 806 330 743
 360  20   1 114 547 681 412 545 655  73 814 581  13 830 238 664 622 379
 700 250 293 651  46   8 732 266 660 591 865 184 359 736  93 381 614  35
 183 313 646  21 456 762 812  70  38  19  10 585 210 361 307 672 705 457
 248 423 272 717 419 320 742 807 819  11 432 502 813 212 204 384 444 648
 632 177 770 442 674 573 537 391 716 340 759 810 392 668 278 729 760 161
 607 528  89 287 640 689 636 222  41 264 576 290 429 385  80 363 165 503
 164 832  97 326 791 317 615 344 185 462 251 859  75 179 321 722 121 598
 410 464 771 818 239  65 431 200 123 449 741 862 230 172 562 724 252 701
 225  53 596 657 295 776 602 438 120 487 518 227 273 730 448 772 561 157
 318 430  82 346 198 141 686 811 766 659 453 246   7 223 704  22 101  95
 232 754  37 311 302 548 461 217 533  91 240  12  39 566  77 472 556 286
 393 407 160 366  33  25 180 844 422 282 375 292 554 243 388 342 784 653
 271 194 637 358 402 353 504 182 314 235 559 633  28 827 621 727 779 445
 426 467 310 631 301 195 676 167 578 673 349 296 294 406 365   3 558 258
  32  27 620 150 129 147 747 768 158 845 785 690 567 702 626 280 108 816
 395 337  50 420 416 579 324 196   2 214 535 519 516 618 315 597 356 490
 583 740 560   9  64 255 514 841 316 247 443 408 152 137 617 218 580 459
 367 539 427 136 623 132 254 649 458 409 309  96 348 837  52 354 347 148
 285 471 159 706 684 588 603 139 390 600 463 275  49 102 629 399 454 612
 517 104 452 281 328 113 613  31 343 728 723 437 564 601 552 725 652 635
 131 508 270 404 536 838 336 610 374 737  81 130 746 178 373 446 156 745
 142 151  61 455 624 397 513 778 259 331 403 128 843 713 345 466 484 149
 398 595 501 815 450 372 276 433  62  57 489 133 512 201 587 599 679 181
 565 447]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6135
INFO voc_eval.py: 171: [127 330 387 248 250 813 383   4 814 363 249 815 746 750 600 658 499 261
 495 130   7 253 747 307 818 301 350 885 816 284 389 184 673 836  16 667
 634 827 819 853 304 828 654 344 378 195 817 497 364  37 910 427 864 664
 838 489 905 900 550 752 672 825 365 432 369 302   9 430 820 221 511 589
 780 545 256 855 487 232 138 837 823  64 370 348  51 385 842 126  55 156
 628  96 575 690 720 912 831 496 285 839 287 479 697 500 826  80 384 161
 808 274 338 359 869 899 257 172 333 129 262 477 581 308  20 346 224 683
 272 252 293 785 513 627 897  38 125 328 154 868 640 153 152 123 566 185
 488 434 861 435 329 557 741 684  98 789 186  65 245  35 655 749  22   8
 160 118 297 735 534  57 142 653 896   6   5  62  66  82 603 493 523 176
 468 580 187 645 351 556 137 522 349 141 263 155 822  17 451 562 401 438
 390 400 806 291 404 396 685  95 502 238 163 366 135 470 559 643 219 558
 244 463  87 183 533 549 551 760 830 246 771 294 644 478 608 194 397 448
 188 773  14  72 769  81 368 403 857 821 213  11 391 574 782 626 788 504
 251 755  61  70 790 538 898 590 572 475  25 225 472 165 879 442  78  39
 565 124 881 524 305 762 241 578 323 695 143 754 371 144 552 316 199 761
 809 340 583  89 439 300 561 902 355 360 758 473 652 332 894 801 336 242
 742 745 542 615 476 666  68 466 702 231 835 214 795 564  77 464 492 196
 517 168 393 230 630 281 311 310 105  52  26 709 772  23 631 787 692 689
 247 804 164 544 191 858 732  60 660 845 173 530 554 921  71 456 353 531
  49 886 526 892  75 592 811  85  29 114 841 786 312 753 698 563 840 484
 573 878 616 243 547  36  40 171 133 178 264 767 874 604  41 324 258 659
 422 182 483 876 429 444 663  30   2 584 887 501 609 373 548 802 668 553
 585 633 849 723 646 361 140 693 621 911 908  76 733  67 485 395 278 121
 596 266 805 516  12 909 486 347  63 309 859 800  24 877 512 335 269 273
 131 916 217 295 303 367 375 382 661 134 670 527 904 582  44 568 728 703
 718 555 778 629 872 189  86 642 425 560  33  34 579 306 480 783 867 567
 529 235 712 824 239 290 736 541 893  58 620 149 520 671 405 240 731 882
 734 650  45  73 216 132 296 117 227 576 177 362 433 460 271 229 372  94
 679 498 657 265 443 104 337 571  56 159 203 490 506 354 169 569 807 459
 570 192  53 356 441 546 588 352 792 528 167 467 577 107  42 895 740 704
 602   1 170 601 277 632  90 318 915 452 922 447 437 386 757 656 157 279
 342 283 595 738 201 919 162  59 594 420 519  46 721 392 856  10 591  74
 381 833 445 446  13 457 675 482 860  54 358 399 622 641 764 436 440 751
 454  97 639 700 428 694 218 458 236 226 674 319 850 106 481 115 423 190
 469 844 765 212 710 128 618 268 119 220 715 415 419 597 207 605 680 234
 254  32 798 112 147 871 687 394 539 331 766 421 148 913 298 280 175 848
  83 677 515  15 681 449  31 607 891 729 327 267 810  88 791 424 866 204
 410 903 322 617 228 509 406 407 883 120 116  28 691 635 812 174 450 920
 237 716 784 743   0 532  69 613 388 374 166 211 339 223 779 299 705 158
 722  92 763 598 537 208 610  99  43 669 593 890  48 901 768 357 377 102
 455 108 665 409 730 233 270 851 832 113 453 637 699 759 587 376 508 198
 205 847 398 503 852 625 321 465 521 777 471 611 259 888 345 317 606 793
 797 854 739 325 799 505 431 880 179 418 725 282 314 756 612  91 536 222
 706 334 122 776 599 713 525 708 507 870 103 917 586 146 426 781 136 727
 623 676 662  50 794 275 540 834 341 863 535 462 474 416 803 889 402 737
 696 678 636 202 181 288 748 914  18 255  27 320 215 744  47 884 139 796
  84 343 907 829 614 313 774 289 380 193 865 210 209  79 417 719 701 647
 711 292 326 638 775 413 414 408 379 200 906 717 260 770 315 286 648 111
 100 110 109  19 101 707 150 714 688 682 724 726 651 649 624 619 543 145
 494 461 197 875 411 873 180 846 412   3 686 491 843 518 510  93 862 206
 276  21 918 151 514]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2243
INFO voc_eval.py: 171: [1374 1375 1595 ... 1125 2054 1887]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3514
INFO voc_eval.py: 171: [1433  830  883 ...  680  433  447]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5430
INFO voc_eval.py: 171: [ 74  47 140 143  19  51   9 224  49  90 180 380 157 284 294  55  33 357
 359  66 248 276 264 309 275 369 208 196  53 378 335 184 136 142 305  44
  52 379 290 299 206 353 261 113 271  56 316 178  80 237  58 188 222  48
 166 219  27  16 265   7 258  18  59 321 165 390 103 385 398 228 310 329
 311 332  22 152 162 391  92 128  95 154 283 148 251 115  34  89 215 287
  39 317 121 101 204 110  40 393 365 344  37  96 243 129 210 282 201 377
 211  28 245 229 392 395  57 145 241  68 137 381 213 207  84 345 239 189
  88 286  64 235 333 343 318 298 352 277 322 167 300 320  50 174 340 244
 289   4  82 177 199 104 325  70 205 161 314  91 361  17  12  23  54 301
 125  21 133  60 102 295 163 114 338 312  11 149 349 293 183 187 107 240
 252 346 179 250 337 141 315 272 246 231 368  38  24 386 109 383 124 358
 108 330 100 116 373 257  43 171 173 158  32 362 291  75 242 159 285 375
  26 191  46 342 181  15  87 197 270   8 232  73 120 334 273  85 112  30
 260 195 313 254 355 306 382 227 130 274  97 259 400  41 370 226   3 221
  71 111 123 150 336  61 139  86  42 288 266 319 354 172 117 234 303 339
 262 155 132 356 156 126 209  93 216 223 253 296 134  69  29 292 323 118
 364 203  78 194 212 267  72 331 220 135 190 160  76 281 268 225 193 122
 182 175 218  25 374 350  10 249 153 168 106 202   2  81 399 384 185 278
 327  98 387   0  35 147 144 200 269 192 371 164 186 105 256 351 326  99
 397 255 396 119 297 170  79 376  63  45 360 151 247  13 217 389 263 198
 236 138  62 394 308 146 233 230 176  31  36 328   5 324  65 304 280 348
 366 127  67  14  94 307 363 341 372 169 367  77 279  83 347  20   6 302
 131 238   1 214 388]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5734
INFO voc_eval.py: 171: [1094 1144  500 ... 1052  980 1537]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4734
INFO voc_eval.py: 171: [ 11   6  24 117  25 105 114   8  45 109  40  62  83   1  82  85 111  58
 108  19  80  81  47  95  86  12  63  14  34  21  57  73   5 115 118  41
  29  49 110  54  10  33  96   7  60  72  78  44  37  30  87 106   9  20
  16  64 119  88  61 103  13  76  70  31  68  46  90  50  92  35 107   0
  32  53  91  71  94 116 104  77  48  79  36  66  27  93  22  18   2  69
  28   4  98 120  84  17  75  39  51  52 112  67  74  23 101  65  42  89
 102  38  56   3  99 100  43  26  55 113  15  97  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0056
INFO voc_eval.py: 171: [5829  646 2533 ... 4062 5869 5451]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5193
INFO voc_eval.py: 171: [  4  77   5 129  47 104  46  43 114 143  76  50 139 145  97  66  78  48
 113 146 106  40  92   8 121 134  34 108 133  63 158  93  33   9  69  61
 112 116  94  53   6 140 120  83 118 156  45 100  82  80 132  41 142 144
  85  49   3 131 101  14  58  17  86  95  44  18 107  27  52  54 141   7
 152  39  22  73  81 137 103  89 128 135 105 136  60 122  57 153  75  25
  38  59  91  26 119 130  56  90  32  11  64  96  42 138 115  62  30  28
 117   1  87 111  21 150   0  79 109 155 125 147  55 151  51  98  99  74
  31  20   2  88  10  29 148 127  35  37  68  70 126  12 149  84 124 110
 102  23  71 154 123  72  24  67  65  13  19 157  15  16  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1818
INFO voc_eval.py: 171: [ 877 1924  471 ...  406 1608  470]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3637
INFO voc_eval.py: 171: [105  97 191  98   3 153 102  19  56 106 211   7  62  40  36  46 209  58
  67  92  33 208   1 189  27 195  12  80  66  81  57  13 168 164 133 145
 155  37 120 154 138  91  34 203 205 222  90 125 223  73  48 175 201  60
 213 126 156  10  86 192 218 160 121 116  15 224  11 190 103  99 170 118
  65 198  26 124  75  68 147 187 129  45 194 128 130 146 132  63 159 135
 122  39 104 210 152 123 176   5 110   2  85 113  16 108  23  84 177 150
  64  43 111 148 112 200  14  71 172  51 206  22 127 188 107 204  29  76
 140  59  96 193 196   8 143  69  31 131 169 171 167   4 165  82  78  47
  41 158  70 174  32 166  25  44 173  21  17 182 142 225  77   0 101 220
 163 217 207  24 114 179 137  61  95  89 185  72 212  49  74 109 161 184
  55 221  83  87  30  28 100  35  94 216   6 214  53  18 117   9 215  52
  42 186  88 119 197  38 181 144 219 139 151  79 115 199 157  93 141 183
 134 149 180  50 136 162 178  20  54 202]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1167
INFO voc_eval.py: 171: [146 129  29  22 173  96 132 183 128 147 190 154  65  24  34  78  23 122
 105 178  66 126 101 149 102 156 171 170 108  71   9 169 157  10 130  70
 131  82  55 111 112  87  50  49  36  33 152  64  90  47  59 179   5 133
  42  44 124 100 163   1  79  56  97  12  76  48   2 150 175 153 167 144
 145  67 181 123 168 107  16  91  77  80   0 114  27  32  84 148 166 164
 185  85 135 187 177 134 158  41  69  72 160 106 188  51 172  52 180  38
  95 155  53 186  98  63  28 127  13  11  93  35   7  14 104  18  19 182
  25  62 136 162 138 137  99   3  21 142 110  15   8 151 116  89  26  92
 113 125  61  81 174 191  83 141 176  20 121 117 119 115  37 120  39  73
 143  94  43  68 103  17   6 159  40  88 165  30 140  46 184  57 139 189
  54  60  75  31  58 118 161  45  74  86   4 109]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3773
INFO voc_eval.py: 171: [ 95 542  73 354 277 584 369 259 393 417 495 425 524 438  50  28 109 290
  24 370 467 408 522  96 266 304 264 562 592 145 183 591 168 231 134  67
 223 184 483  11 174 486  64  60  99 413 201  87 445 166 303 470 335 172
 527 598 260 582 387 305 215 345 334 357 478 410 135 586 328  75 306 273
 248 488 398 158 583 344 553 490 114 545 497 113  70 523 204 480 322 443
 538 203 282  55 295 173 446 590 125 208 342 401 421 371 225 359 521 198
 472  43   3 314 220 437  19 373 275 593 324 544 126 464 180 105 299 157
 123 315 411  40 154  44  57  32 533 594 573 381 195 388 312 189 601 429
  91 272  86 106  65 406 525 548 485 493 239 127 572  29 428  82 156 318
 213 596 412 267 448 506 142 481 399 240 129 579 346 375 227 391 116 191
 529 286 150 252 556 540 350 348 121 224 323 526  69 560  93 498 500 301
 595 214 185 255 367 205 439 597 395 534 148 396 574 469 258 537 509 531
 420 530  56  45 449 285 177  15 392  27 179 307 394 297 289 561 138 317
 440 569 458 536 340 390 484  61 489 587 511 263 432 589 496  68   1 447
 294 355 122 100 155 296 353 580 102 190  80 513  18 236 316 108 382  81
 588 565  21 265 384 175 431 504 200 505 170 476  12  94 535 238 422 528
 402 235 327 333 585  79  42  16 426 570 311 475 403 319 216 551 339 397
 424  26 415 600 186 130  92 501 325 151 479  14  31  97  13 261 365 229
 337   9 124 187 383 418 451 452 164 539  34 247 519  17 462  39 196 442
 326 120 219 482  63 136 308 563 244 140   8 547 571 377 117 456  71 444
 378 169 567 532 188 471 132 466 171  59 356 380 280 349 541 160 568 512
 243 423 419 450 101  98  30 133 234 159 147 218 343 336 193 107 278 454
 288  72 300 309 514   4 520 557 139 131 576  41 143  46 358  22 330 262
 320 491 103 463 194 515 376 249 518 209 400 242 146 182 197  52 414 404
 212  36 149  84 291 499 372 427 362 468 494 211 241 503 270 287 389 110
 292  37 352 176 543 271 250 226 144 281 351  33 363 435 465 441  77 284
 558 602 366  85 162 453 112 141 152  62 118 128   2 599 217  10 405 564
 167  58 549 161 434 492 245 192  38 546  47   7 283 329 206   0 341  88
 575 433 256 360 222 368 385  20 559  48 430 119 276 153 269  53   6 178
 460 508 473 332 477 487 507 502 237 581 338 232 455 517 461 111 347 246
 554  51 364 550 310 457 254 510 268 279 233 104 207 566 331  23 199 251
 253 181 221 409 379  25  54  66 578 577 298 165  78 436  76 210  89 552
   5 516 407 257 202 293 313  49 163  90  74 228 230 459 302 416 137 361
 374 321  35  83 386 474 555 274 115]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7085
INFO voc_eval.py: 171: [ 2082 11623   606 ...  1731  4882 11195]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6214
INFO voc_eval.py: 171: [ 342  435  667 ...   17 1446 1441]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5901
INFO voc_eval.py: 171: [126   3 120   4  67  72 151 105  96 103 180  19 144 119 149  25  71  60
 135  69 158 118  68  45  87  20 130  43  65  27  66   6 166 150 121  70
 111 123  47 155  46 152  32 170  24  35 174  11  38 177  50  41 142  78
  76  83  16 133  23  10 184  59  63  28 143  37 107 104 137  90 159   7
  86  73 134 162 176 129 114 172  82 164  56   2  84  85 127 112  34  51
  14  98  99  53  81  22 145  74 122  79 154  42 146  80  77  29  58 113
 132   0  75  33 156   9 163  21   1  39  62  49  61  88 140 167 106 138
 115 141   5 157 101 153 125  30 165  13 160 117  54 171   8  89  92  97
  64  48  18 102 147 182  12 169  44 175  91  15 109  36  52 100 173  94
 161 168  93  40  17 124  57 181 136 148  26 116  31 108 110  95 139 178
 179 183  55 128 131]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2825
INFO voc_eval.py: 171: [208 205  71 279 299 224 433 145 226  16 167 337 230 314 298 320  75 210
 207 381 193 206 414 176 356  96 319 215 415 284 177 431 251  17 116 209
 260 190 267 144  80  67 253 401 247 127  89 139 258  93  49 228 157 402
 164 174 121 322 140 362 143 321 439 363 440  60 179 351  40 380 123  81
 212 338 160 364 297 315 126  21 302  73  94 173 269 357  41 213 232 292
 117  35 262 202 259 189 408 229 368 138 334  18 285 172  66 241 432 134
  31 147 367 234 288 422 110  23 339 308 214 261   4 418 353 129 221 239
 317 318 195 130 329  56 178 396 383   5  85 373  32 410 366 412  68 434
 252 187 103   3 115 435  46  42 233 333 136 287 276 425 441 155  83 430
 125  77  51  36 387 369 371 278 392 295 326 309 274 263 429 344 146 222
 341 270  82 296 350 184  19  37 301 289 105 325 313 223 182 248 379 417
 398 343  11  55 227 250 360 255 328 395 201  54  44 370 311 132 113 128
 218  79 294 108 180   7 426   6 240  30 158 397  97 354 175  74 191 150
 163 231 249 427  13  45 142 375 135 413  57 181 399  64 403  25   8  62
 153 281 437  48 406 312 348  29 122  61  92 303 165 159  91  72 131 225
 154 291 290  22 192 438 235 323  38 275 421 355 385 378 330 389  50 306
 336 141  26 264 166 352 394 148 361  24 101 100 286   2 188 217 238 271
 365 374  12 266 124 245 199 390 257 359 310   9 168 377 211 420 161 170
 305 407 391 152  78  28 358 424   0  59 149 268  84  98 220 304 197  88
  90 236 196 133 111  99 419 119 416 204  95 156 243  27 283 162 186 109
 405 400 118  69  63  39 384 203 327 106 194 242  70 340 107 293 273 277
   1 151 244 102 376 237  14 411 393 169 316 349 280 372 200 404 345 272
 265 342  52 185 114  86 256  53  47 300 409 436 347 216  10 198  20  34
  43 428  58 423 382 324  15 254 386 307 388 171 183  65 282  87 219  76
 335 246 346 120 104 331 112 332  33 137]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3844
INFO voc_eval.py: 171: [425 269  73  83 139 363 228 138 112 426 143 310 313  75 229 368 119 277
 240 586 339 168 142  82  52 556 366 348 477 274 538  67 484  77 362 271
 598 199 116 108 117 268 333 478 144 301 499 218 314  98 249 545 600 230
 298 103 297 396 215  72 596 217 568 620 614 398 145 567 364  99 494 404
 113 263  13 399 122 486 120 299 355 205 252 204 200 166  16 594 137  66
 326 554 191 347 617 342 308 194 241 467 315 444 121 365 421 358 512 272
  57 262 161 155 427 340 619 104   5 176 522 448 293 195  21 219  45 455
 114 616 169 561  71 319 578  22 236 624 613 431  69 361 441 320 510  53
   8 564 221 481  70 485 553 451 610 489 505 385 458 414 433 609 160 258
 571 369  92 174 335  63  38 170 171 165 483 350 376 289  76 560 500 548
 482 296 434 330 251 612 559 402 311 591 415 479 519 392  30  60 201 497
 354 532 436 304 511 359 387 472 305  46 622 128 593 265 302 471 278 530
 129 312 535 150  93 492 216 259  17 300 226 393 141  47 507 287 480 487
 157  62 203  29 175  96 123 245 454 167 151 416 185 352 473 328  68 411
  42   6 490 189 177 621 134 307 546 457  43 110 127 603 382 248 243 295
 254 470 565 105 213 557  35 232 582   7 615 146 106 234 101 379 459 374
 370 172 443 334 124 584  11  23 440 220 524 318 475 439 476 373 107 324
 563 562 516 136 527 551 570 536 488 599 317 306 140 580 109 606 445 450
 432 555 182  24 583 394 192  34 291 184 279 111 417 389 406 388 331 147
 164 159 253  54 349 285 518 447 188 604 178 179 336 590  18 303 496  58
 397  27  50 377 424 323 357 469  19 587 102 130  88 592 558 115 316 521
 618 585  91   9  49 513 309  41 118 378 410 332 420 400 162 405 493 338
 208 237   1 100 222 321 367  56 523 529  12  28 588 131 438 503  39 581
  94  40 257 266 442  87 437 569 223 589 547  61 625 183  86 412  84 210
 401 526 275 508 491 152 270 408 514 542 186  89 190 344 577 294  55  14
 202 552 390  20 181 153 461 605 156 235 525 429 498 290 409 607 608 419
  78  25 464  97 435 322 351 225 337 380 180  32 504  74 602  90  81 391
 187 575 209 449  44 540  51 239 597 233 539 502 495 329 133 460 281 572
  10  36 207  48 446  15 501 360 544 211 154 341 407 574 325 456 212 276
 541 537  79 550 528 193 244 264 595 343 327 267 238 132 125 206 576 611
 430 283 386 197 566 515 371 573 506   3 214  65 261  64  85 256 534 533
 509 383 260 282 227 250 346 247 280 465 403 428 149  26 372 375 413   2
 395 466 422 517 418 135 453 462 463 474  59  37 345 549 242 452 126  33
 468 288 579 273 158  95 224 231 623 384 356 198 196 246 292  80 520 173
  31 163 353 255   4 601 148 284 531 543 381 423 286   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3437
INFO voc_eval.py: 171: [274 508 727 ... 500 251 167]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4802
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4056
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.357
INFO cross_voc_dataset_evaluator.py: 134: 0.613
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.351
INFO cross_voc_dataset_evaluator.py: 134: 0.543
INFO cross_voc_dataset_evaluator.py: 134: 0.573
INFO cross_voc_dataset_evaluator.py: 134: 0.473
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.519
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.364
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.377
INFO cross_voc_dataset_evaluator.py: 134: 0.708
INFO cross_voc_dataset_evaluator.py: 134: 0.621
INFO cross_voc_dataset_evaluator.py: 134: 0.590
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.384
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 135: 0.406
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
