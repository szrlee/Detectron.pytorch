INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.339s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.373s + 0.003s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.376s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.374s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.377s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.383s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.378s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.379s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.378s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.376s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.374s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.374s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.374s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.551s + 0.001s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.378s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.388s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.383s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.382s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.380s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.383s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.380s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.379s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.379s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.379s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.380s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.376s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.431s + 0.002s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.357s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.348s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.357s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.355s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.356s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.354s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.357s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.360s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.360s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.361s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.362s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.361s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.492s + 0.002s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.352s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.358s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.361s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.366s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.358s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.358s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.360s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.359s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.358s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.356s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.354s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.354s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 54.217s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [104 137  99 118  73  84  48  70  82  74  62  60 138  63 142 144  50  66
  98  81   0  79 266 206  86  91  83 261 108 166 259 102 389  97 168  77
 226  32 116  58 115 139  59  93 183 124  75  42  67 191  54 229 189 129
 215 230 312 289 227 263  41 136  80 327 332  49 361 264 207 247 219   9
 117 195 126 291 132 112 110 177 380 281 257  52 404 399 114  45  65 330
 145 378 317 310   4  30 213 290 185 283 216 260 396 131 360 406 101  11
 123 220 224 358 210 331 302 100 313  47 322 265 395 146 376 394  96  61
 398 127 284  19 169 125 316 397  18 335  53 134 408   5 107 208 113  78
 222 209  68 218 128  72  13   3 403 336 413 211 155  69 204 304 293 212
 341 343 167 320 174 180 385  31 279  90 340 130 233 119  55 140  76 188
 301 205 175 356 161 410 305 362 234  28 314 412 164   2 141 255 135 254
 246 109 276 285 388  94 201 151 414  15  24   8 269 409 333 165   7 192
   6 295 363 298 262 405 386 223 345 292 121  46 402 106 286 156 309 120
 382 377  64 273 103 375 366 202  17 339 190 143  14  92 278 393 196  51
 407 268 176 105  88 133 147 186 198 148  34 243  71 372 400 267  95 354
 237 275 170 367 280  35 329 245 232  85 277  26 334 300 415 160 411 355
 282 122  16  87 319 383 274  89  10  25 381 359 111 179 352 159 328  21
 296  22 387 325 294  20  12 344 200 150 225 158 236  27 357 187  37 221
 244 390 172 249   1  40 337 323 321 178 250 197  23  29 214 392 364 242
 256 253 271 173 193 311 326  57 157 270 272 194 182 203 217 315 306 171
 149 199 154 368 303 324  38 365 391 163  56 379  33 346 308 349 239 297
 350  44 342 251 347 252 369 401 184  43 371 181 353 287 240 235 299 374
 248 348  36 153 384 370 338 231  39 318 228 307 162 258 241 351 238 152
 373 288]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1389
INFO voc_eval.py: 171: [ 61  46  35  82 173  67  86  37 161 172  40  76 144  95 110 167 135  72
 169 160 156 165  55  19   6  39  66 162 109  59 142 159 168  56  73  49
 105 164 103  36  91  74  75 163  13  87 166  15   4 146  41  58  60 125
 117  54  83  64  30 119 141  18 151  70 149   9  25 118 100  85 101  12
  68 111  77 128  69  57 157  78  51  45   8  62 107 170 143  48  27 155
  80 113  20  99 158  84  17  14 129  32  10 171  71  79  81   3  24  63
  94  16 138 108  21   5   7  96 123 147  50  93   1 154 152 130  22  44
 106 131  98 102  23  88  11  29  34  26 127  33  65 133  28  38 112  42
  97 116 137 115 136  52  31 134 126 132 104 122 120   0 139  92  43 140
  90 148  47 145 114  89  53 150 121 153 124   2]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3007
INFO voc_eval.py: 171: [1377 1886  926 ... 2078 1302 1656]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0404
INFO voc_eval.py: 171: [ 146  893  972 ...  431 1051  512]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1057
INFO voc_eval.py: 171: [805 798 663 ... 710  44 233]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1960
INFO voc_eval.py: 171: [ 96  37 142  62  92  45  34  97 108 107  49 110 149  30 148 180  63  32
  52  99  61  48 192 140  43  65  67  47 146  70 147 104  79 189 101  66
 191  36  95  35  14  29 141  23  93 138  39 144 178  71 184  21 112  68
  55  77  33  60 109  27 125 156 105 154 106 162  42 113   9  64  54 111
  13 100  59 143  26  75  50  94 139   0  83  20 177 136  46 179 155 158
  41 185 130  82  81  76  73  22 182 114  51 188 150 193 164 163 187 133
 190 126 151  57 166   8 153 157  88 116 183 169   7  98  16  40   1  53
 172 102   4  44   3 168 152  15  31  85 117 124 131 165 174  38  78  11
  18 135 127 122 170 137  12 161 119 128   6  89 194  87  24 123 181 120
 160 118  80 186  74  58 134 115 167  10 159   2   5 195 103 132 176  19
 121  56  84 175 171 129  69 145  90  86 173  17  28  91  25  72]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4105
INFO voc_eval.py: 171: [777  39 761 494 237 780 548 404 769 787  96  94 241 499 610  55 206 651
 768 495 607 166 279 836 559  93 171 736 284 621 648 235 238 501 737 550
 207 440  48 781 332 613 830  88 653 517 413 549 165 278 285 172 746 163
 276 558 582 245 100 824 191 304 150 513 108 428 445 406  45 762 116 110
  46 419 803   8 608 814 255 107 430 214 114 829 256 414  84 218 184 297
 722 776 503 280 167 632  70 509 498 338 764 774 192 305 838 623 568 247
 745  29 397  59 592 617 507  74 134 748 227 215 742 398 598 709 105 113
 556  89 140 611 553 348 747  52 446 868 197 631 310 173 286 616 158 271
  75 177 300 290 211 435 187 112 676 422 186 299 248 602  81   9 352  82
 779 845 846  64 224 620 622 185 298 564 566 645 786 240 385 377 118 817
 469 844 178 412 291   1  53 738 837 119 641 839  67 532 500 772 561 101
 804 552 741 784 131 468 577 194 307 801   4 514  12 244 778 614 239 258
 763 375 125 470  92 269 156 759 471 818 575 340 717 292 179 139 216 557
 554 339 232 725 584 346 442 533  20 850 587 703  58 265 849 711 102 408
 275 162 210 126 684 642 251 516 436 626 465 421 358 220 511 583 328  34
 506 565 520 579 654  87 104 133 363 429 223   7 273 160 599 416 302 189
 720  10 713 219  73 696 106 368 508 810 595 851 355 871 394 858  85 647
 668 196 309 753 580 157 270 585 570 730 386 242 296 183  77 739 693  69
 472 257 228 294 634 181   6 555 254 349 636 563 342 515 754  24 109 695
 267 630 345 678 336 130 230 222 794 707 425 438  47 860 208 229 209 263
 619 593 217 710 526 121 755 816 796 143  23 639 705 205 721 625 723 731
 848 716  44 250 686 117 329 869 590 141 574 115 233 129 612 534 369 683
 589 351 437 431 478 175 432 409 538 288 527 467 715 382 234 671 132 381
  66 834 383 751  13 236  36 444 665 128 852  63 783 726 473 461 596 833
 138 825 326  25 865 448 322 832 854 249 659 518 159 272 401 685 805  61
 424 835 350 663 135 638  78 504  22 225   2 831 523 727 325 706 357 826
 317 266 484 863 606 411 402 120  79  51 793 393 681  95 481 604  18 603
 744 313 857 547 321 535  16 819 859 146 646 152 231 334  49 870 694 341
  17 597 395 360 370 221 212  83 460 123 782  56  98 124 767 281 701 853
 168 475 512 569 243 333 144 655 867 828  32 365 821 843 740 797 644 618
 454 492  11 760 864 403 792 847 378 539 807 327 505 379  99 802 198 800
 347 788 627 719 615 153 662 758 306 193  50 712 813 274 161 367 400 811
 122 856 637 466  21 812  60 673 282 169 103 316 318 268 785  71 872 581
 195 308 700 319 151 482 588 628 633 331 698  91 551 600 145 601 190 384
 303 147 578 320 426 373 855 203 522 562  35 496 366 441 462 823 262 815
 447 609 586 389 371 155 708 374 734 841 474 457 866 142 733 362 455 567
 323 136 822 137  15 439 148  37 252 420 199 643 691 521 483 791 289 176
 546 752 337 380  14 314 704 689 560  80  97 356 388 497 697 253 530 111
 180 591 434 201 293  40 682 449 649 154 862 543 664  42  27 458 149 335
 688  54  90 315 861 476 127 392 415 311 459 640 372 525  76 479 396 353
 765 226 540 359 757 629 387 343 680 718 261 510 213 771 652 364 670 376
 463 576  57 405 287 456 391 451 174 660 200 519 450 594 260 536 529 571
 820 657  30 605 724 635 728 531 773   5 624 246 842 410 799 656 808 873
   3 427 661 486 650 809 418 537 729 433   0 806 477 330 672 677 491 687
 202  33 390 766 735 361  68 541 452 524 464 493 312  26  86 423 545  62
 490  31 259 840 789 489 669 666 528 667 749 295 699 770 182 354 417 714
 485 658 301 573 170 188 283  19 743  41 702 542 487 750 453 399 344 775
 756 795  65 164 572 674 277  38 324 480  72 679 407 204 675 502 443 264
  28 827 544 690 692 488 732  43 798 790]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1733
INFO voc_eval.py: 171: [251 201  20  31 250  36 255 205 191  23 102 210  37 104 151 101  22  40
 193  28  18 184  32 174 254 252 215 245 192 100 103  99 206  26 212 203
 182 256 195 253  25 175 237  73  68 248 244 119  17 197 230 194 238  64
 222  83  34 152   5  39  81 223 219  19 132 107 243 172 247  41 204   9
 118 249 257   8 209  82 186  35  84  75 181 105 128  61  67 217 242 239
 147 200  53  80 153 111 113 123 196  63 180 149  44 241  29 246   6  98
  66 156 146 139 211 112 232 106 116 158 218 190 185 122  90 178 240 121
 213 208  72  38  77  54 155 221  71  46  52  79 166  76 163 231 235 110
  16 150  62  48 160 199 159 145 168 117  95 171  65 228 169  45 144  51
   4  15  11  88  91  86  30 154 157 176 141 198   3 214  94 224 148 227
 125  13  43   0 173 114 140 138 229  24 226 170 131  49 207 161  78 179
 143  12 130  85  47  55  69  42 133  57 129 108 233  93  50  33  27 216
  92 142 127 234 135 126 124  97 167 236 177 137  58 136 134   7  70 220
 202  96  74 162  14  60  87 188  10 183 187  59 225   1  56 120  21  89
 115   2 189 164 109 165]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0359
INFO voc_eval.py: 171: [ 405  182  648 ...  754  477 1371]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1956
INFO voc_eval.py: 171: [501 106 516 235 413 245 203 201 511 514 208 206 512 419 240 655 237 442
 470 209 287 117 662 197 238 217 670  63 243 119 116 140 213 216 508 276
 230 321 199 695 108 248 260 215 266 190 262 506 653 115 674  23 184 173
 104 835 838  20 271 317 515 425 482 247 185 528 244 251 278 676 211  33
  51 281 738 666 200 195 275 600  82 189 325 667 214 418 178  67 168  49
 231 837 672 665 268 138 696 118 444 654 236 451 179 880 226   2 256 610
  93 893  83 322 399 836 466 362 659 465 879  65 518 431 890 868 269 616
 205 370 233 198 229 314 220 194 543  75  57 678 316 770 421 739 460 850
 101   4 414 257 469  30  88  14 111 882 365 137 382  60 381 259 375 102
 545  96 754  15 155  66 846  72 249 109 274 585  87  32 783 883 458 843
 232 635 829 290 510 560 542 207 865 452 669  22 123 499 440 459 706 817
 513 606   3 818 333 224 188 395 844 841   0 242 327 462 114 525 227 521
 611 228  86  48 527 663 687 135 854 284 186 318 146 551  94 191 225  34
 121 765 128 845 416 640 891 744 539 423 630 267 874 740 329 474 461  68
 872 152 246 363 586  89 847 387 855 432 796 537 241 212 255 344 507 277
 430 187 301 760 300 282 289 221 555 181 603  18 165 234 709 210 866 531
  71 784 886 875 143 261 554  17 520 312 699 467   8 504 730 396 851 183
  10 553 762 857 813 429   7 394 453 170 455 757 279 526 456 122 877 296
 500 598 176  80 729 219 601 707  29  74  97 263 618  50 196 780 656 160
 401 105 364 285 642 773 638 887 450 328 250 571 167 711 239 689 478 833
 862  98 254 483 398  52 443 563 745 782 831  21 544 550 193 337 523 533
 839 605 726 842 830 130 540 484 561 192 558 529 397 628  85 705  62 420
  24 313 392  40 477 352 302 860 577 884 591 331 820 584 448 763  28  56
 502 180 218 372 435 710 819 359 252 811 871 633 778 777 120 366  95 171
  73 297  54 162 759 671  19  53 113 371 166 559 307 100 683 661 389 391
 349 825 142 741 771 487 384 417 265 383 222 145 182 775 280 535 126 415
 379 552 849 376 339 522 388 386 159 700 812 809 479 876 304 562 657   6
  45 592 368 390 360 631 617 892 613 804  58 733 582 597 808 575 572 668
 779 548 822 629 798 358 742 112 703 270 643 127 682 272 149 604  84 702
 471 536  76 530 320 602 342 403  69 299 367 150 755 859 107 634 404 490
 369 341 393 373 298 288 204 534 273 721 769 427  64 704 202 541 303 457
 437 852 519 546 103 258 725  79   9 619 385 691 589 377 503 532 223 736
 720  77   1 538 253 878 139 652 889 636 594 807 441 556 752 177 480 476
 856 380   5 768 292 587 422 408 608 134  92 888 409 747 264 374  38 133
 708 438 684 651 424 346  42 810 823 378 814 748 132 625 156  11 646 144
 677 800 357 713 472 567 609 881 445 664 693 801 354 802 853  81 174 772
 614 110 719 293 751 648 692 764 590 153  61 848 131  47  43 347 361 557
  31 310 305 750 864 599 447 402  46 828 410 627 495 785  39 324 895  59
 688 832 449 658 547 697 172 615  99 596  27 340 735 746 885 781 815 509
 330 434 412 433  41 824 712 827 496 767 673  91 407  55  12 737 147 338
 774 593  78 141 694 576 564 286 568 690 749 698 680 650 306 701 295 446
  44 426 400 858 468 626  26 355 894  16 353 715 491 124 588 873 660 637
 861  90 283 896 724 647 498 463  13 405  70 716  25 718 309 621 570 169
 620 161 583 681 356 790 863 826 579 869 834 840 803 475  37 821 867 336
 607 125 154 485 753 756 758 816 612 493 870 641 351 649 157 505 806 722
 454 734 488 743 411 497 163 731 791 428 151 787 624 319 136 481  35 714
 164 728 294 623 786 308 789 793 323 632 645 348 644 473 343 573 897 723
 524 675 727 334 486 464 799 639 175 794 494 345 685 679 581 622 517 797
 492 436 580 578 569 595 549 335 315 350 761 158 766 795 686 332 311 776
 566 406  36 788 129 326 805 792 732 717 574 565 489 439 148 291]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.4132
INFO voc_eval.py: 171: [528 791 552 ... 879 224  43]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.1473
INFO voc_eval.py: 171: [ 75 216 537 398  77 504 349  72 214 543 356 337  86  95  76 539 220 215
 253  68 350 290 282 540  98 396 500 283 402 310 461  93  70 424 291 544
 168  51 252 545  90 473 401 547  94 299 348 109 105 218  97 499 463  66
 306 464 159 339 187 246  63 367 440 351 535 347 217 251 295 285 503 121
 286 172 509  52 254 124 478 425 428 156  61 309 374 219 276 305 132 101
 175  87 112 176  91  84 470 174 302 443 289  31 359 454 511 250 129 123
 313 372 421 162 271 131 403 235 481  96 437  89 293 442  88 536 263 239
 459 304  58 292 397 261 400 457 249 527 264 311  80 130 265 419 431 343
 301  56 475 529  19 414 505 161 382  85 274 432 166 284 248 467 247 128
 160 522 472 194  73 446 395  57 259 171 399 531 107 474 384 308 233 280
 354 514 179 365 379  78 195 371  43 190  81 237 405 352   9 241 278 173
 193   8 542 102 439  13 341 258 189 515 100 517 223 460 103 140 114 373
  60 158 502 520 410 119 127 471 303  53 212 363 512 178 378 462 530 526
 510 267 436 135 532   5  65 450 224  71 231 198  49  14 380 157  29  35
 370 279 333 468 294 262 108 180 534 519 422 458 257  27 221  79  21 366
 358 260 300 389  67  83   3 355 433  64 164  45 150 236 225 169 441 394
 423 256 111 426 391 376 434 208 117 438 546 453  48 387 238  62 507 506
 201 516 533 234 455 203  32 318  25 314 229  22 417 137 479 319 477 340
 548 476 226 541  54 553  92 104 167 330 155 338 170 383 364 245 192 115
  28 456 393 322 466 325 375  30 513 133 139 270 141 480 418 492  33 486
  15 487 186  10 344 362 528 392  99 200 469 406 145 357  16  40 281  24
 465   6 138 427 106  46   1 268 368 147  82  12 210 232 153 316 336 321
 335 191 554 110 165 144 483 488 277 549 255 222 184 242 413 185 369  18
 420  50 196 188  39 412  69 227 288 444 199 407 497 323   7 353 346 493
 211 501 508 126 360 324 315 312 136  23 204  37 209  26 118 429 451  11
 385 496 381 377 550 213 152 151  20 404 524 430  47 116 490  17  74 331
 113 327 328  44 447 521 182 388 411 163 273 296 334 342 361 148 538 482
 202  55 525 287 177 518 386 320 205 275 329 452 207 332   0 142 491   4
 523 416 181 408 297  36 326  38 149 298 272 143 122 307 240 317 435 154
 125 269 228 197 120 409 445 498 345  59 552  41 206 266 489 134 494  34
 243 244   2 484 485 390 495 146 415 551 448 183 230  42 555 449]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1397
INFO voc_eval.py: 171: [516 127 614 303 819 661 135 332 656 844 605 507 753 663 128 531 734 575
 667 904 335 909 133 761 159 100 606 822 380 130 645 513  99 658 331 850
 604 665 564 504 459 649 305 458 462 216 321 152 680 382 788 747 138 870
 888 108 304 498 568 541 517 325 115 320 678 648 372 608 657 436 817 655
 675 722 644 102 125 855 866 737 847 157 333 334 106 724 537 676 902 486
 416 221 660 413 723 210 898 383 670 448 569 662 864 868  37 317 113 739
 521 379 629 825 577 418 552 654 306 149 572 798 802 204 150 515 105 469
 908 616 428 742 637 840 832 728 736 745 307 151 165 490 366 779  45 651
 410  43 602 687 812 566 636 464 674 480 596 553 140 743 804 473 414 612
 838 634 219 785 824 156 574 641  47 795 842 409 628 884 811 748 374 893
 350 500 388 780 849 376 107 449 551 826 223 435 549 758 510 387 848 539
 857 831 814 109 209 540 764 828 843 818 891 746 244 815 230 757 487 682
 117 607 835 859 846 137 823 235 163 385 703 772 361 863 143 595 773  30
 906 505 104 858 879 851 550  46 131 807 627 762 523 438 213 671 839 886
 664 224 672 318 869 110 314 686 465 195 311 220 829 841 211 821 673 474
 805 759 810 477  39 556 203 217 797 445 836 431 786 894 485 147 234 877
 601  41 647 732 412 889 816 101  25 365 588 483 176 808 806 310 796 442
 468 119  49 319 845 769 164 633 346 740 895 559 363 729 683 338 565 118
 774 460 830 352 502 422 463 312 590 926 121 497 638 705 316 799 520 813
 669 892 617 562 415 536 330 530 378 389 880 425 783  56 461 854 122 584
 284  27 197 862 750 381 111 918 370 236  48 194 484 735 681 770 800 503
 778 809 206 177 598 214 901 646 160 767 784 337 861 364 328 597 820 591
 499 205 453 833 386 570  58 787 199 225 348 652  10 417 283 172 653  28
 775 554 579 532 309 733 373 467 308  69 749 592 455 443 167 538 603 253
  29 368  81 930 875 524 207 876 615 506 427 103 912 343 439 375 116 198
 923 887 296 933  83 896 470 801 910 196 635 837 166 903  80 527 457 215
 766 329 360 229 327 771 362 702 384 430 358 237 727 243 472 585 558 630
 242 900  19 142 763 924 315 659 803  88 208 666 856 913 399 581 446 874
 685 776 712 356 293  24 519 355 124  44  42   8 222 429 927 291 881 582
 287 359 247 709 704 226 282 377 354 695 865 907 699 353 251 571 145 561
 286 546 834 228 587 272 341 451  32 717  31 129 593  60 201 339 535 751
 905  21 640 609 580 827 136 174 246 599 928 297  11 768 586 162 931   7
 254 323 471 369 212 202 684  33   2 576  35  73 919  16 357  85 248 351
 560  17 114 512  40 218 567 292 193 622 227 496 509   6 481  76 873 557
 200 397 489 120 148 694  79 441  97 294 594  86 290 275  14 899  53  71
 276 929 534 278 288 781 404 262 134  94 447 720 298 872 426 782  66 482
 925 852 173 112 153 623 871 367 233 289 916 793 280 914 613 860 175  38
 440 403 261 715 190 706  15 249   5 433 619 161   0 620 123 529 726  74
  72 285 281 245 514 336 883 398 466 419 765   9 543 789 701 708 721 922
 347 777 853  50 158 240 691 707 911 700 650 169 421 326 878 760 790 154
  65 713 698 632 155  36 744 934 921 917 583 269 260  20  57 890 639 573
  61  34 139 408 511 625  13  59 589  51 168 371  91  22 915 611 390 340
 182 526 690 495 267 411  63 518 270 642 475 188 719 897 279  54 349 494
 697 273   4 493 626  77 432 491 144 679 624  18 187 402 867 738 238 295
 252  84 300 668 791 710 450 396 920  26 643 621 533 478 718 434 423 479
 542 508 424 547 407 391 935  68 693 181 794  82 239 146 501 345 456 263
 265  96 882 302 231 189 400 731 178 186  92 454 756 578 241 232 322 405
 170 528 271 264 741 600 696 277 406 126 525 313  87  62 885 730 257 258
   1 555 937 171 266 563 548 184 618 342  89 250 754 344  52  93 299  98
 932 180 522  70 324 452 268 192  90 631 185 792 492 725 444 395 392  55
  64 183 692 259  67 689   3 274 545  78 301 141  95 256  23 755 255 401
 476 191 132 688 394 393  75 716 437 677 711 714 420 179  12 752 544 488
 610 936]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0761
INFO voc_eval.py: 171: [100  42   6   5  46  95  18   7  15  93  45  53  51  70  94  68 113  52
  99  97  30   9  58   8  84 105  66  21  64  96  25  17  57  79  73 114
 104 108  65 101  16  62  27  74  20  48  43 110  32 106  80  22  47  40
  85 107  28  98  61  19  54  11  56   2  90 103  12 112  69  13  92  31
  26  55  88  29  67 116  38  60  72  49 118  78  23   4  82   1  50  37
  34 102  63  39  10  89  86   3 117  87  75  44 109   0  83  24  59  77
  33 115  36  81  71 111  76  14  91  41  35]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3503
INFO voc_eval.py: 171: [ 1584 13926  3375 ...  9817  2253  2939]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.2347
INFO voc_eval.py: 171: [1013  984  110 ...  393  152    2]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2240
INFO voc_eval.py: 171: [209 123 122 139 172   7 210 177  37  74 155 214   3 107  70 110 211 225
 231  71 234 243 118 257 219  17  85   1 224 116  63  53  10 124 136 132
 105 156 114 170 158 213  45  56  80 174 113  72 135 250 233 119 175 230
 112  82 239 154 215  77 171  51 100 252 121 108  44 176 254 134  75 251
  20  83 163 130 109 162 140 125 133  11 131 247 187 179 199 235 151 228
   8  39  66 208 143  32 128 126  76 223  38  95 138 178  79  19 117  78
  89  42   4  34 144 142 227  92 145 180 129  26   6  64 115  18  21 106
  14 167   5  60  16  67  46 141 248 120 192 194 111 242  43 212 258   0
  47  13   9 160 189  58 137 127 240  22 159 236   2  62  48  96 237 186
 232 173  30  12 168  86  15  93 182 197  91 164 255  59 261 238  55 226
  36 253  57 229 256 103 244  31 102  94  97  87 195 165  73 245 263 193
 169 148 260 262 246 185 198 161  81 259  65 196 200  52  24 157 218 216
 221  25 217 205 166  99  84  90 147 188  33  98  68 101 220 241 152  23
 149  61 150 190  40  54  88  27 146 249 204  29  69  28  50 104 202 206
  41 207 203 222 201 183 181 153  49 264 191  35 184]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2482
INFO voc_eval.py: 171: [ 46  39  67  69  32  44  57  86 112 106  82 100  64  36  58  31  35  28
 102  38  55  21  47 101  22 108  88  94  24   0   2 123  73  99  84  77
  30  87 105  65 118  71  34  25 107   6   4  92   3 103  29   8  63 125
   7  74 113 109  26  66  23  76  93 127 114  90  83 124  54  91 134  70
 133  53  11  41   5 111  40  12  60 117  80 121  98 126  27 116 104  95
 128 122 115  72 129  37  17  50  49  62  51  96  89  20 132  59  19  43
  97   9  10  56 131  61  79  16  75  85  68  45 110 119  42  33  48  52
  13 120   1 130  18  78  14  15  81]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0307
INFO voc_eval.py: 171: [228 235  56 229 234 328 230 227 165 330 111 325 324 322  63 237  30 241
 172  31 326 136  60 231 242 255  82 167 329  99 293 135  66 204 144  45
  33 166 115 112 168  58 347 291 292  81  57  61 243  67 335 320 283  32
 290 103 171 185  75  59  49  52   7 246 266  29  53  84 170 104 240 256
 257 327 338  54  83  15 252  12  48 319 188   9 106   2 183  64 197 120
 276 206  16   3 107 340 233 267 334  96 344 337 332 223 259 109 305  74
 339 296 333 175 205 268  34  50 341 247 254 239 309  76 219 138   5 271
 141 248 342  80  94 177 180  47 307 130 270 253 294 207 129 143 117 274
 161 182 118 121 232 134 174 236 263 343  87 193   8 356  40 152 273  37
 346 355 299 119 116 210 262 145  46 101 272 278 126  14 173  23 251  55
 224 298 209 287   0  98 218 196 295 282 345  43   6 312 348  92 114  36
 302 110 358  72 187 261 163 213  68 195 108 281 260 100 186 264  73 147
 200 181 158 164  26 269 349 194  88 178  11 142 102 137  91  95 220 212
 201 275 357 354 160  25 184 127 208 280 132 249 113 133  93  97 105 122
 352 226 304 321 331 162 353  86  39  10 149  65 189 336 238 300 157 306
 313 148 286  71 202 285 311 150  13 279  70 139 222  51 128 308 297  28
 215 156 131 350  18  22 179  44  79 153 154 176 258  62  41 303  21 225
 244  42 124 151 159 217  89 140 146  20 221 318 301 314 245 310  35 169
  77 250  24  90  17 288 359 265 192 360 203 198 191  38 362 315 214 155
  78 216  27 363 277 125 316 211  19  85 289 351 284  69 190 361 317   4
   1 323 123 199]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.1114
INFO voc_eval.py: 171: [186 257 161 183 317 206 120 289 198 160 188 259 194 114  45 100   5 109
 197 320 269 166  41 175 260 233 264 107 232 201 287   2 189 208 254 171
  65 333 292 167 111  79 324 331 274   9 226 192 108 185 241 236 217 139
 293 102 321 116 158 187  80 191   4 267 205  35  58 249  72 117 271 113
 164 284 181  93 119 248 239  21   3  90 235  56 297 133 298 202 128  69
 272  33 174 213 124  15 283 335 214 228 330 307 131 325 182  89 122 219
 244 218 140  18  39 172 280  81  87 251 224 170   7   8 204 300 240 103
 290 148 247  13 276 334  52 294 136 329 318  82 291 221 199 266  88 308
 207 223 286 288 328 281  60  40 296 295 250  67 212 279 326 237 162 104
  50 310 193  34  20 125  71  83 178 246 210  66 319 130  98 306 245 176
  97  27  57  92 252  17  84  38  54 101 303  25 159  59 302 277 305  99
 115 203  48 231 275 301  16 110 220  85 215 327 234 168 144 142  37 106
   0 323  32  78 163 270 338  23  19 105 258  49  77 121  74 225  36 242
 126 227 138  42  12 153  22 256 137 169  70 143 118 157 262  43 196 337
 230  94  31  63  11  46 165 200  10  64 141 180 216 314 312 253  55  30
 229  96 315 282  68  61 261 149  29 127 316 209 145 132   1  75 268  91
  86 147 238 299 336 146 179 332 156 243 184 211 285  73 313 222 134 255
  62 309  47 123  95 304   6  53 265 173  44 135  26 322 273  76 177 311
  28 152 129  14 263 195  51 155 278 151 190 154  24 150 112]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1438
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1858
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.139
INFO cross_voc_dataset_evaluator.py: 134: 0.301
INFO cross_voc_dataset_evaluator.py: 134: 0.040
INFO cross_voc_dataset_evaluator.py: 134: 0.106
INFO cross_voc_dataset_evaluator.py: 134: 0.196
INFO cross_voc_dataset_evaluator.py: 134: 0.410
INFO cross_voc_dataset_evaluator.py: 134: 0.173
INFO cross_voc_dataset_evaluator.py: 134: 0.036
INFO cross_voc_dataset_evaluator.py: 134: 0.196
INFO cross_voc_dataset_evaluator.py: 134: 0.413
INFO cross_voc_dataset_evaluator.py: 134: 0.147
INFO cross_voc_dataset_evaluator.py: 134: 0.140
INFO cross_voc_dataset_evaluator.py: 134: 0.076
INFO cross_voc_dataset_evaluator.py: 134: 0.350
INFO cross_voc_dataset_evaluator.py: 134: 0.235
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.248
INFO cross_voc_dataset_evaluator.py: 134: 0.031
INFO cross_voc_dataset_evaluator.py: 134: 0.111
INFO cross_voc_dataset_evaluator.py: 134: 0.144
INFO cross_voc_dataset_evaluator.py: 135: 0.186
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.513s + 0.002s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.377s + 0.003s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.365s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.360s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.367s + 0.003s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.372s + 0.003s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.369s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.371s + 0.003s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.370s + 0.003s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.372s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.373s + 0.003s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.374s + 0.003s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.373s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.516s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.393s + 0.003s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.385s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.376s + 0.003s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.378s + 0.003s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.374s + 0.003s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.380s + 0.003s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.376s + 0.003s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.376s + 0.003s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.375s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.375s + 0.003s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.375s + 0.003s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.373s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.455s + 0.002s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.353s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.355s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.367s + 0.004s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.363s + 0.003s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.362s + 0.003s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.362s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.359s + 0.003s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.364s + 0.003s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.361s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.363s + 0.003s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.364s + 0.003s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.363s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.573s + 0.003s (eta: 0:01:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.382s + 0.004s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.363s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.365s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.371s + 0.003s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.369s + 0.003s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.368s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.366s + 0.003s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.364s + 0.003s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.363s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.361s + 0.003s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.361s + 0.003s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.361s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 54.110s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 22  11   6  83  29  63   8   9  12  77  15  30  85  24   5  69  26  65
  10  88  78  39  14  71   7  64  45  74  43  73  90  52  23  28 104  31
   4 100  20  27  51  25  32  61  33  98   3  76  79 111  56 110  47  38
  13  49  42  19  41  17  34  37  58  53  44  46  18  16  21  36  67  66
 108  96 103  40  55  92  95 101   0  99  54 102  87  60  82  80   2  57
  97  84  94 106  50   1  62 109 105  35  70  91  48 107  86  89  68  93
  72  59  75  81]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1154
INFO voc_eval.py: 171: [ 48  56 150 170 154  85 151  50  88  37 171  93  68  86  64 103  55  92
   3  23  67 128 149  91 158  99  95 102  33 113  54 159 155 173   4  36
 174 166 160  66  34  61 167 163 153 100  97  45 161  16 172  12 107  94
  57  58  60  49  82 129 156  27  89  53  69 169 117  32  71 105  74 137
  73 125 152  59  13  51   6  52  46 157 168  63  65 165 115  29 134  62
  44  14 164  26 162  87 141  42 120 135 139 109   2 110  17  28   5  40
   7   9 132  11  21  90  10  30  25 108  96 119 131  84 112  15   8  35
 130  78 145  47  18 111  81  98 142 118  83  24 106  79 126 122  19  22
 124 104   0  70  20  75 121 146 148 116 143 138 133 140 123   1  39 101
  76  72 144 127 136  43 114  31  41  80 147  77  38]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.1376
INFO voc_eval.py: 171: [1132 1131 1134 ...  717  166  171]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0208
INFO voc_eval.py: 171: [1205 1424 1494 ...  213  607  612]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1169
INFO voc_eval.py: 171: [797 245 314 787 484 386 415 770 628 761 385 254 273 627 756 220 260 752
 742 231 251 242 277 413 638 777 407 579 392 495 182 253 219 727 288 233
 759 793 647 224 259 645 749 565 772 606 406 634 817 790 808 743 490 227
 545 485 766 270 555 248 303 177 501 486 222 540 773 543 672 424 419 723
 633 428 368 753 515 599 785 781 232 427 556 315 491 503 226 722 728 625
 230 425 434 539 217 522 381 528 268 521   4 172 482 764 395 732 355 499
 559 180 189 237 546 643 779 401 730 806 580 550 388 729 795 571 553 412
 269 792 429 737 747 670 655 524   7 181 533 598 498 439  14 531 692 477
 185 250 776 356 319 229 221 135 547 430 218 593 308 157 244 391 151 750
 751 646 271 398 725 574 554 734 660 283 591 542 278 518 320 502 146 703
 605  86 255 421 304 516 626 621 488 551 252 757 663 257 582 597 520 595
 738 788 782 279 414 441 187 370   6 164 166 814 168 367 500 557 165 667
 387 119 791 183 480 805 572 475 291 581  72 650 144 313 525 437 537 744
 358  61 239 366 563 299 444 399 735 549  13 519 575 530 394 529 225 566
 416 576 789 532 241  11 630 745 446  87 588 426 408 138 548  32 285  98
 243  27 811 390   0 422 258 558  15 596 450 644 403 668 481 440 411 794
 739 280 544 758 234 397 170 305 212 534  82   1 438 784 733 536 523  29
 746 604 418 601 256 240 812  80 478 641   8 310  31   9 184 615 120 228
 301 178 578 298 389 223 570 468 505 348  62 562 676 431 603 809 436 677
  71 235 153 211 763 432 175 608 275 107  89 396  12 796 656 673 284 631
 150 494 156 361 309 249 317 517 247 535 357 435 215 433 568 145 469 569
 724 203 106 382 236 276 496 333 786 371 246 451 526 760  55  28 423  16
 136 169 472 654 527 171 105 445 802 567 640 238 132 611 302  83 594 538
 400 155 642 118 637  65 167 420 691 602 359 272   3 479  88 311 564 101
 123 113 442 552 214 322 810 583 455 307 669 497 648 404 584 678 803 286
 807 463  69 376 589 504 778 780 148 206 696 405 592 329 300   2 158 449
 577 328 754 375 454 607 619 800 590 632 541 573 297 126 453 192 108 736
 561 173 117 467 459 466 748  67 762 332 204 363 335 471 373 372 402 688
 726 384 675 586 661 585 316 103 731 609 112 560 813 600 587 149 321 306
 179 127 417 154 350 483 614 694 493 369 702 152 774 104  34 443 815 639
 635 122 651 799  79 765 456 364 102 473 360 798 160 176 378 341 695 775
 324 804 290 462 620  37  94 188 769 312 133 377 457 374  70 659 131 476
  84 124 351  10 323 697 195 509 690  18 199 110 514 339 289 618 721 447
 687  17 174 137 161 318 147 213 470  81 393 448  43   5 622 100 629 197
 342 465 380 767  46  66 662 665 664 711 657 658 492 282 771 349 741 452
 337 362 783 410 487 460  74 816 489 513 464 652 293  48 383  23 623  19
  51 159 281 716 347 713 186 458 190  95  24 198 666 194 700 409 142 109
 679 718 693  78 209 653  40 343 461 267 616 326 801 325 755 768 111  64
 649 334 474 714  50  52  92 344  93 365  85  57 709 717 712  97 636 130
 353 707 340 296 193 740 121 114 143  63 685 141 720 263  47 294  21 162
  44 710 346 261 671 612 338  77  73 208 129 708 140 330 274 674 210 327
  33 139 134 202  90  49 295 200 201 704  76 292  68  26 352  75 354 345
 705 706 686 116 715 683 125  91 287 682 508 115 510 680  30 379 719 613
 684 506 207  53 128 216 191  99 617  56  22 331 699 262 163 511 701 336
  42  38 205 266 681 196  59  96  36 265  45 689 624 264  54 698  20  39
  58 610  25  35  41  60 512 507]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1501
INFO voc_eval.py: 171: [115 136 112 108 116  67  10 114  57 126 110  12 164 131 107  58  84  11
  61  47  82  27 132  16  85  42 127 128 125  28  14 173 109  83  59  21
  54  65 175 166 167 169  19  70 119   6  41  32 134  38 135  60  81  64
 106  50 151 124 157  91  35  20  71  48 117  26   9  40  43 122  74   8
 171 129  55  94  92 170 145  25  51 123  37  98  96 168  39  73 150 165
  34 133  97  29  62 161  93  33  72 120 160 100 142   2  80   7 137  68
  56 156 158  99  45 159 147 101  31 121  90 148  89  88  78  75 154  17
 113 140  63 174 141 162 144 130 111  79  49  36  24 163  44  53 118 172
  15   3  13  46   1  23  76 153 155  66 149   4 138  77  95  18  69  87
  22  86 143 152  30  52 146 105 139   0 104 102 103   5]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2259
INFO voc_eval.py: 171: [2026 2032 2038 ...    1    0   39]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1300
INFO voc_eval.py: 171: [23 43 44 19 39 31 38 15  8 25  0 13 16 34 21  5 22 14 42 32 20 41 28 26
 30 12 40 10  7  6 37 33 24 18 35 29  1 36  9 11 17  2  3  4 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1104
INFO voc_eval.py: 171: [ 593  766 1626 ... 1720   45   49]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0583
INFO voc_eval.py: 171: [156 117 135 163 124 165 149 121 131 159 160 140 102  99 107  95  97 186
 115 148 154 331 141  89 557 349 104  96 146 116 551 142 435 187 179 134
 138 174 176 166 155 167 103 431 183 350  94 157 161 126 335 169 555 133
 261 128 150 343 168 508 130 252 153 338 139 158 143 545 503 125 308 108
  80 332 164 113 181 341 127 558 424  92 188 518 106  91 118  42 172 301
  61 333 105  98 267 137  20 219 180 339 182 114  66 426  19  23 100 120
 152 111  78 289 250 151 299  39 427 185 145 173 144 376 437 359 283 438
 368 300 511 310 258 112 251 488 367 428 548  67 324 326  54 499  22 110
 221 227 233 260 330 162 445 525  49 535 344 184 348 432 494 266 360 177
 415  56 123 357 262  93 136  41   5 549 287  40 534  72   1 132 273 178
 175 122 298 361 285 369 529 109 189 513 305 517 486  21 502 101 170 229
  28  82 553 129 119  81  10 147 541 487 253 288  76 483 380 247 171 223
 521 436  84 501 248 490  90 531 419 224 279 522 297 274 425   4  71 296
 439  52 403 345 321 232 538 457 440 316  46 446 239 302  17 444 381 544
 466  53 510 265 249 358 255 407 327 371  85 230 309 485 291 495  50 364
  64 290 191 277 422 527 481 356   3 423 226 417 399 434 520 409 379 209
 244 366 317  13 524 547 373 208  48 552 554 220 318 319 246 201 496 493
 286 471 468 506  62  35 413 410 411  18 323 559 550 214 543 514 194 241
 245 546 254 528  15 225   0 556  29 370  51 515 523 329 190 398 530 458
 497 198 498 307 204 206 236 405  30 243 416 275 459 533 342 203 512 322
 196 482 442 363 306 212 240  63 340 314 391 295  60 256  74 195 197 461
 460 532 222 231  27 412 491 473 465 401 394 313 453 509 278 500  14 492
  45 383 507 536 476  47 420 228 504 294 200  32 386 449 516 484 365 199
 238 505 408 430 202 540 268  37 396 242   9 372 537 292 526 418   8 336
  16  26 235 264 304  77 237  44  31  43 234  57 519 293  70 312 263  55
   2 539 542 489 353 414 193 192  83 346 259 388  68 257 375  34 218 272
 211 207  73  24 215 280 334  12 377 448 362 311 213 374 389 400  36  38
 347 393 395  25  79 433 328 205 284   6 282  75 281 337 421 470  33 443
  65 429 479 382 392 270 462 210 352 441  58   7 276 450 397 271 355 217
 447  11  69 315 320 216 387  59 303 402 406 404 269 325 469  87 378 474
 454 451 384 475 385 390  86 351 464 354 452 463 477 478 467 472  88 480
 455 456]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2939
INFO voc_eval.py: 171: [ 95  62  28 176 152  85  31  25  68 172 177  94 195 166  41  96  93  64
  33  23  81  39 178 168 197  42  45 102 180 167 214 169 183 179 163 200
  65  97  89  24 175  91   3 174  55  36  15 165 215 156 173 133  61  19
 154 208  35  11  90 164  40  86 161 170 120 131 211  29 190  56  66  60
  63 210 127 218  54 213 184 155  37 106 171 100 109  34  67   4 181   9
  12  92 206 185  44  27 202 104 151  88  38  77  22  78 129  14 188  17
  46  30  83 132 128 110  57 149  18 134 143 119 122 207 217 148 103  51
  20   0   8 196 135  50  32  10   5 182  26 114  98 189 209  70 121 101
 187 150  43  21 118 186 153 158 212 117 160  13  59  47 126 123  82  69
  99  16 199 112 147 105  84  53  52 111  72 193 194 115 124 220  48 191
 137  73 192 216 144  49 142 198  87 125 204  79 141 146   6 145 203 159
 205   7 113 140 136 157  80  71   1  75 108 201 219 116  74  76 221 162
 107 130  58 138 139   2]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.1891
INFO voc_eval.py: 171: [ 37  83  59 198 111  52  33 205  31 189  60 280 200  71  53 258  43 291
 199  66 194   5 110 290  29 186 165  40 179  81  47  34  41 176   9 162
 284 108 177 292  48  69  89 195 119 261 248  50 294  35  78  80  62 192
 196 191  57 244 123 202 257  46  42 273  51 182 181  36  72 238  13  10
 122 193 116  94  55  45 187 161   8 286 234 167  77  49 147  76 246 166
 265 216 245 117 226 180 109  92 121 149 296 178 115  38 188  56 268  14
 289 260 237 223 153 263 114  90 212 242 255 256  91  61 287  68 169 264
  63  75 220 209 267 190  22 206 175 170 213  70  11 120 250 283 272 160
 134 183 275 285  93 262 197  32 140 172 251  67 288  54  64 254 217 278
  12  88 125 266 129 210  84 104 279  24 146 277 136 204 132   6 124 298
 150  21  79 232 103  39 276 185 231 126  25 253  65   2 270  30  28 100
 225  44 219 247 243 133 236 154 269  99 259 164 282  82   4 101 159 106
 173 207  87 229  27  85 105  17 241 158 168 239 152 128 240 271 141  86
   1  73 163 249   3  58 107 299  74 131 102  15 156 171 227 297 174 201
 252   0  97 208 228 130  96 218 184 274  26   7 295 139 300 155 281 203
 112 127  95 148  23 221 135  20 138 233 211 137 214 143 224 222  16 215
 293  98 151 142 118 230 113  18 145  19 157 235 144]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0870
INFO voc_eval.py: 171: [399 190 402  56 188 419 214 187  69 215 189 413  47 101 409 195  54 406
  97  86  39 198 221 471 403  64 408  57  44  42 535  92 365  40 510 104
  41  61 404 203  91 391 407 374  98 196 205  88 396 223 279 373 193 212
 457  81  85 470  45  66 105  49 296 392 318 508 370 530  52 299 293 347
 142  80  59 282 420  46  50 330 509 460 224 103 194 192 451 375 526 378
 410  67 308 107 156 200 154 412 362 367 348 394 278 416  72 300 140 227
 294 461 519  89  58   6 253 442 337 518 201 418 421 452 468 361  78 466
 401 424  63 191 257 314 292 153 463 467 436 160 283 372 345 234 326 528
 411 417 520 385 228 449  70 415 320   1 380 343 478 250 462 363 453 393
 360 350  94 238 521 108 211 513  65 544 515   4  55 231 242 165 316 254
 319 286 369 480  43   7 445 338 243 303 447  11 298 368 441 458 148 270
 258 219 531 287  74 210 265   8  96 446 388 474 381 448  51 229 179 323
 339 562  23 344 236 167 143 324 529 271 483 213 225 475 437 517  99 400
 220 384 358 539 489  33 244 129 355   9  62 523 482 432  10 113  93 174
 423 280  77 152 538 504 290 553 496 327 235 435 245 514 502  53 317 332
 443 494 575 114 507 336 548 386 551 112 177 564 547 170   5  34 175  75
 512 208 307 572 389  73 328 246 450 255  24   0  95  14 549 465 206 252
 383 226 102 456  82 261  17   2 573 251 159 422 540 505 506 566 127 440
  38 537 100 485 546 331 464  71 569 249 322 273 163 147 398 525   3 568
 476 172 155 182 131 164  84 247 128 576 492 173 584  12 284 376 197 248
 288 511 543 563 473 110 135 534 571 183  79 240 186 558 264 555 499 302
 106  15  13 321 301 444 561 291 387 275 479  16 132  30 574 557 371 545
 556 484 262 306  32 554 488  18 304 469 342 136 312 256 125 111 495 145
 285 434 325 295 560 359 178 414  37 354 580 185 176 276  48  83 431 405
 583 477 239 567 524 533 274 144 120 581 126 503 260 541 241 356 357 281
 491 490 493 582 550 536 454 232 169  25 552 138 565 578  22 522 340 259
  21 209 216 455 532  76 266 309 438 139 130 439 481  31  60 500 559 137
 297 352 527 570  20 459 149  35 122 472 141 204 516 230  19 382 501 433
 351 310 487 577 346 579 397 349  90 268 542 390 269 150  87 395 202 109
 313 133 217 305  68 486 166 497 272 353 233 289 377 311 168 117 180 329
 146 498 315  29 425 263 267 333 364 218 341 158 162 199 151 161 427 184
  26  28 334 124 118 335  36 366 115 171 379 222 277 181 119 207 237 134
 123  27 157 121 116 429 428 426 430]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0642
INFO voc_eval.py: 171: [100 270 264 109 121 164 160 115 105 173  39 286 141 285 300 139 134 133
 184 142 155 275 180 280 265 163  65 108 217 282 177  52 199 149 101 122
 195 266 193 216  37 114 171  20 140  26 231 272 288 238 232 132  73 268
 273 274 183 269  22 161 179 225 111 186 214 290 119 170  41 127 138  32
  69  42 129 154  21 112 230 263 103  64  35 277 178  58 175 176  49 301
 229 150 234 254 240 205  27 102 241 200  45 126 315 296 246  53 152  84
 271 287 191 319 289 128 190 125 117 168 262 291 131 169 239  13  18 192
 237  54 137 281 166 198 297  25 223 143 189 219 130  43 226 295  66 110
  90  61  15  72 162  19 306  70 243  40 201 165 293 220 113 215  36 260
 151 253 188  10  89 158 222 148 181  31 174 144 228  38 212  50 145 283
 167  51 279 172 318  48 196 197  68  44 298   3  55  14 267 278 314 204
 304  81  86  47 185  29   8  92 307 194  82 236 209  78  75  30 284 153
 135 313 218  24 256 309  59 257 211  34  28 249 182 299 233 247 106 187
  11  60 118  67 123 259 308 124  12 303  87 159 305 136 107   1 292 210
 255  88 242  23 116 120 221 276 251  33  74 146 213  83 302 252   6 250
 104   2 312  96 261 244  16  80 311 294 235   7 208 224  79  85 316 206
  71  17  91 258 310 317 203  93   4  57  76   5 157 248 156   9 227 245
  77 147 207  99  63 202  46  95   0  94  56  62  97  98]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2887
INFO voc_eval.py: 171: [32123 25890 32103 ... 26318 26302 26310]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.1766
INFO voc_eval.py: 171: [527 276 425  55  90 368  52 374 258 418 511 273 512 456 509 534 279 260
 455  87 499 490  56 515 531  23   3  44 179 516 177  72  53 498  93 402
 334 446   5  66 449 422 489 272 347 373 401 426 523 408 264 358 526  80
  57 549  71 428 182 382  13  70 375  27 520 219 110 452  38 535 504  95
 274 501 389 118  37 189  50 190 475  81  73 217  76 513 388 275 139   6
 414 332  94 176 538 186 197  84 459 477  85 432  82  10 345 417 120 178
 286 438 399  83 122 411 364 262   8  51 496 493 529  69  34  65  97 518
 153 136 514 497 163 377 546 330 367 205 533 352 129  75 434  40 188 280
 376 149   4 528 240 416  32 145  49 354 439  96 397 431  89 127  98  59
 537  46 491 357 103 335 292 419 233 435  62 300 283 460 487 483 204 392
 106 391 503 168  30  60  39 282  64 541 270 519 236 311  91 444  61 315
 269 171 429 508 349 525 243 412 409  77 100 102 385 451 295  35 227 124
 370 390 169  79  54 453 540 128 494 474   9 362 251 530 164 381 447 181
 301 500 297  74 249  86 488 225 522 333 218 413 484  99 480 366 544  58
 384 507 454 242 316 510  25 321 441 157 351 199 195 304 220 160 420 113
  78 506  47 202 112 293 383 229 433 379   7 380 361 394  92 478 302 232
 469 485 372 191 442  63 214 284 314 161  68  16 265 337 159 539 545 405
  67 271 524  88 248 237 424 342 423 213 331 107 263 222 291 143 140 183
 211 482 105 547 141 543 261 101  11 536 162  19 210 231 194 303   2 148
 473 479 123 472 481 147 492 427 150 294 421  42  18 502 371 221 289 448
 131  45 378 476 403 180 338 155 228 137 319  17 154  20 400  31 532 437
 174 517 166 369 108  22 495 542 209 505 415 133 290 167  29 254 298 245
 310 299 326 203  14 223 151 430 170 328 144  36 521 288 198 206 165 396
  33  15 235 486 212 230 244 208 359 307  43 471 287 234 224 406 152 296
 450 327 329 465 285 187 404 109 324  26 463 548 130 407 461 259 268   1
 200 250 363  24 226 313 142 309 464 201 387 440 185 241 253 121 216 175
 305 173 117 365 356 353 215 207 138 184 458 343 281 325 395 336 135 360
 172  41 104 467 308 398 132 114 386 312 134 156 255  48 278 266 158 277
 238 267 468 111 306  28  21 256 252 322 116 344 393 247  12   0 193 196
 340 257 346 410 246 466 445 239 119 348 355 318 436 192 126 350 470 341
 462 115 457 146 320 323 125 339 317 443]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.1509
INFO voc_eval.py: 171: [229 160 340 308 323 230 283 271 306 268 210 204 348 273 380 350 176 171
 235  98 377 138 365 277 309 353 190 141 313 249  95 233 314 355 186 109
 228 118 221 178 232 367 152 240   0 148 158 203 166 215 207 358 333 312
 369  74 174 112  89 274 366 279 357 294  99 100 191  70 211 319 269   9
 381 275 208 198 110 239 150   3 356  46 219 282 258 149 349  41   2  75
 114 168 320 251  44  97 285  39 161 220 130 364 125  90 136 173 256 376
  78  96  14 144 234 267 354 182  88 195 200  91 199  29 359 341 292  56
 222 276 342 187  24 120 329 378 128 362 371 284 180  18 126 163  22 224
 346 192 370 287  25 347 165 175 132 139  86  83 311 361  49 206 146 243
 244  43  69 172  50 135 169 119  42  80 185 183 255 291   4  37 372 237
  92 124 153 289  51 193 216 108 332  20 374 231  15 209 140  59  33   8
   7 246 242 297 106 373  85 201 351 375 155 154  64 288  77 105 162 260
  36 245 383 213 107  72  79 384 181  66  35 147  52 316  17  68  45 382
 248  21  38 227 189 113 262 143  16  34  31  32 177   5 104 270 281 334
  65  40  48 327 272 352 325 266 328 188  76  13 145 343 301 304 117  81
 264 179 302 250 278 338 133 157 170  84 164  71 280  87 123  82  93 167
 196  19 218  28 290   6 217 159 194 368  30  57 238  26 257  53  23 101
  12 286   1  10 296  58  60  54 142 307  55 202 310 305  27  47 300  63
 363 127  67  11 122 226 326 324 214 253 265 303  73 339 254 315 247 151
 317 205 337 336 225 360 236 298 212 321 131 111 197 331 137 223 184 379
 134 318 259 116 115 103 263 261 335 252 344  94 330 241 299 102 129 156
 121 293 322  62 345  61 295]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1428
INFO voc_eval.py: 171: [16 71 68  1  0 17  5 20 23  6 66  3 26 25 67 18 19  2  4 57 21 70 73 28
 27 22 10 14 45 32 81 24  9 40 43 38 88  7 29 11 61 59 69 83 60 86 78 75
 58 37 72 31 84 76 47 56 64 54 12 33  8 82 85 62 39 87 34 79 36 74 80 55
 52 41 30 77 15 50 35 51 44 13 42 53 46 48 49 63 65]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0977
INFO voc_eval.py: 171: [267 369 187 272 266 363 268 374 278 367 362 365 364 383 273 271 188 277
 192 195 228 368 288   3 370 366 234 270 191 190 269  71 290 335 377 381
 336 380  41 274 144 287 379 333 116   1 279 284  40  76 239 169 232 241
 334 373  37 140 387  39 235   8 225 293 111 338 292 119 282  11 170   7
 351  75   0 260  35 231  62 186  51 112  10 346  57  30 173 344 200 189
  13 276  52 206 107 376 133  72 286 113  34  89 157 340 193 229 283 209
 117 220 108 372 205 110 143 309 168   2 249 299  47 355 175 320 263 242
 230 247 132 347 308 215  90  56  70  73  46 103 185 139 294 375 385  27
 204   4 301 318 147  42 158 125 378 159  59 350 194 339  82 182 315 386
 165 105 130 226 197  88 214 161 264 183  38  26   5 153 280 245  19  29
 354 291  48  44 207 184 109 122  69  74 371 388 382 152 261 106 150 167
 172 104 238 212 100 217 337 179  50  36  96 198  95 319  87 312 262 246
  66  33  32 317 146 295 178  67 114 171 142 361 313 323  78 281 275 160
 314  84 208 305 289 384 101 343 265  81  55 233 174  93  20 243 304 177
 357 148 156  61 257 259  58 253 345  14 255 118 321 307 252  91 210  25
 390 393 297 316 221 356 180   6 136 216 127 341 176 391 248 201 237 331
 302 330 360 162 115 392 303 218 123  86  24 358 135 128  65 126  53  83
  79  16 342 213 300  63 181 145 102 236  45 352 124 311 306 134  23  12
  68 256  80  98  49 325 163  64  21 353  92 211 202  43  31  99 222 138
  22 131  15 240 349 166 199 137 298  94 251 326  60  54 149 151 196 203
 250 389 394 155  28 359 154 310 121 254 227   9 141 348 332 129  18 219
  97 120 164 324 223 285 244  85 296  77 322 258 328  17 329 327 224]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0921
INFO voc_eval.py: 171: [485 477  18 542 474 576 456  11 534 478 586 834   4 522 373 377 577 610
  16 532 384 638 570 828 111 450 455 585 725 632 475 655 672 599 855 461
 559 564  33 545  22 555 422 370 235 824 427 572 188 540 553 654 167 460
 574 148 857 657 676 630 659 525 363 588  29 528 596 594 561 729 629 231
 527 237 589 523 578 681 616 275 285 367  12 615 467 626 362 435 238 684
 375 484 150 420 633 186 682  52 342 621 853 730 620 187  40  21 557 451
 823 543 230 669 453 833 476 185 609 110 479  61 607 680 602 685 360 138
 441 636 327 125 283 107 531  66 436 140 191 173 149 826 689 438 240  46
 335 719 839 387  31 829 515 165 658 687 866 726 344 865 739 858 628 379
 276 863 229 850 469 458  62 675 239 603 625 738 442 464 184 660   5 584
 199  69 821 330 651 345 144 608 549 888 838 340 228 668 674 383 847 516
 416  63 556 747 129 721 174 802 840 646 893  79 170 880 470  55 861 234
 391 117 712 120 194 390 580 481 190 421 511 419 547 176 648 153 480 471
 483 583 688 172 859 683 579 755 445 662  20 541 742 372 571 425 195 288
 592  59 171 890  49 462 575  41  25 244 405 593 854 463 807 670 286 332
  38 536 766 126 810 551 837 548 677 875  24 309 212 519  54 882 512 868
   8 159 568 440  53 634 381   9 832 329 465 119 673  95   6 311 582 644
 226 671 429  60 704  27 380 359 514 812  93 157 232 678 797 886 841  58
 168  45 410 247 281 695 884 301  64 142 803 517 486  15 679 318 723 667
 278 223 179 364 735 277 889 189 305 590 690 108 734  14 878 152 649 885
 406 131 538 718 203 785 744  37 699 227 385 627 245  67 457 151 715 376
 686   3 508 256 423 290 233 799 741  96 506 518 503 591 520 274 830 801
 825 707 656 535 258 145 160 452 524 696 341 817 156 154 566 641 433 808
 513 492 751 213 182 716 569  88 328 133 753 205 279 642 773 158 428  65
 143 317 877 552 554 202 320 852 790 806  50 204 408 122 163 369 717 313
 831 192 819  90 623 161 220 141 236 883 879 211 692 529  39 818 805 793
 502 365 898 137 650 705 426  13 215 640 811 348 166 899 284 123 424 306
 200 242 789  70 397 468 459 537 310 207  57 196 392  56 271 835 895 869
 612 472 798 349 444 488 756 473 316 700 764 822 139 600 319 197 439 778
 368 398 864 694 794 324 448 510  43  89 777 101  68 887 714 697 437 871
 378 896 219  94 336  48 322 193 872 619 338 326 206   2 892 432 393 162
  19 407 112 618 702  51  23 418 169 709 175 560 289 417 130 581 604 720
 891 788 693 116 346 509 487 334 403 400 248 181 816 862 809 295 748 814
  84 155 504 386 631 132 209 124 118  47 762 351 366 361 222 813 796 312
 703  44  34  17 431 257 784 732 371 836 842 104 595 851 183 783 848 353
 573 323 208 728  10 752 128 643 779  73 761 164 653 251 411 314 505 860
 701 352  42 496 881 146 795 217 201 844 413 214  99 792 740 466  72 782
  92 412 874   7 443 414 100 303 315 507 894 760  82 243 635  28 774 746
 115 374 254 708 113 178 136  30 827 134 135 265 691  32 722  86 273 114
 304 754 347 776 622 121 876 647 389 180 339 198 224 698 102 637 482 897
 606 225 767 109 321 255 759 434 587  74  80 252 758 177 820 791 210 780
 786 343 447 216 775 299 287 724 382  26 430 250 873 147 454 298 639 127
 259 713 706 601 401 771 399 260 280 800 446  35 613 757 307 499 272 768
 409 870 765 249 787   1  76 489  98 394 302 404 745 497 402 710 846 711
 501  83 218 308 103 614 867 331 781 521 221 282 264 388 645 550 743 736
 261 598  78 763 750 395  71 562 727  81 415 300 845 804 749 597 449 246
 241 268 546 539 815 337 565 856  87 530 770  91  85 350  77 105 253 325
 526 106 849 772 737 611 563 605 731 617  75 396 769 652 270 293 661  97
 733 262 292 296 294 358 843 355 291 333 533 269 266 558 498 267 263 356
 665 664 500 354 357 493 663   0 624 495 544 567 490 491 494 666  36 297]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.1176
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1383
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.115
INFO cross_voc_dataset_evaluator.py: 134: 0.138
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.117
INFO cross_voc_dataset_evaluator.py: 134: 0.150
INFO cross_voc_dataset_evaluator.py: 134: 0.226
INFO cross_voc_dataset_evaluator.py: 134: 0.130
INFO cross_voc_dataset_evaluator.py: 134: 0.110
INFO cross_voc_dataset_evaluator.py: 134: 0.058
INFO cross_voc_dataset_evaluator.py: 134: 0.294
INFO cross_voc_dataset_evaluator.py: 134: 0.189
INFO cross_voc_dataset_evaluator.py: 134: 0.087
INFO cross_voc_dataset_evaluator.py: 134: 0.064
INFO cross_voc_dataset_evaluator.py: 134: 0.289
INFO cross_voc_dataset_evaluator.py: 134: 0.177
INFO cross_voc_dataset_evaluator.py: 134: 0.151
INFO cross_voc_dataset_evaluator.py: 134: 0.143
INFO cross_voc_dataset_evaluator.py: 134: 0.098
INFO cross_voc_dataset_evaluator.py: 134: 0.092
INFO cross_voc_dataset_evaluator.py: 134: 0.118
INFO cross_voc_dataset_evaluator.py: 135: 0.138
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.505s + 0.004s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.380s + 0.003s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.366s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.368s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.369s + 0.003s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.368s + 0.003s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.364s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.367s + 0.003s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.367s + 0.003s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.368s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.367s + 0.003s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.371s + 0.003s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.372s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.414s + 0.003s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.367s + 0.003s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.366s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.363s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.362s + 0.003s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.362s + 0.003s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.372s + 0.003s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.373s + 0.003s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.371s + 0.003s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.372s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.369s + 0.003s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.369s + 0.003s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.367s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.476s + 0.002s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.354s + 0.004s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.353s + 0.004s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.363s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.366s + 0.003s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.364s + 0.003s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.363s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.362s + 0.003s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.364s + 0.003s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.365s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.365s + 0.003s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.365s + 0.003s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.364s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.438s + 0.004s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.362s + 0.004s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.369s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.362s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.363s + 0.003s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.359s + 0.003s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.363s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.362s + 0.003s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.360s + 0.003s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.359s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.358s + 0.003s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.358s + 0.003s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.357s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.677s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [150 144 175  55  28  19 160  16  52  36  42  82 153 206  65  54  87 158
  92  51  45  24 133  97  47  43  35 162  32 212  31  27  21 155  88  63
 143 137 145 172 152  26  53 131 134 209  85  84  90  34 140  56 203  58
  39  23 147  99  98 214 159 199  75  22  68  62  20 207 132 204 210 174
 216 201   5 177 142  96  57 135  48 123 168  17 157  70  77  38 148  91
  72  25  60 215  94 171 202 195 208  14   9  15 225 180 213  41 193  95
  86 223  33  46 103 120 221 151 141 224   3  30 205   1 124 138 112 125
 197 126  81  10 136  83 165  37  44 211 110  29  64 176   4  49  40 170
 218 102 108 181  50 187 122  18 200 129 179 196  13 109  93 101   8  59
 194 139   0 169   7  78  67 166 149  61 154 161 191 220 146 105  71 219
 167 222 100  74 114 121 119 183   2 106 189 164 227 190 127 188 178 111
  69  76  80  66 104 185 156 182 107 128  79  11 198 184  73  12 118 186
   6 217 192 113 130 173 115 163 117 116 228  89 226]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0223
INFO voc_eval.py: 171: [149 134 135 202 277 142  53 139  63 133 136 152 141 144 168 279 137 286
 258 161 259 181 200 162 169 179 140 269 138 272 198  51 153 157 129 273
 116  59 267  71 113 102 261 270 145 281  28 275 278  24 154 260 175 104
 276 151  57 183  60 229  75  65 159 265 173 176 264 206 117 174 280 150
   6  54 147 163  10 165 167 148 106 182 287  69  64  16 263  18 203 184
 146 126 127  49  40 158  43 171 187 192  93  30 191  67  19 282 204 285
 231 101 109  25  66  70  20  76 274 177   9  42 262  68  96  27 230 205
 185 283 156 284 225 216  97 266 268 233 213 271 188 201  21 193   2 238
 235 105  82 221 186 189 211 118 120  11  84 130  72 215  31  22 234  38
 212  58 197 223   5 240 239 190  17 255  15  13   8  23 122 207 226 237
 112 224 172 194 232  85 100  14 209 248  74 166 214  29  90  26  87 251
  12  79 103  47 252 143 123 228 195 247 128 246 124 253 121   7  52 249
 199  41  95 115  62 170  46  73  78 244 219 245  36 236  89 241 160  50
 250 164  55 119 114  91  80  94 180 155 256 243 220 178 257 196 210 227
   4 254   3 242  45  33  98  99  81 125 131 217  39 110   0 108  83   1
  34 208 218 111 222  32  77  44  88  56 107 132  92  37  61  35  48  86]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.1076
INFO voc_eval.py: 171: [ 820  811  807 ... 1962  235  236]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0192
INFO voc_eval.py: 171: [ 378  374  482 ...  208 1020  194]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0328
INFO voc_eval.py: 171: [267 125 181  98 108 115 205 238 199 236 235 151 334  55 233 270 273  85
  64  65 179 269 101  67 322 256  63 202  68 156 261 160 206 338  99 259
 207  82 214 241  53  48 325 128 203 224 272 226 295 157 222 161 114 223
 118 219 243 167 201 271 268 196 332  60 301 109 155 210  70  47 302 221
 162 296  57 211  96 213 102  73  74 227 230 250 158 187 327 228 276  15
 177 274 217 180 253 258 225  44 308 249  54  62 112  58  69 320  88 127
  36 265 218 220  39 290  72 136 129 254  83 209  43  40  66 105  52 245
 237  86 264 330  81 204 198 123  38 252 172  23 113 298 208 166  24 134
  77 260  16 159  22 251 329 200  76  19  45 232 117 141 281 212 234 328
 283  34  89  78 335 244 107 240 262 288 255  59  50 314  97 175  51  41
   0 171 263  87 257  42 266 164  35  75 231 110 135 126 337 103 106 197
  61  71 229 293 215 239 183 247 131  84 294 154 216 169 246 248 242  20
 324 289 124 297  80 104 163 287  46 300 150 152 138  49 121  12   2  79
 144  56 130 309 299  21  17 304 153 174   1 342 142 315 100  14  29   4
  27 147 140 282 344  26 116   5 312  33 275 120 133 137 303  28 173 318
   3  10 193  18 122 149 188 292 111 132 326 178 343 339  25   9 119 313
 317 336  95 185 176  91 170  90 286 277 182 145 143 284  11 285 291   6
 189  30 165 333 168 146  13 139 278  32  93 148  31 192 195 191   8   7
 194  37 280 279 319 184 186  94  92 323 190 306 305 340 316 311 310 341
 331 345 321 307]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0444
INFO voc_eval.py: 171: [114  99 100  31   3  92  43  39 107 109   7  44  97  98 124  65  38  61
  16  29  78   8 110  77  90  94  32 104  73 101  49  67  28  95  17 111
  53 117  93 108 116  47  26  10  63  33   1  59 123   9  51  91  45 113
  24   4  74 112 103  34 105  35 118  46  20  71   6 115  12  72  27  22
  70 129 128  25  54  66  79  21 102  40 125  62 120  68  18  75 121  96
  82   0  30  60  19  84  58  56  69  52  64  23  37 126  57  42  11  13
 127  48  76  15 122  55 106 119  85  41  89  80  50  88  81  14  86  83
  36   5   2  87]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2250
INFO voc_eval.py: 171: [ 370  614  616 ... 2069 1112 1123]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0211
INFO voc_eval.py: 171: [ 16  83  29  76  37  77  36  13  30  33  96  17  87  95   6  97  25  65
  99  89  75  78  18  47  74   9  10  27  62  11  35  85  93  82   5  61
  66  12  39 100  88   3  94  40   1  64  34   8  28   4  50  44  90  45
  98  80  63  23  46  49  21  84  70  32  19  52  38  20  22  42  86  43
   7  31  24  56  79   0  14  26  59  73  81   2  58  69  68  57  71  72
  15  48  91  51  55  92  41  53  60  67  54]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0291
INFO voc_eval.py: 171: [2501  646  651 ... 1496  455 1497]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0310
INFO voc_eval.py: 171: [ 14 185 262  18 274 120 263 190 189 131  16 290 271 112  71 196 119 115
 333 122 109 152 186  96 102 267  74 133 291   7 308 191 108 276  77 266
 163 116 334 171 170 338 198 167 164 195  72 105 166 101 307   3  78 157
 117  99 161 180 286 111  82 128 181  81 146 127 147 174 107 148 104 178
 173 165 145 144 103 177 113 123 130 278 182 118 172  89  86  91 340 124
 374 135  80 155 273  90 289 277 150 330 129  76 329 126 168 179 121  13
 162 396  22 183  10 110 134 142 188 268 392  95 125 154 365 269 264 285
 149 158 281 140   2 159 136 197 151 297 394 187 138 331 106 406 193 137
 397 356 160 287  40  37 309 139  49 348 386 363  75  17 335 169 114  21
 132 175  30 153  68 156  48 265   5  38 407 304 270 283 395 326 201  85
  15  42 275 293 282 279 306  83 222 259 184 141  73  58 341 311  51 387
 192 143 332 255 100 221 176 389  92 194  36 358 231  45 366 353 379 280
 284  79  19 245 372  44 337 371 343 232 294  94 200  33 250   9  35  61
 272 230 310 219 298 202  87 328 220 403 208  50 239  26 258  47  27 251
 325  60 299  12 224 312 319  34 300   4 382  59   1 336  65  24 384 405
 247  97  52 360 215 209  41 380 254 288  31 346  29 388   0 373 249 349
  64 240 228 354  43 352 345  23  32 391 361  55 235  20 381 296 383 393
  25 205 410 248 327 223 401 399 213 400 339 350 207 316  63 295  11 344
  54 225 385 378 218 367 246 212 233 229 211 236 234 241 351 369 227 216
  57 199 257   8 252 390 402 318 226  88 408 217 261 244 203   6 253 324
 237 347 368  28 398 206  62 214 260 204 210 305 362 256 303 314 313  56
  67 357 370  98 323 355 409  53 364  93 321 302 404 359 238 292  84  46
 243 242  39  66 342 317 301 315  69  70 322 320 376 377 375]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1042
INFO voc_eval.py: 171: [ 82  81  88 545 274 276 542 541 514  87 568 249 256 521 586 522 283 590
 327 547  90 596 588 304  85 571   2 302 300 278 325 280 203 532 525 292
 323  94  93 570 515 544  89  35 248 593 251  86 540 608 534 167   3 100
 275 605 552 519 328  14 202  84 266 326 517 273   4 450 492 284 445 397
 550 632 573 112 261 620 393 587 470 119  95 122 268 107 600 442 599  43
   1 379 530 363   9  59 520 516 554 394 567 350 603 602 562 589 555 481
 306 551 398 598 282 607 612 626 606  12 199 111 449 546 518 254 307  38
 526 250 548 624 446 533  39 572  99 395 101 493 485 272 255 543 549 504
 296 279 499 497 270 269 452 337 129 659 271 252  11 592 386 454 380 609
 290 369 633 281 263 479 615 513  37 287 651 206 413 150  41 561 370 267
   6 637 627 613 438 227 495 309 172  92 399 291 569 578 285 591 464 618
 322  34 496 297 318 102 163 405 171 113 448 465 575 401 531  62 124 303
  80  28 559 631 563 564 480  91 378 331 641 595  18 557 553 262  98 253
 576 105 455 348 577 628 104  26 257 259 389 188 457 359 295 412 579 440
 432  60 286 647 558  19 574 635 235 392 366 433 277   0 352 447  44  57
 217 638  27 123 168 117 467 471 384  10 229  16 489  32 462 146 441 556
 162 294  51 524   8 658 365 604 120 118 610 463 319 601 127  83 205 342
 639 374 636 305 298 490 494 209 560 403 565 213 106 125 640 335 505 527
 408  46 402 103  97 523  36 437 208  96 538  42 663 617 585 368  33 197
 210 200 312 566 483 293 236 317  25 466  48 332 646 152  23 385 597   7
 487 503 115 391  70 169 656 660  30 196 336  53 301 528 535 509 324 313
  15 207 649 529 177 367 144 265 439 187 201 189  52 308 616 344  22 204
  17 364  31 215  49 653 611 132 157 400 435 661  20 314 536 361 451 622
 258 260 411  40  72 634  65  29 507 147  45 151 126 136 288 488 644 484
  75  50 407 594 409 121 506  71  58 410 619 621 140 114 382 299 510 264
 614 404  56 316 652 376 645 148 116 539 623 214 406 358 238 190 453 431
 498  13 109 537 458 377 422 315 443 665 355 340 239 491 630 329 339 371
 289 159 131 195 383  55 142 212 108 135 154  68 477 444 145 185  64 353
 160 501 180 134 474 110 662 153 166 242 511 345  21 349 244 155 149 625
 330  47 164  76  73 482 320 198 648 179 475 194 486 211 338 418 508 387
 416 130 390 655 216 141 650 223 175 191 472  66 580 218 583 436  63 388
 468 461 657 629 228 500  24 469 128 246  67 237   5 343 220 186 581 381
 165  61 582 642 192 158 434 346 420 373 512 372 193 341 460  54  69 333
 143 311 357  78 643 476  77 664 184 396 241 222 182 138 233 243 502 351
 133 226 139 347 473 310 426 245 240 584 137 478 375 183 156 231  74 321
  79 224 173 247 174 230 654 176 170 360 181 354 424 459 161 334 232 178
 362 221 430 414 427 423 219 234 225 356 429 456 419 421 428 415 417 425]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0997
INFO voc_eval.py: 171: [ 13 172  10 128  32 139 132 222 215  31  23   4 223 171 134  24   8 127
  16 219 163  33 159 142  34 115  30 109   5 228 133 157 107 170 138  78
 165  40 125 104 160 113   3   9 154 177 156  27 180 108 229 225 213  74
 188 173 118   6  96 203  41 122 117 131 164 226 185  93 182   1   7 193
 214 178  20 199  12 106 176 148  76 123 110 116  84  77 191   2  26 152
 239 155 136 153 150  88 151 201 175  94 241  28  38 166 114  81 227 169
 162 231  49 103 102 161 137  69 158 140 130   0 194  17  14  42 149  70
 181 217  19 111  87 197  37 179 184 183  90  36 216  25 105 204  50 100
  21  39  45  73  92  15 121 124 238 174 167  97  79 135 224  29 240 232
  66 144  53  35 212 187 195 230 198 233 208 205 196  89 120 141 211 119
 192  11  82 189 221  60 145  99  71  80  47  48 202  68 237 112 200 186
  18  46 147 210 190  98  22 234  67  44 206 209  72  57 101 168  58 143
 207  95 129  85 235 220  83  43  59 218  54  65  75 236 146  55  62  63
  64  91  51  56  61  52 126  86]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0395
INFO voc_eval.py: 171: [156  34  52  59  66 292 202  25 132 171 131 129  53 157  80 283  45 137
  55 168  77  76  27 332 327 251  72 159 139 135  61 293 271 247 100 359
  89  24  31 191 214  65 126 331  37 151  64 121 101 194 360  36 184 264
 309  14 237 289 242 109 282 328  79 280 142 294 179 204 267 266 158  58
  17 162  10 246 123 124  47 252 190  12  74 305 136   8   9 356  35 284
 148 291 192  99 120  15 133 104  73  84  22 274  70 208 125 141 147  50
 143  69 320 307 299 152 316 285 324  18  85 134 286  95  71 174 243 270
 102 130 164 238 149 364 176 256 336 188  33 253 187  42 313  51 333  83
 362 259  91 213 217  41 249 358 229 344 326  62  97 146 182 339  20 348
 106 276  67 209 114 138 297  19 354 317 195 351 268 224  56 255 257 310
 346 163 155 330  32 178 166 319  44 233 334 154 193  63  49 189  16 383
 140 183 206   5 199 262 244 186 153 378  57 181 185 200 352 329 357 216
 269 172 361 345 239 314  81  78 366 105 290 248   7 258  48 369 232 128
 347  96 173 165  13 205  82 103 119 278  30 197 201 219 308 375 122 318
 363 160 379 323 384 108 221 161 234 301 281 353 325 350  87  86 210 218
 300 371 175   0  39 223   3   4  23 385 110 177 302  93 367 279 342 312
  92 322 107 240 273 203  28 343  60  11 380 303 288 295 241 212 335 306
 372 287 368 337 261 277 272 298 311 254 150 180 198 226 296 169 250 370
 376 340 117 315  43 115 365  88  40 355 260 111 382 170  29 387   2 263
 236 245 338 227 215   1 275  68 321 230 341 374  54 196 373  94 113 228
 349  90  26  98  21 118 304   6 377 144  46 225  75 222 386 220 265 381
 167 211 235 112 127  38 116 207 231 145 388 389]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0311
INFO voc_eval.py: 171: [223  25  83 127  75 212 226  31  80 225  20 120  99  51 130 144 197 133
 215 110 205 100 218 208  77 156 132  18  87 135  17 240 219 204 146  93
 119 186  19 207  78 220  54  29 105  36 141 238 209 134 162  40 140 190
 199 149  85 237 211 217 179  50 178 114 168 104 235 201 175 107  23  57
  82  43 158 131 122  30 210 206 200  81  97 239 222  92  90 221 184 230
 173  88 159 236  95 164 196 151 117 112 106  98 224 174  91  79 241 213
 188  76 138 192 180 143 145  24  12 169 185   1  37 116 176 155  89  52
  66  96   8 193  59 115 142  58 124 121 232 103 231  33 108 153  38 129
 148 216  10 154 229 177  11 152  86  44  28 111 233  62 128  47 228 157
 163 214  32 202 139  71  15 125 181 195 147 171 243  35 170   6  55 227
  48  69 172 113 165   5 167   4 123  21 234 101   9  63   0  13 198 161
 166 136 194  84 126   3  34 242 203 160 109  60   2  27  53  26  61  94
   7 150 102 182  16  41  67 244  14 187 137  49  64  45  42 183 189 191
 118  65  22  56  39  46  74  70  73  68  72]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2831
INFO voc_eval.py: 171: [19487 17716 17715 ...  4994  4955  4991]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0862
INFO voc_eval.py: 171: [158 927 854 ... 415 667 530]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.2014
INFO voc_eval.py: 171: [106 167  90 150  83 121 110 137 169 177  50 196 166  48 163  94  77  44
 164  56  86  95 171 142 102  91 174 120  76  55 158  51 128  99  80  33
 205  10 199 152 115   5  96 173  53  12 201 148 105  37   6 138 193 111
  93 183  42 165  79  41 202  45  98  75 160  27 119 133 155 187  61  54
 203 117 101 125 103  34 145  30 179 161 123 134 154 147 112  57  26 140
  78  47 124 122 175 149 185  20 221  43 159 172  36 136 168   2   8 212
  73 189 200 143 157 132 108  70 118  31 213 141 151 156 104 204  87  49
 153  32 130  92  88  46  81   4 170  24 139  15  82  84 113  74  62  38
 162 135 230  64 127 211 190 146 181   9  52 144 207 224  14 188  40 206
 214  28 180  29 129 126   7 227  35   1 209  58  39  13 191  25 192 131
 210  69 178 114 225  72  97  17 231  11 198  65  85  71  60 195 197 109
 116  18 194  23 208   0  22  68  19  89 107  67  16   3 229 100 215  21
  66 219 186  63 218 223  59 176 226 182 184 216 232 222 220 228 217]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0422
INFO voc_eval.py: 171: [125 117 129 122   1 118 128 123 126  27  26  25   0  13  17 114  20   5
   4  14   3  32  35  69  63  10  68 127  37   2  12  57 120 119   8  29
  50   6  30  19  39  18  21 108  52  15 121  75  49  41  53  76  45  83
  54 112 124  97 134  46 111  62  81 104  94  99  84  77 110  22 106 149
  65  24  90  91  74  38  70 140  78  79 101 116  34 136  87  48 150 132
 100  16 147  51 146   9  31  36  95  80 115  11 144  60  47  89  33  23
   7 154  85  55 131 155  28 145 151 107 148 113  82 103  71  98  88 138
 143 102  86  64 133 105  42  92 135 142 109 137  93  96 141 153  73  72
 139 156  67  61  56  44  66 130  59  43  58  40 152]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0350
INFO voc_eval.py: 171: [189  46 278 279 378 285 376 374 283 170 168 380 166 293  70 386 347 304
  54 167 301 191  51 190 201  49 284 345 377 311 300 306  73 296 302 349
 307 126 384 395 398 314 265 132   9 281 291 407 401 282 280 165 294 408
 389 393 388 204 128 192 352 316 107 310 271 400 305  37 210 375 360 289
 178   2 342 410 253 129 343 205 237 365 243 203 379 335 241 196   1 383
 144 324 257 179 356 199  31 169 266 390  71  72 248  33 344 299 102  80
 200  77  26 358 361   4  36  78 273  35 208  39  50 131  29 287  60 405
  43 194 267  16 298 197  27 322 382 262 292 184 174  81 145  41 159 149
 105 136 206 325 323 177 173 110  57 391 236  76  67 387 214 261   7 109
 153 238 252 101 213 385 309 350 244 404 211  99  64 137 158 256 100 180
 317 274 403 247  59 318 351 326  19  12 226 277 399  82 108 340 332  14
 290 157 215 202 319 268  30 103 218  98  34  42 106 348  55 249 333 143
 328 239 216  48 127  74   3  66 272 224  63 269 229 251 367 104  32  87
 175 138 162  69 139 346 195 111 312 355 240 113 338 172 397 357  25 234
 151 334 182 406  38 161 245   5 185 122 116 392  28  53 320 198 130 176
 264  83 417 414 250 331 187 124 415  24  68 212 160 259 242  45  92 337
 381 402 308 341 223  56 275  13 142 232 219 412 150 181  58  94 217 147
 288 286 134 411 416 246 354  96 123 186 183 228 409  52 303 396 164 363
  21 230 114 225 330 120  88 235 295 133 362 276 315  23  22 339 233 193
 222 152 135 321 313  79  85 413 154  47 163 156  91  40 112 188 125 368
  90 171 353 258 209 118  65 327 373   6 254 329 369  44  89 394  61 359
 220 297 364 148 370 141 270 366  62 207 255 221 227 140 119  11   0 372
  93 263 146 155  10 260 371  17   8  15  95  97 121 117 231  75  20 115
  84  86 336  18]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.1401
INFO voc_eval.py: 171: [247 271 306 309  65  17  13 266 335 116  83  15 338 208   1  10 394 489
 207 355 299 106 301 357 323  70 318 473  62 124 492 490 418 274 272 341
 173 184 345  16 248 253 425 259 212 185  87 482 457 152 117 120 369 189
 374 255  30  33 494 119 372 379 354  19 362 268  94 313  80 260 496 217
 249  67   4 359 507 324   2 269  43   7  91 461  89 484 361 197 480 393
 415 262 115 438 108 448  73   6   5 312 200  42 329 390 196 370  35 250
 122 460 215 233 325 331  44  14 205 154 298 343 504 172 178 411 187  56
  95  53  38 498 118 179  20 308 275 433 465 328 486 147 487 483 423 421
 417 169 444  66  26 455 452 292 264 351 201 105 254 360 419 288 136 111
 155 113  59 497 509 295 228 367  86 377 406 294 442  34 206 414 199 107
 420 391 462  74  11  99 202 153 157 232 267 352  77  60 378 198 443 447
 213 265 481 170 273 175 293  81  52 373 177 456  48 429 458 166 103 128
 263 236  27 382 409 182 383 353 330  18 386 491 252 503 191 181 192 123
   8 190 422 485 112 466 439  71 320 129  51 336 204 326 282 381 348  90
  21 100 416 186 371  97  98 146  85 104 138 316 276 278 193 303 342   9
  69 305 317  41  63 500 426 286 437 446 477  37 139 174  31 502 188  39
 506  96 209 134 297 495 424 459 363 346 211 311 114 475 488  72 337 470
 164 493 160  57 396  23 140 176 322  79 388 451 435 257 214 366 453 102
 505 210 163 334 161 358 405 454 221 356 441  84 141 314  40 376  64 167
 283 291 238 224  76  25 110 510  58  45  55 127   0   3 463  93  12 101
  46 296 471  49 227 440  54  36 310  68 256  24 251 427 144 344 130 364
  92 183 445 216  88  82  32 434 195 241  47 315 395 304 333 261 137 150
 132 149  75 467  50 401  78 436 431 392 180 156 131 499 476  61 168 258
 432 339 501 327 165 151 413 145 289 270 240 368 246 162 430 400 284 450
 219 478 412 290 142 403 235 287 428 508 109 349 468 347 277 449 242 408
 229 222 143 220 340 398 279 280 397 404 126 300 281 285 121 307 384 474
 158  22 148 133 350 230 135 203 375 159 469 244  29 194 321 464  28 332
 171 365 385 226 380 243 231 239 479 472 302 237 407 410 125 399 389 402
 319 223 234 387 218 245 225]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0572
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0826
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.022
INFO cross_voc_dataset_evaluator.py: 134: 0.108
INFO cross_voc_dataset_evaluator.py: 134: 0.019
INFO cross_voc_dataset_evaluator.py: 134: 0.033
INFO cross_voc_dataset_evaluator.py: 134: 0.044
INFO cross_voc_dataset_evaluator.py: 134: 0.225
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.029
INFO cross_voc_dataset_evaluator.py: 134: 0.031
INFO cross_voc_dataset_evaluator.py: 134: 0.104
INFO cross_voc_dataset_evaluator.py: 134: 0.100
INFO cross_voc_dataset_evaluator.py: 134: 0.040
INFO cross_voc_dataset_evaluator.py: 134: 0.031
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.086
INFO cross_voc_dataset_evaluator.py: 134: 0.201
INFO cross_voc_dataset_evaluator.py: 134: 0.042
INFO cross_voc_dataset_evaluator.py: 134: 0.035
INFO cross_voc_dataset_evaluator.py: 134: 0.140
INFO cross_voc_dataset_evaluator.py: 134: 0.057
INFO cross_voc_dataset_evaluator.py: 135: 0.083
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.498s + 0.004s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.370s + 0.004s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.373s + 0.004s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.372s + 0.004s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.370s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.374s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.368s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.366s + 0.003s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.368s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.368s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.367s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.366s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.367s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.377s + 0.004s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.345s + 0.004s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.365s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.357s + 0.004s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.362s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.364s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.374s + 0.004s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.376s + 0.004s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.374s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.376s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.374s + 0.004s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.375s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.372s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.486s + 0.004s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.375s + 0.003s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.361s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.365s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.358s + 0.003s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.359s + 0.003s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.360s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.361s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.361s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.362s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.361s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.361s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.362s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.428s + 0.002s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.351s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.349s + 0.004s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.350s + 0.003s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.351s + 0.003s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.351s + 0.003s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.351s + 0.003s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.353s + 0.003s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.352s + 0.003s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.353s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.350s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.352s + 0.004s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.350s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.861s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 74 142  69 102  70  88  90 169 136  48 137  76  81  71  89  53 174 129
 170 135 120 167 133 171  91  61  60 111 166 106  55 131 127 154  49  67
 159 172  66  62  36  83 150  52 163 130  18 141  14 105 148  50  80  17
 119 108   5  92 162  38  57 151  54 144  63  27 157 173  16 134  44   3
  11  24  10 117  30 176 140 175 189 100 156 149  94  86  21 103  22 152
   2  32  26  85  75 155  56  15 104  77 158  25 138 125  93  42  13 180
  43 164  33 181 101  72  34  20 107  84  64  23 122  65   8  19  39  68
  58   1 165  79  35 160 146 126  82  73   0 190 109  31 178  40  51  87
 132  78  28 168  59 185 187 118 179  47 153  45 184 124  46 177  29 183
 161 143 121 145   9 116 182 188  12  41 147 123 139 113 114   7  37   6
   4 128 186  95  99  97 115 110 112  98  96]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0255
INFO voc_eval.py: 171: [ 91  87  37 106 111  53  54  16 107 190 113 110  65 172  63  40  51 174
  57  83  79  52 108  36 145 123  88  43  39  85 133  70 112  58  23  28
 116 175  41  45  64 189   5 153   9  75 118 124   7 180  84  27  93 186
  42 176  73 177  29 196 181 115  60 194 195  11  48  90 138 117 114  26
  56  34  19 126  24  10   4 102  69  94 187 184 178 155  13 188  59 127
 103   6 131  62  14 125  61 159 179  15  92 185  17   2 198  81 141   0
  86  66  38  68 191 158  95  55 134 182 197 151 183   8  21 121 120  97
  12 193 148  89   1  22 173 137 192  20 105 101 104 156  18  25 162 168
  30  47 139  78 109 132 142 144  72 130 167 140 147 100 152  74 160 166
 119 135  98 129 122 146  35  50  76  77 157 163  32 150   3  82  49 161
 165 149  99  71 171 136  80  31 128 164  44  46 154 143 170  67 169  33
  96]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.2197
INFO voc_eval.py: 171: [300  86 504  52  48 508  77 289 303 500 535 498  50  35 523 495 489 525
  67 321  25  40 292 532 283 268 511  70 501  28 284 644 255 492 253  59
 490 251 282 704  58 703  51 249 509 510 518 264 323 269 531 506 262 632
 297  30  33  85 556 528 273 694 102  93 301  72 331 553 493 712 515 279
 643  46 695 707 494 316 324 267 478 308 700 559 210 708 650 554 334 261
 577 527 454 561 218  88 709 270 174  83 347 213 243  34 449 252 633 260
 649  37 471  39 702 686 107 496 298 663 101 522 502 640 595  32 285 453
  41 512  80 208 691 410 514 469 177 355 197 687 265 660 651 491 450  45
 337 295 415 476 503 573 304  66 529 560  55  61 188 473 118 103 286  84
 237 108 186 116 466 353 684 336 479 254 626 135  74 710 184 325 555 204
 668 574 130 655  92 460 187 113 430 487 457 257 354 637 228 348 570 447
 338 664 563  76 566 394 224 299 179 521 519  62 634 582 229 631 429 183
 206 412 474 656 380 614 638 172 572 212 317 692 236 166 390 137 126 277
 499 175 403 258 117 239 682 604 567 305 697  43 328 319 360  71 481 266
 526 516 329 132  75 165 441 198 422  13 477  82 452 568 311  69 635 367
 180 711 226 645 647 351 358 569 689 608 578 465 513  54 327 393 486 207
 483 667 370 223 366 517 678 157  68 562 698  87 362 484 431 195 222 680
 121 636 364  57 461  38  24  47 346 683 409 658 505 396 596 111  42 125
 200 374 375 616  79 288  18 363 159  19 312 389 302 167 662 150 485 593
 391 423  36 534 455 685 488 598 688 293 467 420 622 699 623 705 230 241
  95 671 215 256 591 176  65 136 339 442 185 306 557 140  21 399 540 427
  78  44 149 281 580 322 326 625  11 376 343 171 333 584  89 654 397 344
 416 690 278 603 151 352 115 296 579 181 332 641  22 653 433 205  31 211
 592 128 480 435 290   6  26  10 520 451 248 112 361 472 432 609 342 106
 139 291 617 259 141 417 666 220 234 462 610 406 192 216  16  23 242 693
  73 232 388 575 541 371  91 368 657  81 359 425 402 170 443 385 129 482
 294 217 162 459 335  53 263 133 665 384 550 669 392 271 581 182 169 646
 615 679 105 190 250 701 309 696 543 418 209 227 611 341 219 583 606 530
 124  60 350 437 131 548 440 203 196 104 601  94 414 189 648 238 109 588
 280   5 463 602 168 426 565  27 448   1  14 272 122 564 287 539 307 164
 424   3 158 470 127 434 571  49   8 706 134 537 408 356 173 100 193 576
 398 458 382 240 639   0 558 138   9 178 235  56 589  15  97  90 605 233
 191 438 315 676 445 155 542  12 147  64 143 456 199 244 681 194 629 428
 618 400 468 405   4 163 161 524 381 628 276   2 547 231 379 599 675  29
 387   7 612 585 114 677 123 145 597 313 314 275 225 395 642 673 160 310
 444 246 357 345 201 533 538 551  99  96 464 419 436  63 411 590 110 274
 674  20 630 407 404  98 672 546 439 214 378 401 221 544 661 120 536 247
 202 507 369 619 475 349 607 119 320 413 153 587 446 652 545 600 613 659
 156 421 330 627 386 142 670 549 497 586 373 594  17 245 144 552 365 154
 152 713 318 383 146 620 148 340 714 624 621 372 377]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0264
INFO voc_eval.py: 171: [545 702 569 567 123 758 699 215 710 691 549 537 750 313 327 744 318 122
 269 330 205 210 131 128 470 154 564 218 538 427 463 542 444 264 574 701
 357 117 765  47 753 308 693 548 803 696 480 557 339 377 359 761 479 769
 371 361  81 708 151 751 434  16 448 314 748 414 121 120 356 246 422 436
 709   5  64  15 322 840  62 581 823 447 437  55 366 178 698 809 216 321
 727 424 762 842 183 167 127 554 333 251 795 568 119 118 469  96 217 368
 388 539 735 265 345 430 337 429 342  77  59 721 200 755 329 768 346 235
 173 787 563 153 705 450 802 531  10 747 852 556 420 507 343 323 298  69
 582 440 296 319  66 782 307 477 544  46 415 508 522 746  58 169  56 816
  97  43 646 723  11 678  49 209 304 497 600 268 536 788 713  99 760 756
  45 454 485  71 234  50 811 461 302 433 835 226 287 290 445  92 142 759
 528 431 790 146   4 150 223 363 370 808 214 179 171 810 375 681  22 619
 605 254 227 837 472 724 821  18 203 130 355 112 157 295 783  63 441 500
 515 275 421 475 294 245  67 423 731 416 468 543 529  76 700  95 248  80
 571 692 514 579 639 553 786 365 764 449 478  88 326 439  87 312 351 575
 738 382 111  75  28  23 247 617 584 367 438 483 309 641 386 667 499 552
 858 687  72 334 597 505 566 845  83 390 690 757 349 745 181 195 310 274
 297 572 525 820 161 813  48 801 559 225 300   3 373 426 784 706 729 754
 263 580 244 870 629  44 815 697 715 653  26 162  73 466 303 627  85 722
 242 828 267 317 560  14 493  84 491 110 871  74 347  65 546 219 595 752
 176 262  60 655 257 827  31 487 670 453 389 372 631   9 409 799 488 471
 767 737 156 720 593  78 565 473 626 725 834 352 489 589 460 826 733 663
 703 260 662 239 797 587 598  29 806 293 862 630 272 158 163 238 376 378
 621 854 812 341 868 869 107 230 152 419  91 132 332 849 286 573 578 457
 428 618 482  53 804 459 187 451 495 674 455 521   0   8 180 258 174 490
 228 185 249 182 712 728 253 732  12 443 207  39 273 640 504 704 417 855
 452 103  21 102 481 502 511 166 474 540 647 170 456 124 671 446 292  68
 642 749 793 558 604 324  13 637  34 623  19  36  94 841 736 547  70 498
  51 763 586 585 165 856 570 832 270 484 384 289 742 643 707 590 435 838
 496 711 177 412 486 774 740 101 344 829  82 208 250  57 608  79 418 730
 669 638 594 336 305  20 664 350 523 741 291 688 425 695 861 125 847   6
 288 766 147 220 533 464 846 492 864 126 831 611  41 503 236 331 353 407
 320 462  38 785 651 819 583 328 104 635 335 791   2 615 789 385 716 833
  33 689 659 266 657 241 364 665 494 850 243 432 518 282 532 192 636 202
 680 814  17 865 718 233 524 259 770 467 105 279 672 550 197 860 535 229
  35 777 805 612 610 211 848 398 859 116 866 401  42 616 622 781 607 734
 108 201 113 863 633 379 516 602 315 825 530 403  27 285 660 800 168  93
 476 358 624 383 645  32 541 830 106 676 509 794 129 164 726 442 652 186
 400 137 628 189 513 683 362 501 867   1 634 506 458 717 576 141 776  25
  40 857   7 658 148 348  30 354 191 196 510 668  37 743 465 591 792 252
 739 843 817 654 340 374 596 555 405 283 519 109 193 311 592 853 824 325
 277 780 387 316 410 306 675 206 393 526 839 714 255 599 100 338 396 656
 517 649 551 175 577  54 392 172 399 773 562 280 719 408 534 512 648 779
 561 601 278 661 818 609 360  86 199  89 632  52 394 677 381  90 281 284
 520  61 159 625  24 402 836 614 588 682 851  98 771 212 603 620 644 231
 772 822 369 397 778 844 380 138 256 798 527 190 404 411 685 149 221 606
 224 213 650 114 613 240 686 261 204 673 194 413 395 276 301 155 406 299
 143 391 796 807 184 160 222 232 134 679 136 115 666 145 237 188 198 775
 684 271 694 144 133 135 139 140]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0962
INFO voc_eval.py: 171: [314  88 204 304 108 309 295 321 319  84 266 264 248 182 103 359  99 177
 138  75  60 285 342 280 401 244 465 334 456 297 482  81 200 287  89 351
 286 394 366 367  67  76 260 267  66  59 269 404 349 431 469 311 339  85
  70  96 112 172 292 299 300 276 445 441 116 346 407 212 377  93 184  50
 388  51 210  80 357 360  39 474 290 378 439 240  86 115  77 403 429 385
 437 213 100 183 107 249 343  98 275 330  48 102 283  95  90 259 368 298
 426 352 106 173 409 180 411 386 278  68 432 137 393 237 262  34 113 406
 274 405 251 284  94 179 331 371 470 114 341 345 126 120 379 372 171  78
 265  44 211 101  52 436  82 233  19 471  92 375 181 123 443 425  49 190
 219  97 198 408 203 324 333 340 245 174  26 370  91 438 442 205 254 376
 216  63 328 186 397 448 477 317 373 270  38 302 176 336 332 125 105 207
 206 335 383  79 288 365 197 109  83  24 350  62 218 337 110 111 122 427
 104  41 326 475 117  13 358  35 486 338 256  43 306  87  57 361 398 224
 136 348 364 395  46  71 355 363 389 387 175 289 308  64 318   3 447 214
 472 369  31 124 396 118  40 294  54 356 301 229  65 195 423 434  42 384
 420 133 139 381 374 344 399 412 225 353 382 293 141 168 391 178 354   7
 400 129 325 189 315 392 236 135 243 169 362 327 253 170  20 476 453 257
 347 208 380 402   0  61 487 192  53 252 282 410 279 162 250 322 132   9
 239 329 390 119 164  29 121 221 268 202 235 462  10 424 414 446 228 230
 272 134  45 323 320 209 247 480 196 312 220  69  30  12 261 191 281 127
 188 440 217 131 167 128  56 468 258  28 222 433 185 223  47 316 454   6
  55 231 273  58 227 158 140 246  17  22 130 277 255 310 457 187 484 163
  11 271 165 481 450 232 199 193 241 422 305 435  23  15 215 415 226 419
 160   2 142 417   4 238  14 449 194 303 459 478  25 242 263  21 161  27
  18 166 421 418 143 307 413 234 430 159 473 452   5   8 153  16 146 157
 201 455 149   1 463 416 461 150 464 144  37 458  32 148 313  33 156 485
 488 154 291  36 444 151 147  74 296 152 145 466 155  72 428 460  73 467
 483 451 479]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0782
INFO voc_eval.py: 171: [106 127 103 105  85 108 128  75 120 102 107  83 109 123  88 130  71  37
  52 121  94  38 101   9 129 131   7  18  84 126 125  93  87  30 115  91
 140   8  89  82 118  69  10  78 138  26  39  81 114 142  19 116  21  68
  90  17  80 117 122  50  55   4  34  70  76  77  56  23  57  32 145 110
 148  33 113  92  72  95   3  79  24  31  15  16 147  63  22  47  62  58
  14  36 104  67  97 124  25  86 135  74 137  35  66 143  48  98  40  20
 139  73  27  29 111  43 119  12  13  64 136   2  51 141  65 146 112 133
   6  11  53 144  54 132   5 134  45  41  49 100  28  46  59   1  99  60
  61  96  44  42   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2297
INFO voc_eval.py: 171: [638 389 427 ...  40 531  42]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0455
INFO voc_eval.py: 171: [21 24 18 20 16 34 59 43 61 15 47 74 31 60  9 11 53 36 76 77 22 75 14 35
 13 73 41  8 17 33 32  7  3 42 46 62 45 70 10 23 52 50 29 19 56 71 78 65
  0 25  4 30 68 49 37  5 58 26  2  1 12  6 63 72 67 44 27 69 55 28 66 51
 38 64 40 48 54 57 39]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0182
INFO voc_eval.py: 171: [ 738  351  293 ... 1310  243  755]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0168
INFO voc_eval.py: 171: [180 190 269 155 150 271 144 145 133 131 268 186 148 310 312 166 195 260
 114 267 158 161  94 185 196 276 121 122 104 126 160 177 179 270 259 117
 165 183 134 194 107 202 174 171 182 199 154  99  83  87 308 119 173 358
 277 106 164 266 101 175 197 176 201 168 123  95 375 129 264 278  91 100
  86 205 139  93 132 142 172 304  89 361 127 140  57 109 124 184 141 110
 118 116 372 113 366 163 159 111 307 198 156 128  90 188 120 115 200 191
 356 153 130 280  39 305 167  84 293 137 224 149 151  92 368  29 314  82
 152 147 181 143  54 138 316 157  55 377 354 211 136 125 108 279  16 227
 250 229 103  13  28  75  56 226 238 187  62 261 203 286  40 228 146 255
 355 162 223 349 246 244  72  74 265 207 306  35 273 170 209 370 112 232
  50 352 313  63 274 189 318 263 193 178 204 192 360  77   5  78  49  73
 169 258 102 347  15  38 374 231  97  61 241  76 135  25 381 105 351 303
 240 362  47 379 214   6  31 206  88  37 222   3 248 217 298 357 294  96
 218  98 213  69  81 287 323  11 301 245  30 350 221  43 215  22 367 371
 284 369  52  21   1 236 230  58  85 317 382  17 288  19 376 291 326 345
   2 299 249 216 364  46  12  26  36 237 210  34 344 309 242  27  51 282
 302 220  33 262  59 253 219  23  14 234 225  18  44  67   7   0 257 297
 283 383  32  24 319 254 239  68  10 212   4 328 300  42 289 315 295  70
 384 311   9   8 348 380 325 290 275  71 208  79  80  65 324  64 252 346
 272 251 292  60 243  41 337 330 247 285 320  48  20 336 353 329  66 378
 333  45 233 235 327 332 322  53 373 281 321 363 331 296 359 256 365 340
 341 339 342 343 338 334 335]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0335
INFO voc_eval.py: 171: [201  72 381 171 164 179 380 419 198  64 231 427 247  69 241 391 386 404
   6 172 244  74 344 399 242 398 403 353  89 187  67 423 230 396 281 414
 366 329  68 385 212 439  70  24 461 289 450 457 252 131 328 333 402 345
 184 213 180 159 197 196 155  85 401 286 183  81  80 341  90 217 175 449
 410 433 265 240 397  86 383 330   1 367 205 109  12  97 334 339 406 215
 154 429 139 137 443 135 280 389 283 227 147 204 337 191 243 246 216 356
  63  50 437   2  27 400 361  17 392   5 368 456 276 152  20  65 209 393
  66 390  18  79 431 239 465 210 352 347 422 157 151 474 475 173 224 466
 260 464  25 214  59 165 354 338 234 169 376 454  77 411 134 261 441 256
 145 144 409 186 442 219 238 434 251 395 133 360 279 296 348 200 211 116
 364 462 118  84 233 287 203 178 444 405 408 170 327 193 421 435 460 221
 438 384 473 343   0  30  96 189  56  71 301  33 412 369  75  76  87 277
 111 340 245  22 452 472 373 150  31 199 222 355 263 413 258 188 249 372
 185 394 177 130 253   8  83  88 288  14  47 167 181 162 127 107   3 463
 254 114 202  60 182  13  78 166 357 105 358 458 121 229 232 160 190 336
 149 207 371 332 292  26 363 335 126 192  10 284 206 266  53 208 168 308
 365 223  38 259 467 228  98 176 388 174 351 158  11 124  58 470 319 447
 331  42 342 350 143 418  92 300 123  35  95 271  29  99 104 326  45 161
 194 273 417 141 359  55 248   4 142 374 370 375 362 294  15 218 455 382
 225 303  94 106 119  93 220 282 306 128 136 318 299 275 451   9 250 291
 377 407 122 321  49  34 302 148  82 305 387 272 459  41 297  91 379 153
 378 320 430  73 324 262 236  19 476  43  62 270  57 264 298 138 290 310
 103 102 295 278 132 420 226 477 446 120 267  51 428  36 453 317 125 255
   7 156  44 316 163 140 322  52 432 101  23 237 468 293  28 117 146  21
 445 440  61 323  37 349  16  48 235 346  40 415  32 448 313 425 416 426
 285 471 424 436 469  46  54 129 274 195 115 113 108 110 311 268  39 257
 269 314 312 112 309 100 307 304 315 325]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.1428
INFO voc_eval.py: 171: [ 24  10 135  35 140  27  11  19  96 107 111  94 136 105 139  95 109  23
  56 100 101  91  58 146  33   9  84 106  21  32   4  65  80 122  22 143
 147 103 102   5 145  77  78  37  87  39  30 110  79 148  89  81 121 115
  28  64  98  76 134  25  60  72   0   7  83   3  82  34 142  20  17 123
  14  53  13  46 141 138  69  18 130 118 116 125 137  41  26  54  40 108
  90 120   8 104  63  97  45  15 117 113  92  70 119 133 129  12  43  85
 114  44  88  31 149 112   2 126  74  93  86  67 128  36  62 144 127   1
  38  99  71  55  57  52 131  68   6 124  61 132  59  73  42  29  16  75
  66  50  51  49  48  47]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0466
INFO voc_eval.py: 171: [ 77 302  79 354  71  70  56 159  62  28 133 135  44  53  17 119 162 294
  10 155  72 226 131  68  84  46 291  41  39 293  35 303  19  16 117   6
 124  25  59  58  22 164  40 126  18  60  34 188  51 163 197  13 211 283
 318 243   7 230 118   9 141  55 299 258 147 122 280 321 136 121 140 353
  38 137 138 103 339 130 305 185  76  15 178 232 187 203  14  11 355 346
  48 304 297 312 120 376 215 225 277 268  45  50 209 252 198 194 217 184
 161 257 196  78 351  33  67 292  43 150  65 301 113 115 139 256  49 128
 127 282 179 231  61 272 267  85 375 171 182 123 143  99 322 276 296  64
 309 192 298  83  26  29 347 180 175  31 134  21 153 341 308 247 106 316
 377 324 326 269 350  23 264 148 279 165  91 378 151 218 229 174 342  81
  75  92  82 212  89 234 101 168  12 265 381 100 241 216 386 200 357 186
 319 219  80 250 109 330 214 166 356 156 313 281 266 176 220 195 345 254
  69 142 172 388 262   3  54  37  66 287 374 349 205 235   0 157 224 183
 170 245 311 329 372 190  86 104 251 278 129 270  32 125  42 332 360  57
 107 193 191   1 244  74 358 343 285 371  87 383 146 348 393  30 201 221
 328 261 114 333 181  27 286 102  36 382 169 210 391 152 335 189 238 288
 325 228 314 177 239  88 336 337  20 199 352 240 379 249  96 320  97 385
 390 380 167 327 290 260 259 284 367 364 295 236 207 132  95 275 274 340
 344  52 116 255 173 271 331 363   8 338 237 366 263 289 389 307 206 222
 368  98 315 365 248 334 105 202 369 373 233 300 111  90   4 242  63  47
 323 160 359 384 145 253 273 310   2 246 223 306 110 112 213 317 392  94
  24 108 158 154 361 370 149  73  93 362 208 144 227 387 204   5]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0627
INFO voc_eval.py: 171: [ 31 119  71  40  36  84  33  46 113 111  50  75  42 100  13  45  11  70
  85  64  38 118  90  34  82  65 116 123  87 125 110 117  86  96  32  43
  95  62  18 122 134  51  39  67  54  89   5  76  15  88 103  26  58 106
  49  21 115 121  73  55 133  93  35  98  22  97  60 128 135 131 112  20
 129 126  24   2  16  47  28  92 105   6  77  94 127 138  72 114  79 132
  52  44 143  74  59  53 124   3  63  57 130  29 102 107   0   1 142  81
  69  48  66 136 108 141  41 137   4 120  37  68  83   9  91 109  27 104
  56  14 140  99  80 101 139  61  78  17  30  23   7  10  12   8  25  19]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3007
INFO voc_eval.py: 171: [38372 36299  6675 ...  5432  5430  5455]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0997
INFO voc_eval.py: 171: [526 732 722 487 331 751 733  98 112 712 765 486  60 781 525 530 632 558
 777  68 725 803  73 587 493 714 743  99 329 524 716 314 758  94 106 640
 100 675 527 784 531 689 785 492 494 288 567 500  83 497 628  62   7 620
 529 563 417 330 720 682 793 786 115 719 721 323 528  86 726  90 507  79
 670   4 562 334 636 810 108  78  12 320 262 468 639 253 582 508 471  63
 692 379 797 356 499  96 625  80 723 110 638 663 779 619 193 561 729  87
 380 728 197   8 668 623 118 401 420 104 378 363 248  65 772 107 691 470
 660  69 245 516 259 300 740 143 206  75 503 133 364 662 282 289 546 608
 514  85  61 424 482  64 681 434 435  66 734 538 688 762 278 715 196 337
 281 421 122 250 146 604 769  82 187 755 566 713 805 763 137 397 458 247
 672 319 752 738 358 151  70 391 475 175 788 599 795 432 744 186  10  17
 694 512 790 303 318 627 787 614 287 666 141 163 126 129 560 433 554 792
 321 384 804 436 102 302 474 408 120  51 271  77  97 483 208  54  67 506
 393 293 534 184 808 477 407 125  11  74 132  95 215 194 573 612 737 256
 140 610 428 664 748 396 774  72 265 676 252 572 656 387 172 418  44 704
 510 687 594 236  52 284 370  71 291 142 192 277 357 422 311 747 111 476
 764 285 152 454 473  81 359 498 405 273  88 780 776 290 690  16 607 101
 345 613 547 717 138 757 344 761 766 442 279 463 123 169 246 324 160  19
 446 149  93 600  92 539 645 791 171 598 491 742 185 553 134 739 621 495
 459  43 176 128 158 559 629  42 389 229 369 242 515 596 322 109 238 745
 509 501 135  76 577 361 392 809 205 697 103  30 167 806 170 224 711  14
 212 210 304 161 778 257 341 388 395  23 113 513 383 800 615 376 298 274
 220 578 411  84 673 404   6 268 462 154  91 119 234 445 597 586 710 611
 799 767 746 347 213 754 556 603  89 654 343 232 231 771 775 178 650 466
 390 542 162 630 412 373 209 684 727  20 519 550 414 543 105 426 637 147
 144 354 333 760 227 301  48 794 574 633 283 340 796 449 136 575 148 707
 207 591 595 460 606 605 718 520 204 349 807 339 447 365 429 286 523 571
 609 368 617 736 173 372 200 235 648 416 643 313 276 166 237 671  47 622
 753 685 759 674 646 665  40 557 651 153 258 451 315 350 724  18 292 706
 741 798 678  45 750 168 695 489 116 488 770 647 181 472 121 211 655 669
 157 328 686 756  13 702 593   0 174   3 703 130 443 782 254 260 342 549
 658  50 517 275 544 602 584 150 219 659 564 267 264 159  39  34 312 552
 195 374 296 464 131 657 801 385 661   5 585 413  26 352 644 124 165 522
  59 680 479 249 399 709 641  22 699 504 452 425 652 381 233 353 749 693
 469   1 351 336 346 783 216 332 419 601 481 222 263 183  15 461 540  53
 240 190 182 730 511 218 382 362 355 164 394 156  29 243 768 214   2 683
 375 244 377  27 545 155 533 438 518 700 145 272 326  32 708 297 127 536
 569 270 576 117 667 360 269 485 348 635 203 773 114 618 735 226 325 338
 441 731 555 415 402 266 410 802 478 590 679 139 811 367 280 505 294 502
 789 251 548 241 677 423 223 592 386 465 295 705 450 541 701 448 299  38
 532 179 398 427 317 589  33 403 366 521 583 455 440 306 551 480 198 444
  49 180  25 371 239 653  37 535 457 626  28 698 189 581 580   9 188 467
 255  31 225 400 177 230 335 202 631 191 221  57 490  21  36 537  35 496
 453  24 456 217 327 305 616 316 649 437 406 634 588 696 261 642 308 201
 570 439 624 568 431 430 228  46  41 199 565  55 484  58 309  56 579 307
 409 310]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.1129
INFO voc_eval.py: 171: [ 97  87 101  60 111  76  81  23  83 116 105  90  79  84 146  64 206  71
 119 150 132 143  59 167 142  78 163 126  15 155 128 172  63  94  36  73
 148  92  61 102  96 173 161 100 124  26  35 203 162 130  28 154  16 199
 114  10  37 147  91  89 136 121 123 157   3 153 193  21 160 196  30 118
   2 106 174 164 131  66  17  67 129 205 145  32  14  86  70   9 112 139
 141  27   4 192 159 175 176 204  18  95   8 151  75  29 125 156  33 177
  25  22  11 144 140   6 198 191  39 108 133 186  80  93  69 158   1 188
  68  98   7  20  88 166  77 187  24  62 117  34 149 189 152  12  19 120
   0 137 168  13  99 127  42 134 122 135 138 171 195 183  31 194 185  85
 169 165   5  48  74  50 201 104 184 103 190  72 109 180  55 170  44 197
 113  82 182 115 200  65  57  56 202  45 107 110 178  53 181  41  54 179
  52  49  46  38  51  58  40  47  43]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0292
INFO voc_eval.py: 171: [ 71  70   1  23  12  27   2   4  72   6  18  13  10  66   5  75  26  74
  28   3  24  68  17  64  73  36  45   0  76  98  21  25 105  95  14  30
  16  60  69  65  41  96  67  20  47   8  11  59  49  94  54  46 104  57
  22  52  53  55  15  90  50  39  19  44  87  48 103  99  82  58  40  78
 100  85  92  80 106   9  56  79 102  91 101  61  97   7  51  42  62  88
  29  77  35  89  93  84  86  81  63  38  83  43  37  33  32  34  31]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0603
INFO voc_eval.py: 171: [327 369 349 462 345 337 221 366 490 479 196 309  62 525 207 463 198 465
 466 340 468 339 338  29 222 336 236 491 311 481 484 330 487 228 495 328
 361 350 293 348  74 219  80 123 271  83 269 118 437 516  31 140 199 473
 478 503 486 474 306  94 499 206 347 216  86 263 461 432 212  52 427 200
 215 233 279 280 272 509 514 234 126 244 226 436 467 329 364  46 344 460
 147 295 304  84 205 312 268 104 382 383 420 211 423 428 110 294 292  41
 298  97 225 202 469 471  81 497  42 494 203 334 113 431 384 285 513   5
  93   8 167 157 218 130 224 519 275 331  36 143 523 489  24 241  40 210
 239 335  99  39 447 209 305 303 520 273 177   0 135 132 144  87  30 488
 395  76 367 133 426 159 459 485 381 475 197  68 112 267 270 429 386  50
 464 417 120 442 125 357 358 472 161   4 371 172 201 297 483  57 245 127
 333 343 397 387 424 117 456 508 493 360 422  75   3  71 455  26 243 505
 421 283 498 171 214 446 162 274 121 374 388 290 445  90  79 332 302 242
  37 155 128 396 390 230 393 438  59  88 192 138 151 518  28 231 342 300
  35 346   6  72 163  16  34 288 204  65 169 154 134  45  92 122 265   1
 238 416 307 517 105 435  56 174 430  14 181 451 376 264  64 385 281 119
 501  19  55 434 136 282 391 370 299 276  25 403 217 142  32 308  44 377
  43 173 235  54 418 354 363 287 470 193 450 251  51 392 259 277 141 227
  53 443 524 240 223 310 414 262 153 246 415 131 129 439 444 254 109 191
  48 115 449 409 315 111 266 355 250  58 124 394 296 195 139 247  95 145
 441  23  27 322 100 492 213 260 454 165  70 362 146 411 375 261 356 114
 116 402 522 179 359  78 398 325  73 320 515 190 457 314 368 178 170 440
 103 152 521 253 286 476 406 477 410 507 318 278 189 504 148  15 194  82
 400  20 425 352 291  98   7  60 168  12 208  17 419 500 452 284   9 405
 373  38 404 316  91 433 108 248 458 351 256  33 317 502 158 496 453 183
  11 511 188 229 506 185 166 326 220 102 365 407 301 323 413 232 480 510
 341  66 353 160 401 289  67 412 372 482  49  21  47 255  10 389 257 321
 408  22 150 512  61 186 237 176 448 180 149  63 156 313 258  77  69 252
 137 164  85 249 324  13 399 101  96 187 380  89 319 526 106   2 527 175
 182 107  18 184 379 378]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0547
INFO voc_eval.py: 171: [ 18  75 373 296   6 194  97 248 174  13 140  59 182 361  40   4  12  39
 185 273 269 283 331 177 250  10 365 139 187 131  64  92  89  78  51  77
  36 266 367 364 284 341 334  19 330 264 260 375   5 363 101 129 319 176
 316 274 276 282 226 181 290 343 193 125 180 238 300  93 249  91 163 297
   0 392  57  72  61  55  45 135 132 121 166 356   2  32 196  33 357 239
 178 215   7 219  41 218 335  62 337 190 189 188  47 252 333 321  81 265
 320  11  95 302 308 136 275 305 317 256 143  99 235 198   3  85 134 123
 109 156 133 285  60  71 202 162 388 298 389 243 318 328 371 104  87   9
  21 288 120 197 242 322 153 382 385 216 221  96 312 299 199 191 210 329
  22 224 214  65 277 152 183 208 268 303 387  20 292 184 280 141 311 186
  48 359 306 211 314 360 222 362 161 126 336 339 200 225 217 378  88 207
 205 168 358 151  63 102  90 122  84 342 347 170 130 340 384  66  16 158
  86 380 100 267  14 124 301  17 262  15 271 137 338 390 173 206 381  52
 257 344 379 175  30 376 128 247 230  42 279  74  76 157 167 368 324 259
  83 234  69 127 220 165   1   8 310 278 370 369 350 255 149 383 241  58
 291 204  25 270 323 263 281  80 289  24 315 237 179  27 142 114 227  38
 138 209 106  94 164 345 294 147 253 307 348  54 386  26 286 145  73 354
 201  79 261  70 295  44 212  28  31 353 195 112 117 148 355 118 155 115
 372 223  37  43  68 192 391  67 105 107 377 119 232 366 113  53  98 110
 213 351 229 287 103 272 203 346 246  50  23 231 293  49 150 352  46  35
 374  56 326 251  82 327 116 236  34 111 244 108 325 349 146 159 228 169
 233 309 332  29 240 304 254 171 313 245 154 144 258 160 172]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0942
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0897
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.025
INFO cross_voc_dataset_evaluator.py: 134: 0.220
INFO cross_voc_dataset_evaluator.py: 134: 0.026
INFO cross_voc_dataset_evaluator.py: 134: 0.096
INFO cross_voc_dataset_evaluator.py: 134: 0.078
INFO cross_voc_dataset_evaluator.py: 134: 0.230
INFO cross_voc_dataset_evaluator.py: 134: 0.046
INFO cross_voc_dataset_evaluator.py: 134: 0.018
INFO cross_voc_dataset_evaluator.py: 134: 0.017
INFO cross_voc_dataset_evaluator.py: 134: 0.034
INFO cross_voc_dataset_evaluator.py: 134: 0.143
INFO cross_voc_dataset_evaluator.py: 134: 0.047
INFO cross_voc_dataset_evaluator.py: 134: 0.063
INFO cross_voc_dataset_evaluator.py: 134: 0.301
INFO cross_voc_dataset_evaluator.py: 134: 0.100
INFO cross_voc_dataset_evaluator.py: 134: 0.113
INFO cross_voc_dataset_evaluator.py: 134: 0.029
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.055
INFO cross_voc_dataset_evaluator.py: 134: 0.094
INFO cross_voc_dataset_evaluator.py: 135: 0.090
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.495s + 0.004s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.361s + 0.003s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.351s + 0.004s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.354s + 0.004s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.359s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.358s + 0.004s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.360s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.360s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.360s + 0.004s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.360s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.359s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.361s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.363s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.483s + 0.003s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.393s + 0.003s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.379s + 0.003s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.374s + 0.004s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.373s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.373s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.374s + 0.004s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.372s + 0.004s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.371s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.374s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.373s + 0.004s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.373s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.371s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.361s + 0.003s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.357s + 0.003s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.357s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.363s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.361s + 0.003s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.361s + 0.004s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.363s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.365s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.365s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.363s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.363s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.360s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.361s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.448s + 0.002s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.365s + 0.004s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.363s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.357s + 0.004s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.346s + 0.004s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.350s + 0.003s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.358s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.356s + 0.003s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.359s + 0.003s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.358s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.357s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.357s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.358s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.807s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 52  87  30 121  47  69  65 122  49  60 117  72  64  27 112  50  76 163
 113 119  53  97  88 162  26 109 110  37 152  91  31 102  56 153  34  42
 157 149 115 133 147 125  90  86  45 141 132 123  46 148 155  89 146  71
  32  78 118  57  58 108  17  75 139  98  48  33 144 158 131 160  43 124
 140  85  99  54 138   8  28  19  41 120  44  66  13  35  55  61 170 174
 114 107  95 156  59  94 134 101   4 135 142 165  36 137  63  62  51  22
 145  74  93 159  77  29   3   0  68   2 116 136 105 143   1   6 168 106
  67  92 129 128  40  73  25  18   7 169  21  23  84  70 164  38  10   9
  16   5 130  12 100  15 151  39 150 173  20 111 166 154 161 167  81 104
  14  11 103  82 126  24  79  96  80  83 172 171 127]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0171
INFO voc_eval.py: 171: [ 87 234  55  16 114  40 110 109  62  53 111  50 206  13  29  54 221 133
 204  84  59 233  23 229 154 224  42 112 121 125  63  90 117  19   3 205
  38 216  56 203 223 151 209  28 175  95  12 113 231  64  79  78   4  41
 167 138 232  44 218 222  18  91 140  58  14  45  17 107   9 210  47 116
  32  60 215  11 214 219 152 213 211  26 135 100 122 208 217  68 227 103
  15  93 172   5 131  30 228 165 153  51 115  37  83 166  21 230 118  85
 128  22   2  10  61 137  98 123 145   8   7  48  88  31 162  20 225  89
  96 159 207 164  57  24  27 106 174 130 157 220 173 226 212  66 132  52
 127 124   1   6  94 158 143 150  35 183  49  39  25 136 141  76 168 120
 177 160 119 188 176  69 147 104   0  97  75 178 182  82 146 179 180 161
 190 181  70 148 199 187  72 129 126  71 192 171 186 108  43 185 149 194
 197 193 134 139  67 156  80  86 169 170 144 189 195 101 163  81 201 142
 105  65 155 202  33 200 191  92 184 198 196  46 102  73  77  36  34  74
  99]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.1727
INFO voc_eval.py: 171: [101 138 118 ... 590 500 562]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0224
INFO voc_eval.py: 171: [618 289 236 118 275 779 853 239 643 774 610 226 352 370 788 127 119 532
 233 573 576 544 270 625 605 511 846 479 124 792 192 362 369 356 160 230
 783 221 832 847 903 775 154 900 839 515  43 271 342 287 535 770 772 777
  75  16 607 809 824 126 391 468 432 114 483 153 210 282 237 232 580 582
 631 922 600 638 641 222 115 263 481 418 122 480 389 117 177 660 793 285
 493 500 630 504 566 298 509 771 811 339 487 934 893  15 845 334 292 569
 407 807 379 315 530   3 325 213 927 620 948 886 419  59 834 363 478 387
 802 606  60 597 637 612 623 421 938 626 123 399 946 162 502 836 884 372
 497 172  57 212 475 755 274  56 361 952 354 508 324 318 229 548 302 290
 269 604 616  53   9 169 371 842 147 806 859 636 628 247 795 411 596 272
 273 278 857 125 855 848 518 512  96 835 357 878  62  52 709 176 758 833
  50  55 166 484 145 488 259 856 879 840 155 885 337 915 425 426 568 340
 514  46 477 388 897 284 674 909 429 769 358 860 844   8 929 743 581 650
 301 815 434 264 152 401 552 174  68 664 366 784  95  81 286 655 440 889
 746   7 288  93  10 492  42 326 398 682 669 170 498 105 521 971 785  22
 614 563 516 611 791 556 558 517 489 850 316 945 826 818 925 491 579 683
  20 265 656 184 494 963 651 395 345 964 350 241  47 646  77 121  61 439
 279  11  85 570 293  69  72 789 469 950 496 373 323 351 343 113  70 931
 591 744 918 402 919 805 482 473 320 640 652 311 703 882 106 627 238 240
 503 896 365 470 466  44 768 128  23 346 283 331 575 812  92 649 423 215
 216 353 700 186 632 935 814 565 505 967 381 393 261 658  39 765 175  80
 804  25  83 830  99 360  45 251 827 143 102  58 415 843 507 268 513 534
 555 968 947 414   2 549 159  74 880 178 613 647 499 932 716 957 738 912
 698 696 780 524 898 904 553 547   0 359 308  87  65   6  12 545  71 584
 542 961 823 619 540 260 254 522 877 797 661 377 422 430 690 427 190 364
 831 453 829 689 813 693 692 103 892 317 173 937 920 910 527 907 396  63
 382 330 277  29 428 745 490 416 723  90 476 659 841 193 747 941 151 550
 276 883 781 380 495 116 668 959 684  66 589 713 750 603 280 759 146 828
 706 329 786  54 572  79 629 678 974 338 409 347 635 761 928 367 262 645
 537  78 299 838 248 609  38 101 657 663 438 235 742  19  67 120 782 760
 167 307 523 526 962 486 704 975 310 849 670 578 413 258  13 917 564 965
 837  34 472 762 543 810 255 653  94 621 740 464 710 953 557 294 702 224
 970  37 112 936 691 973 394 158 798 397 943 790 969 588 732 194 538 642
 304 474 149 459 529 536  49 773  64 725 822 583 533 764 617 722  35  17
 951 921 335 852 753  27 551 457 940 300 108 854 390 754 602 227 891 295
 403 400 109 525 562 662  86 376 171 471 368 799 933 322 595  82 344 733
 249 697 694 816 519 341 452 865 895 246 386  18 654 207 144 412 819 731
 163 257 291  89  41  32 266  76 252 206 593  88 195 501 686 435 231 349
 559 930 868  40 599 198 608 485 392 665 681 858 183 902 728 586 456 960
 873 374 404 448 528 305 825 333 899 297 881 520 955 622 104 633 676 867
 887 699 679 680 714 506   4 451 701 179 872 202 888 778 410 205 375 866
 594 666 355  98 561 454 741 424 461 800 870 756 958 794 648 675  31  24
 601   1 510 531 309 634 100 554  97 585 688 966  14 408 129 204  21 437
 874  84 735 592 539 135 949 138 442 560 405 685 705 720 730 766 787 214
  36  33 313 168 639 851 182 467 296 546 972 225 541 667 385 672  30 181
 571 763 751 644 131 924 911 444 243 894 156 729 711  28  73 447 406 567
 321 107 433  91 695 253 719 383 749 460 615 942 913 944 718 384 130 801
 817 203 458 314 197 449 598 923 803 228 901 267 808 332  51 157  48 378
 417 726 796 164 223 462 687 111 303   5 450  26 939 574 821 319 200 876
 577 914 871 188 587 707 180 140 455 708 196 199 139 748 328 757 306 431
 624 712 863 715 348 161 250 956 721 189 864 441 191 187 245 242 671 954
 443 890 767 677 905 916 820 327 724 752 110 727 926 737 908 211 734 436
 133 312 445 875 861 336 739 717 673 590 217 906 256 465 736 463 446 869
 420 165 209 141 862 137 219 776 218 148 201 220 281 150 208 234 185 244
 132 136 142 134]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0965
INFO voc_eval.py: 171: [262  70 260 249  84 164 270 309  83 207  45 264 235 148  63  60 244 234
 251 139 236 354 205 255 317 109 228  65 263 254 294 215 414 316 238 297
  72 204 306 211  52 242 435 222 321 253 259 348  61 198 420 384 225  37
  46 158 382  67 174 357 266 303  57 134 293  53 330  76 142  78  38 247
  86 208 231 388 106 172 145 311 220  91  64  79  81  73 383 334  62  68
  71  30 175 149 358 281 232 377 409 393 159  80  77 261 391 342 301 339
  75 212  89 271 344 243 286 206 322  82 307  36 376  69 135 324 333  55
 192  26  74 144 329 298 136  87 177 356  88  90 283 325 140 419 374 179
 173  85 291 163  39 318 102 433 210 422 320 146  40 387 390  95 165 100
  48 113 152  66  18 278 287 327 295 332 343 252 143 351 352 178 250 224
 226 396 269 282 276 230 240  99 427 349 138 167  16 112 304 285 168 289
 153 290 336 310  93 288 176  31 107 155 305  96 203  29  25 181 395 312
  13 185 401 345 209 314  49  32 426 300 369 346 308 193  33 375  42  44
  56  97  34 437  94 423 218 219 331 187 279 347 151 133 328 373 299 302
  21 313 170 111 399 350 296  51 157 385 197  50 340 108 341 338 355 141
 323 315 202 186 360 412 280 105 130 265 169 104 194 389   9 267 127 110
  92 353 337  47 126 223 284 319 213 335   2 429  22 132  35 101 137 326
 292 162  14  98 147 380 171 229  41 402  27 103 180 404 392 199 434 156
   4  23 161  20 166   0 275 237  54 370 188 268   5 217 367 128 245  12
 182 394 184 233 398 362 272 421 154 221 183 227 150 129   6 246  28 386
 216 417 258  43   8 189 125 436 359 371 431 415 257 241 195 201 366  11
 131 196 239 190 256 214  17 364  19 160   3 191 200 378 274  15 277  10
 248 368 430 410 428  24   7   1 361 365 273 363 117 116 403 121 124 413
 397 372 118 411 379 425 406 123 416 407 115 120 119 122 405 424 432 408
 114 400  59 381  58 418]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0731
INFO voc_eval.py: 171: [ 98  88  71  84 103  99  91  62  69  85  89  82  81  74 101 108  96   4
  59 107 106  29   1  57  39  70  13  68  94 102  24  73 105 112  77 109
  58  86 104  10  67  92   6  76  87  72  93  19  66  83  22  97  12  15
  79  35  51  95 100  16  11  65 111   7   2  33  55  14 110  64  27 113
  17  25  63  42  61  41  78 115  38   9  43  23   3  49  75  52  30  90
  28  32 114  31  20 116 117  60  18  48  45   5  53  26  36   8  47  44
  37  54  34  21  40  80  56  50  46   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.1353
INFO voc_eval.py: 171: [423 609 666 ...  41 493  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0358
INFO voc_eval.py: 171: [24 19 23 34 18 15 53 11 44 17 55 30 31 47 76 20 67 54 10 41 65 75 72 16
 74  9  3 13 64 70 28 35 73 21 38 33 45  8 49 43 32 69 58 50  7  0  4 56
 52 46  2 29  6  5 36 14 25 42 22 62 57 27  1 61 48 71 12 26 59 60 63 40
 68 66 51 37 39]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0170
INFO voc_eval.py: 171: [1770  908 1798 ... 1022 1325 1323]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0246
INFO voc_eval.py: 171: [169 156 181 133 188 131 228 230 123 108 238 243 239 107 162 161 179 153
 102 151 168 281 116 111  94 139 152 132 167 120  67  75  76 104 231 101
 241 237  87  83  72 170 118 119 185 147 165 144 158 154 106 174 234  78
  70  42 180 126 276 148 187 183 135 112 233 150  73  89  92 140  93 248
 145 240  95 160  99 100 109  84 128 182 114  80  77 232  91 115  96 125
 178  74 103  51 319 242 249 271 113 171  97 117 163 146 141 122  68 127
  30 105  46 124 138 121 129 149 136 137 110  90 329 279 176 273   6 318
 143  54 130 270 159 164 221 317 157 155 142 229  19 166  41 227  58  10
 184 315 272 177 245  59 173  85  25 322 258 175  82 134  52  98 186   3
 208  35 218 257  53 203 235 172 215  29  57 283  13  86  63 285 323 269
  48  37 306 236 207  49 311  23 204 205 199 316  61  71 250 312  38 313
 246 324  88  66  81 277 278  12  27 274 282 255  62 263 286 333 209  69
 328  11  31   1 198 265 256   5 307 201 195 275 189  22 268   7  32 290
  15 305 284 219 223  39  55  64  20   2  34  24   0  79  26 200  56 326
 314 214 327 226 280 337   4 244  50 308 212  40 197 225 192 262  60 196
 217 220 335  44  33  14 304 193  45 266 264 222 252 253 194 330 206 213
 247  17  47 332 224   8   9 202 292 211 320 287 309 191 190 261  21  36
 334  18  16 321 299 254  28 289 216 267 293 331 291 260 288 300 259 310
  65  43 336 298 210 325 294 302 296 251 301 295 297 303]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0451
INFO voc_eval.py: 171: [ 74  76  68 232 325  73 248  78 383 231 416 229 379 222 215 212 186 184
   7 378 172 168 347  97 320 338 334 341 342 161 240  70 354 234 225  96
 182 166 165 164 259 362 321  15  11 381 400   6 419 397 401   1 394 402
  81 239  84 236 412 363  82 227 431 201 197 189 444 183 173 169   4   3
  89  75 135 382 335 329  37 390 285 348 344 364 274 332 366 346 171 399
 349 439 369 352 452 207 209 213 398 217  72 395 405 386 328 359 223 219
 357 433 255 211 241 200 448 174 283 429 406 389 422 280 284   0  39  95
  56  19  85 105  27 170 221  26 287 432  21 193 317 251 360 139 194 195
 337 198 192 336 175 114  94 327 326 427  88 323  38 246 316 275  69 307
 409 202 167  20 458 120 142 361  47 144 146 350  29 132 456  62 238  60
 137 322 454  34 235 271 163 162  33  35 208 388  98 301  10 435 333 176
 151 121   2 442   5 339 324 256 178 112 417 428 230 214 423 282 149 153
 459 196 455 447 111 414 116 188 237 353  50 355 136 216 460 370 308 384
 242 392 145 445 119 396  32 407  83 220 425   9  54 154 286 365 177  71
 253  45  65  41 273 441 302 368 125  92 100 351 179  25 403 300 304 393
  30 358 330  18 375 318 130  59  57 265 377 426 446 191 408 147 150 152
 345 205 185 103 210  93 289 343  23 128 306 124  79 451 156 113 101 373
 356 380 160 266 190 387  87 367 404 107 140  22 155  13 199 438  14 436
  86  16 310  63 117 331 254 106  80  77   8  52 159 288 102  99 250  90
  91  53 243  17  12 305 462 252 258  40 464 143 110 340 309 424 453 465
 148  67  58 434 264 374 281 418 257 233 463 461 104 391  51 203 303 277
  55 204 279 158 294 449 385 420 437 276  44 127 272 108 126 122 129 278
  31 206 141 138 421  36 443 218  24 292 133 315 293 115  66 181 226 118
 372  28 224 260 228  42  61  46 269 187 413 440  43 268 270  48  64 411
 415 457 180 311 247 410 312 123 249 291 267 430 261 376  49 450 134 298
 244 109 245 319 262 157 297 263 131 299 296 314 371 295 290 313]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.1197
INFO voc_eval.py: 171: [ 12  35  19  15 157  25 158  17  36 106 109 165 108 116 168 103 121  60
 123 113  62 111 114  11 172 142  39   7  69  99 169  27   9  92 136  30
  32 170 115  89 138 140  42  21 101 119 163  87  18  13 133  70  91 110
 144 102  97  28  64  98 156 134  38  26   5  51   4 166 118  22 120 107
 125  24  37  86  75 122  93  58 159  20   2 105  29 130  10  79 141 126
 151 139  72   8  52 128  14 167 146  67  31  46  43 131 117 100  90  47
 160 143 154  59 164 150 104 173 124  48  76  95  44 148 132  73   6 162
  66  78  50 112  40 129 149   3  61 161  94 147 127  65  96  82 153 174
  63  88 152 171  83  16  23 145  71  77  45 135  84  81   1   0  74  85
  80 137  33 155  34  41  49  68  57  55  53  56  54]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0434
INFO voc_eval.py: 171: [ 66  64 172  19 166 230 165 359  59  71  61 309  87  55 162 140 136  30
  62 233  39  41 131  60  58 215  34  43  49 310 306 304 303   6 158 122
  27  11  81  16 124  20 144 299 134 358  56 170  63 146  53 200  48  50
 320  15 256 132  14 126  13 133  40   9  45 213 107  10  24 160 125 150
  69 143  68 198 130  65 336  36 229 142 348 207 270 311 123 312 163 302
   8 288  18  23 260 259 254 157  38  52 117 199 191 186  73 354  79 272
 201 115 357  67   7 381 394  12 258 248  86 244 159 291 138 236 171  51
 193 339 181 192 195  93 267  80 211 286  99 283  32 135  83 167 203  29
 187 396 237 391 301  35 355 281 343 322 179  57 108 105 189 380  90 329
 313  77 220 218 257  95 275 314 216 183 319  21 378 389 205 360 129 101
  28 174 194 235  82   3 188 145 282 128  22  91 344 222 217 265 298 111
 190 137 196 110   0 384  96 308 273 197 176 332 321 228 271 168 264 307
 249 276 112 379 293   2 285 330 334 223 356 392 182  33 351 390 335 292
 180  76 247 161 326  26 353 177 376 315 178 262  31 118 238 184 385 297
 352  84  72 120 375 365  88 239 175 210 305 346 148 316 324 369   1  97
 340 345 139 206 269 209 400 405 225 279  85 274  17  42 404 386 261 277
 397   5 368  70  89 263 147 296 224 338 290 387 395 245 268 402 328 362
 212  44 383 202 185 372 231 204 246 141 347 116 294 327 243 388 252  47
 363 151 226 253 242 370 350  46 367 240 104 113  94 341  92 371 323 280
 278 154 373 364 403 295  54 109 289 227  98 349 266 106  37 241   4 232
 214 333 284 393 119 250 331 342 382 366 337 361  75 325 300 173 221 398
 155 317 287 255 153 234 103 374 318 399 406 169 127  74 377 401 114  78
 100 251 156 102  25 164 152 407 149 208 219 121 408]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0252
INFO voc_eval.py: 171: [ 34 110  40  71  36  45 106  10  51 103  81  50 104 108 116  75  18  54
 112  82  46  85 123   8  49 118  37  52  94  67  84  26  29  35  80  70
  39   3  87 102  42  30 121  64   5 124  41  48  23  38  86  12 115  73
 101  43  66 113 114  83  57 119  91  90  65  19  97  69  32  33  78  28
 109  59 105  61  68   2  95  17  89  98  22 111 107 117  74 122  93  56
  31  60  88  53  47 120  63   4  16   9  58   0   1  96  99  92  72  20
  77  62  44  55  76  79 100  15   7  11  13  27  14   6  24  25  21]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2855
INFO voc_eval.py: 171: [36044  6582 18866 ...  5379  5376  5327]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0914
INFO voc_eval.py: 171: [901 637 854 858 600 120 852 839 598 894 402 117 109 676 750 119 636 643
 603 907  90 111  82 124 712 892 911 853 635 116 609 869 325 759 844 383
 758 602 910 455 638 304 801 843 860 818 606 613 686  10  89 629 734  83
 680 384 756  91 641   9 285 396 457 400 849 919 121 454 897 851  85 916
 127 518 754 107 303 639 100 487  97  98 814  17 575 429 708 471 656 216
 679 653 871 623 797 578 845  87 403 110 211 118  11 898 395 475 796 323
 904 327 102 220 742 114 789 741 737 925 681 104 642 663 624 217 129 577
 861 302 493 282  88 790 640 612 866 611 730 207 857 158 530 437 615 514
 286 859 115 318 568 231 122 803  75  84 850 326 490 483 831 324 805 524
 604 288 847 463 812 291 206 161 136 684 808 583 647 480 134  58 753 920
 106 424 746 621 196 537 721 887 527 292 103 731 296 410  68 733 813  16
 841 922 218 209 242 221 453 744 883 913 586 880 580 581  73  19  23 893
 617 917 724 328 591  93 678 716 320 322 343 870  94 112 534 881 105 918
 389 170 293 795 799 392 113 339 176 208 614 492 141 428 540 594 315  64
 101 930 199  92  96 523 155 608 531 412 931  65 662 435 420 648 903 625
  59 875  86 156 536 478  27 418 933 174 823 284 317 138 466  99 671 272
  63 479 426 474 321 584 227 157 879 692 785 726 622 262 253  95 232 246
 329 469 749 695 713 149 735 153 182 711 661 886 675 546 163 498 868 108
 767 728 786 569 277 442 148 473 173 842 547 877 239 230 672  51 739 601
 125 175 921 382 802  28 123 252 280 915  66 745 906 214 817  57 433 628
 717 183 406 549 229 186   8  40  26 472 150 397 757 146  70  31 443 865
 237 626 776 236 573 308 335 888  38 465 393 550 160 874 307 665 194 677
 416 840 143 574 715 422 722 467 895 394  47 934 470  50 619 885 787 147
 699 548 697 552 314 927 766 192 195 269 279 342 414 743  32 289 204 202
 567   6 819 191 440 462 707 341 718 270 791 793 876 654 788 763 275 519
 255 266 826  46 413 579 535 838 441  25 747 655 456 447  69 878 238 482
 213 234 836 768  71 135 283 159  56 254 657 404 294 166 777 340 912 417
 446 553 132 769 738 165 693   3 837 794 201 171 862 151 902 810 605 515
 932 673 142 188 210 301 399 162 572  42 154 258  18 778 891 319 506 233
 890   2 415 924 200 510 309 180  15 554 332 848 407 290 177 330 658 247
 228 300 696 278 570 501 691   7 499 720 846 646 235 306 529 528 834 620
 313 719 250 889 189 203 333 521 193 882 219 511 334 497 419 522 133 241
  24 533 856 240 264 727 351 714 816 806 287 130 222 245 451 923 765 460
 867 137 725 259 223 588 128 338 668  76 265 391 385 563 855  36 798 145
 599 800   1 872 773 832 732 610 502 168 645 914 425 709 827 496 634 908
  74 703 491 905 929  48 900 509 312 538 477 427 779 627 452 761 271 197
 172  80 650 660 630 551 740 358 833 347  22 545 683 362 274 928 181 909
 439 649 444 666  41 431 704  45 644 432 504 421 755 263  21  54 485 355
 423 555 190 884 607 297 873 804 702 484 476 267 500 674 438 593 164 436
 448  61 752 935 226 257 140  79 459  43 616 430   5 589 434 513 205 698
 331  78 590  55 667 688 411 268 815 631 139 565 809 316 345 596 926 367
 863 281 664 370 276 542 556 464   4 659 520  67 807 167 899 179 775 508
 445 896 587 705 829 131 144 864 260 187 353  77 651 126 830 764  39 770
 835 820  44 198 375 736 710 398 387 811 774 388 771 571  29 828  34  52
 152 381 505 633 481 618 792 336 337 256 507 390 566 821 273 526 458  35
 564 349  49 532 365 244 295 409 539 185 344 489 461 562 706 782 401 585
 450 348 670  81 700  13 248  72  14 561 669 224 597 354 582  20 249 772
 386 169   0 251 373  12 305 449 729 557 781 298 357 243 632  30 525 723
 652 685 215 682 184 212 517 486 760 299 543 178 371 225 261 595  33 488
 544 558 751 495 352 824 689 311 592 780 762 346 405 690 369 516 822 468
 378  62 825  37 783 541 512  53 380 784 687 694 368 559 701 408 503 372
 560 374 576 377  60 748 366 360 359 361 356 310 363 494 350 376 364 379]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0994
INFO voc_eval.py: 171: [103 106  94 108  65  87  27  90  92  89 127 128 110  72  70  81  83 196
 162 164 154 158 136  66 149  75 172 150 140  95 113 115 193  86  32 116
  96 163 219 118 151  41  97 174 167 157 109   8  19  34  31 166  68  29
  78  42 123 119 194 188 126 138  79  25 176 101 134 195 145 141 144  24
  15   3  76   9 217 177  21 200 202  47 100  33 148 131 205 133  26  91
 147 155  18 153  35  17 129 218 104  37 198 156 168 182 130  30 135 186
   7 160 143  84 201 192   6 189  67 187  71 175 190 185 142  16  85  45
 114  20 191  22 159  74 161  40  23  39 171 132 152 211 146  28 137 180
 204 197 139  11 102   1  69 169  93  48 199 124 184 121 173 216 120  51
 203   5  38  13   0  77 170  12  14  10  55 165  88 213 117 112  82 105
 125   2  60 183  46   4  62  36  98 107 111  57 122  73  80  99  50  61
  59  43 210  52  53  56 178  49 214 208 207 181 215 179 206  44 212 209
  64  63  54  58]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0504
INFO voc_eval.py: 171: [ 96 102 103  33   0 101 106  25  13  10  30   5   2   3 107   4  20  32
  21   6  93 105  23   9  11  12 100  53  43   1  95  31  35 130 125  97
  24  50  18  28 114 122  15  55 123  51  85  47  76 129 121   7  49  27
  60  63  14   8  81  72  94 132  64  62  45 124 126 127  77  74  89 109
  84  91  70 108 111 112 131  69  34  26  59  44  40  79  22  41  73 104
 113  19 117  29  56  68  99  78  16  17  61 128  52 120  38  87  65 119
  82  86  88  46  36  98  75  80  71  83  48 116  90 110  67  42 115  66
 118  54  92  37  57  58  39]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0364
INFO voc_eval.py: 171: [181 268 283 192 113 207 253 190 210 186 191 260 184 193 278 272 136 118
  19 267 280 115 262 213 266 122 178 185  57 126 275 265  59 200 202  22
  30 287 197 198 259 269 171 182 289  37 117 175 264 146 137 208  60 227
 199 141 261  78 116 134 257 147 203 219 119 236   0  44 183  53 139 125
 271 255 243 145 120  29 263  45 135  61  58 237 158 128 217  70 214 206
 140 274 256  43  24 285 228 177 220 294 179 201 194 221  75  34 172 258
 159  92 218  18  73 174 144  31 254  74  64  85  76 286  33  65 153 242
  67   1 241 187   9 245 270 148 292 173  90  50 167 209 195  77  72 142
 290   2 165 291 247 279 189  14  79 130 273  39 232  28  66  80 121 235
 240 132  20 244  94 108 166  71 123  49 138 160 295 169 129   4 222 205
 131 176 164   5 196 151 127 297  26  62 225 188 212 231  52 277 293  54
 133  56 252  99 238 157  41 226 276 112  55 168  83  23 124  96  17  91
  46  10 109 296  47  95 298  38 251 229 149  98  13 114 233  81 224 156
 248  32 215 288 105 180 154  88 102 230  15 250  27 100  42 163 170 111
  40 246 234  21  84 150 223 249  63 239  87  51 152   7  48 161   6   3
 110  68 204 211 143 281 155 284  36  69  35  16 282  25 101 162  89 104
  93  11   8  12 106  82  86  97 103 107 216]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0301
INFO voc_eval.py: 171: [ 14 128 286  19 153  10  72 150  50 279 256 277 186 141  33 103   4 230
 142 281 188 285 200  63   9 222 208 101  18 238   3 131  51  94 224 207
  90  67 260  75 189 243 214  41 144 264 289  24  25  26 133 280 151   7
 132   1 290 134  93 266  92  39  84 175 212 225 199 197  99  30 257 235
  83 165 303  69 245 135 180  35 242  56 136  27 172  46 276 104 244 147
 239 231 232 233 194 152 154  52   2 248  47 261 262 113 202 211  21  64
 170  15 164 205  16 302 226 249 157 190 130  95  71 301 216 259 100  96
  91 254 252 148  76  61 137 255   5 251 298 106 109 115 121 234 294 284
 227 145 304 183   8 278 173 168 221 174 171   0  74  59 119 139 267 275
 297 143 156 161 263 140  85 218   6 282  40  66  98  22  20 178 291  87
 246 176 167 265 159 240 179 219 102 149  89 155 198  44 293 204  79 299
  12  13  54  48 129 292 138  17 206 237 288 162 163 126 269 169 287  37
  11 146 196 166 108 213 295 272 250  28 158 296  58 273 118  88  32  45
 203 160  43 177 283  42 193 191  55  97  34  86  53 110  57 305 271 223
  70 185 187 215 241  29  78 229 127 201 125  49  38  65  60  62 258 107
 274  36 209 124  82 220 247 228 123 270  77  68 300  23 192  73  31  81
 253 217  80 236 184 268 182 111 210 114 117 105 112 120 181 195 122 116]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0518
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0737
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.017
INFO cross_voc_dataset_evaluator.py: 134: 0.173
INFO cross_voc_dataset_evaluator.py: 134: 0.022
INFO cross_voc_dataset_evaluator.py: 134: 0.096
INFO cross_voc_dataset_evaluator.py: 134: 0.073
INFO cross_voc_dataset_evaluator.py: 134: 0.135
INFO cross_voc_dataset_evaluator.py: 134: 0.036
INFO cross_voc_dataset_evaluator.py: 134: 0.017
INFO cross_voc_dataset_evaluator.py: 134: 0.025
INFO cross_voc_dataset_evaluator.py: 134: 0.045
INFO cross_voc_dataset_evaluator.py: 134: 0.120
INFO cross_voc_dataset_evaluator.py: 134: 0.043
INFO cross_voc_dataset_evaluator.py: 134: 0.025
INFO cross_voc_dataset_evaluator.py: 134: 0.285
INFO cross_voc_dataset_evaluator.py: 134: 0.091
INFO cross_voc_dataset_evaluator.py: 134: 0.099
INFO cross_voc_dataset_evaluator.py: 134: 0.050
INFO cross_voc_dataset_evaluator.py: 134: 0.036
INFO cross_voc_dataset_evaluator.py: 134: 0.030
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 135: 0.074
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.395s + 0.003s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.360s + 0.003s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.353s + 0.004s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.354s + 0.003s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.365s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.364s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.362s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.364s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.366s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.367s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.366s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.367s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.368s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.504s + 0.003s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.376s + 0.004s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.380s + 0.004s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.376s + 0.004s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.374s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.378s + 0.004s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.388s + 0.004s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.386s + 0.004s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.383s + 0.004s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.384s + 0.004s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.380s + 0.004s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.380s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.376s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.354s + 0.003s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.370s + 0.004s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.366s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.371s + 0.004s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.368s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.359s + 0.004s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.359s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.362s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.361s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.361s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.362s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.362s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.361s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.550s + 0.003s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.358s + 0.003s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.364s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.358s + 0.004s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.356s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.354s + 0.004s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.353s + 0.004s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.351s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.351s + 0.004s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.353s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.352s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.353s + 0.004s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.355s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 54.396s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 47  86  25  41  22 126  75  65  44  55 123  58  64  71 113 122 118  68
 125  49  87 169  46  79  90  31  96 168  21 114  74  29 155 166  34 110
  36 101  51 117 159 167  85  89  42 137 129 157 128  76 136  40 150 145
 119  88 149 158 153  72  69 121  28  54  70 108 144  43  38  97  91  35
 147 127 162 135 165  84  27  13 143 116  26  39 174  98  61  50  60  59
 142 124  48  24 179  30  56  16  94 120   8  57 161  52 100 171  15 141
 164 139 146  23 115  93  67 151  73 132   2   7  66  78 106  77  18  92
  32 138 170 148 104 134   0 140  63  83 173  33  99 172   1 107  37 156
 152   3  10 178 154  20  53 111  45 160 130  19 163  17  62   4  11   9
 102 112  80   5 103 109 131 133  12  14   6 105  82  95  81 177 175 176]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0176
INFO voc_eval.py: 171: [237  54  40  86  49  61  14 113 107 229  28  81 223 236  22 225 123  19
  53 109  11  57 130 208 219 206 119  42 114 151  93 155  88 224 110 205
 111 150  36 167 211   3 233  84  55 234 207 221  10 135  56  16 220  18
  21  41 212   7  27 226  89 153 117  98  39  31 216  62 213 102  12  58
  59   4 127  45 122  91  78 218  23 132 231  17 116   9  47 154  26 210
 165 162  13 115 232 106 118  29 134 145  63 161  97   5   2 228 163  60
 128  50 126  37   8 101  30 222  72  25 170   6  79 158 124  85 227 168
 129 215 217  35 209 177 104  87  83 230 108 139  15 121 105 147  65 112
 137 214  52 156  92 142 172 235 188   1 144 133  48 164  24  38  76  20
 125 173 182 141 193  68 186 160 146  70 148 185 184   0 103 120 197 176
 189  82 131 166  69 171  80 136 149 175 196 180 174  96 199 191 138 192
  44  51  66 187  43 194 159 169 143  75 204  64 140 201  71 152 183 157
  34 190 100 200 195  77  90 181 179  74  73 178  94 202 198 203  33  32
  99  46  95  67]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.1599
INFO voc_eval.py: 171: [ 528  120  481 ...  636 1119  643]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0258
INFO voc_eval.py: 171: [628 228 296 796 120 219 654 229 280 862 786 366 593 121 616 379 272 291
 791 618 383 634 358 232 130 544 789 222  44 787 493 841 840 904 161 525
 526 289 589 153 530 275 548 127 781 652 346 813 478 151 650 355 128 491
 849 124 193 669 116 115 830 344 852 797 304 620 611 233 264 234 906 400
 235 279 582 441  16 584 925 434 818 397 497 495 117 175 816 184 164 937
 782  14 784 542 293 502 790 420 119 340 496 382  74 640 898 854 492 333
 518 788 494 843 322   2 629  57 295 608 424 415 845 270 241  60 365 896
 928 248 338 579 179 171 203 323 635 636 519 617 282 765 641 623 524 277
 276 648 609 330 211 126 392 407 942 385 810 375 951  61  55 931 891 364
 490 885 428  99 197 186 380 418 627 647  64 859 861 637 487 485 260  11
 145  53 815 561 804  52 950 332 886 503 597 780 892 895 292 498 863 155
 500 612 520 144 858 770 846 370 345 367  54 678 720 306 528 575 288  50
  46 660 432 914 921 583 409 237 149 661 527 976 294   7 753 178  66 342
 842 686 666 572 935 334 412 436 901  10 954 591 962 266 967 529 298 373
  49 510 451 622 363 694 625 757 331 100 848 174 944 696 826 445 123 287
   6 504 799 297 823 507  21 446 522 239 405 681 209 317 890 856 577 664
  42 665  97 191  83 354 488 476 506 533 353 327 489 422 783 129 833 655
 221  85 236  69 568 855  20 444 408 649 290 838 948 847 114 811 800 969
 715 508 778 419 638  93 938 349  80  62  65 924 934 427 479 851  40 356
 483 362 822 109 369  70 386 480 776 361 679 512 515 603 517 278  71 244
 903 391  84 920 540 642  95 106 262 321  43 192 555 711 141 668 401 302
 565 265 883 261  39 534  78 545 285 429 814 213 672  59 819  25 182 562
 558 912 936 169 551 396 730 586 774 158  88  94  51 836 511 550 707 806
 554 462 933 630 624 523 563 543 656   0  75  63 103  12 252 335 181 225
   3 368 212 794 710 888 194 752  67 556 902 315   9 758   4 125 949 835
 104 821 667 831 751 974 437 704 897 706 966 961 254 952 700  92 378 122
 940 703 564 853 537 162 172 148 839 343  48 773 283 505 739 570 918 793
 910 673 152 359  29 614 416 486 798 602 909 411  77 850 785 118 724 834
 675 763   8 771 484 639 284 695 580 403 170 887 509 723 756 592 663 263
 547 307 374 113 389 689 325 110 645 932 596 316 682 501 448 274 621 242
 535 557 410 964 970  13 105 670  56 339 716 658 313  34 971 532 979 552
 922 185  45 256 253  23 259 421 301 482 474 404 601 147 595  98 576 166
 224 631 273 567 714 626  41 894 889 549 337 844 755 514 466 398 705 546
 837  28  58 801 802 258 975 541 573 768 150 722 828 915 653 735 973  35
 402 423 968 406 352  17 142  79 395 470 958 978 740 674 481 857 957 393
 376 196 477 941  26 746 220 600 657 945 825 860 227 173 919  73 195  91
  81 245 394 461 606 329 956 677 458 807 390 939 249 165 702 108  18 613
 531 388 269 873 449 350  31 605  86 619 464 769 442 900 351 560 708 499
 399 880  38 893 183 513 759 372 643 281 417 536 697 384 632 305 599 683
 872 539 727 761 832 745 314 425 585 102 177  96 792 107 965 749 868 712
 691 467 312 738 440 820 764 963 676 869 180 204 644 438 471 516 574 690
 311 413 680 688 959 598 463 521 426 879 154 870  30 566  82 538 131  22
 874 713 615 569 671 960 604 646   1 808 803 243 101 578 607 216 766 201
 687 303  76  15 206 953 336  89 701 435 414 972 553 721 736 320  68  36
  33  37  47 760 268 775 469 460 226 651 189 559 157 588 319 447 929 684
 132  32 698  27 472 594 111  19 913 326 341 328 112 133 217 159 718 719
 734 455 610 257 160 728 744 917 743 347 450 911 590 136 659 453 955 240
 360 905  90 310 824 457  87 809 899 138 581 709 829 662 748 571 812 866
 371 878 946 805 300 930 926 309 357 377 199 817 163 348 387 324 246 223
   5 699  24 251 767 732 156 456 200 439 202 633 772 187 210 198 717 381
 737 884 433 977 143 250 465 947  72 473 827 168 943 916 725 877 747 692
 587 779 318 741 882 208 308 238 726 731 139 865 871 795 733 881 454 137
 167 443 468 430 431 762 729 754 452 864 876 190 693 908 750 742 685 146
 907 475 255 777 286 927 214 271 215 459 267 923 188 875 134 135 176 867
 218 230 247 207 205 299 231 140]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0966
INFO voc_eval.py: 171: [264  79 260 248  83 159 271 278  84 314  49 204  67 201 272 253 242 142
 231  64  69 330 352 108 146 236 249 306 441 273 322  73 208 202 237 229
 317 257  54 207 295 138 203 232 261 221 418 296  82 429  38  65 298 148
 147  75 335 226  76 387 252 389  51 303  77 246  61 171 346 395 206 265
  86 230  81  78 216 311  62  57 326 356  74  68  39 136  66 388 169 334
  70 363 382  92 227 144  71  31 220 223  72 109  80 173 413 209 399 270
 155  85 302 342 396 140 287 241 290 309 355 381  27  90 189 288 339 158
 323 133 327 358 345  58  37 285 174 379  89  91 170 176  88 319 425 286
 198 428 394 251 292 258 308 435 222  40 282 354 362 337  52  41 161  18
 141  98 100 197 134 228 392 324 280 150 274 175 332 218 225 244 343 402
 350 434  16 135 297 289 284 336  97 102 139 149 166  93 103 107 132 304
 313 263  53 305 172 168 104 283  32 180  56 190 348 294 401 152  26  95
  30 315  87  44 331 320 105 310  96 301 405 293  11 185 364 238 219 205
 318 347 427 212 380 329  34  33 373  94 433  43 268  46 194 164 281 349
  20 183 300 266 247 316 106 404 299 163 328 128 378 307 340 390 361 151
 416 312 262 353 181 366 341  55 101  50 167 137 351 125 123 360 391 344
 338 333   9 131 214 291 165 359 325 357  99 437  21 196 321 217   2  59
 157  14  35  28 192 235 210 143 233 407  22 385 177 438 243  42 397 156
 200 154  36 409   3 234 160 184  12 213 129 276 179   0  60 403 145 193
 215 277 162 426   5 398 374 371 269 178  29 182   6 393 121 224 439 130
 368 421  45  47 436  23  48   8 186 419 153  25 256 199 365 122 375 191
  19 124  17 250 127   1 211 188 195  15 239  13 240 126 187   4  10 377
 245 410 384  24 432 367 431 279 370 415 440 372 376   7 114 408 112 400
 369 118 420 120 255 417 117 430 411 113 422 383 119 412 115 116 424 254
 259 275 267 443 414 111 110 386 406 423  63 442]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0419
INFO voc_eval.py: 171: [101  91  80  92  70  98  58  88  82  69  81  95  66  90  85   4 103 102
 105   2  26  37  73  14  87  22  67  59  96 108  21  72  56  18   9  75
  99   6  86 104  65  83  74  16  71 100  79  97  89  64  11  44  13  77
  68  62 107  10  93  52  94  25  35   1  54   3  63  12 106  24 109  23
  76  53  60  38  41  42  36  48 111  20  17  50   8  61 110  84  30  27
  28  19  49   7  43  29  45   5  15  57 112 113  46  32  34  55  33  78
  51  47  39  31  40   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.1099
INFO voc_eval.py: 171: [1742  603  407 ...  486   43   42]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0291
INFO voc_eval.py: 171: [22 18 16 21 36 62 13 11 57 23 60 15 31 32 44 49 61 76 86 74  9 14 84 51
 82 33  6 73 28  7 41  8 20 81 37 30 85 19 59 46  5 45 17 53 67 80 34 54
 83  2 38  4 71 64  0 12 52 69 42 27 68 25 10 50 43 55 70 35 24 72  3  1
 39 29 63 26 66 78 47 65 58 77 75 79 40 48 56]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0156
INFO voc_eval.py: 171: [ 565  582  603 ... 1146  456 1300]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0245
INFO voc_eval.py: 171: [136 225 131 158 171 183 189 190 109 234 232 237 229  71 227 125 152 155
 277 160 115 110 164 230 233  99 169 182 105 104  80 119 120 111  50 122
  91 108  88 133 141  77 146  79 153 156 157 180 174 172 140  73 239  81
  96 185 175 107 150 163 147 128 168 228 151 243  89 143  95 121  97 154
 103 162 187  92 165 112  76  74 272 129 114 113 102  98  87 184  83 117
 116 238 127  35   9 244 100 118 167 132 268  49 106 124 312 173  93  53
  32 148 145 137 138 139 186 126 179 144 134 123 269 315 324 275 166 130
  60 235 159 149  94 170 135 274 220 224  13 267 177  57 188 226   3  63
  12 161 178 222  55  86  90 181  23 314 325 142  41 191 311 231 327  29
  42  61  59 101 176 266  37 255 216  10 204 256 278 213  78  68  66 281
 200 240  18 242 201  75  33 307  84 322  43  27 198 245 203 223 313  24
   1 259 273 308 270 246 282 253 276 264  36  72 210 304  67 254 197  28
 271   6  69  85 193 321  54 262 303 218   0 247  25  39  16 280  82 301
 199 219  45  30 265 192  38  56 309 196  51 205 302 202  62  58   4 206
  64   2  44 217   8 310   7 194 221  46 248 319  22 286 330  65 215 212
  21 236 195 279  20   5  52 250 326  11  15 251 207 209 241 323  70 291
 258  19  48 318  26 260 283 211 305 263 328  34 252 214 287  14 257 316
 208 296  40 261 290 317 288  17 289  47  31 284 285 297 320 329 295 306
 293 300 249 299 298 292 294]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0958
INFO voc_eval.py: 171: [362 244 263 429 267 194 387 249 258  75  76  77 383 272 274  78 268 359
 269 293 361 214 196 188 107 105  91  90  88  83 259 216  79 460 453 408
   6 406 396 424 427 465 376   4 373   1 381 425 185 186 184 181 369 101
 444  87 476 488 448 191 436 442 371 375 357 317 382 386 264 193 407 410
 230 222 220 370 212 441 262   0 253   7  38  14 391 484 243 447 446 256
 401 494 475 404   3  19 156 155 270 273 309 275 430 283 428 291 241 240
 288  54 195 218  32  95  61  97  30  29 457 385 364 102 232 360 356   8
  56 478 477 284 282 467 434 239 451 192 189 440 231  10 445 197 242  23
 426  67 340 116 398 403 353 366  94 492 126  20 316  74  25 320 393 313
 499  99  62 379 219  35 389 233 372 106 390 374 100 205 471 161 345 308
 304 265  11 254 135 202   2 157 276 416 368 367  17  45 223 226  52 251
 342 247 246 392 335 245 450 163  69 167 473  89 480 198   9 472 266 498
 395 397 496 145 290 495 121 122 409 307 318 182 183 130  31 227 505  26
  33  28 463  12 355  58 229 287 165 330 491 487 415 458 315 377 168 433
  65  92 103 449 177 199 133  68 394 438  55 437 158 501 336 298 405 236
 319 384 469 151 187  81 162 255 109 431 489 454 338 380 175 104  40  34
  86 399 306 153 299 365 346 170 166 461  36 347 138 482 143 497 470  37
 217 113  21 411 358  22  85 203 423 485  59 280 169 124 400  48 172  84
 213 439 455 490  44  18  47 118 248 137 108 147 176 140 237 402 388 204
 422 285 171 224 321 459  71  93 111  72 221 412 154 201 123  15 286 225
 228 363 481 339 200 452 117  16 297 378 420 215 152 432 277 413 341 414
 131 129  82  41 211  13 114  96 468 479 507  80  98  49 160 257 281 208
 337 149 110 173 305 164  63 112  64 435 134 503 421 350 252 334  73 443
  27 325 464 311 348 115   5 235 141  46 504 502  50 178 119 271 310  39
 125 289 466 250 261 483 302 462 493 344 314 312  70  42 159 139 136 294
 179 506 142 303 301 419 234 324 238 206 180 326 279 132  24 260  66 296
 486 148 456 417 295  53 174 127 323 500 474 349  57 278  60 352 190 207
 333 332 300 150 209 292 120 210 329 331  51  43 144 146 418 128 322 343
 328 354 327 351]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0773
INFO voc_eval.py: 171: [ 37  30  19  11 111 167  36  20  17  16 166 172 175 113 130 114 122 131
 108 117 116  64  62 118 151  12 176  28  40 180  34   8  72   9 129  31
 140  96 102  13 149 177   5  93 142  43 125  91  53  95  73 169 127   6
 144 148 128 100 107 105 165 115  26  38 101  25  66  39  27  22  23 178
 132 139   4  90  77  18  97 171 147  24  60  82 173 112 150  29   2  52
 110  10 160 170 133 119 120  94 138  69 136  76  32 103 174  14 153  44
  50  78  99 121 141 123 179  81 164   7  61  45 135 159 109 137  46 134
   3  49  68  51 126  65 124  98 168  48 104  63  67  41 154 156 182  85
 161  21  80  92  87 157  15 145  33 146 181 152 106  75 162  74  89  47
  84 155 143  35  83  86 158 184  59  71 183   1  42  79  88 163  70   0
  55  56  57  58  54]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0256
INFO voc_eval.py: 171: [ 77  78 151  22  23  71 180  68 177  32 321  65 378  64  62 246  45 243
 175  66  24  48 145 144 134  14 184 328 142  90 158  56  53 133 284 113
  54  61  74 327 149 139  67  87  69  72  49 322 329 156  15 228 230 179
  29  12  33  19 227  11  18 213  41   8 148 414  16 357 125 245 135 320
 211 337 220 140 136  86 270 367 161  59 171  55 155 174  43  73 154  17
 306 317 200 273  42 132 275 212 279 214 374 224  38 268 172 176  84  26
  21 122  60 290 150 169 301 262 272 303 204  51  80  58 372 375  39 380
  10  13 146 377   9 400 209 195 417  81 215  97 205 107 318 208 249  76
  57  30  35 308 236 239 373 409 181  91 359 258 271 339 115 302 231 407
 364 114 319 399 330 201 111 411 379 206 252  95 218 289 207 237   7 202
 232 332 101 157  63  28 348 233 196 138 333 147 365  52 117 242 316  25
 397 137 173 304 108 210 336   0  34 190 105  27  47 426 193 118 203 234
 412 291 126 326 130 350 261 323 402  50 159 313 297   2 152 187 162 295
 194 276 366 188 351 408 250 294 225 293 112 385 191 356 197 185 312 346
 338 360 198 352 325 324 396  31 362 427 183 283 371 403 398 415 141 280
 363 285 315 222 277   6   1 307  92 274 189 383 192 395 421 131 219 106
 342  83 388 163 226 334  70 229  44  98 199  75  40 265  20 418 253 370
 241 103 347 292 165 314  46 405  89  82 309 305  36 416 153 124 404  94
 358 387 216 423 278 401 406 281 368 389 217 310 267 369 331 286 260 256
 259 282 119 376 420 240 361 263 266 349 110 238 386 413 390  99 300 264
 244 391 269 128 382  93 381 116 298 254 340 251 428 353 354 341 355 255
 186 288 160 164   4 345 248 102 287 299  85  37 384 109 296 104 344 422
 343 424 127   3 311 143 166 257 168 120 419 335  79 178 430   5  96 394
 392 170 393 410  88 182 425 123 235 121 100 129 247 167 223 221 429 431]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0213
INFO voc_eval.py: 171: [ 35 112  43  72  36   9 109  47  51  12 106  83 107 119 110  90  18  42
 126  50  76  38  84  53  27  28  37  68  41  86   4  99 123  52  71  92
 127 117  87 105   6 115  45  40  11  49  24  65  30  82 118  73 114  56
  89  67  85 124  29  95 101  33  66  94 116  78 108 111  88  21  62  93
  70   3  91  54  61 102  23  39 113  58  44  48  17  75  69 125  16 121
  60 122   8 120  19  31  96 100  97   5   2  34  98 103  57  22  64  59
  81   1  55  63  77  46  74   7  80  79  15  32 104   0  10  25  20  26
  14  13]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2857
INFO voc_eval.py: 171: [ 6540 18499 18175 ...  5297  5288  5232]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0594
INFO voc_eval.py: 171: [952 690 653 135 905 906 944 645 903 889 651 106 440 689 695 961 769 130
 913 733 122 799  94 659 103 943 960 133 352 975 433 904 916 127 494 688
 807 871 128 808 856 663  11 900 650 691 682 332 902 743 646 424 894 785
 909 306 496 105 737 234  99 948 138  96 693 120 493 962 111 712 437 508
   9 969 331 806 528 867 125 984 692 564 512 467 801 763 899 622 624 676
  97 708 966 860 736 102  18 426 119 355 226 233 532 129 366 134 845 349
 844 919 953  10 694 100 789 739 132 661 231 716 678 101 788 325 965 116
 115 438 625 883 431 784 654 439 957 792 311 140 852 113 124 660 307 615
 912 131 345  85 326 529 666 170 850 126 313 559 910 247 504 914  63 677
 985 575 783 627 518 901 701  75  95  98 236 908 864 475 857 223 353 463
 232 118 569 261 800 630 176 774  82 136 145 147 633 183 227 628 314 516
  19 582 937 519 859 741 450 673 572 794  25 639 123 225 117 973 351 374
  71 791 210 208 357 971 735 778 787 702 668 305  22 664 369 930 216 933
 893 717 444 356 945 517 869 983 865 533 209 680 958 108 109 877 264 274
 576 771 461 333 472 642 153 579  30 731 918 121 347 316 342 114 344 110
  67 568  69 932 797 586 434  73 464 840 303 224 285 453 976 207 514 954
 107  65 947 149 581 159 809 257 296 245 460 169 510 656 167 468 924 538
 248 748 730 780 631 616 164 212 655 713 972  74 823 922 359 317 790 168
 186 765 112 190 175 803 158  31 489 979 750  57 927 506 229 764 513 511
 855 652 920 273 479 478 793 590 160 949  61  84 192 203 715 318 872 370
 895 772  79 471  44 166 341 675 104 505  55 891 879 967  33 246 592 870
  46 446 545 253 968 367 915 155 621 556 251  72 432 302   8 810 722 826
 230 252 196 189 846 219 430 457 839  36 812 492 542 598 522 978 707 873
 709 276 162  60 729 776 288  77  58 926 301 934 842 619 462 671 665 754
 154  37 614 220 503 595 626 329 205 312 375 171 591 956 843 847 553 939
  53 822 495 454 291 782 946   4 890 734 218 292 753 566 165 796 277 436
 184 185 762 254 177 580 551 849 977 423 373 146 199 970 819   3  26 500
 697 841 963 509 217 372 499 888 928 255 521 824 458 315   2 256 173 280
 327 599 142 265 540   5 163  21 447 250 294 832 286 299 484 648 862 887
 911 670 249 387 560 324 594 365 258 172 206 259  32 723 825 833 346 485
 270 537 620 238 235  13 456 574 197 779 362 749 721  86 465 938 942 600
 773 448 770 699 390 775 950 567 459  90 263 211  91 617 851 878 898 480
 488 747 144 193 752 201 896 237 573 319 287 577 386 269 403  56 179 515
 687 964  81 308 931 935 941 781 635 487 520  43 455 658 527 180 886 141
 178 549   1 821 917 429 550 157 139 491 815  28 361 657 704 392 786 371
 214 148 703 340 802 608 861 200 309 452 711 239 758 583 535 921 298  88
 502  52 284 498 383 268 923 539  89 854  24 407 848 853 725 907 589 547
 364 681 814 182 684 696 980 524 244 884 767 466 955 378 322 328 640  15
 202 546 541 412 623 470 732  49 981 959 974 759 191 525 469 602 830 435
 282 389 416 360 204 982  93 221 477 482 377 473 474 188 638  64 481 402
 936 986 304 756 603 152 940 242 348 929 925 422 565 667 289 885 798  51
   7 874 486 343 740 636 557 323 649 724 720 310 882 413 683 379 728 215
 150  78 543 866 858 451 829 644 719 441 951 714 705 497 612 418 610 300
 194 354 892 718 805 881 745 552 863   6  45 820 281 181 290 880 672 744
 897 391 587 868 394 523 240 368 363 818 384 604 634 875 198 698 618 685
 760 156 555 828 827  47 143 669  40 262  34  39 321 804 137 405 161 271
  92 222 427 151 350 409 376 293 700  16 174  66 425 297 554 679  41 428
 597 338 578 358 601 278 571 632 187  27 768 761 449 727 836 585 544 443
 613 442   0 501 295 647 726 662 584  68 415 813 388  54  42  23 382  29
 260 530  48 411 686 605  59 611 526 414 706  20 811  35 777 629  83  12
 507 563 195  50 835 483 243 757 674 641 267 570  38 816 213 490 593 876
 241 330 385 320 738  62 228 410 393 643 400  14 795 536 588 746 334 742
 266 283 837  76 396 562 445 279 398 831 710 404 838 275  17 417 399 606
 419 609 401 272 751 531 834 755 607 766 558 596 637  70 561 406 476 548
 817  87 381 335  80 339 337 395 421 534 420 336 397 380 408]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.1163
INFO voc_eval.py: 171: [105 108  94  83 113  64  89 130  93  86 131 112 200  81  72  78  26  70
 165 163 157  66 140  69 158 150 173 117  98 141 154 195  21  33 118  85
 164 123  97  43 166  95 175   8  31 168  67 196  34 126 190 109  18  44
 121  75 110  27 104 198 177 226 144 149 202 204   2 129  22 136  74   9
 178  48  15 101 142 208 151  91  23 147  35 156 203  17 155  29 169 225
 183 135  37  30 152 137  25   7 139 106 176 191 189 188  32 134 206 159
 133   6 205  14 161  80 160 146  46 138  20  16  82  65 116  71 172 145
 193 214 181 162  24 132 153  42 199 148  73 192  28  39   3 143  11  19
   1 219 171 187 103 174  49  96  68 186   5 122  13 207  10  53 120  77
  76 197 115 170   0 167  12   4  56  84 114 128 184  61  58  36  47 100
 194 107 124 201  79 111  55  60  88 185 119  92  99  51 102 125 127  90
  87  41 218  62  38  45  54 182 180 216 223 217 224 213 179  59  52  50
 222 212 211 210 215 221 209  40  63 220  57]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0665
INFO voc_eval.py: 171: [104   0 106  32   3  28 105  25 100  14 110  18 109   2   4  99   6  11
  31   5  23  12  20   8  30  13  98  29   1 137  42 130  47 120  24 101
 103  27  34 127 107  82  51  46 128  54  19  84 135   7 126  45  15  65
   9  59 139  49  85  73  62  57 111  88 114  92 131  94  80 132 119  78
 112 129  79  61 116  64 138 133  97  53  70  71  22 108  38  89  39  26
  21  52  36  75  68  72 117  17  66  83  69 121 102  50 115 124  44  91
  35  60  48 136  74 134 123  90  33  67  16  87  56  76  86  43  63 125
  41 113 122  77  81  10 118  96  40  58  95  93  55  37]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0591
INFO voc_eval.py: 171: [178 265 250 186 261 111 183 182 281 260  21 133 277 113 206 120 117 252
 280 201 177 283 253 180 254 188  58 203  61 269 190 123  23 167 174 193
 255  29 266 199 196  36 142  62 268 116 251 171 288 219 262 138 143 286
 195 134  81 200 191 198 275 257 232 210  44 114 239 141 130 184 115   0
  41  30 155  54 228 270 267 119 179 122 230  63  59  40 208 135  73   1
 256 258  28 129  19 197 220 212  34 125 282 154 213 259 209  90  33 234
  86 140 175  77  75 189 235 284  76  66  22 148  60  65 207 164   9  68
 238 181 240  78 137 161  51 170 144 225  18 139  14  79  72 118 168 247
 271 243 264  42  47 290 278  82  74  92  67 136 289 163  71 156 127 263
  27  94  97 169 245 274 166 131 105 292 160   4  31 273 126 291 216 217
   5  20 146  64 211 152 172  53 276 202 222 295  57  56 248 218  37 110
  55 185 204 124 165 109  49 173 147  45  98  84  17 192   2 128  95  24
 249  13 231 107  91 221 227  11 223 176  50 153 121   6 293 226  43 187
 159  99 233  39 236 214 132 294 237 272 162  15 112 194 104 287  88 149
 246  46 205  80 224  52 100  25 244  87  38 215 229 279 157 241  89 285
  32 150   8   7 242  83 108 158   3  85  35  48 145 151  16  26  69  12
  70  10 102 101  93 103  96 106]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0250
INFO voc_eval.py: 171: [ 13 158 134 155 196  38  18 299   9  72 146 266   4  54 245 290   8 234
 230 215 217 301 136 109 197   3 216 137 264  55 250  66   1 107  76 209
 270  98  99 100 148  44 242  26  27   6  25 289 294 293 156  33 249 159
 205 221 222  91  90 188  70 252  43 287  41 150 275 274 302 139 103 102
 298 141 140 240 183 258 152  73 247 110 316  22 233 154  61 203 172 238
 239  49 186 180 251 200  50  56   2 211 157 315 121 199  19 314 220  68
 272 236 288 177  75  67 257 262 263  16 271  17 101  40 214 162 265 260
 269 256 273 295  28 241 224  97  31 304  64 142 104 297  78 187 237 219
 184 232 317 126   5 114  24 116 149   7 311 123 111  51 292  92 182 229
 181 125  77 145  34 151 178  62 276 286 144 147 106 227   0 174 253 267
 161 167 303 194  42  83 291  94 175 108  20  95 165 228 246  47 173 206
  58 312 171 244 268 135 213  52  14  11  12 164  15 300 305 306 195  46
  69 259 204 278 310 143 130 163  29 170 281 202  48  10 115  37 169  21
 153 296 307 176 282 308  45  35 283  96 138  59  30 198 166 192 255  57
 105  81 179 280 231  53 185  79  60 248 284  87 223 235  93 160 210  84
 117 285 122  74 132 226  65 127  63  86  80 313 218  85 279  88 168  36
 212 261 131 133  23  39 112  71 225 201 309 243 207 193  89  82 191 277
 124  32 128 120 254 119 190 118 113 208 189 129]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0614
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0707
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.018
INFO cross_voc_dataset_evaluator.py: 134: 0.160
INFO cross_voc_dataset_evaluator.py: 134: 0.026
INFO cross_voc_dataset_evaluator.py: 134: 0.097
INFO cross_voc_dataset_evaluator.py: 134: 0.042
INFO cross_voc_dataset_evaluator.py: 134: 0.110
INFO cross_voc_dataset_evaluator.py: 134: 0.029
INFO cross_voc_dataset_evaluator.py: 134: 0.016
INFO cross_voc_dataset_evaluator.py: 134: 0.024
INFO cross_voc_dataset_evaluator.py: 134: 0.096
INFO cross_voc_dataset_evaluator.py: 134: 0.077
INFO cross_voc_dataset_evaluator.py: 134: 0.026
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.286
INFO cross_voc_dataset_evaluator.py: 134: 0.059
INFO cross_voc_dataset_evaluator.py: 134: 0.116
INFO cross_voc_dataset_evaluator.py: 134: 0.067
INFO cross_voc_dataset_evaluator.py: 134: 0.059
INFO cross_voc_dataset_evaluator.py: 134: 0.025
INFO cross_voc_dataset_evaluator.py: 134: 0.061
INFO cross_voc_dataset_evaluator.py: 135: 0.071
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.396s + 0.008s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.370s + 0.004s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.359s + 0.004s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.357s + 0.004s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.363s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.361s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.358s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.359s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.359s + 0.004s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.360s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.358s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.360s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.363s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.397s + 0.004s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.363s + 0.004s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.364s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.368s + 0.004s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.372s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.372s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.376s + 0.004s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.372s + 0.004s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.370s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.371s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.369s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.367s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.366s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.499s + 0.005s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.373s + 0.005s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.373s + 0.004s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.367s + 0.004s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.361s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.367s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.364s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.366s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.366s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.364s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.364s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.363s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.363s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.384s + 0.003s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.352s + 0.004s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.361s + 0.004s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.361s + 0.004s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.362s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.358s + 0.003s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.358s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.357s + 0.003s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.358s + 0.003s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.358s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.357s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.359s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.358s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.272s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 46  22 121  81  73  71  39  52  18 118  58  59  74  42  68 106  19  83
  62 126 127  92 122 110 116  29  17  31  35  65 165  86  49 160 152  32
  98  26 147 112  40  85 136 158 109  41 161  38 142 135 148  84 157 125
  67  66  79 149  25 151 104  51  43  64  33  93  34  87  82 164 139 171
 141 124 159  23  24  55 145  80 163 134  48  45  60  28 120  54  27  50
 133  47 103  21 162 176   9 156  53  90  97 113 114 167  20 140  96  10
  37  63  89 138 150 111 102  69 144   6  12  57  72 130 166 123 137  88
  30   3  36 170  14 101  56 154 146  95 132 169 108   0  78  61  44 175
  94 153 107   1 155 128  99  70  16  15 168  11  13   2 100   5 119 143
 131 115 129   8   4   7 105 117  91  75  77 174  76 172 173]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0104
INFO voc_eval.py: 171: [246  86  55  40  58 114 106 115 117  15  23 110 234 245  60 217  29 162
 136 127  39 164  81  20 232 236 235  13 228  88  56 242  50 131 159 243
 122  83 218 175 216 118  37  92 116  19 166   3 227 141   5  16 237  89
  63  59 121  61  62  42 224 138  41  11  12  30 230 223  22   9  78  54
  57 165 134  24 241 222 103 233 120 128 123  48  44  18  17 154 225 229
  14  27 173  10  99 140 244   2  64  94 100 221 220 171   4 163 219 169
 133  96 167  45  28  85 239  49  31 231  25  35  74 226  79 238 135 180
 109   6 155 185 125 240  87  84 129 144 145  65   7 150 132 151 148 107
  21 196  91   1  76 142  52 108 124   8 111 181 126 101 190  26 172 176
 174  38  72 158 200 195 170 137 152 130 193 182 191 143  82   0  95 207
 186  98 197 156  70 178 189 119 139 146  80 184 183 206 204 194  66 199
 168  51 213 157 161  69 160  36 147 153  73 201  43  67  34 112 149 104
 177 192  77 208 198 105  71  33  47 205 202 203 209  68  75 210 211 188
  93 187 212 113 179 215  90 214 102  32  97  46  53]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.1589
INFO voc_eval.py: 171: [ 155  254  262 ... 1184 1204 1198]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0398
INFO voc_eval.py: 171: [638 296 116 216 804 225 277 604 398 286 794 118 272 631 666 228 229 127
 910 125 799 789 845 348 605 553 643 235 223 391 161 367 376 629 543 633
 279 597 287 499 839 821 797 795 113 557 121 124 664 677 233 261 534 539
 149 112 450 598 911  45 416 409 365 934 389 866 330 111 503 805 430 663
 798 796 861 336 502 162 160 230 231 903 790 944 506 353 268 510 263  16
 178 530 826  77 440 623 500 624 859 858 849 184 200 305 487 593 420 792
 854 941 936 243 343 824 422 326 570 652 295 505 293 588 647  15 362 335
 177 407 212 584   2 392 960 634 662 374 644 194 947 388 646 655 776 274
  52 504 818 280 435 276 428 498 896 501 901 122 123 418  62 141  57 438
  59  61 649 981 607 864 594 386 306 847 641 190 494 625 453 630  55  51
 529 822 781 269 788 179 760 115 890 668 957 868  11 812 258 425 151 540
 538 512 350 232 352 507 304 496 589 289 443 578 379 145  68  56 726 865
  47  72 891 851 897 900 928 155 674 150 378 973 384 220 340 528 176 637
 906 327 514 920 322 650 848 765 267 354 100 678 234   6 942 962 481  70
 372  50 454 455 373 363 819 537 415 692 791 349 686 676 969 671   9 853
 834  84 297 292 595  43 895 417 939 403 364   5 371 863 832 840 810 667
 654 669 516 786 281 703 521 700 518  91 429  67 532 949 439 138  22 433
 240 724 687 517  41 332 966 497  99 660 291 290 651   7 843 976 488 275
 852 366 787 829 616 377 380 657 927 186 545 237 400 393 620 527 828  63
 860 470 436 513 489 432 807 325 337  74 567 195 278  96 104 823 106  87
 582 767 716 526 522 909 410 329 766 542 926 259 679 319 437 742 617 888
 575   0 492  73 351  44 166  71 173  75 154 211  76 183  27 120 294 943
 844 918 639 908 581  64 814 282 169 550  13  12 554 191 757 718 210 958
  28 519 648  24 221 563 402 551  54 609 558 562 560 370 445  69 576 608
 893 665 802 972 955 182 566 358 785 412  92  79 103 846 838 717 148 250
 156 830 603 707 448 345 174  10   3 592   8  21 902 509 251 806 779 387
 793 574 110 109 606 801 614 946 711 252  23 117 426 171 924 338 684 746
 236 763 675 270 586 114  49 105 321 773 515 710 734 762 411 164 919 368
  31 857 978 892 419 688 544 733 493 915 568 317 590 307 495 855  81 731
 413  66 940 406 484 683 271 841 260 262 961 632 784 476 256 695 444 565
 702 569 381 975 302 931 405 977 397 144 511 520 672  58  42  17 899 653
 466 635 101 808 673 482 146   1 536 535 764 722  36  39  46 561 577  14
 218 139  65  60 385 780 254 452 982 555 723 680  80 835 480 361 203 491
 894 468 255 523 408 951 850 980 713 984 414 580 556 596 399 923 334  86
 552 547  94  32 126 963  35  89 974 809 720 862 708 192 341 771 573 685
 755 938 748 457 239 741 867  18 948 181 922 549 298 729 945 471 283  82
 284 224 815 170 752 159 266 965 571 879 357 619 316 246 383 656 288 626
 587 541 905 382 778 508 490 318 477 682  19 339 434 642 390 886 427 472
  33 303 898  40  97 147 524 423 645 369 689 712 837 591 800 449 347 768
 705 971 615 102 548 881 875 745 469 661 827 772 610 314 681 601 185 213
 698 107 199 241  95 401 956 431 475 299  83 719 697 658 324  85 198 885
 811 323 602 533 265 622 128 816 167 880  30 959 157 525 579 979 546 694
 618 751  25 442 670 564 611 599 421 559 628  98 204 693 872 446 967 783
 769 727  53 201 465 583 467 531 483 775 709 627  48 970 168 747 129 108
  37 313 333 331 462 249 933 874 917 950  38 916 621 856 925 158 572 690
 715  29 193 300 222 456  34 833 163 836 725 312 929 952 964 187 441 831
 215 820 355 825  88 459 257 704 738 479 817 238 983 842 404 245  90 328
 346 743 640 904 813 932 873 739 754 714 612 877 308 750 613 342 876 884
 395 315 153 424 954 140 447 600 356 396 197  26 320 208 777 132  20 782
 189   4 463 937 474 394 706 585 196 907 473 375 730 219 636 753 134 359
 460 659 728 721 953 889 253  93 921 310 883 248 119 244 871 344 878 696
 165 311 478 207 968 143 887 736 735 699 285 360 803  78 732 761 869 309
 882 756 691 740 458 264 759 136 770 485 188 935 180 749 913 701 451 133
 461 758 744 226 737 301 205 914 774 912 930 175 486 214 464 242 202 217
 870 152 137 247 209 142 227 273 172 206 130 131 135]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0968
INFO voc_eval.py: 171: [252  75 268 248  79 153  80 261 267  65 302  48 197 235 192 259 136 244
 234 231 140 224 325 243 241 230  62 347 297 213  67 251 313 316 424 194
  54 199 220  69 203 433 133 298 412 376 330  77  72 379 306 143  63 287
 195  71 421 219 293  57 242 285 383 141  50  39 166  81 321 204  73 236
 342  70  66 413 198  58 101 378 352 152 132  98 222 315  76  74  64  59
 223  40 249  85 331 137 274 402 164 215 168  68  78 229 210 209 403 387
  32 202 372 260  27 278 292 147 151 193 279 300 311 354 336 385  84 339
 318 239 183 409 295  55 355 322 334 130 280 128 171 233 189  89 349 369
 169  83 381  82 308 418 247 356 420 200 165 104  19 283  41  37 135 275
 240  95  42 288  90 188 129 307 319 264 271  94  51 327 144 218  16 391
 382 269 131 170 253 343 100 208 286 329 162 102  92  87 103 276 250  99
 337 134 333 294 281 167  52 296 301  33 428 161 175 146 303 299  28  53
 389  91  45 344 277 254 328  96 184  31 214 310 309 226 324 284  88  11
 305 392 426 257 363 157 345 371 196  44 127  20 341 180 154  35 351 272
  34 255 178 304 291 312 393 323 314 368 289 290 406 346 332 384 145 159
  97 155 160 353  49 273 348 380 340 335 122 256 326 358 121 205 338 119
 176  47  86  56 370 320 282 419   9  22 190 317 431 350 125  93   2  29
  21 221 397  46 410  14 149 138 227 163 430  36 211 386 377 148 186 191
  43 172 225 139 217   3 173 364 179  12 394 158 398   0  38 400 156 201
  60 212 266 265 207   5  30 187 123 390 142 367 416   6  23 429 177 124
 359 150 174 116 216 238  25 411   8 206 362 360 117 427  18 181 245 228
  17 361 118 232 185  13   1 126 120  10   4 399 182 374  24 237 423 414
 366 425 432 270 395 357   7 365 108  26 388 408 106 111  15 112 115 401
 422 107 417 373 109 110 404 114 434 263 258 405 105 246 375 113 262 407
 415 396  61]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0380
INFO voc_eval.py: 171: [101 105  71 106 102  70  86  91  96  64  85 100  75 103  84   4  62 112
 110   2 111  29  77  95  40  81  17  61  88  68 115 104  25  98   6  10
  24  74  21  94 107  79  19  87 108  99  93  73  78 109  12  13  92  48
  16  82  69  66  11 114  97  60   3   1  55 116  67  38  28 113  58  14
  27  26  80  57  63 117  42  44  52  65  72  47  51  41  20  53   9  89
  15  32  23  22 118   5  30  76  54   7  49  46  31  18  90 120  83  50
 119  34  59  37   8  35  56  43  33  39  45  36   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.0761
INFO voc_eval.py: 171: [ 416  667 1809 ...   42   41   39]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0309
INFO voc_eval.py: 171: [17 16 23 19 36 11 58 56 31 61 13 15 45 49 32 59 84  9 73 60 27 82 14 52
 80 18 72 41  6  8  7 29 28 22 79 10 47  5 83 35 44 66 53 78 34 20 71 54
 81 57 38  2  0  4 26 12 21 51 42 67 63 50 25 43 33 69 70 39  3 24  1 77
 62 76 65 30 46 64 74 68 75 37 40 55 48]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0156
INFO voc_eval.py: 171: [ 574  592  577 ... 1147 1478 1144]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0216
INFO voc_eval.py: 171: [182 135 187 155 161 130 122 167 188 189 226 228 107 157 231 233 149 234
 101 102 141 164 103 281 171 113  70 178 151  96 109 108  84  78 106 235
 150 230 143 144 166 133 126 119  71  73  80  93 140 145  48 152 180 154
 176 160 173 172 241 110  72 104  85 184 245 120  92 127  94 159 142 153
 100 229  98 277  82  95 237  10 112 146 114 162 179 117  35 170 115 239
 174 118  99 128  97 247 165 156 134  75 111  47 105 316 116 125 273 168
 147  52 137 132 185  90 123  31  77 131 139 183 274 319 124 136 328 163
 121 275 129 158  58 250  79 175   4  13 169 225 148 186 138  91 278 227
 272  83  56  55 177  61 181  86  40 318 232 329 315 257 238  28 331  20
  69  41  24  54 258  59 221 214 271 217 244 211 202 282  63 286  64  12
 199  74  81 246 311  32   2 200  42 279 196 326 263  11 287 317 255 201
 312  76 280 256 249 269  30 223   7 216  34  27 276 283 209  66  26   8
 308 267 195 325  68 248 307 192 203 285  18 220  38   9  88  53 191  21
 270 305 215  44  25   1   5  57 194  60 323 197  36  50 190  37 266   6
 213   3 291 314 313 222 210  62  46 306  19 236  43 253 284 204 252  23
 333 330 242  14 243 193  51  65 260  16   0  89  67 205 198 327 240 321
  15 288 262 254 292  45  22 206 268 218  49 259 309  17 296 212  33 208
 224 293 265 320 290 324 300 332  29 294 322  87 289 219  39 264 207 301
 295 299 310 261 251 298 304 302 303 297]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0587
INFO voc_eval.py: 171: [256 282 272 453 261 387 457 433 410  78  79  80 277  82  81 386   1 226
 195 228  91   4 413  88 201  86 280 455 394 471 452 396 382 468   0 400
 438 403 437 408 472 434  83 223 423 414 397  96 473 306   6   8 199 197
 233 192 191 190 186 273 275 276 283  97 290 487 286 476  41 493 107 334
 412 204 157 231 225 415 416 100 328 337 384 289 242 245 287 409 252 255
 307 267 270 402 389 158 381 265 106 484 475 478  34  30  29  56 458 456
 504  24  63 512  19 517 444 436   3 419 432  14 429 244 305 300 285 495
   7  32 205 202 503 520 232 523 505  25 506 474 254  20 237  10 469  99
 425 406  77 418 465 118 460  93 391 454  71  58 399 367 378 445 246 249
 251 401  18 108  12 430  11 200 203 526 528   2 210 440 398 214 375 499
 159  35 257 480 340 278 359  31 417  55 263 393 332  47 323 298 354 424
 303 420 330 368 501 279 519 443 392 259 253 258 508 234 327 268 124 181
  38 187 189 164  64  26 198 206  72  89 136 162 125   9  90 326 479 463
 380 532 485 366 168 336 490 404 188  94 527 302 411 194  69 147 525 132
 239 500 169 166 461  95  61 466 266  84 390 173 531 358 530 238 360 183
  33 207 131 497  37  57 193 109 153  23 316 248 111 407 338 160 163 510
 459 370 482  21 372 427 296 470 317  62 488  40 325  87 115 514 230 421
 435  50 155  46 212 127 383 343 167 110 224  22 498  39 175 120  49 260
 281 467 170 179 431 426 301 513 518 145 139 213 442 365 536 211 364 486
 342 483 422 229  43 314  27  76 149 428 172 119 227 297 209 451 395 126
  52 171 103 385 156 388 102 142  15 140 208 344  98 405 221 439 481 333
  17  92 441 516  68  60 105 509 104 529 447  85 123 114 112 133 247 154
 176 218  13 496 113 335 294 271 361  16 295 345 264 507  65 101  67 462
  48 235 339 284 309 324  53 151 184 241  28 349 524  36 533   5 121 374
 450 165  66 117 236 182 322  75 274 310 464 492 240 143 329 262 535 331
 128 243 116 320 494 362 177  44 521  74 489 161 299 313 477 511 363 135
 348 369 371 319 144 137 341 321 141 250 312 515  70 269  73 534 150 178
  59 350 346  42 308 376 219 292 491 220 449 134 216 502 152 353  54 196
 129 293  45 288 304 356 122 357 318 351 448  51 522 355 148 217 180 291
 215 311 379 446 222 146 130 138 185 352 373 315 174 347 377]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.1285
INFO voc_eval.py: 171: [ 41  21  34  18  20  15 114  26 115 170 184  39 122 123 178 111 136 172
 126 121  66   5  68 182   7  32 152 132 189 124  76  10 142   6  35 150
 187 100 104  17  96   3 118  56 145  22  38 131 125  48 130  98 146 151
 127  94  77 176  23 116 110  44  29 108 135  25 171 105 180   9  93 149
  81  70  16  30 143  27  55 173  99 117 153   0  64  33   2  28   8 113
  31 139  73 164 174 138 133 141 119  80 177  13  82  37 154  65 183 175
 144  86 103  51  95   4 185 156  11 168 140 129 137  49 112  53 163  54
  69 102  72  52  71  36  67  85  45 134  24 107 128 148 190 191  14 101
 157 179 147 166  91 160  89 195 155 109  40 192  42  79 106  97  50 165
  83  12 193 159 120  63  78 186  88  90 181 188  87  46 194 196  47 167
 161 169  43  75  92   1  19  84 158 162  74  59  58  60  61  57  62]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0232
INFO voc_eval.py: 171: [ 77  76  71 148  22  25 142 240  28 178  44  64  42 174 382  69 144 183
 228  19 333 211 324  59  50  89  61  66 244  11 150 225  63 134 139 153
  62 160  58 147  48  68  70  72  73 140  84 334 132 247  90 125 113 269
  67 143 284  53 326 215   7   9 224  12  14  15 226  17  18  56 181  27
  30  23 332  38  51 156 155  31  49 177 272 210 122 214 145 372 152 135
 363 158 233 383 176 179 279 164 274 416 343  37  82  81  60  16 308  54
  24  34 173 169 151  10 220 267 307 384 198 203  75 327  29 380 419 170
  79 141 146 260 236 320 292  21  20 402 264   8 202 271 154 338  57 229
 381 322 246  26 376  13 323 195  95 204  33 377 378 311 411 104 239 401
  80 270  55  91 410 209 413 369 111 234 133 335 218 370 114 337 109 175
 303 241 345  43 256 291 249  47 321 200  94 137 199 115 212 353 207 361
 367 316   5 192 325  65  41 116 126 130   0 138 201 428 194 193 208 190
   3 106 103 206 252 342 294 205 157 364 414 344 315 259 283 275 299 355
 319 329 184 368 227 280 232 223  32 285 168 331 328 403 404 217 356 253
 188 187 310 409  45 357 429 196 351 388 417 110 221 191 397 295 405 276
 309 237 250 273 375 339 398 330 301   1 107 131 159 298 189 197 162 304
 358 245 161 386  35  46 395  74  92 390 235 263 400 124 313 213 243  88
 346  96 423 318   6 293  36 420 101  40 314 418 282  93 281 266 373 312
 341 278 399 362   4 407 216 305 425 222 371 128 117  83 391 374 165 238
 365 268 366 422 408 255 286 108 219 261 336 379 306 350 257 290 262 258
 392 393 265 242 389 288  99 302 230 359 186 430 385 360 352 231 297 112
 163 296 251 424 415 426 127   2 254 354 300 185 387 180 123 349  87 136
 347  97 340 119 248 287 421 172 105 396 348 317  39  78 166 129  85 171
 102 432 289 277 394 149  98 427 121 182  86 120  52 100 406 412 118 431
 167 433]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0188
INFO voc_eval.py: 171: [ 39 117  48  76  41  10  49  55 114  52  13  92 110  85 125 132  56 109
 115  18  46  80  30  42  87  32  40   4  44  89  75 124  67 102 128  58
  50  90 122 127  14  45  25  54  11 129   6 120 108  88 123  61  33 119
  86  73  78 121  72  34 104  98  97  37  83 112  93 116  91  71   3  57
 105  23  62 126  96 118  43  94  53  66  47  79  65 130  17 111  74  77
 134  20 103   2  21  99 100  63 101  35 106   5  68  64 133  38 131 113
  82   1  95  69  81  70  59  19  60   9  51   7  84  36  16  24 107  31
  27   0  12  15  29   8  22  28  26]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2851
INFO voc_eval.py: 171: [33369  7127  5014 ...  5242  5271  5263]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0515
INFO voc_eval.py: 171: [1033  738  705 ...  440  459  458]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0905
INFO voc_eval.py: 171: [114 117  92 101  68 121 137  97  99  93 138 120 209  90  87  80  78 162
 175 171  30  70  76 125 163 183 106 154 147 159 202  37 103 169 131 174
  10 134  25 127  34  46 181  36 176  72 203 179  94 236  47 129  83  22
 111 206 119 150 155 172 212 143  32   2 214  82  26  11  31 198  51 208
 109 182 136  19 157  96 160  38 213 161 191 148  39 164  21  86 145   9
  41  29 199 140 158 141   8 215  35 216 196 146 187  18  24 166 151  49
 165  69 178  23  89 189  91 124  20 139 152 167  79 222 201 102  27 207
 210 156 200  81 184  45 144  33  43 173 142  15 118  28 149 113   1 194
 115   5 153 227 180 112  50  16   6 105  74 130  12   7 128 193  85  84
 177  77  17 126   0 168 205  58 108 122  60 135 170  71 186  66  40 107
  64 197 116  54 204  75  52  63 132  65   4   3  59 192 195  14  98  13
 211 100 104 110  88  56  95 133 226 123  44  73  67  42 190  48 188  53
 225 234 224 232 221  57 219  55 220 218 223 230 185 217 229 233 235 228
 231  62  61]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0725
INFO voc_eval.py: 171: [115   0   3  38 109 120  15 113 114  29  23  33 122 119  12   9   8 110
   4   2  13 108  36  32 106  27  37   7   5  24  14  35   1  28  58 116
  60 111 151  92  62 129 136  16  25  40 112 105  55 148  63 157  49 149
  86 147  70  11  10  67  80  54  21 123 117 101  69 150 132  64 152  66
  17  94 128 153 158  77  96  78  57  84  46  73 139  88  44  59  90 121
 145  31  99  81  26  89  72  71 126  42 127  53  22  74  56  85 141  20
  41 118  95 107 155  34  45  83  52 134  30  91 103 154 140  47  75  68
 102 156  39 104  98   6  76  87  82 133 130  51 138 143  93  79  19 124
 142  61 144  18 146  97 125 137 135 131  65  43  50 100  48]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0674
INFO voc_eval.py: 171: [173 128 277 108 249  17 176 179 268 247 183 258 260 200 248 197 203 273
 113  53 116 117  27 266 250 110 180 172 175 177 279 185 254 195 265  56
 263 163 255 189  58 194  31 191 169 257 216 137  57 130 196  23 284 129
 166 256 207 109 126 190 226 139  75 282  41 253 193 233 259 112   0 271
  39 119 181 138  37 174  24 111 151  48 205 262 224 228 206  59 267 252
  55 115 208  54  25   1  30 217 168 192 150 278  20 125 227  64  89 246
 121  28 230 144 232 134  72 236 170  82  70 186  65  71 280 182 251  73
 160 221   5 157 229  68 178  86  62  50 133  16 264 242 261 140  12 270
 127  38  46  14 131  74 165 275 114 286 104  90 152  76  22 159  35  61
 285 164  69   4 240 202  19 124 288 162 156 214 287 213   3 141  18  60
 235  52 122 148  26 272 215 219  51 198 199 234  49 291 106 274 209  33
 243 161 107 103 167 201 123  78  91 143  15  94 120 223  42 135  88 187
   9  87  11 218 171 225 245 210 211 222  77 220 149 231   6  92  40  44
 101 158  36 289 155 188 184 118 244  34 290  43 145  97 204 136 283 142
  13 238  84 132   2 276 239 241  21 212  47  80  79 269 153  32 147   7
 281 237  81 146 154  95  10   8  85  29  67 105  66  45  63  83  93 102
  96  98 100  99]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0671
INFO voc_eval.py: 171: [ 15  19 132 155  70 156  39  54 194  13 305 144 135 269 196 108 247 240
 218 219 233  26 293  93  11 299   9 307   3  10 204  63 211 251   6  58
 103 242 217 147   1  95  94 224  74 267 158 292  27  28  20 134  44 136
 272 276 157 254 225 320  88  87 180 235 149 300  33 105 187 277  22 202
 296  42  60  68 304 172  61 252 152 273 241 138 109 139  71 290 238 183
 177 102 106 246  49  50 259 146 154   2 319  97 237 223 117 213 199 275
 291 198  89  65 249  73 318 253 274  41  32 162 265 173 137 264 301 261
 243 216 258  57  29 310 160  64 239  96 227 271 266  76 303 153 234 222
 186   0 321  14 140 120 118  82 263  24  16 182  52  75 148 316 181 151
 279 178 308   7 231 179  34   8 174 143 126 309 145 142 101 278 294   5
  62 161 297  46 167  59 256 195  91 104 176 268 250  47 165 230 171  55
 205 248 203 215  51  98 306 133 245  81  18  12  17 193 255 314 312  43
 298 282 315  36 311 114  30 164  38  67 125 185 285   4  48 260 302 201
 168 113 122 286  45 141  21 287  79 163 123 244 150 175 197 169 313 232
 191 166  92  83 284  35  56  31  78 100 226  77 295 288  53 236 270 257
 159 184 107 289 220  85 212 228 131  37  90  72 119  80  84 170 115  66
 221 112 317 121 283 130 127  86 206 214 229  99 262  23  69 129 192 281
 124 280 110  40 200 190  25 209 116 188 111 189 208 210 207 128]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0432
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0697
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.010
INFO cross_voc_dataset_evaluator.py: 134: 0.159
INFO cross_voc_dataset_evaluator.py: 134: 0.040
INFO cross_voc_dataset_evaluator.py: 134: 0.097
INFO cross_voc_dataset_evaluator.py: 134: 0.038
INFO cross_voc_dataset_evaluator.py: 134: 0.076
INFO cross_voc_dataset_evaluator.py: 134: 0.031
INFO cross_voc_dataset_evaluator.py: 134: 0.016
INFO cross_voc_dataset_evaluator.py: 134: 0.022
INFO cross_voc_dataset_evaluator.py: 134: 0.059
INFO cross_voc_dataset_evaluator.py: 134: 0.128
INFO cross_voc_dataset_evaluator.py: 134: 0.023
INFO cross_voc_dataset_evaluator.py: 134: 0.019
INFO cross_voc_dataset_evaluator.py: 134: 0.285
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.090
INFO cross_voc_dataset_evaluator.py: 134: 0.073
INFO cross_voc_dataset_evaluator.py: 134: 0.067
INFO cross_voc_dataset_evaluator.py: 134: 0.067
INFO cross_voc_dataset_evaluator.py: 134: 0.043
INFO cross_voc_dataset_evaluator.py: 135: 0.070
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.352s + 0.003s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.333s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.341s + 0.003s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.350s + 0.003s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.361s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.361s + 0.004s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.361s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.363s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.363s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.361s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.362s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.365s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.366s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.378s + 0.003s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.361s + 0.004s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.380s + 0.004s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.367s + 0.004s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.368s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.370s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.374s + 0.004s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.371s + 0.004s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.369s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.367s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.368s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.367s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.363s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.631s + 0.003s (eta: 0:01:18)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.384s + 0.003s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.365s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.370s + 0.003s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.362s + 0.003s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.362s + 0.003s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.364s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.365s + 0.003s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.364s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.365s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.367s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.366s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.364s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.405s + 0.003s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.389s + 0.004s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.376s + 0.004s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.369s + 0.004s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.364s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.362s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.360s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.359s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.358s + 0.004s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.360s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.356s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.356s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.355s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.240s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 48  21  63  25  86 126  76  57 133  61 129  71  74  42 110  77 120  62
  87  34  32  22  20 128 124  44  37 102  54  97  91  51  29 119  68 159
 113 166  35 153 157  45  43  90 168 163 143 149  41 142  88 127 165  70
  72 132  28 122  84 155 123 106  36  38  46  67  93  26  98  53 169  85
 176  27 150 164 146 148 131  31 167 154  47 170 141  50  52  65  89 107
  49  30 125  58 140 116 162 100  55  95  24  23 101 115 181  11  40 172
 147 105  66 114  14  56  94 104 156 130 145  73  60 152 136  75  33 171
 144   6  39  59 161  92  10 175 174  17 112 138  83  99   0  64 180 160
  69 121 111 158 134 139 117  82  19 173 103  16   2 137 135  13   1   4
   5 151   9   7   3 118   8 109  12  81  96  15  18  79 179 108  80 178
  78 177]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0098
INFO voc_eval.py: 171: [263  96  47 128  56  16  40  31 262  25 123 119  13 124 248 250  64  61
 115  49  88 135 166 132  97 172 253 141 232  23 127 125  53 144 251 246
 102  90  58 147 231 260 164  32 175 258 252 187  22   5 233 237  62  63
 145  65  98   7 245 240 241  17 140 239  46 244 126 176 130 259 138  85
 131  42  57  59  26  14  12  10  33  66  20  19 111 146  45 249  18 136
 238  94 114 109 106  54 181 261  75 171  44 162  11 174 243 137  91   6
  15 185 180 142 112 236   3  34 255  28 167 247 256 161  60 173  21  86
 118  52  41 103  92  72 254 191  30  37 154 257 199 149   8  67 157 151
 234  81 133 148 116  24 117 188 235 242  43 110 156   9  84 192 204 168
 120  27  29 139  77 184 214 182 150 186 193 210 159  89 208 107  69 205
 169 194 165 196   0 183 222 170 211 200  38 153  93 121 203  50 163 129
 228 190  74 209 177 197 152  36  68 220  95 215 213  78 160  70 158 217
  83   2   4 216 178 206 143 179 101 134 223  76 189 212 207 198 155 226
  71 221  79 225 218  35 224 227 122 105  87   1 230 202 201 229 100 113
  80 219 195  55 108  48 104  51  39  99  82  73]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.0900
INFO voc_eval.py: 171: [ 227  267  529 ...  692 1209 1215]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0244
INFO voc_eval.py: 171: [323 683 670 ... 232 143 147]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0496
INFO voc_eval.py: 171: [262 247  72 147 243 230  74 257 252 295  76 184 129 191  51  67 316 216
 415 132 301 341 308 235 234 233 223  68 127 220  57 212 421  65 293 290
 204 246 255 202 321  69  71 370 403 367 188 192 278 136  73 187 299  66
 133 277 213 412  40 404  52 194 283  95 157  77 313  70 228  59 190  60
 307 146 337 375 369  78  97  75 186  41 126 347 239 211 265 322 219 214
 201 130 394 363  63 156 231 160  82 142 144 253  34  29 198 377 195 387
 238 305 333 272 271 174 401 314 224 286 346 294 124 309  81 182 376 291
  58 273 326 328 122 360 340 180 345 163 318  80 225  20  79  85  62 300
 242 226 161 409 411 276 193 128  99  39 372  42 310  86 158 266 123 298
 280  43  53 179 284 384  18 121  90 258  83 335 137 120 319  93 279 162
 199 249 325  94 292 330  98 269 267  88 373 125 419 244 287  55 141 379
 275  35  30 296 159  56 166  91  96  47 323  87 288 270 320 289 150 268
  84 302 334 207 175  33 317 215  44 381 149 332  14 250 383 315 134 336
  22 342 145 355  37 418 361 263  46  36 152 172 189 227 385 297 303 170
 338  92 331 359 151 397 140 155 281  50 343 312 374 282 139 264 236 245
 344 327 371 329 197 362 349  49 285 112 168 115  24 304  61 274 240 410
 324  54 222 113   9 181 114 423 339  89 390  31 248 306 118  23 311  48
 209 402   2 422  16 221 378 143  38 119 148 425 117 368 131 177  45 203
 183 164 393 154 165 218 388 153   7  12 354   0  32 206 171 259 205 135
   5 357 392 200  25 178 260 382 420  27 406   6 138 350 169 229 167 185
 417 426  21 208 116 196 210 351  19 173 217 352  15 176   1 237 391   4
  26 413  10 405   3 366  13 358 232 386 416 103  28   8  17 424 356  11
 400 353 106 348 101 380 111 414 107 395 102 407 364 408 104 105 109 110
 396 398 256 108 427 251 241 254 365 261 399 100 389  64]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0372
INFO voc_eval.py: 171: [110  84 100 105  73 103  95  99  98  86  65  71 102  85  76   5  90  67
  63   1  92 112  29  70 106  39  17  68 116 111  62  59 104  75  97   8
  11  24 109  93  80 107  78  96  87  20  16  14  74 108  91  48  82  23
  77  13  81 115 117   2  69 113  61 101  56  38  15  58  27  26 119   4
  19  79  10  64  45  53  52  47  66  72  21  41  88  44 120  54  25  40
  43  22   7   6  83 122  55  46  49  28  30 121  89  18 114  94   9  12
  60  35  34 118  51   3  42  57  36  31  32  37  50  33   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.1291
INFO voc_eval.py: 171: [1866 1870  424 ...   40   42   43]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0287
INFO voc_eval.py: 171: [19 17 24 36 60 20 11 57 32 62 13 46 33 50 84 16 82 74 61  9 28 14 85 18
 55 30 29  6 86 73  8 42  7 58 23 83 10 48  5 45 67 37 53 80 35 22 72 59
 54 39  0  2 21  4 12 52 68 43 64 27 51 78 26 81 34 71 70 44 25 40  3  1
 66 77 31 75 63 47 15 65 69 76 79 38 41 56 49]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0202
INFO voc_eval.py: 171: [ 566  564  581 ... 1285 1880 1874]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0229
INFO voc_eval.py: 171: [164 134 232 181 184 224 127 147 153 157 162 112 109 108 185 188 121 104
 226  89 228 229 231 233 103 151 146 107 160 142 138 285  91 145  69  78
  84 179 172 105 110 119 132 137 106 143 177 175  72  94 150 170 154 240
 125 168  38 161 174  73  80  95  82 118 120 101 129  96 243 159 163 227
 189 187 126 180 280 238 236  86  10 173  97 165  99 114 115 144 111 113
 141 176 149  98  50 169 102 100 117 130 245 136  75 234 182 311  70 156
 152 155 276 131 124 116  92  41  79 167  29 122 190 123 321 166 133 277
 158 178 135 278 171 139 186  55  77 313 128 223 281 148  93 225 249  43
 140 275 183   4  83  52  59   8 230  56 261 258 325 310  67 219 274  49
 213  57  27 237 244  25 215  21  85 210  53  58  87 286  12 289 202 324
   3  14 306  40  74  60 282 246 318 266  11 283  81 323 198  28 290  30
  37 257 312 320 195 284  64 307 209   7 200 272   1 279 287  18 303  26
  76 221   9 247  32  71 201 192 270 193 288 304 218 248 214 273  88  22
 301 317 316 220   5 194 207  65  47  34  54   2  42   6  23 235  62 262
 253  16 212  35  13 309 196 269 252  44 191 199 260  48 292  45 241  51
 308  19 204 197   0  90 208  68 302 327 203 242  33  36 239  66 250 254
  20  15 265  61  63 322 216 222 326  39 211 255 263  24 296 291 305 293
 271 205  46  31 319 217  17 268 300 294 328 314 259 206 295 315 267 298
 264 299 251 256 297]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0767
INFO voc_eval.py: 171: [394 442 231 235  94 421 418 416 409 395 390 262 102 265 267  98 290 286
  84  85  86  87  88 285 284  92 460 465 422  12   2 202  17 208  62 303
 100 310  13 495 261 389 484 105 339 499 293 291 273 198   8   7 509 197
   4  89 196 193 281 294 466 277 249 229 443 437  40 459 432 474 476 477
 423 478 446 479 206 417 481 247 411 410 204 405 404 462 402 239 447 211
 237 199 252 258 230 209   0 289 530 523 511 508 502 493 483 480 475 464
 461 445 434 428 427 424 420 414 399 397 164 392 387 374 336 330 308 306
 302 280 163 271  97  34   9  77 114  33  19  20  69  28  24 113 123  65
 104 212   6 500 406 256 425 487 488 344 407 510 342 268  35 217  46 438
 408 448 426 242 282 527 334  15 250 534  23 259  61 207 325 367  83 238
  32 112 260 453 383 329 433 205 255  39 485  96 452  78 240  26 283 210
  95 473   1 168 165 513 143 401 400 429 131 375 505 170 187 520  70  11
 195 341 138 537 388 398 533  67 243 373 301 538 304 174 469 270 194 340
 188 468 213 506  99 419  37 179 412  75 245 525 531 365 263 266 536 253
 201 172  22  90 153  14 366 515 361 440 173 274 436 318 377 467 379  30
 368  63 415 430  68 169 496 200 501 137 159 101 214 444 490 338 521 327
  38  45 299  48 347 116  50 316 128 319 120 517 503 236 472 264 391 439
  93 524 180  21 161  31 307  42 115 125 184 450 219 346 133 435 491 305
 232 519  53 175 372 272  10 220 151   3 145 177 403 370 348 542 431 233
 300 124 278 218 130 227 155 234 393 492 287 216 449 335 504 396 107 108
 176 110  16 162 215 178 103  74 413 535 181 146 149 458 119 254 129 296
 451 514 117  18 298 189 223  66 349  71  91 118 109  73 489 269 471 337
 139 518 160 455 441  56 543 343   5 512 106  47  72 350 532 354 326  36
 312  49 111  64 186 136 324 241 157 381 470 539 251 371 182 122  82  81
 463 171 315 498 541 246 311 457 526 148 332  43 248 322 121  80 333 244
 331 482 516 314 353 132 486 166 369 345  60 497 323 321 378 288 257 150
 141 376 142 522 126 309 167 540 147  79 352  76 226 384 328 225  58  41
  25 156 494  27 356 295  44  29 276 203 507 221 158 275 297 292  52 456
 279 140 313 363 360 529 127 320 192 364 134 359 222 317 528  55  59 224
 386 183 154  57 185 190  51  54 362 228 144 152 135 454 380 191 358 357
 351 385 355 382]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0985
INFO voc_eval.py: 171: [ 20  38  31  40  44  19 113  30 178 114 172 184 171 135 110 168 121 120
  18   5 122 124 151  66  68  76  10 123  25   7 131 183  24  36 149 141
  16   6  56   3 143 117  95  99 103 126 130  21  48 125 150  92 129 145
 181  77  35  97 175  29 133 115  22 170 107  27 109   9  80 148  33  55
 116  93  71 142 152 174   0  15  98  64  37  26 176 105   2   8 173 177
 132 164  82 138 137 112 118 182  79  85  12 153 140 185   4  11  65 144
  52 155  94 102 128 161  49 136 169  51 162  53  39  69 166  72  54  73
 111  81 188 101 134 147 127  45  23  67 156 106  14 146 165 159 193 100
  90  41  83  88 154 189 119 108 104 163  42  50  43 190  78  96 158 187
  13 179  63  89  84 186 139  87 167 192  70  46  28  47  86  34 160  17
  91  32  75 180   1 191 157  74  59  58  60  61  57  62]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0206
INFO voc_eval.py: 171: [185  81  76  82  77 228  61 338  29  31 133 227  71  26 229 231  41 392
  44 248 348  51  85  49  62 142 186 180 178  69   7  67 141 216  66  10
  64 151 147  63 144  20 143  22 157 246  89  92 340 251  75 150 349 182
 177 376 214 159 156 220 145 347 140 137 134 226 384 388 126 123 113 138
  74  94  72  48  38 279  73  32  27  23  19  18  17  16  15  12  11 295
  50  53 191  57  55  59  60 284 393 428  24 154  37 290 183 175 148 283
 277  30 362  90  65 218 174 163 164  58 331  13   9  87 304  35 233  78
 254 339 169 333 242 158 273 204 323 321 281 356 311 172 152 210   8  25
 167 136 415 431 395 269  21 208 241 336 201  97 335 413 344 173 390  33
 146  84 352 219 211 209 280 389 111 381 222 424 423 106 351 179 317 426
 238  28  43 115  79 110 217 215 256 303  40 365 372 116 239 206 205 184
 341  96  52   5 334 264 198  68 359 320 117 382  14   3 442 230 131   0
 213 193 199 328 200  54 306 187 370 105 294 427 291 212 107 330 355 296
 286 380 161 268 207 417 367  34 194 379 313 170 377 259  80 342 368 397
 325  46 195 401 416 422 192 441 364 410 418 287 285 162 337 139 112 282
 243 346 430 202  47 307 197 315 411  45 310  42   1 385 387 375 203 166
 108 252 221 132 196 247 155 399 312 257 125 408  70 403 223 160 345  95
 414 234 272 434 232 250  98   6 293 329  86 276 432 149 168 255 244 305
  36 357 292 429 326 386 103 412 289 265 420 374   4 190 438 327 366 419
 394 278 354 378 129 224 318 404 118 245 319 383 109 435 406 297 421 302
 165 405 443 270 128 135 235 263 363 396 271 300 398 350 391 262 181 402
 316 237 309 101 266 324 249 436 153 439  39 369 371 225 127 274 189 114
   2 267 299 308 261 120 314 400 298 124 373  83 445  88 361 343  93 258
  99 433 322  56 332 130 171 260 288 358 407 437 409  91 353 122 301 360
 440 253 240 188 102 100 104 119 121 444 176 425 236 446 275]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0190
INFO voc_eval.py: 171: [ 39 119  47  40  73  10 112 115  55 130  12  85 135  92  50  17  52 111
  44 117  79  27  42  86  29   4  43  41 123  88 127  48  72 102  57  65
 125  87  89  46  13  23  11  30 131  54 122  61   6 110 126  91 124  71
  76 106  84 121  99  98  70  93 132  63 120  90 114  36  81 129   3 128
  22  97 107  69  53  95  33  66  64  56 113  18 133  51  45  21 118  77
   2 103  20  74 101 100   5 108  67  62 137  34  80 134 116 104  38  59
  58 136  83  96   1  19  68  94   9  49  60 105  35   7  75  78  37  32
  82  16  28  24 109   0  25  15   8  14  26  31]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2846
INFO voc_eval.py: 171: [22871  7162  7159 ...  5189  5201  5231]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0516
INFO voc_eval.py: 171: [1073  763 1063 ...  440  447  476]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0830
INFO voc_eval.py: 171: [119  97 122 109  73 143 128 104 107 144  99 223 126  96 167  92  84  33
 181 222  83 178  94  74 170 151 191 130  81 160 213 210 168 150  11  40
 108 180 139 176 203  39  30 131  37 140 182 189  47 141  77 101 134 186
  49 116 252  25 154   3 214 161 125  34 100 175  12  20  27 142  87 216
 115  38 190  55 166 152 163 105 198 165  10  31  90  23 221 164 157  43
  22 187 209   9 147 220 204 146 218  41 194  53  21 172 158  28 196 155
 153  93 145 215 171 219  95 236 173  26  29 183  75 156  35 149   6  85
  32 112 162 208  44   1  36 169  86 148  18 201 179 224  46  45 159 123
 199 118 120 188 245  17   7 205   8  13  54 207  24 200 133 111   0  79
  72 211  82  19 136  88  89 184 177 135 103 193 129 174 206  42  62  76
  65 217 132  69   2 114   4 121 212  14  68  71  15 202  59 124   5  16
  56  70  64  91 106 233 117 113 137 110 102  61 225  78  80 138  98 127
  51 197 243  48 231 195 192 247 229 244  58 185 246 227 240 241 230 226
  57  60 238 235 251 250 237 248 239 249  50 232  52 234 242 228  66  67
  63]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0488
INFO voc_eval.py: 171: [123   0  36  43 128 122  31 121 117  16   3  27  42   2 130   9 127   4
   8 116  14 118  11   5  35   7  41  28 114  13  39  30 124  40   1  65
 119 144  25 162 100  70  17  52 138 112 120  64 168 159 102 160  45 158
 126  79  12  75  68  34  60  89  10  24  19 108  77 142  74 131 161  69
  99 163  90  86  71 137  61 150 129 170  73  93 165  82  95 156  56  51
  87 106  81 125  97  29 135  78  63 152  49  83  38  26  59 136 110  32
 103 167  62  22  47 115  92  58  50 151 109  37  46  98  33 101  84 169
  94 145   6  55 166  76 111 164 105  96 149  85 141 139 153 154 104  91
  88 157  15  66  44  80  23 155 133  67  18 146  21 140 134 107 132 143
  53 147  20 148  72  48 113  57  54]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0679
INFO voc_eval.py: 171: [183 194 133 280 272 271 283 260  20 261 289 205 119 188 275 278 192 266
 117 189 214 113  27 211 208 291 185 262  56  58 120 204 186 144 179 264
  36 200  59 277 173 129  60 116 246 269 134 191 296 202 203 285  19 135
 240 207 145 132  31 268 176 294  78 226 217 199 115   0  44 273 237  41
 143 270 242 121 158 184 215 274  52  25  61 124 216 279   1  34 122 218
 265 267 206 190  57  26 227 201 241 150  92 187 178 157 243 290  30 245
 259 238  70 140 249 114 193  75 292  67  84 180 136 139 169  65 164 118
 234  89  73  74 125   6  53  76  16 276  18 282  43  14 287  48  93 175
  77 146 298 255 159  64  79   5  21 168 297 147  72 300 195 299  22 174
   4 171  97 128 223 163 224 253 248 247 284  55  62 229  29  54 225 131
  35 108  32 154 196  51 127 210 209 182  95 110 170  82 212 305 177 111
 126  38  90 141 281 256  49 239  45  10 149 221 219 137 235  13  12 181
 231 197 228 213  98 105 155  96 244  63 251  46  80 198 258  17  39 165
 301  42  28 151 162  40 257 123 142 172 233 232   2 230  87 303 295 288
 130 138  83  23  50  15 222 148 252  86 236 254   8 152 160  91 250  68
 286  11 100  69 153 161 293 166 302 156 263 112 304 109   3 220 167  37
  66  85  47  33  24  99 106  88   9 101 104 103  81  71   7 107 102  94]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0568
INFO voc_eval.py: 171: [ 15 125  51  66 297 149 181 147  38  18  13 135 291  24 261 131 258 285
 233 234 136 224  61  88  86   9   8 184 209   3 240 298 221 215 244  55
 207  90  89 235  81  97 203 176 139 148  19   5  42 150  26  27 300  10
 130   1 128 272  69 267 196 287  80 202 141  93 168 100 225 143 247 314
  57 265 269  32 292  22  64 295  40 164 283 102  67 232 245  58 132 231
 263 138 133  63 144 239  49 214 229 111 167  98 173 194 188 205  48 268
   2 311 312 145 284 227 126 186 129  39 248 154  62  82  91  87  68  54
 242 260 165 252 152  28 253 246 249 236 303 264  92 257 226 218  71 266
 294 175 262  60  14  77 146 313 255 309   0  70 112 105 299  33 271 172
  12 170 140 134   7 142 223 301 169 117 174   6  96 211   4 137 286 270
  59 206 289 153  56  44 208  21 250  84 256 160 183  99 222  45 210 166
 193 243 151 116 302 158 259 296 195 163  52  47 182 190 127  76  17 307
  37  35 241  11 290  16 275 305  30  41 304 308 106 278 238 115 293  65
 171 191 157 251 101  46 279 114 162 281  20  74 185  43  29  25 179 155
 213  78 237 156 306 277 159  95  53 216  73 103  85 280 288  72 228 161
 230  34 123 282  50  79 217  75  83 108 113 212 310 124 219 118 276 204
  94 180  23 273 121  36 119 254 201  31 178 274 189 220 110 198 192 177
 104 200 199 187 107 197 109 120 122]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0559
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0638
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.010
INFO cross_voc_dataset_evaluator.py: 134: 0.090
INFO cross_voc_dataset_evaluator.py: 134: 0.024
INFO cross_voc_dataset_evaluator.py: 134: 0.050
INFO cross_voc_dataset_evaluator.py: 134: 0.037
INFO cross_voc_dataset_evaluator.py: 134: 0.129
INFO cross_voc_dataset_evaluator.py: 134: 0.029
INFO cross_voc_dataset_evaluator.py: 134: 0.020
INFO cross_voc_dataset_evaluator.py: 134: 0.023
INFO cross_voc_dataset_evaluator.py: 134: 0.077
INFO cross_voc_dataset_evaluator.py: 134: 0.099
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.019
INFO cross_voc_dataset_evaluator.py: 134: 0.285
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.083
INFO cross_voc_dataset_evaluator.py: 134: 0.049
INFO cross_voc_dataset_evaluator.py: 134: 0.068
INFO cross_voc_dataset_evaluator.py: 134: 0.057
INFO cross_voc_dataset_evaluator.py: 134: 0.056
INFO cross_voc_dataset_evaluator.py: 135: 0.064
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.448s + 0.003s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.365s + 0.003s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.367s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.368s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.376s + 0.003s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.375s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.368s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.370s + 0.003s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.367s + 0.003s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.369s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.364s + 0.003s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.364s + 0.003s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.366s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.376s + 0.003s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.350s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.360s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.359s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.358s + 0.003s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.358s + 0.003s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.367s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.364s + 0.003s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.362s + 0.003s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.365s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.364s + 0.003s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.365s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.363s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.519s + 0.004s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.378s + 0.004s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.369s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.361s + 0.004s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.363s + 0.005s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.365s + 0.005s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.368s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.369s + 0.004s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.368s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.370s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.371s + 0.004s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.371s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.369s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.565s + 0.003s (eta: 0:01:10)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.372s + 0.004s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.366s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.359s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.356s + 0.003s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.349s + 0.003s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.354s + 0.003s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.354s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.357s + 0.004s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.356s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.356s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.356s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.358s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.395s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 48  76  25 126  63  21  85 132  43  56 120 129  62  71  74  58 110  61
  86  44  37  34  32 128  22  20 124 101  68 113  35 160 119  96  29  90
 158 154 166  45 164  89 168 144 150  41 142  72  87 167 127  70  28 131
 156 123 121  83  38 105  36  46  65  92  26  97  84 170  53 177 151  27
 149 165 147  31 133 155 171 169  52  47 141  66  51 106  88  30  49 125
  59 163 116  54 140  99  94  24  23 100 115 183  11  50  40 148 173 104
 114  67  55  14  93 130 103 157 146  73  60 136 153  33  75  39 145 172
  57 162   6  91 176  10 112 175  17 138  82 143  98  42   0  64 182 122
  69 161 118 139 111 134 159  81  19 174 102  16 137   2 178 135  13 152
   1   4   5 117   9   3   7   8 109  12  80  15  95  18  78 181 107 108
  79 180  77 179]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0100
INFO voc_eval.py: 171: [261  95  47 127  56  16  40  31 260  25 122 118  13 123 114 246 248  63
  60  87  89 165 134 132  96 171 251 101 244 231  23 126  53 124 140 249
  49 143  58 146 163 235 232  22 230 250 186 174  32 256 258   5  64 243
  97   7  62  61 242 238  17 144 237  46  33 139 239 125 129 175 137 257
  84 130  57  14  12  65  42  59  10  26  18 110 236 113  93 247  20  19
  45 259 135 145 108 105  15 170 161  44 180  54  74 184 141 111 173 136
 179   6 241  11  90  28 234   3  34 253 160  21 245 166 254  71  85 117
 172 102 252 190  37  41  52  91 255 153  30   8 198 148 156  80  66 150
 131 233 147  24 240 187 155 109 115 116   9  83  43 191 203 168 119  27
  29 213  76 183 138 185 149 181 192 158 209 207  88 208 106 204 167  68
 164 195 193   0 182 221 169 199 210  38 152  92 120 202 162  50 189 227
 128  73 176 196 151  36 219  94  67  77 212 214 159  69 157  82 216   4
   2 215 177 142 178 205 100 133 222  75 188 211 206 154 197 225  70 220
  78 224  35 217 223 226 104 121 229  86   1 201 200 228  99 112  79 218
 194  55 107  48 103  51  39  98  81  72]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.0897
INFO voc_eval.py: 171: [ 175  194  186 ...  635 1216 1210]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0999
INFO voc_eval.py: 171: [321 681 668 ... 230 141 145]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0498
INFO voc_eval.py: 171: [260 245  71 146 241 228  73  51 250 255  75 294  66 183 128 189 233 314
 289 339 232 131 231 306 299 221 413 214  57 218 210 202 253 200  64  67
 292 419 126 244  68  70 186 190 365 368 319 401 132  65 402 274 185 275
  52  72 410 297 211  40 135 282  69  94  76  60  59 311 226 188 192 335
 156 145 320 125 305 372  96  74 209 367 237 184  41 345  77 217 199 263
 212  81 391  63 129 361 229 155 159 251 141  29 143 196 269 374 193  34
 293 236 331 303 270 384 326 324 173 399 312  80 222 290 344 373 285  58
 296 180 121 123 307 178 358 338 316 223  78  79 343 162  62  84 298 280
  20 370  98 409 160 157 240 191 273 407  39 127 224  43 278 122 281  85
 308 177 264  42  18  53 257 317 333  82 136 381 119  89 120  92 276 197
 161 247 323  93 291 267 417 328  97 265 124  87 371 286  55 375 283 242
 140 268 272  30 321  35  95 295 158  90  86 165 318  56  47 287 288 332
  83 149 266 174  33 315 300 205 213 148  14 330 378  44  22 133 340 248
 334 313 380 353  37 416 144 261 150  46 359 171  36 187 225 382 301 336
  91 169 329 279 154 139 395 357 151 310  50 138 341 243 342 325 234 262
 369 327 195  49 360 347 284 302  24 167 111 271 114  61 322 238 408  54
 220 112 113 179   9 421 387  88  31 337 246 117  23 304 309 420 277 208
   2  48 400 142 219  16 376  38 147 118 422 130 116 366  45 175 181 201
 163 390 153 164 216 385 152   7  12 352   0  32 170 204 256 134 203 389
 355 198   5  25 176 379 258  27 404 418   6 137 348 168 227 166 415  21
 182 424 194 115 206 207 349  19 172 215 350  15   1 388 235   4  26 411
  10 403   3 364  13 230 356 383 414  28 102  17   8 393 354 423  11 398
 351 105 100 346 377 110 392 106 412 101 405 406 362 103 104 108 109 396
 394 254 107 425 249 252 363 239 259 397  99 386]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0383
INFO voc_eval.py: 171: [112 102  86 105 107  74 101  97 104 100  66  72  87  88  77   5  92  68
  64   1 114  30  94 108  71  40  18 118  69 113  63  60 106  99  11   8
  76 111  25  95  81  79 109  98  89  17  21  14  75 110  93  49  78  24
  83  82  13 117 119   2  70 115  62 103  57  39  16  59  28  27 121   4
  20  80  10  46  65  54  53  48  67  73  22  42  90  45 122  55  26  41
  44   7  23  84   6 124  56  50  47  29  31 123  91  19 116  96   9  12
 120  61  36  35  52   3  43  58  15  37  32  38  33  85  51  34   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.0987
INFO voc_eval.py: 171: [ 369  646 1474 ...   40   42   43]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0284
INFO voc_eval.py: 171: [19 17 24 20 36 60 11 32 57 13 62 46 33 50 74 16 82 61 84  9 28 14 18 85
 55 30 29 86 73  8  6  7 42 23 58 10 83 48  5 45 67 37 80 53 35 22 72 59
 54 39  2  0 21  4 12 52 68 43 64 27 78 51 81 25 34 71 70 44 40  3 66 31
  1 77 75 63 47 15 26 65 69 76 79 38 41 56 49]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0202
INFO voc_eval.py: 171: [ 577  562  561 ... 1866 1281 1873]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0218
INFO voc_eval.py: 171: [224 133 180 182 231 163 120 156 152 126  88 184 103 187 111 161 146 108
 107 230 102 232 225  68 159  77 149 141 144  90 106  83 145 104 137 285
 228 172 178 167 240 105  72  71 109 226 118 124 131 136 176  93 143 174
 169  37 153 150 173 160 117 234 158 243 100 188  79  81 162  95 119 186
  94 125 179 129  98  85 280  96 171 235  10 142 113 114 237 227 110 112
 148 175 140 164 168 101  99  50  97 128  69 116 238  74 135 245 155 312
 154 151 181 130  91 276 123  29 115  78 166 121 189 122 322 157 165 277
 278 132 134 177 170 138  41  54 185  76 127 314 281 223 147  92  43   4
 249 139 275  52 183  82  58   8 229  55 259 326 311  66 219 274  49  56
 244 213  27 236 260 215  26 210  21  84  53  57  86 286  12 289 202 325
   3  14 307  40  73  59 282 246 266 320 198 283  80  11 324  28 257 290
  30 196  38 313 321 284 209 200  63   7 308 272   1 279 287  18  25 304
 247  75 221   9  70 201  32 191 270 193 218 288 305 214 248 273  87  22
 302 316 220   5 318 207 194  64  47  34 263  42   6   2  23 233  61 253
 212  16  35 269  13 190 252  44 195 310 293 199  48  45 241 309  51 262
  19 192 204 197  89   0 208  67 303 328 203 239 242  33  36  65 250 254
  20  15 265  60 222 216 323  62 327 261  39 211 297 255 291  24 271 294
 306 205 258  46  31 319 217 268 295  17 301 329 296 206 315 317 292 267
 299 264 300 256 251 298]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1355
INFO voc_eval.py: 171: [423 269 203  92  88  87  86  85  84  94 448 288 289 398 397 293 393 462
 467 287 209 444   2 418  98 424  12 102 413 232 420 236  17 264 267 392
 342 461 468 422  62 395 464 409 434 449 105 415 414  89 405 445 100 313
 440 408 419 476 479 194 247 249  40 284  13 511 280 478 276 197 198 199
 273 205 207 263 497 501 296 425 480 481 240   4 294 483   7   8 230 486
 297 306  19 253 123 231 259 416 311 411 333 164 163 339 310 377 212 390
 292 400 402 283 200 210 305 426   0 114 466  69  65 477 482 495 504 510
  34  33 512 513  28 525  24  23  20 531   9 463  77 485 238 436 430 447
 113  97 429 104 208 428 328  61 410 427 270 491 529  46 502 412 404 285
 439  35 489 337  32 112  83   1 450 243   6 455 536 386 213 250 239 262
 257 218 347 345  15 370 260  26 435 241 515 206  11 256 522 431 211 403
  95 401 454  78 138 407 378  70 344 196  96 165 475 286  39 332 507 186
 170 174 131 168 487 527 540 539 391 244 245 307 535 254 376 369 343 470
 533 471 421 304 508 272  75 214 179  99 153 195  67 187  37 140 202 517
 368 265 172 364 268 173  22  90  14 538 442 417  29 503 277 498 302  63
 382 201 350 321 101 169 371 116 432  68 159 469 215 380 137 438 446  45
 341  48 523 330  38  50 128 120 492 322 319 505 519  21 237 266 180 183
 233 349 220 115 521 474 526  93 452  30 125 441 394 309 308  42 161 437
 133 493 175 274  53 375 351 152 221 150 373 433 303   3  10 234 290 177
 219 406 281 544 228 124 396 155 130 235 506 494 338 451 107 217 181 178
 176 216 110  74 399 108  16 162 103 537 147 516 119 460 255  18 453 129
 261 117 301 299 190 224 352  66  71  73 118 109  91 271 473 340 490  56
 142 141 443 545 520 139 346 160 457 144 514  47 106 534   5 353 357  72
 184  36 315 329  49  64 111 136 242 327 157 472 384 541 252 374  82 122
  81 465 318 171 543 314 149 500 246 528 459 335 146 248  43  80 325 121
 336 251 334 317 484 518 356 132 488 166 348 372  60 326 499 324 381 291
 258 148 524 379 143 126 312 542 167  79 355  76 227 387 226  58 331  41
  25 156 496  27 359 298  44 279 204 509 222 278 158  52 295 300  31 282
 316 275 458 363 532 127 193 323 367 320 362 366 134 223  59  55 225 530
 389 182  57 154 191  51 185  54 365 229 145 188 151 135 456 383 192 361
 360 354 388 189 358 385]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0968
INFO voc_eval.py: 171: [ 20  31  19 112  44  40  39 177 170 171 134 113  30 182  18 120 119 109
   5 167 121  66  68 123 150   7 130  75  25  36 122  10 181   6  24 148
 140  16 102   3  55  98 142  94  21 116 125 129 179 124  48 149  91 144
 128  35 173  76  96  29  27  23 132 108 114 106   9 169  79  33 147  56
  92 115  71 151 141 174   0  37  15  97  64  26 175   2 104   8 172 176
 131 163 137  81 136 180  78 117 111  84  12 152 183 139  11   4  52 143
  65 154  93 101 127 160  49 135 168 161  38  51  69  53 166  72  54  80
 110 133 126 186 146 100  22  45  67 155 105  14 145 164 158  41  89  99
 191  82  87 153 118 107 187 103 162  42  43  50  95  77 188  13 157 185
 178  63  88  83 184 138 165  86  28  70 190  46  47  85  34 159  17  90
  32  74   1 156 189  73  59  58  60  61  57  62]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0191
INFO voc_eval.py: 171: [185  77  79  83  82  62  60 230  48 228 227  43 184 133  40  64  65  51
 226  68  70  72 179 337  86 177 215 347 244 391  67 247 143  20  10 147
   7 158 151  22 142  25 144 141  30 140  28  92 145  12  73  74  75  76
 250  11 157 294  94 374 213 348 190 181  90 176 114 339 225  15  16 138
 137  37 134 278  26  23 382  47  49 346  31 386  52 123  17  18 219  19
  54  59 150 160  56 126  58 217 165 361 154 148 392 164  78 156 276 426
  57 182  63  36  66 282  13  29 174 283 289  91 331 159 322 280  34   9
 303 152 311 320 272 232 332 355 253 241 169 203 172  88 338 413 240 429
   8 167  21 207  85 209 394 335 136 268  24 218 173 200 343 334 422  97
 411 351  32 388 146 208 387 421 221 161 379 106 279 111 210 315 237 350
 180 424  42 116 390 110  80  27 214 255 216 212 340 302 183  39   5 364
  96 115 238 371  50 333 204 205 263 197  69 380 319 358 117 440 131 229
   3   0  14 327 198 199 192 304  53 186 293 425 211 370 105 107 290 295
 329 354 285 267  33 308 378 163 206 367 193 341  81 258 170 375 377 415
 368 414 324 420  45 194 399 363 191 439 408 416 286 281 284 242 112 428
 336 139 345 201  46 196  44 306 409  41   1 310 385 166 383 202 220 108
 246 251 155 195 132 397 316 366 312 256 125 406  71 401 222 162 344 412
  61 233  95 271 432 231 249 307  98   6 292 328  87 275 168 254 430 243
 149  35 305 356 427 291 325 384 410 288 103 373 264 418 188 436   4 365
 277 393 417 326 353 223 129 376 317 402 245 118 318 381 109 433 296 404
 419 301 441 234 403 269 128 262 135 362 395 270 299 396 178 349 389 236
 309 261 400 314 265 101 153 323 248 434 437  38 224 127 369 189 273   2
 266 298 113 260 120 124 297 398 313  89 372 443  84 342 360  93 257 431
 321  99  55 330 130 259 171 287 357 435 407 405 122 352 300 252 359 438
 239 187 102 119 121 100 104 442 175 235 423 444 274]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0185
INFO voc_eval.py: 171: [ 39 119  47  40  73  11 115  55 130 112 135  92  50  85 111  17  44  52
 117  27  79  42  86  29   9   7   4  43  41 123 127  88  48  72 125  58
 102  65  87  89  13  46  23  30  54 131  12  61 122 110 126  91 124  71
  76 121 106  98  99  84  93  63 132  70 120  90 114  36  81 129   3 128
  20 107  97  53  95  69  33   8  66  56  64 113  51 133  45  19 118   2
  77 103  18  74 101 100   5 108  67  62 137  34  80 134 116 104  59  38
 136  57  96  83  94  68   1  10  49  60 105  35   6  78  75  37  32  82
  16  28  24 109  15   0  14  25  26  31  22  21]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2841
INFO voc_eval.py: 171: [26528 22849 22851 ...  5192  5233  5203]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.1334
INFO voc_eval.py: 171: [1076  767  497 ...  441  448  478]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0894
INFO voc_eval.py: 171: [122  99 125 112  75 146 131 106 109 147 101 228 129  97 172  94  86 185
  34  85 227 182  96  76 154 196 133  83 217 165 215 173  12 153  41 110
 142 180 184 208  40  31 134  38 143 186  48 144 194  79 103 137 191 119
  50  26 257 158 218 102 166   4 128  35  21  13 179  28  89 145 221 118
  39  56 195 171 155 168 107 170 203  11  32  92  24 226 169 192 161  44
  23 214  10 150 225 209 149  42 199 223  54  22 163  29 176 201  95 159
 157 162 219  98 175 148 224 241 177  27 188  77  30 160  36   7  88 152
  33 167 115 220 213  45  37   1 174  87 229  19 183  47 206 151  46 126
 164 204 156 121 123 193 250  18   8 210  14   9  55 212  25 114 136 205
   0  80  73  84 216  90  20 139  74  91 189 181 138 105 198 132 178  43
  63  78 211  66 135  70 222   2 117   5 124  69  15  72  16 207  60 127
  17   6  57  71  65  93   3 108 238 116 120 140 104 113  62 230  81 187
  82 141 100  52 130 202 111 248  49 236 200 197 252 249 234  59 190 251
 232 245 246 235 231  58 243  61 240 256 255 242 253 244 254  51 237  53
 239 247 233  67  68  64]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0492
INFO voc_eval.py: 171: [123   0  36  43 128 122  31 121 117  16   3  27  42   2 130   9 127   4
   8 116  14 118  11   5  35   7  41  28 114  13  39  30 124  40   1  65
 119 144  25 162  70  17 101  52 138 112 120  64 168 159 100 160 126  45
 158  79  12  75  69  34  60  89  10  19 108  24  77 142 161 131  74  68
  99  86 163  90  71 150 129  73 137 170  61  93 165  82  95 156  56  51
 106  87 125  81  97  29 135  78 152  62  83  49  38  59  26 136 110 103
  32 167  22  63  92 115  47  58  50 151 109  37  98  33  46  84 102   6
  94 145 169  55 166  76 111 164 105 149  96  85 153 141 154  88 139 157
  15 104  91  66 155  80  23  44 133  18  67 146  21 140 107 134 132 143
 147  53  20 148  72 113  48  54  57]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0676
INFO voc_eval.py: 171: [181 286 278 131 281 269 258 259 203  19 117 192 270 186 111 190 264 184
  26 209 206  55 212 183 202 115 260  57 276 289 273 118 187 127  35 262
 142 275 198  58 171 177 244  59 114 133 267  18 132 189 294 283 200 201
 266  77 292 174 130 205 143  30 238 215 113 224   0 197  43 268  40 271
 156 119 239 141 235 213 182  24 272  60  51 277 122 214  25 263 216   1
  33 120  56 265 204 188 148 240 225 185 199  90 176 155 288  29 257 243
 236 247 138 112  69 134 191 116  66 162  74  82 290  64 232  87 178 167
 137   6  73  72 123  52  15  75  17  13 280  42 274 285  76  91  47 253
  63 144 296 157 173  78   5 166  20 295 145  71 298 193 297  21   4 172
  95 169 222 126 161 221 251 245 282  54 246  61 227 129  53  28  34 106
 223  32 152 194  50   9 125 208 207  93 180 108 168 210  80 109 303  37
 124 175  88 139 279  44  48 254 237 147 135 219 217 233 241  11  12 179
 195 229 226 211  96 103  94  62 153 242 249  45 196  79  16 256  38 163
  41 299  27 149  39 160 255 140 121 170 231   2 230 228  84 301 293 128
 287 136  81  49  22  14 250 146 220 234  85   8 150 252 158  89  67 284
 248  10  98  68 151 159 164 300 291 110 154 261 302 107   3  36 218 165
  65  83  46  31  23 104  97  86 100 102 101   7  70  99 105  92]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0540
INFO voc_eval.py: 171: [ 14 125  38  13 298 181 147  18 149  67  51 259  24 131 135 286  89 262
 292 136  87  61 234 235   9   8 184 210   3 241 225 299 216 222 245  55
  91 208  90 236  82  97 204 176  26   1 150   5 148  10 301  19 139  42
  27  70 130 128 273 268 195 288  94 100 141 226 143 168 248  81 202 315
  32 266  57 296  64 293  40  22 270 102 164 264 232 233 284  68  58 240
 144  63 138 132 133 246 194 188 206 111 269 215  48  49 172 230  98 167
 312 145 285 313 228   2  69 154 243  83  39  88  62  92 249  54 129 186
  29 152 261 254 253 250 237 247 126 165 304  72 219 227 258  93 295 265
 175 267 263  15 314 146  60 310 255   0 112 105  78  71 272 134 173 140
 224 170   7  12 142 300  33 302 174 117 169 212 137   4  96   6 287  66
  59 271 207 153 290  44  56 209  21  85 251 160 257  99 183 223 211  45
 166 193 151 244 116 158 303 203 297 260 163  52  47 127 182  77 190  35
  37 308  17  11 242 291  16 276 306  30  41 305 115 309 239 106 279 294
  65 191 171 157 252 101  46 280 114 162  20 282  75 185  28  43  25 179
 214 155  79 238 156 307 278  95 159 217  53  74 103 281 289  86  73 229
  34 231 161 198 123 283  80  50 218  76  84 113 108 213 311 124 118 220
 277 205 180  23  36 274 121 119 201 256  31 178 275 221 189 110 197 192
 177 104 200 199 187 196 107 109 120 122]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0398
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0722
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.010
INFO cross_voc_dataset_evaluator.py: 134: 0.090
INFO cross_voc_dataset_evaluator.py: 134: 0.100
INFO cross_voc_dataset_evaluator.py: 134: 0.050
INFO cross_voc_dataset_evaluator.py: 134: 0.038
INFO cross_voc_dataset_evaluator.py: 134: 0.099
INFO cross_voc_dataset_evaluator.py: 134: 0.028
INFO cross_voc_dataset_evaluator.py: 134: 0.020
INFO cross_voc_dataset_evaluator.py: 134: 0.022
INFO cross_voc_dataset_evaluator.py: 134: 0.135
INFO cross_voc_dataset_evaluator.py: 134: 0.097
INFO cross_voc_dataset_evaluator.py: 134: 0.019
INFO cross_voc_dataset_evaluator.py: 134: 0.019
INFO cross_voc_dataset_evaluator.py: 134: 0.284
INFO cross_voc_dataset_evaluator.py: 134: 0.133
INFO cross_voc_dataset_evaluator.py: 134: 0.089
INFO cross_voc_dataset_evaluator.py: 134: 0.049
INFO cross_voc_dataset_evaluator.py: 134: 0.068
INFO cross_voc_dataset_evaluator.py: 134: 0.054
INFO cross_voc_dataset_evaluator.py: 134: 0.040
INFO cross_voc_dataset_evaluator.py: 135: 0.072
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.443s + 0.003s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.356s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.360s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.364s + 0.004s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.373s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.369s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.366s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.365s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.366s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.367s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.366s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.368s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.370s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.492s + 0.004s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.376s + 0.004s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.374s + 0.003s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.373s + 0.003s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.365s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.369s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.378s + 0.004s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.373s + 0.004s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.372s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.374s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.373s + 0.004s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.373s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.368s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.373s + 0.003s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.353s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.361s + 0.003s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.363s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.360s + 0.003s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.356s + 0.003s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.356s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.357s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.357s + 0.004s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.356s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.356s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.357s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.355s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.594s + 0.004s (eta: 0:01:14)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.345s + 0.004s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.345s + 0.005s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.349s + 0.005s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.348s + 0.004s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.346s + 0.004s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.349s + 0.004s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.348s + 0.004s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.351s + 0.004s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.352s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.350s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.351s + 0.004s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.353s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.514s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 48  76  63  21  85 125  23 128  42  71 119  56 109  74  62  58 130  86
  43  20  95 127  60  33 123  31  36 100  66 118 158  89 160  28  34 165
 111 154 164  44 167  88 144 150 116  40  72 126 142  87 168  70  27 131
  83 156 122 120  35 104  37  45  64  25  84  91  53 170  96 176  26 149
 151 166 147  30 132 169  46  52 155 141  50  68 115  51 105  29  59 124
 163  54 140  98  22  24  93  99 182  49  11  39 113 172 148 103  67 129
  55  92  14 157 102 146  73  61 136  32  75  38 153 162 145  57 171  90
   6 112 175  10 174  18 138  82 143  97  41   0  65 181 121  69 117 139
 161  81 134 110 159  19  47 173 137  16 101 135 177   2  13 152   1   4
   5 114   9   3   7   8 108  12  80  15  94  17  78 133 180 106 107  79
 179 178  77]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0108
INFO voc_eval.py: 171: [264  97  48 130  57  16  41 263  31  25 146 125 121 126 117 249 251 254
  62  13  64 129  91  89 137 168 135  98 174 143  50 234  23 127  54 238
 252 247 103  59 149 261 166 259  32   5 177 246 189 233 253  22 235 147
   7  65  99  63 245 241  17 142 240  47  33 242 115 128 178 133 260 140
  86 132  10  60  19  26  66  14  12  43  58  45  55 107  46  18 239  95
 250  20 112 138 148 262 110  15 182 164 183  77 173  11 113 176 244 237
   6 144 139  92 187  28 248 257  34   3 163 169  21 256  61 255 175  87
 104 120  72  93  37 193  53  42 201  30 156 258 151   8 159 134  83  67
 150 153 236  75  24 243 190  85 118 158 111   9  44 119 194 206 171  27
  29 122 141  79 185 188 186 152 215 195 160 210 212  90 108 211 170 207
  69 196 167 198 184   0 172 224 202 213 155  38  94 123  51 165 205 192
 131 230  76 179 199 154  36 222  68  96 216 217 162  70 161  84   2 219
   4 218 180 145 181 208 136 102 225  78 191 209 214 157 200 228  71 223
  80 227  35 220 229 226 106 124 232   1  88 204 203 231 101 114  81 221
 197  56 109  49  40 105  52  39 100  74 116  82  73]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.0899
INFO voc_eval.py: 171: [ 259 1049  571 ...  638 1216 1221]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0234
INFO voc_eval.py: 171: [318 677 231 ... 227 137 141]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0498
INFO voc_eval.py: 171: [257 243  70 239 144  72 227  73  49 253 291 248 187 181  65 130 232 311
 213 220 229 231 286 337 410 296 123 303 201 209  63  66 199  55 217 415
 242 289 251 184 398  67 362  69 365 196 316 294 183  58 407  50 272 271
 126  71 399 210 118  39 138  64  68 308 333 225 279  57 188 155  94 143
  74 186  93 131 369 182 364  40 236 342 302 317  75 208 197 216 260 211
 158 141  28 228  79 358 388 154  62 137 249 194 266 190 371  33 328 293
 290 380  56 323 234 267 300 335 396 321 341 172 129 221 309 121 287 282
 304 117  78 370 119 340 192 161 177 313 355  77 222  60  76  82 270 277
  19 124 295 367 238 156 159 404  38 128 223  97 406 189 278 305 330 134
 261  42  41  83  80 275 120  17  51 255 314 176  90 378  87 320 195  85
 264 127  91 420 273 245 160 325 413 122  95 288 262 368 372  53 240 136
 283 280 292  29 157 269  34 265 318  92 315  46 284 164  84  54  88 148
  32 263 285 329 135  81 145 327 173 212  43 204 312 297  13 375  21 377
 336 331 310 350  36 142 149 412 246  45 258 356  35 170 185 379 298 224
 332  96  89 326 168 276 150 354 153 392 307 339 338  48 322 259 233 241
 125 366  47 324 193 357 344 319 281 299  23 166  59 268 112 237 405 109
  52 219 110   9  86 334 419 111 178 384  30 301 244  22 306 115 146 140
 416 132 274 397   2 207  15 139 218 373  37 147 133 417 116 363  44 114
 200 174 387 162 179 152 382 163 151 215   7  31  11  61 349   0 254 169
 203 202 352   5  24 198 386 175  26 256 414 376 401   6 345 167 226 165
  20 411 421 180 191 206 113 205  18 346 171 347 214  14   1 385   4  25
 235 408  10 400   3 361  12 230 353 381  27 100  16   8 390 351 418 395
 103 348  98 374 343 108 104 389 409  99 402 403 101 359 102 106 393 107
 391 105 252 247 422 250 360 394 383]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0267
INFO voc_eval.py: 171: [105 108 109  74  93  99  88 104 103  66 106  72  86  87   5  77  68   1
  64  30  96  69  71 110  90  18  40 119 114  60  63 102 107  11   8  76
  25 113  97  81 111  79 100  21  17  14  89  95  75  83 112  24  49  78
  94  13  82 115 118 120   2  70 101 116  62  56  39  59  16  28  27 122
   4  80  20  10  46  65  54  53  48  67  73  22  42  91  45 123  55  26
  41   7  44  84   6  23 125  57  50  47  29  31 124  92  19 117  98   9
  12 121  61  36  35   3  52  43  58  15  37  32  38  33  85  34  51   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.0762
INFO voc_eval.py: 171: [1479  368  623 ...   40   42   43]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0279
INFO voc_eval.py: 171: [19 17 36 60 24 20 11 32 62 57 13 33 46 50  9 82 73 16 61 84 28 18 14 55
 85 70  8 29 30  6 86 42  7 23 58 10 83 71 48 67 76  5 45 37 80 53 35 22
 59 72 54 39  0  2 21  4 12 52 43 64 27 78 51 81 25 34 44 40  3 31 66  1
 77 63 74 47 15 26 65 69 75 68 79 38 41 56 49]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0195
INFO voc_eval.py: 171: [2375  564  579 ...  459 1864 1279]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0202
INFO voc_eval.py: 171: [162 179 181 229 131 222 150 123 119 109 105 104 100  98  87 154 160 143
 186 230 228 223 183  76  82 281 101 103 226 108 134 137 171  67 141 142
 147 176 158  71 168 166 173  89 238  99 159 102 138 140 106 148 224 115
 122 172 129  70 174 151 135  39 124  80  78  91  94 241  96 232 178 127
 156 114 185 155 117 161 107 139 111 170  84  10 235 276 233  92 116 110
 145 225 163 167  93  95  97  49 126 113  68 133 243  73 236 308 149 180
 152 128 153  90 272 121 165  28  77 118 188 120 112 164 175 318 157 169
 273 274 130 132  43 184  53  75 146 310 277 221 144 125  45 271 182 136
   4 177  51 247 187  81  56   8 227 256  54  65 322 217 307  58 270 212
 242  26 234  25 214 209  21  83 253  52  55  85 282  12 201 285   3 321
  14  42 303  35  72  57 278 262 197 244 316  11  79 279  37 320 286 195
 309  29  41 208 317 199 255 280   7  62 304 268   1 275  27 283  18 245
 219 300   9  74 200  69 190 266 192 216 213 301 284 269 246  86  22 299
 218 312   5 314 206 193  31  63  33  44   6   2 231 251  23  60 211  16
 265  13  34 189 250  46 306 194 289 239 198  48 259 305  50  19 191 203
 196  88   0 207 237  66 298  36 202 324 240  32  38 248  64 252  20 261
  15 220 215  59 319  61 258 293 210 287 323  40 267  24 204 290 302 257
  47  30 315 291 264  17 297 325 292 205 311 313 288 263 260 295 254 296
 249 294]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0814
INFO voc_eval.py: 171: [420 285 289 393 395 408 409 414 416 419 102 440  98  94  92 444  88  87
  86  85  84 463 284 283 389 458 202  17  12 208 267 262 265 235 231   2
  89 113 430 445 105 436 100 229 421 441 246 261 249 206 193 280 196 197
 198 290 292 293 302 309 329 338 388 457 391 277 273 401 404 405 410 271
 415 418 204 239 460 493  40 507 482 479 476 475 474 521  62 472  19 477
 497   7  13   4   8 464 396 230 506 508 500 237 252 386 373 459 163 164
 335 398  34 199 258  20 209 527 279  23  24  33 211 407 288  28 509 301
 306   9 307   0 411 422 426  69  97 123 462 473 442  65 481 478 112 491
 432 412  77 425  46 281 525  61  32 435 207 532 212 268 217  83 260 446
 256   6 248 451  78 242   1  15 324 104 487 423 366 424 238 485 343 341
 400  35 382 406 111 498 333 467  11 210 131 240 483 255 503  70  39 397
 328 399 403 138 450 170 205 531 511 518 374 186 340  26 168 282  95 195
 431 471 427 174  96 165 243 244 535 536 303 300 339 253 417 523 266 534
 372 466 387 529 504 365 270 179 194  90  99 153  37 140 187 213  67  75
 201 263 513  63 360 438 172  22 173  14  29 364 298 200  68 465 274 169
 159 494 346 214 317 413 378 376 367 116 434 101 137 428 499 318 326  47
 488  49  45 315 337 128  38 120 519 443 501  21 264 183 236 180 515  51
 448 437 470 433 345 115 125 161  93  53 390  30 522  42 305 304 219 232
 517 489 175 371 272 133 347  10 369 429 152 278 299 150 220   3 540 218
 402 227 286 177 233 124 490 130 447 392 502 155 334 234  16 215 216 178
 108 107 110 533 394 147 176 162  74 181 103 254  18 297 512 189 449 456
 119 129 295 223 117 259  73  71 348  66  91 109 118  56 336 469 486 269
 439 141 541 142 342 144 139 453 160 510 516  72 106   5 530 184 349 354
 311  36 325  48 323  64 114 241 136 468 380 157 537 314 251  81 370  82
 368 461 122 539 171 149 496 310 245 524 331 455 146 247  43  80 332 321
 121 250 313 480 330 514 352 484 132 344 166  60 322 495 287 257 320 377
 148 143 520 308 375 126 538  79 351 167  76 226 383  58 327 225  41  25
 492 156  27  44 294 355 276 203 505 275 221 158  52 312 296 291  31 454
 192 359 528 127 319 316 363 358  59 362 222 224  55 134 526  57 385 182
 190  50 154  54 185 361 228 145 188 151 135 452 379 191 357 350 356 384
 353 381]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0471
INFO voc_eval.py: 171: [ 21 113  31  22  40  19  41 177 182 114 121  36 170 171  30 135 120   5
 122 167  76 110  67 131   9   6 124 151  69  37 123  24 149 181  27 141
   2  17 103  10  43  95 117  56  99 143 130 126 150 129 125 179  49 145
  92  35  77  97  29 173  20 133   8 107 115  23 109 169  13  80  33 148
 116  93  57  72 152 142 174  16   0  38  98  65  26 175 105   7  25 172
 176 132  82 163 138 137  42 180 112  79 118  12  85 153 183 140  11   3
 144  53  94 155  66 128 102 166  50 168 161 136  39  52  70  54  73  55
  81 111 186 134 127 147 101  46  68   4 156  15 106 146 164  44 159  90
 100 190  83 154  88  14 119 108 187 104 162  45  96  51 158  78 185 188
 178  64  89 184 165  84 139  28  87 189  47  71  86  34 160  48  18  91
  32  75   1 157  74  60  59  61  62  58  63]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0205
INFO voc_eval.py: 171: [ 83  82  79  77 186 145  86  72  70  68  67  65  64  62  59  51  48 246
 135 244 142 215 344 226 228 229 231 185  43 180 160 153 334 149 146 144
 143 178  40 388  10  30  20  28   7  22  25 115 383  94 379 140 124 128
  92 371  90 136  19 139  96 423  16  15 213 219 293 343 225 345 190 182
 177 336 162  11 159  12 152  14 147  17  23 249  74  49  54  73  52  75
  76  37  26  31  47  60  58 277  56  36 183 155 329  63 166 358 150  29
  66  57 175  13 158  91  78 281 389 275 288 328 282 217 301 241 272 252
 203  34 161 308 335 279 170 173 154 319 352 233  88   9 317 240  24 209
   8 207 138 410  21 332  85 391 426 168 348 340 174 200 218 148  98 408
  32 385 331 419 112 210 208 221 107 384 278 376 418 163  42 421 347 312
 181 237 111  27 117 387  80 268 216 337  39 254 212 184 214 238 205 204
  97 368 306 127 273  50   5 330 361 116 197 262 230 377 118 355  18 316
  69 227 325   0 437 133   3 199 198 192 303  53 187 108 106 211 292 367
 422 289 284 294 327 351 206 165 364 375  33 302 193  81 412 372 338 171
 257 365 374 321 411 194 396 436  45 267 417 360 191 405 285 413 310 242
 283 280 333 141 201 113 342 425 324 305  46  44 196 406  41   1 167 380
 245 202 382 220 250 157 109 195 134 394 255 313 363 126 403  71 398 222
 164 234 409 341  61  95 429 270 232  87   6 248 291 307  99 274 169 326
 243 253 427 151 304  35 353 290 424 322 407 381 104 287 370 263 415 433
 276   4 362 414 271 390 223 350 323 131 373 314 399 119 315 378 110 430
 401 416 295 300 235 438 137 130 261 400 359 392 269 298 393 236 179 346
 386 397 311 260 264 156 102 320 431 434 247 129  38 224 366 189   2 297
 265 114 259 121 309 125  89 296  84 440 395 369 357 339 256  93 318 428
  55 100 132 258 172 286 432 404 354 402 123 349 299 251 435 356 239 188
 103 120 122 101 176 439 105 420 441 266]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0184
INFO voc_eval.py: 171: [ 39 119  47  73  40  11  55 112  92 115 134 128  85  50 111  17  44 117
  52  27  79 129  42  86   9  29   7   4  41  43 125  88  48  64 123  72
  58 102  87  13  46  89 131  23  12 130  30  54  61 121 110 124 122  91
  71  76 106  98  93  84  63  99  70  90 114  36 118  81 127   3 120 126
 107  20  97  53  95  69   8  33  56  66  65  51 132 113  45  19   2 103
  18  77 101  74 100   5 108  67  62 136  34 133 116  80 104  59  38 135
  96  57 137  94  83  68   1  10  49  60 105  35   6  32  37  78  75  82
  16  28  24 109  15  14  25   0  26  31  22  21]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2843
INFO voc_eval.py: 171: [18702  8707  2607 ...  5193  5234  5204]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0850
INFO voc_eval.py: 171: [1080  771  729 ...  445  452  481]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0889
INFO voc_eval.py: 171: [125 101 127 115  77 148 133 109 112 149 104 229 131  96  99 174  88 187
  87 228 184  98  78 167 156 197 136  85 218 216 175  43  13 103 155 113
 144 182 186 209  40  36  42 135 145  51 188 146 195  80 106 139 192  53
 120 258  27 160 219   4 168 105 181  22  38 130  14  29  91 147 222 119
  58  41 196 173 170 157 110 204 171  12  35  94  25  46 172 227 193 215
 163  24  11 226 152 210 151  44 224 200  56  23  31 165 178  97 202 161
 159 220 164 100 242 179 150 177 225  33  28 189  79 162  90   7 154 169
  37 214 221  34  39   1  89 176  50  20 230 185 207  48 128 153 166 205
 158 123  32 126 251 194  19   8 211  10  30  15  57 213  26 116 138   0
  81 206  75  92 141  86  21 217  76  93 190 183 140 108 134 199 180  65
  83  45  68 137  72 223 122   2   5 118  71  16  74  17 208  62 129  18
  59   6  73  67  95 212 111   3   9 239 121 142 117 107 124  64 102 231
  82  84  49  47 143  55 203 132 114 249 237  52 253 198 201 235 250  61
 191 252 233 245 247 236 232  60 244  63 241 257 256 243 254 246 255  54
 238 240 248 234  69  70  66]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0491
INFO voc_eval.py: 171: [122   0 120 121   4 116 127  16  41  30  35  26 129  40 126 117  34  25
 115   9   2   8   3   5  11  14  37  39  29   7 113  13   1  38 123  63
 118 143 100  24 161  68 111 137  50  17 159 158 119  98 166  62 157  77
 125  43  12  73  33  67  87  58  10 107  23 141  75  72 160 130  64  27
  97 162  84 103  88  71 149 128  69 136 168  59  91 163  80  93 155  54
  49 105 124  85  79  95 134  28  76 151  60  81  47  57 109 135 101  31
 165  61  21  90 114  56  48  45 150 108  96  36  32  44  99  82   6  92
 167 144  53 164  74 110 148 104  94  83 152 153 156 140  15  86 102  66
 138  89 154  78  22 132  42  18  65 145  20 139 106 133 131 142 146  51
  19 147  70 112  46  52  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0682
INFO voc_eval.py: 171: [180 271 282 116 279 259 287 202 130  17 260 191 270 185 186 189 277 117
 110  26 274 211 265 208  53 183 182  55 201 290 261 114 205 263  56 276
 126 197 131 176 170 141  33 113 132 199 188 268 244 295 200 284  57 224
 267 293 173  42 204 129 238 215 142 112  75  25 196 269   0  40 118 235
  37 272 155 239 213  23  58 273 181  49 135 278 121 264 214  24 187  31
 119 216   1 203 266  54 147 225 240 184  88 198  29 154 175 289  18 258
 243  67 236 247 111 138 190 133 115  64  72 291 232 161  80 166  62 177
 137  85  71  70   6 122  50  73  15  16 275 281  13  39 297  61  74  46
  89 286 156 143 254 172   5  76  19 165 296 144  69  35 299 192 298  20
  93   4 125 168 222 160 221 171 252 246 245 283  52 128  51 227  59  30
  28 105 223 151 193   9  48 207 124 206  91 179 107 167 209  78 108  34
 304 174 139  86 123 280  43 255 237 146  41 134 219 217 233  11 241  12
 178 194 229 226  94 210 101  60  92 152 250  44 242 195  77 162 257 300
  38  27  36 148 256 159 140 120 169 231   2 228 230  82 127 294 302 288
 136  79  47  21  14 251 145 234   8 149 220  83 253 157  66  87  10 285
 249  96  65 150 163 158 301 292 109  32 262 153 303 248 106   3  63 218
 164  45  81 102  22  95  84  98 100  99   7  68  97 103 212  90 104]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0428
INFO voc_eval.py: 171: [ 15 126  40  68  52  20 183  12 300 148 151  90 206 226 236 288 264   3
 294 237 261 132  88 186  25 137 212 243 301  62 136  11  13  65 224  91
  98  83  92  56 238 228 218 247 270   1 152 150   8 303  43 178  27 141
 131 290 129 197 275 210  28  71  95 142 144 101 204 169  82 250 317 298
  23 295  33  58 272  41 268  69 265 235 103 234 286 134 139 145 133  59
 165  64 248 242 190 208 112 196 232 271 217  99  50 314 173 168  49   7
 315 230 287   2 146 251 130 245 155  89  84  70  94  55  63 249 252  30
 188 256 255 263 166 127 239 306 260  93  73 221 267 297 229 147 177 266
  19 269 316  61  16   9 312 257 113  79  72 106 302 274 171 225 174   0
 140 135  34 143  10 118 170 175   4 304 214 138   6  97 289 209  60  67
 273  45 154 292  57 211  86 253  22 100 176 161 185 213 259 227  46 149
 167 195 153 246 117 159 305 205 299  53 262 164  48 128 184  78   5  37
 192 310  18  39  14  17 293 244 278 308  31  42 307 281 116 296 311 241
 107  66 193 172 158 102 254 115 282  47 163  21 284  76  29 187  44  26
 181  80 156 216 240 157 280  96 309 160  54 219  75 283 104 291  87  74
 231  36 233 162 124 285 200  81  51 220  77  85 114 109 215 313 125 119
 279 222 207 182  24  35  38 120 122 276 203 258  32 180 277 223 111 191
 199 194 179 105 202 189 201 198 110 108 121 123]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0422
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0586
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.011
INFO cross_voc_dataset_evaluator.py: 134: 0.090
INFO cross_voc_dataset_evaluator.py: 134: 0.023
INFO cross_voc_dataset_evaluator.py: 134: 0.050
INFO cross_voc_dataset_evaluator.py: 134: 0.027
INFO cross_voc_dataset_evaluator.py: 134: 0.076
INFO cross_voc_dataset_evaluator.py: 134: 0.028
INFO cross_voc_dataset_evaluator.py: 134: 0.019
INFO cross_voc_dataset_evaluator.py: 134: 0.020
INFO cross_voc_dataset_evaluator.py: 134: 0.081
INFO cross_voc_dataset_evaluator.py: 134: 0.047
INFO cross_voc_dataset_evaluator.py: 134: 0.020
INFO cross_voc_dataset_evaluator.py: 134: 0.018
INFO cross_voc_dataset_evaluator.py: 134: 0.284
INFO cross_voc_dataset_evaluator.py: 134: 0.085
INFO cross_voc_dataset_evaluator.py: 134: 0.089
INFO cross_voc_dataset_evaluator.py: 134: 0.049
INFO cross_voc_dataset_evaluator.py: 134: 0.068
INFO cross_voc_dataset_evaluator.py: 134: 0.043
INFO cross_voc_dataset_evaluator.py: 134: 0.042
INFO cross_voc_dataset_evaluator.py: 135: 0.059
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.391s + 0.003s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.359s + 0.004s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.366s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.369s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.381s + 0.004s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.379s + 0.004s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.378s + 0.004s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.374s + 0.003s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.373s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.377s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.375s + 0.004s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.374s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.374s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.432s + 0.014s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.378s + 0.004s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.375s + 0.004s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.371s + 0.004s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.366s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.371s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.379s + 0.004s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.374s + 0.004s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.369s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.367s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.365s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.364s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.366s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.438s + 0.004s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.374s + 0.004s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.361s + 0.004s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.367s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.363s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.362s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.356s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.361s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.362s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.361s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.360s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.361s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.363s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.442s + 0.004s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.373s + 0.004s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.372s + 0.004s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.358s + 0.003s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.354s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.350s + 0.004s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.356s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.355s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.358s + 0.004s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.356s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.355s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.356s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.357s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 54.011s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 48  21  63 128  74  85  23 115  58  75 111 119  86  70  62 132  56 110
  44  45  36  33 131  31  41 125  60  95  20 101 164 162  28  89 158 168
  35 169 171 154 148  49  39  88  87  42 146  71 129 172  69  27 134 123
 160 126  83  38  34 106  46  64  25  91  54  84  96 174 153 180  26 156
  30 170 151 136 173  47 159  53 117 107  68  51 145 167  52  59  29 127
  55 144  22  93  24 100  50 114 186 121  40  12 152 176 133 105  66  99
  92  14 161  61  72 150 139  73  32 166  37  57 149 157 175 113   6  90
 179 142  11  82  19 104 147  97  43   0  65 185 122 118  67 143  81 165
 120 137  98 112 163 140  17 177  16 102 138 181 103   2 178 155  13   1
   4   5 116   9   3   7   8 109  80 141  15  94  18  10 135 130 124  77
 184 108  78 183  79 182  76]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0105
INFO voc_eval.py: 171: [133  99  59  50  16 266 253  66  25  43 265 129 256  13  64 149 128 124
 251 119  31  56 237 146 100 241 249  52 171 138 132 263  93  91 254 105
 140 130  23 177 236 192 152 238  65  61 169 180   5 261 248  32  22 255
 150   7  17 101 247 145  33  67 245 244  49 243  88 135 143 136 181  62
  60  10  12 131  14 262  45 117  68  26  19 264 115  18 252  20 242  47
  48 151 167  57 109 141  97 114 186 147  79 176 179 185  15  11 142 112
 190 240 246   6  94 250 166  34  27   3 259  63 178 172  21 258 106 123
 257  74  89 260  55  30 159  95 196  39  44 204 154   8 162  85  69 137
 156  77 153  24 239  87 161 113  46 193 122 121   9 197 174 209  29  28
 125  81 218 188 144 189 191 155 198 213 163 215 110  92 214  71 173 210
 170 201 199 187 175   0 227 205 216 158  40  96 126 168  53 208 233 195
 134 182  78 157 202  36 225  98  70 219 220 165  72 164  86   4   2 222
 183 221 184 148 139 211 104 228  80 194 212 217  38 160 203 231  73 226
  82 230  35 120 223 232 108 229 127 235   1 207  90 206 234 116 103  83
 224 200  37  58 111  51  42 107  54  41 102  76 118  84  75]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.1083
INFO voc_eval.py: 171: [ 259 1051  119 ...  639 1219 1225]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0338
INFO voc_eval.py: 171: [318 677 305 ... 248 136 140]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0500
INFO voc_eval.py: 171: [258 244  71 145 240 228  73 292 254  74 249  50 188 182  66 410 131 233
 312 230 202 221 232 214 339 124 304 298 286 398  67  56 415 210 218 200
  64 289 243 252 362 365 197 317  70 272  68 185  59  58 184 119 139 407
 273 399  72  40  65 212 296  51 127  69 132  93 156 369 334 187 189  94
 226 144 309 279 318 237 343 364 183 303 209 211  41  75 198 217 260 358
 159  79  29 387  62 229 155 142 195 266 190 330 371 250 138  34 324 291
 295 370 301 267 310 287 396 380 120  57 236 130 122  78 305 342 173 322
 222 336 118 282 162 341 178 193 355 314 297 271  61 239  98 125 223  82
  19  76 277 129  77 192 160 406 224 404  39 367 157 306 331  87  83  43
 135 276  80 278  42 261 121  17 256  52 378  92 177 315 274  95 321  96
 196 264 420 246  85 161 128 326 288 372 123 262 368 283 413  54  35 241
 137 280 293 265 319 270  30  84 158  88  55 165  47 316 149 284  81  33
 263 285 146 136 299 329 328 213 313  44  13 174 205  21 311 375 337 377
 333 143  91 247  37 235 150 412 351  46 259 356  36 186 171 379 332 320
 225  89  97 169 327 151 290 154 392 354  49 242 340 338 308 323 234 126
  48 366 325 294 357 300 194 281 349 167  23 268  60 238 113 110 405  53
 384 111 219  86   9 419  31 335 112 179 245 302  22 307 116 147 275 133
 416 141 397 269   2 140 208  90  15 373 220  38 148 417 134 117 363  45
 115 175 201 388 163 180 153 152 382 164   7 216  32  11  63 350   0 170
 204 255 203  24 353   5 199 386 176  26 376 414 257 401   6 345 168  20
 411 166 227 421 181 207 191 114 206  18 344 172 348 346 215  14   1 385
  25   4 408  10 400 361   3  12 231 381  28  16 101 390   8 352 418 395
 104  27 347  99 374 109 105 389 409 100 402 403 102 359 103 393 107 108
 391 106 248 422 253 251 360 394 383]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0281
INFO voc_eval.py: 171: [109  93 114 110  74 106  90  98 102  89 107 105  66  72  92  77   5  68
  64   1 100  88  30  69  71 111  40  18 116  95  60  63  76   8  11 104
 108 115  25 101 118  82 112 103  80  21  99  14  17  91  75 113  84  49
  24 120  79  13  94  83 117 122 121   2  70  62  56  39  59  16  28 124
  27   4  81  20  10  46  65  54  53  48  67  73  42  22  96  45 125  55
  26  41  85   7  44   6  23 127  57  50  47  29  31  78 126  97  19 119
   9  12  36  61  35 123   3  52  43  15  58  37  32  38  33  86  87  34
  51   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.1257
INFO voc_eval.py: 171: [651 395 369 ...  41  43  44]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0313
INFO voc_eval.py: 171: [19 17 24 61 38 20 11 34 58 64 13 47 35 51 62 84 75 16 86  9 28 14 18 87
 30 56 29 88 72  8  6  7 59 43 23 85 10 73 49 78  5 69 46 82 54 31 37 21
 60 74 32 55 40  0  2 22  4 12 53 63 44 80 27 66 52 83 36 25 45 41  3 33
 68 79  1 65 76 48 15 67 26 71 70 77 81 39 42 57 50]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0202
INFO voc_eval.py: 171: [ 595 2372  578 ...  457 1863 1279]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0247
INFO voc_eval.py: 171: [222 181 179 161 230 131 183 160 152 149 118 109 123  87 105  98 100 186
 142 104 223 231 229 101 281  82  76 108  67 134 140 141 146 103 157 136
 227 176 171 139 116 137 135  71 129 165 240 122 106  89 224  99 102 114
 174  70 173 147 172 150 158  39 167  80 127  78 159 155 154 242 124  84
  94 178  96 233 185 234 113  91 170 276  92 138 107   9 111 236 144 115
 110 162 225 166  93  95  48  97 126 244 112 237  73 133  68 309 148 151
 180 121  77 128 272 153  28  90 164 117 188 120 163 119 319 175 156 168
 132 130 274 273 184  43  53  75 221 145 311  44 277 125  51 143   4 182
 177 248 187  56  81   8 228 270  55 257 323 217 243  65 308  58 169 212
 271  26 235 214  25 209 254  83  21  54  52  85 282  12 201 286 322  14
   3  42 304  72  35  57 262 278 245 197  11 317 279  79  37 321 208 287
 195  29 310 256 318 199  41 280   7 305 268   1  62  27 275 246 284  18
  10 200 219 301  69  74 190 266 216 192 213 302 285  86 247  22 218 300
   5 313 269 315 206 193  31  34   6  63   2 252  45 232 211  60  23  16
 265 283  13 239 189  33 251  46 307 198 289 194  49 260 306 191  50  19
 196 203  88 238   0 207 299  66  36 202 325 241 226 253 249  32  64  38
 261  20  15 220 215  59 320 259 293 267  61 210 288 324  40  24 204 258
 303 290  30  47 316 291 264  17 326 298 292 205 312 314 294 263 296 255
 297 250 295]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1391
INFO voc_eval.py: 171: [424  94 411 284 285  92 289  88  87 210  86  85  84 462 201 398 396 499
 468 392 412  98 283 445   2 423 261 418 420 264  12 449 435 233 266  17
 235 102 478 341 344 391 441 469 113 394 425 105 419 464 404 422 407 450
 446  62  89 408 413 100 481 290  13 205 293 513  40 208 202 238 480  19
 270 280 527 276 292 229 245   8 483 309 485 248   4 272 260 302 196   7
 503 488 251 257 426 123 429 230 237 329 414 336 307 375 164 163 389 306
 199 200 399 401 301 211 212 410 288 279 416 430   0  24  28 472  23 467
  77 533  20 465 463  33  34 515  65 479  97 514  69 506 437 497 447 512
   9 112 487 484 281 492  32 209 368 326  61  35 213 345 165 431 339 335
 218 256 504 259 111 451 247   1 241 385 538 455 456 409  15  83 440 403
 267  78 427 428 531  46 436 104   6 509  26  39  11 537 517 524 282 529
 207 239 432 198 406  95  70 473  96 168 376 170 138 400 175 402 489 477
 131 255 332 330 187 535  90 443 140 252  99 265 421 242 243 541 262 542
 540 390 153 197 303 300 510 180  37 214 269 188 374  67 384 471  75 367
 362 519 380 500 366 273 101 172  14  29  63 173 195  22 448 215 505 298
 203 417 348 137 494 369  68 470 439 169 319 116 433 340  51  49  47  45
 320 317 507 120  38 128 525 125 453  21 521 181 184 263 528 236 476 115
 160  93 442 393 438 219 490 347 305 304 523  30  42 206  53 231 176 133
 373 161 150   3 271 495 220 178 277 434 299 546 152 232 371  10 349 286
 405 395 227 124 155 234 130 452 337 496 508 107 108  16 110  74 397 177
 217 162 182 216 179 415 482 147 539 191  18 103 518 119 117 461 129 454
 258 253 223 297 295 350  73  71  66 118  91 109  56 493 475 268 547 338
 444 141 343 142 522 458 139 144 159 516 536  72   5 351 185 174 356 106
 312  36  48 327  64 325 114 240 136 474 254 157 543 382 316 250  82  81
 466 372 122 545 370 310 502 171 530 149 244 333 146 246  43 460  80 121
 315 334 486 323 249 354 331 520 491 166 346 132  60 324 287 322 501 379
 148 526 308 143 377 544 126  79 353 167 226  76  58 225 386 328  25  41
 498 156  27  44 357 275 294 204 511 274 221 158  52 313 296 278  31 291
 194 459 361 534 127 321 318  59 360 222 224 365  55 364  57 532 134 388
 192 183  50  54 154 186 363 228 190 145 311 342 151 135 381 457 193 359
 314 352 358 189 387 378 355 383]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0454
INFO voc_eval.py: 171: [ 21 111  22  19  38  31  34 180 175 112  12 119 169 133 168   5 120 118
 165 108   9 129  65  67 149 122   6 121  35  74  28 179 147  97  10 139
  17 101  68   2  24 115  40 141  53  93 124 128 123  46 148 127 177  90
  95 143  20 171  29  75 131  23  14 113   8 107 105 167  78  32  54 146
 114  91  71 140 150  36 172   0  26  96 173 103  63   7  25 170 174 130
 162  80 136 135  39 178  77 116  83 110  13 181 151 138   3  11 142  92
 153  50 126 100 164  64 159 166  47  37 134  69  49  51  72  79  52 109
 184 125 132 145  99  43  33   4  66 154  16 104 144  41 161 157  98  88
 188  81  86  15 152 117 106  42 102 160 185  94 156  48 183 176  62  76
 186  87 182  82 163  27  85 137  44 187  84  70 158  45  18  89  30  61
  73   1 155  57  56  58  59  55  60]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0207
INFO voc_eval.py: 171: [ 83  77  79  82 184  34 176 178  41 226  44 385 227 229  48 183 224 341
  58 245 243  62  64  65 152  67  68  70  87  30  72 143  29 141 142  20
 159  22  10 144 134 217 147 149 331 214   7 145 368 151  75 139 138 175
  90 291 340 114 342  74 276 248 161 180 326  96 123 127 158  94  92 135
 333  76  38  23  17 223  73  28  16  14  31  12  18  11  49 211  51  19
  52  59 188 376  54  61  56 380  78  63 148 165 274 156 327  24 157 286
  66 215  57  13 281  37 280  91 420 172 386 181 355  15 299 278   9 306
 239 153 315 349  35  88 201 251 231 332 168 271 160 317 171 238  85 423
 207 407 388  21 137 206  25   8 330 198 216 416 415  32 382 329 146  98
 345 277 208 205 405 381 337 106 373 162 111 219 344 418 310 179  43 235
 267 213  80 116 110 182 334 212 210 253 384  40 358 115 304 272  26 126
 203 202 328  50 236 365  97 261   5 195 225 228 314 352 117  69 374 323
 434   0 132 197   3 190 196 185  53 287 290 301 209 107 105 419 364 348
 292 282 164 361 325 191 409  33  81 300 372 169 204 371 335 362 256 369
 319 192 408 414 266 357  47 433 393 189 402 283 308 240 279 410 140 199
 422 322  45 339 303 112 194  42 403   1 167 244 200 377 155 379 218 249
 108 133 193 391 125 311 254 360 400  71 220 233 163 395  60 406 338 230
  95 426  86 247 269 166 273 305 289 252 241   6 324  99 150 424 302 421
 288  36 320 404 350  46 378 367 285 104 262 412 359 275 430 270 221 387
 411 130   4 321 347 312 370  27 396 118 313 375 427 109 298 413 232 293
 398 435 129 136 260 397 356 389 268 234 296 177 390 343 309 154 394 383
 263 259 428 102 318 431 246 128 222  39 187 363 295 264   2 120 258  89
 307 124 294  84 113 392 242 366 336 437 354 255 316 425  93  55 100 131
 284 257 170 429 122 351 401 399 346 297 250 432 353 173 237 119 103 186
 121 174 436 101 417 438 265]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0196
INFO voc_eval.py: 171: [ 39 118  47  73  40  11  55 111  92  85 114 133 127  50  17 110 116  52
  44  27  79  86  42 128  29   9   7   4  41  43 124  88  48  58  64 122
 101  87  72  46  13  89 130  23  54 129  12  30  61 120 109 123  91 121
  71  76 105  93  97  98  63  84  70  90 113  36 117  81 126   3   8 119
 125  20 106  53  95  69  33  66  56  65  51 112 131  45  19   2 102  18
 100  77  74  99   5 107  67  62 135  34 132 115  80 103  59 134  38 136
  57  96  94  68  83   1  10  60  49 104  35   6  32  37  78  75  82  28
  24 108  16  15  25   0  14  26  31  22  21]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2843
INFO voc_eval.py: 171: [27537 27557 27551 ...  5236  5204  5227]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0638
INFO voc_eval.py: 171: [1085  729  772 ...  455  465  484]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0885
INFO voc_eval.py: 171: [124 100 127 115  77 148 133 109 112 149 104 230 131  99  96 174 187  89
  87 229 184  98  78 167 156 136 197 219  85 217 175  13  43 103 113 155
 145 186 182 210  40 135  42  36 144  51 195 188 146  80 106 139 192  53
 119 260 125  27 160 220 168   4 181 105 130  14  22  38  29  91 147 223
  41  57 196 172 170 110 157 204 171  12  35  94  25  46 228 173 163 216
 193  24  11 227 211 152 151  44 225 200  56  23  31 165 164  97 161 178
 202 221 159 101 179 243 226 177 189 150  79  28  33 162  90   7 169 154
  37 215  34 222  39   1  88 176  50 231  20 128 185 207  48 158 153 166
 205 122  32 126 252 194  19   8 212  15  10  30 214  26 116 138   0  81
 206  75  86  76  92 141  21 218  93 209 183 190 108 140 134 199 180  45
  83  64  68  72 137 224 121   2 118   5  71  74  16  17 129  61 208  18
  58  73  66   6  95 111   3 213   9 120 240 142 117 107 123  63 102  82
 232  84  49  47  55 143 132 203 114 250  67 238  52 254 198 201 251 236
  59 191 234 253 246 248 237  60 233 245  62 242 259 257 244 255 247 256
  54 239 241 249 235  69  70  65 258]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0495
INFO voc_eval.py: 171: [120   0  30  41 125 119  35  16 114 118   4 113  34  40   2  26  25 127
   9 124   3 115  14   8  11   5 111   7  13  29  39  37   1 121  38 116
  63  99 160 140  24 109  17  50 135  62 157 158 165 117  97 123 156  58
  76  12  72  43  33  86  10  66 105  57  23  74 139  71 159 128  64  27
  96 161  83  87  70 126 147 167  68 134 162  90  79  59  92 154 103  53
 122  49  78  84  94  28 132  75 149  80  60  47  56 107 100 133  31 164
  89  61  21  55 112  48 106 148  36  45  32  95  98   6  44  91  81 166
 142 163  73 108 146 102  93 151  82  15 155 152  85  67 101 138  88 136
 153  77 130  22  18  42 143  65  20 104 137 129 131 144 141  51  19 145
 150  69 110  46  52  54]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0834
INFO voc_eval.py: 171: [180 271 116 279 259 288 260 130  17 202 283 270 185 186 191 261 117 274
 114 277 110 211  26 208 201  53  55 189 265 290 182 183 205 263 268 276
 126  56  33 131 197 141 176 170 200 188 295  57 245 199 132 285 113 225
 239  75 267  42 293 173 142 129  25 112 215 204 196 269   0  40 236 155
 240 118 272  37  58 135  23 181 213  50 214 273 121 278 264  31 216  24
 203   1 187 119 241 266  54 198 184 147  88 175  29  18 154 289 248 244
 258 138  67 237 111 133 115 190  85 233  62 166 137  72 291  80  64 161
 177  71  70   6  49 122  15  39  16  73 275  13 281  89  46  61 297 156
 287  74 172 143 254   5  76 165  19 144 296  69  35 299 298 192  20   4
  93 160 168 222 125 252 284 171 246 221 128 247  52  51  59  28 105 228
  30 151 226 223 193   9  48 207  91 124 206 179 107 167 209  78 108  86
  34 139 304 174 123 280  43 238 257 146  41 134 219 217  11 234 242 178
  12 194 253  94 230 101 227  60 210  92 152 251  44 243 195 162  77  38
 300  27  36 148 255 140 159 224 120 232 169   2 229 231 127  81 294 256
 302 136  79  47  21  14   8 149 235 145 220  83 282  66  87 157 286  10
 250  65  96 163 158 150 301 109 292  32 262 153 303 249 106   3  63 218
 164  45  82 102  22  95  84 100  98  99   7  97  68 103 212  90 104]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0354
INFO voc_eval.py: 171: [ 15 183  53 127  69 298  20  12  40 148 151 243 286 206 137 292 226  63
 186  25 138 237 212  91 133 299 236  89 261  13  11 262   3 210 197 268
 218 224 228  99  72  93  92 247  83  57  66 238 178 130  27  28 288 301
   8 150 141 152   1  41  84  43 132 273 142 169 250  96 144 266  23 102
 293  33  59 205 270 296 315 235 234 104 145 147 135 134 165 284 263  65
  70 248  60 242  50 312 113 208 173 196 168 100 217 269 232 190  49 313
 230   2 285 146   7  56 131 155  85 245  90  95  71 251  64 166 249 252
 128 256 188 255 260 239  30 304 221 229 295  94 265  74 264 177  19   9
 267 314  62 107  16 310 257 114  80  73 300 171 174 225 272   0  34 136
  10 143 140 302 119 170 175   4 214   6 139  98 287 271 209  68  61  45
 154 290  58 253 211  87 101 161  22 176  46 227 213 259 185 149 195 167
 153 303 246 118 159 204 297 164  54  48 129 184  79 308   5  39  37  18
 192  17  14 291 244 276 306  31 305  42 117 279 108 294 309 241  67 172
 193 103 254 158 116  47 280 163 282  21  29  77  44 187  26 181  81 216
 156 240 157 278  97 307 219 160  55 281  76 105 289  75 231  88  36 233
 125 162 283 200  82  51 220  78  86 115 110 215 311 126 120  52 277 222
 207  35 182  24  38 121 123 274 203 258  32 180 275 223 112 191 199 194
 179 106 202 189 198 201 111 109 122 124]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0564
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0659
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.011
INFO cross_voc_dataset_evaluator.py: 134: 0.108
INFO cross_voc_dataset_evaluator.py: 134: 0.034
INFO cross_voc_dataset_evaluator.py: 134: 0.050
INFO cross_voc_dataset_evaluator.py: 134: 0.028
INFO cross_voc_dataset_evaluator.py: 134: 0.126
INFO cross_voc_dataset_evaluator.py: 134: 0.031
INFO cross_voc_dataset_evaluator.py: 134: 0.020
INFO cross_voc_dataset_evaluator.py: 134: 0.025
INFO cross_voc_dataset_evaluator.py: 134: 0.139
INFO cross_voc_dataset_evaluator.py: 134: 0.045
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.020
INFO cross_voc_dataset_evaluator.py: 134: 0.284
INFO cross_voc_dataset_evaluator.py: 134: 0.064
INFO cross_voc_dataset_evaluator.py: 134: 0.089
INFO cross_voc_dataset_evaluator.py: 134: 0.049
INFO cross_voc_dataset_evaluator.py: 134: 0.083
INFO cross_voc_dataset_evaluator.py: 134: 0.035
INFO cross_voc_dataset_evaluator.py: 134: 0.056
INFO cross_voc_dataset_evaluator.py: 135: 0.066
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.426s + 0.003s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.363s + 0.003s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.363s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.360s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.364s + 0.003s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.365s + 0.003s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.366s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.366s + 0.003s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.364s + 0.003s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.365s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.366s + 0.003s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.368s + 0.003s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.371s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.540s + 0.004s (eta: 0:01:07)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.378s + 0.003s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.382s + 0.004s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.369s + 0.004s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.368s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.376s + 0.004s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.384s + 0.004s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.381s + 0.004s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.377s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.377s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.374s + 0.004s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.374s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.372s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.521s + 0.005s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.371s + 0.003s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.372s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.369s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.367s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.365s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.363s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.361s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.364s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.363s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.366s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.366s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.365s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.450s + 0.002s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.354s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.363s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.368s + 0.004s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.371s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.365s + 0.004s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.369s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.365s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.365s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.364s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.363s + 0.004s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.364s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.364s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct29-17-33-06_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.956s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 50  90  23  77  66  21 136  91 140  45 115 127  60  61  73 121  78 133
  65  33  35  20  41  63  46 139  31 100 118 107  94 171 170  28 175  36
 165 176 161  51  93 177  40 155  43  74 153  92  71  26 137 179 142 131
  34  86 111 134 167  39  47  25  67 101  96  56 180 187 160  89  27 178
 163  30 158  87 124  49 181  55 166  72 112  53  54 174 152 135  62  29
  57  22 151  98  24  52 105 120 129 193  42  11 159 141 183 110  70 122
  97 104  14 168  64 157  76 146  75 173  32  38 156  37 182 117 164  12
  95   6  58 149 186  85  19 109 154  44   0  68 130  59 125 192  69 150
  84  88 172 144 128 102 116 147 169  48  17 103 184  16 145 188 108   2
 185 162  13   1   4   5 123   9   3   7 114 148   8  83 132  15 138 119
  99  18 143  10 126 191  80 113  81 190  82 106 189  79]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0106
INFO voc_eval.py: 171: [133  99  59  50  16 266 128  31 149  25 256  23 124 119 251 177 253  64
  13 129 265  43  66  56 249 146 100 241  52 238 171 169 130 140 138  91
  93 263 132 254 105  61 193 237 152 239  65 180   5 261 248  32  22 255
 150   7  17 101 143 145  33  67 245 244  49 243 247  88 135 136 181  20
  26  68 117 262  45  47  19  14 131  12  10  60  62 264  18 115 252 242
  48 151 168  57 109 141  97 114 186 147  79 176 179 185  15  11 142 112
 191 240 246   6  94 259 166  34   4 250  27 178 258  63  21 106 172  89
  44 197  39  95  74 257 123  30 159 260  55 203 154 162   8 153  85  69
 137 156  77  24 161 113   9  87  46 195 122 121 208 174  28 144  81  29
 188 125 190 217 155 198 192 163 214 212  92 110 213 173 209  71 170 187
 199 201 189   0 175 226 215 158  40  96 167  53 126 207 233 134 205 182
  78 157  36 224  98  70 196 218 165 219  72 164  86   2   3 221 183 220
 184 148 139 210 104 227 202  80 211 216 160  38 231  73 225  82  35 230
 120 222 232 108 229 127 235   1 206  90 204 116 234  83 103 194 223 200
 228 236  37  58  51 111  42 107  54  41 118 102  76  84  75]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.1084
INFO voc_eval.py: 171: [1056  260  120 ...  643 1224 1230]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0286
INFO voc_eval.py: 171: [125 678 320 ... 250 137 141]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0500
INFO voc_eval.py: 171: [258 244  70 240 146  72 228 249  50  73 290 254  75 311  66 183 189 408
 132 229 233 215 284 297 221 232 303 203 338 125  56 201 252 211 413  63
 243 219 287  65 397 186 316  69 271 197 364 361  67 273  64 120  71 128
  59  58 140  51 398 405 295 214 185  40 145 133 368  74 188 157  94  95
 226 333 190 277  68 308 237 184 317 342 363 210 302  76 212  41 260 199
 217 143 386 230  62 156 160  29 357  80 250 139 370 196 192 329  34 266
 123 309 379 236 131 285 289 293 121 267  57 322 369 395 300 119  79 280
 335 341 321 222 174 304 354 194 313  83 163 296 340 179 223 275  78  19
  61  77  99 270 130 126 239 366 193 402 161 404 158 224  39 276 305 261
  81 330 136  42  88  17  43 274  85 122 178 314 377 256  93  52 320 272
  96 129  86 419 264 246  97 198 162 325 286 262 281 367  54 124 371 412
  35 278 241 138 291 265 315 269  84  89  47  55 159  30 318 283 150 282
  82 166 147 137 328 263  33 298 312 213  13 206 175 327  44  21 336  92
 310 332 374 376 144 151  37 350 410  46 259 355 235 247 187  36 319 172
 378 331 225  90 326  98 170 288 152 155 339 353 337 307 242  49 391 323
 234 127  48 365 324 292 356  23 279 299 195 348 168 268  60 238 403  53
 111 114  31   9 218  87 112 383 418 180 334 301  22 245 113 306 117 148
 142 294 134 415  91 396 141   2 209  15 372 220  38 149 416 135 362 118
  45 176 164 387 202 154 116 181 153 165 381   7  32  11 216 349   0 255
 171 205  24 204 352 200   5 385  26 177 414 400 375   6 257 344  20 169
 409 167 227 182 191 208 115 207  18 343 173 347 345  14   1  25   4 384
 406 399  10   3 360  12 231  28 380  16 102 389   8 351 411 417 394  27
 105 346 100 373 110 106 388 407 101 401 103 358 104 392 109 108 390 107
 248 253 251 420 359 393 382]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0284
INFO voc_eval.py: 171: [110  93 114 109  74 106  90  98 102  89 107 105  66  72  92  77   5  68
  64   1 100  88  30  69  71 112  40  18 115  95  60  63  76   8  11 104
 108 116  25 101 118  81 111 103  80  21  99  14  17  91  75 113  84  49
 120  24  79  13  83  94 117 122 121   2  70  62  56  39  59  16  28 124
  27   4  82  20  10  46  65  54  53  48  67  73  42  22  96  45 125  55
  26  85   7  41   6  44  23 127  57  50  47  29  31  78 126  97  19 119
   9  12  36  61  35 123   3  52  15  43  58  37  32  38  33  87  86  34
   0  51]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.1257
INFO voc_eval.py: 171: [1871  366 1850 ...   40   42   43]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0329
INFO voc_eval.py: 171: [19 17 24 38 61 20 11 34 60 58 13 51 35 47 62  9 74 16 83 85 28 14 18 29
 56 30 86 71 43 87  8  6 59  7 23 63 84 10 72 49 77 68  5 46 81 31 54 37
 73 21 32 55 40  2  0 22  4 12 53 44 79 27 52 65 82 36 25 45 41 33  3 67
  1 78 64 75 48 15 66 26 70 69 76 80 39 42 57 50]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0227
INFO voc_eval.py: 171: [ 597  565  566 ... 1867 1874 1282]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0283
INFO voc_eval.py: 171: [222 180 245 179 129  98 163 109 100 152 149 123 159 140 104 118 182 229
 185 105 223 103 144 132  75 147 135 108 139 101 158  66 169 282 174 227
  82 224  70 172 136  88 116  69 114  99 232 238 171 160 102 133 157 106
  38 165 146 148 167 170  84  78 122 126  91  76  80  96 233 240 187 138
 183  94 150 154 156 176 113  92   8 168 111 107 137 235 277 134 166 110
 161 225 115 142  47  97 236  93  95  72 310 125 112 131  67 242 178 151
 121 145  77 153 273 127  25  89 177 117 162 120 164 173 186 155 119 320
  41 274 130 275 128 184  74  51 221 143  42 278 312   3 181 124  49 247
 175  90 141  54  81   7 228  53 271 256 243 325 309  65 230 217  56 212
 272  35 234  23 253 214  22 209  83  50  52  85 283  11 201  13 287 324
  40 305  71  32 263 241  55 197 279 318  10  79 280  34 322 208 311 255
   0 199 288 319  26 194 306   6 281  39   5 269  24  61 276 285 244 302
 200  68   9  73 219  16 267 189 216 213  86 191 246 303 286 301 218  19
   4 314 316 206 270 192  31  28 251   2 231 237  43  62 211  60  20 266
 284  14 308 250  12 188  30  45 198  46 260 290 193 307 196 190  87  18
 203  48   1 207 300  33  64 252 248 226 239 202 327 262 195  63  29 220
  36  17 259 268  57 321 215 294 210 326 204  37 304 258 289  59  21  27
 317 265 293  44  15 328 313 205  58 299 292 315 264 291 295 323 297 254
 249 257 298 261 296]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1501
INFO voc_eval.py: 171: [  0  95 426  91 272 270 267  90  87 401  86 506 432 405 431 407 428 239
 205 237 214 420  96 443 421 475 291 102 292 104 296  17 453   9 290 457
 470 200 493 491 477  99 427 510 212 489 488 206 209 472 117 116 496 433
 458  88  89 454 449  93 430  92  63 283 266 276 279 422 287  22 297 300
 400 301 310  11  10 317   5 352 349   1  13 256 486 403  41 252 534 233
 520 416 417 243 413 434 309 471 419 215 315 473 216 476 424 344 423 313
 455 204 337 258 264 398 408 244 437 384 438 286 411 445 167 203 166 295
 235 480 334 487 106 101  80 513  71  66 503 519  35  34 540  27  26  23
 521 495 522 126 492 436 346 439 134 265  36   3 435 263  85   4 545  82
 511 353 255 377 213 217  47 222 412 115  62 236 538  33 418 444 448 105
 459 168 500 273 343 463 464 288 394 536 211  72  16 385 516 409 544  29
 410 531 497 415 289  40 485 202 178 173 440  20 171 524 190 340 481 141
 262 399 338 271 249 383 393 275 376 259 268 542  38 248  18 218 429 308
  81 191 504 548 311  97 183  68 549 100 201 143 451 517 479 156 547 199
 176 103 280 389  25  31 175 526 371  12  64 245 478 219   6 375   7 356
 378 456 327 119 306 501 507 425 532  48 172 441  69 348  46 207 140 512
 447 328 325  52 131 123 528  50  39 514 118 269 184  24 223 128 187 355
 163 107 535 461  54 446 450  98 498 179  76 402 210 312 136 484 530  32
  15  43  74 314 505 382 357 277 108 224 234 153 164 307 380 285 414 442
 553 181 155 109 460 293 404 127 231 158 133 502 246 345 515 221 165 182
 185 220 406   8 490 180 112 114 111 151 546 195 241  19 305 462 242 525
 122 260 469 227 303  75 132 120 358  73  67  94 483  57 121 274 113  14
 554 347 452 351 529 144 145 466 147 543 523 142 162 188 320 365 359 110
   2 177  37  49 335  65 247 333 482 139 161 261 324 391 550  84 474 381
 552 240 125 379 318 509 174 537 152 341 253 250 149 468  83 323 254 342
 124 494 331 257 363 339 499 527 354 135 169  61 332 294 330 508 388 150
 238 533 316 146 386 551 129 362  79  59 230 229 395 170 336  42  28 159
  44  30 282 366 302 208 281 518 160 225  53 321  21  70 198 284 304 251
 298 278 370 467 541 326 329 130  60 369 228  58 226  56 299 374 373 397
 539 196 186 137  51  55  78 157  77 189 372 193  45 232 319 148 350 154
 138 390 197 465 368 322 360 192 367 194 396 387 364 392 361]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0547
INFO voc_eval.py: 171: [ 18  31  26 110  24  37  19 168 111 169  11 118 181 132   5 175 165 117
 119 107  34 148 120 121  64  66   6 128  72 180   9  15 146 138  95  23
  67 100   2  52  20  39  92 123 114 127 140   4 126 122 178  45 147  89
 142  93 171  74  21 130 112 104 167   8 106  77  32 145  29  53 113  90
 149  70 139 172  35  94   0 173  27   7 102  62  25 170 174 129  79 162
  38 179 135 134 115  82  76  16 109  13 182 150  10 137   3  49 152 141
  91 125  99 183 158  63 159 166  36  73 133  68  46  48  50  78 164 108
 124 186 131  51  22  98  97 144  12  33  14  42  65 103 153  40 161 143
  87  96 156  80 190 116  85 151  41 105 101 160 187 185 176 155  61  81
  47 184  86  28 163 188  84  75 136  83 157 189  17  43  44  88  30  69
 177  60  71   1 154  56  55  57  58  54  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0217
INFO voc_eval.py: 171: [ 76  74  79  80 180  63 179 143  29  30  62  84 174  37 337 145 381 148
  44  46 173 239  24 155  61  65 327  66 210  58   7 220 222  10 130 241
  69 137 138 223 139  19 225 140  68  73  70  72  71 376 147 338  89 141
  92 135 134 131 123 272 119 287 336  87 244  60  13  14  15  16  18  20
 184  25  28  31 154 176 171  12 321  52  59  56 157 110  53  39  51  11
  49 329 213  41 219 270 207 276 277 211 177 144 364 382 168 161 153 372
 351 282 323 416  88  26  57  64  78  34  85 235  75 247 295 152 156 311
 164 167 149 227 313 345 328 274 197 303   9   8 419 203  82 403 202 133
 267  27 234  22 384 326 325  32 212 194  23 142 341 401  95 201  21 333
 411 215 204 369 273 158 378 412 377 175 231 107  36 102 414 340 209 263
 307 112 208 106 206 330 178  43 300  50 268 199 380 354 232 258 324 111
 198 191   5 122 361 310  94 113 249 221 370 224  40 430 348  67 128  17
 319   0 186   3 181 192 193 283 286 296 288 205 278 360 357 415 103 101
 344 368 160 165 322 187 200 299  77  33 331 365 367 405 262 410 358 188
 315 404 429 389 353 304 279 397  48 185 252 136 275 236 418  47 318 195
 406 335 298  45 399 190 108   1 151 163 214 375 196 245 240 373 129 189
 104 250 121 387 356 396 216 253 229 159 391 334 226 422  83 265 402  93
  55 243 301 285 302 162 269 248 237 297 146 420 417 320   6 284 316  96
  90  38 363 400 346 374 281 217 355 271 408 100 266 426 383 126 317 343
 407 308  35   4 366 114 392 371 309 423 409 294 228 105 257 132 289 125
 352 431 394 393 385 230 150 264 292 339 172 306 390 259 424 386 379 314
  98 242 427 218 124 183  42 359 291 260 116   2  86 120 290 305 255  81
 362 332 238 251 388 350 312 421 109  54 256 433  91 127  97 280 118 166
 254 425 246 293 347 342 398 395 428 169 115 233 349  99 117 182 170 432
 413 434 261]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0208
INFO voc_eval.py: 171: [ 39 119  47  40  73  11 115  55 112 134  50  92 128  85  17 111  44  52
 117  27  79 129  86  42  29   9   4   7  43  41 125  88  48  58 123  64
  87 101  13  72  89  46 131  30  23  12 130  54  61 121 110 124 122  91
  71  76 105  97  63  98  93  84  70  90 114  36 118 127  81   8   3 120
 126 106  20  53  95  69  33  56  66  65  51 132 113  45  19   2  18 102
 100  77  74   5  99  67 108  62 136  34 133 116  80 103  59 135  38 137
  94  96  57  68   1  83  10 104  60  49  35   6  32  37  21 107  78  75
  82  28  24 109  16  15  25   0  14  26  31  22]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2843
INFO voc_eval.py: 171: [6437 7154 7151 ... 5187 5228 5197]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0485
INFO voc_eval.py: 171: [1083  772  728 ...  455  465  484]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0873
INFO voc_eval.py: 171: [124  99 127 114 149  78 133 109 112 150 104 232 131 101 175  96 137 188
  88 231 185  98  79 167 157 136 198 176 218  86 221  13 103  44  90 156
 113 147 187 183  37 211  41  43 135 145  51 189 146  81 106 196 140 193
  54 118 123 261  29 222 161   4 169 182 105  23  14 130  39  30  92 148
 223  42  58 197 171 172 110 205 173  12 159  36  26 230  47 194 164 216
 174 229  25  11 152 158  45 212 201 153 227  57  32 166 165  24  97 224
 203 162 179 100 180 245  80 228 190 178  34  28 151 163  91 170   7 155
 225  38  40 215  35   1  89  50 128 177 233 208  20 186  48 126 160 121
 206 168 154 125  33 254 195  19   8  15 213  10  31 115  27  82 139 214
   0  76  87  77 207  93  21 142 219  94 184 210 191 108 141 200 134  46
 181  65  73  84  69 138 226 120   2 117  72   5  75  16  17 209  62 129
  18  74  59  67   6  95   3 111 217   9 119  22 242 107 144 116 102  64
 122  83 234  53 220  49  85  56 143 132 204  68  52 252 199 240 202 256
 253 238  60 192 255 236 248 250  61 239 235  63 247 244 260 258 246 257
 249 241  55 259 243 251  70  71  66 237]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0481
INFO voc_eval.py: 171: [122   0  31  42 127  18 121  36 116 120   4  10  27  26  41   2 129   3
   9 126 115  15 117  12   5  35 113  14   8  40  38  30   1 123  39 118
  64 141 162 101  25  17  51 111 137  63 119 159 160  98 167 125 158  59
  77  73  13  44  34  87  66  24  11 107  58 142  75  72 161 130  65  28
  97 163 103  88 128 149  71 169  68  91 164  60  80 136  93 156  54 105
 124  85  50  79  95  84 134  76  29 151  81  61 109  57  48 100 135 166
  32  90  56  22 108  62  49 114 150   7  33  46  37  96  99  92  82  45
 168 144 165  74 110 148 104  94  69  16 153 157  86 154  67  83 102 140
 138  89 155  78  19 132  23  43 145 106 139  21   6 131 133 146 143  52
  20 147 152 112  70  47  53  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0832
INFO voc_eval.py: 171: [181 279 117 131 203 288 259 283 260 192  17 270 271 187 186 190 212 206
 118 265 114  26 277 183 202 184 111 274  53 261 290  56 209 276 263  55
  33 284 127 177 132 171 198 142 268  76 189 294  57 200 247 226 201 133
 115 143 216 292 113  25  42 267 174 205 240 197 269   0  41 119  50  37
 272 237 241 155 273 215  23 136  58 182 128 214 122 264 278   1 217  65
  32  24 204 188 120 266 148 243  54 199 185  89  29 176 154  18 289 249
 246 258  69 238 139 191 112 134  74 138  66 116 162  62 291 167  86  81
 234 178  73  72   6 123  49  15  16  39  13 275 281  46  61  90 296 156
  75 287 144 255 173   5  77  19 145 166 295  71  35 299 297 193  20   4
  94 169 161 223 285 130  52 172 248 252 222 106  51  59 159  28  30 229
 152 227 224   9  48 194 208  92 207 125 180 168 126 108  79 210 109 140
  34  87 175 242 304 124 280  43 239 254 135  40 147 220 218 244  11 235
 179  12  95  60 195 102 228 253 231  93 211 251  44 245 196 163  78  38
 300  27  36 149 256 141 160 121 225 233 170   2 230 232 129  82 293 257
 137 302  47  80  21 150   8 236  14 146  84  68 221 282  88 157 286  10
  67  97 164 158 151 301 110  31 298 262 153 250 303 107   3  63 165  45
 219  83 103  22  96 101  99  85 100   7  98  70 104  64 213  91 105]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0463
INFO voc_eval.py: 171: [ 14  10 125  18  38 181 146 149  51  67 296 297 241  89 204  61 184 260
 224  24 259 210 290 135  87  11 235  97 131 234   3 136 284  91 139  94
 226  90 266  70 208 130 244 128  55  64  41 216   1 176   7 151 236 195
 148 222 299  26  27  82 271  81  39 285 100 248 264 142 291 140 314 165
 203 268  57  32 294  22 240 233 163 232 102 246 261 282 194  19 143  58
  48  68 133 132  63 145 230 311 171 111 166 188 206 267  47 215  98 310
 144   6   2 228 283 129 249  69  54  62  93  88 243  83 153  29 164 258
 255 253 250 186 247 126 237 302 293 219 263 227  72  92 262 175 265   8
  17 105  60 312 308 112 298  71  78 141 254 172   0 134 223 270  33   9
 138 169 117 173  96 168 300 212 315   4 137 286 207  66  59 269  43 152
 288  56 209 251  85  99  21 174 225 147  44 159 211 193 167 257 150 301
 245 116 157 202 295  52 162  12  46 127 183  77  36   5  34  16 190  13
  15 289 304 242 274 303  30 115  40 277 106 292 307 239  65 170 191 101
 156 114 252 278  45  28 280 161  20  75 185  42  25 179  79 154 214 238
 155 276  95 305 182 217  53 158 279  74 103 287 313  73 229  86  35 231
 123 160 281 306 198  80  49 218  76  84 113 108 213 309 118 124  50 275
 220 205 180  37  23 119 121 201  31 256 272 178 221 273 110 189 197 192
 177 104 200 187 196 109 199 120 107 122]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0387
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0660
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.011
INFO cross_voc_dataset_evaluator.py: 134: 0.108
INFO cross_voc_dataset_evaluator.py: 134: 0.029
INFO cross_voc_dataset_evaluator.py: 134: 0.050
INFO cross_voc_dataset_evaluator.py: 134: 0.028
INFO cross_voc_dataset_evaluator.py: 134: 0.126
INFO cross_voc_dataset_evaluator.py: 134: 0.033
INFO cross_voc_dataset_evaluator.py: 134: 0.023
INFO cross_voc_dataset_evaluator.py: 134: 0.028
INFO cross_voc_dataset_evaluator.py: 134: 0.150
INFO cross_voc_dataset_evaluator.py: 134: 0.055
INFO cross_voc_dataset_evaluator.py: 134: 0.022
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.284
INFO cross_voc_dataset_evaluator.py: 134: 0.048
INFO cross_voc_dataset_evaluator.py: 134: 0.087
INFO cross_voc_dataset_evaluator.py: 134: 0.048
INFO cross_voc_dataset_evaluator.py: 134: 0.083
INFO cross_voc_dataset_evaluator.py: 134: 0.046
INFO cross_voc_dataset_evaluator.py: 134: 0.039
INFO cross_voc_dataset_evaluator.py: 135: 0.066
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
