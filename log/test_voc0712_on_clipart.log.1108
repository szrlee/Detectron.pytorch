INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/baselines/e2e_faster_rcnn_R-101-FPN_1x-VOC.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 60000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 20000, 40000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 20000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 60000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 20000, 40000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 20000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.379s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.352s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.351s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.356s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.366s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.366s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.364s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.363s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.362s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.359s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.361s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.366s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.366s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 60000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 20000, 40000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 20000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.443s + 0.001s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.365s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.361s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.355s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.351s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.354s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.369s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.371s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.372s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.372s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.373s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.372s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.369s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 60000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 20000, 40000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 20000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.495s + 0.002s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.360s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.363s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.364s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.361s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.361s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.359s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.361s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.360s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.362s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.361s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.362s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.363s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 60000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 20000, 40000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 20000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.505s + 0.001s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.352s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.352s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.356s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.360s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.358s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.362s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.362s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.366s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.363s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.363s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.362s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.362s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_1x-VOC/Oct08-14-54-41_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 53.442s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 69 219 366   9  56 256  35 309  58 428  86 371  71  64 140 228 426  14
  42 227 133 429 377 257  89 353  68 299 324 237 233 313 135 368  66 131
 447 262 130   6  39  88 269  15 211  44 351 168 445  92 134 410 231  87
  10 225 298 367  40 306 224 167 296 320 391 297  47  11 151 187  13 132
 369  94  93 449 361 234  43 229  99 154 275  32 108 407 317 279  81  57
 226 350 260 438 278 312 254 276 310   4  36 464 169 147 422 144 430 253
 282 359 452 270 179 123  76 364 102 300 199 396 311  27 303 263  59 342
 230 152 138  90  21 418 470 214 295 448   0 283 346 476 251 362 141 408
  67 160 412 475  72 176 273 172 289 352  80 437 284 197 264 334  79 286
 403 348 333  91 193 137  46 280 373 100 394  28 287  37 107 125 163 379
 274 414 380 314 119 246 415 247 139 120  45 454 455  70 392  61 318 294
 128 195 143 386 290 456  85 236 198  95  16 432 319 404 281 316 166 305
  29 453 259 111  41  54 339 184 171 252  31 292 291 443 462 442 466 196
 109  25 212 215 421 218 389 363 173 148 330 390 327 397 387 393 145 384
 459 235 156 344 374 315 170  20 136 474 155  83 124 451 345 240 181 272
 180  77  18 338 420 177  23 419  53 150 183 416  65 202 243 424 268 423
 205 249 244 322  52 349 204 271 217   7 467 149 266 110  38 220 186 341
  84  55  34  26 267 307 458 329 162 413  19 365 248 232 321 165 101 121
  97 208 417 431 435 471 265  33 376 206 440 200 343 436 360 127 223 354
 446 400 245 118 406 159 242 450 370  63 190 325 399  50 433  24 288 158
 129 302 103   5 258 293  49 434 308 114  73  82 116 207 241 122  98 465
 164 192 378 157 201 203 304 277 388 238  51 356 210 472 178 382 405  17
 216  48 222 188 347  96 239 336 463 468 473   1 340 381 383   8 323 358
  62 301 441 189 385 427 185   2 332 161  12 461 425 460 457 357 115 255
 372 153 213  60  74 105 175  75 194  78 411 469 182  22 439 106 191 112
 117 337 335 398 174 402 285 250   3 126 261 326 395 209 328 355 146 113
  30 142 401 331 221 104 375 409 444]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2416
INFO voc_eval.py: 171: [ 95 352  50 219  51 304 391 394 386  55 417 110 409  49 415 248 293 302
 408 395 354 407  68  54 312 101 218 103 382 257 175  26 107 357  56 336
  96 427 239 413 106  39  30 174 414 221 253 356 154 291 142 147 343 303
 227 358 418 306 246 426 290 385 211 389 297 387  33  59 206 383 400 361
 420 102 243  34 244  61  29 346 149 126 282 338 322  58 140  42 305 165
 295 176  45 337 360 324  64 397 207  60 388 245 109  62 428 333 225 184
 255 163 267  69 367 375 139  70 231 425  65 266 313 122 276 401 316 136
 113  52 203 268 138 421 390  16 419 118 170 131 355 378 237 318  94 179
 319 233  57 342  47 339 104 241 369 160  73 288 217 105 100 196 121  85
 247 240 434 328  66 116 410 347  36 191 251 286 373 377   5 259 402  74
 201  19 150 270 156 216 277  40   3 340 348  17 114 430 363 287  89 370
 299 133 315 242 144 194 238 327  79  92  46   1 317  83 392 308  31 432
  22 202 198  14 215 332 166  32 157 137 283  35 183 258 365  43 359 252
  78 285 159  88 200 396   2  53 134 422 372 173 325 307 292 300 220  44
 209 195 284  11 193 301 412 380 368  10 279 213 311 261  23 145 224 416
 424 115 379 112 222 192 123  80 381 168 271 281 345  98 384   0 185  18
 323  38 330  37 274  99 435 177 132  84 236 344 260 171 263 262 120 187
 181 334 264 152  20 331  90   7  86 411 230 172 155 210 167  91 228 351
 353 280  63 431  72 254 320  97 404  28 108 130  21 326 169 362 398 436
 265  15 204 141 117   4 423 214 161 366 371  27  24 376 143  87   6  77
 129 146 314 223 190 226 374  71 429   8 272 399 364  82 180 186 269 162
 405 188 275 111 298 119 153 127 294 182 278 135 309  41 197 164 249 124
 189 403 199  81 349 148 178 296  13 235 212  67 125  76 350 406 310  25
 335 433 273 151  93 205 289   9  12 208 256 393 234 232  48  75 329 250
 341 128 321 158 229]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5072
INFO voc_eval.py: 171: [146 178  54 184 112 114 113 156 328   3 387 188  25 124 301 173 384 333
 422 175  60 427 330 389 421 189  75 119 297 193 394 423 497  27 117 487
 116 183 425 229   2 326 180 424 130  26 385  77 392 142  57 129 331 174
 429  10 128 373 434 393 122 447 463 231 159  63 232 126 177  65 242 439
 361 118 144 430  37 332  50 266 454 203 153  85 163 241 442 314 300 248
 120 202  93 302 428 431   8 264 401 470 249 115  76 194 340 343 448 179
  80 449 386 162 123  87 293 398 499 148  74  38 501  29 359 494 498   6
 164 492 151 388 325  71 261 104 322  62 367 222 309 234 233 166 412 201
 451 364 484 323 391  28 190 362 475  84 235 329  78  83   5  92 100  16
 208 310 255 186 462 152 348 495 408 238  46 414 370 450 482 154 182  58
 467 336 338 247 125 143  35  30 253 225 433 176 210 262 399 465 236 404
 147 160 455 403 371  14 379  66 199 230  45 341 157 254  23  24 295 488
 196 273 136 426 460 459 155  17 217 312 368 350 355 305 320  52 170 224
 294 504 419 127 476 269 161 275 149 438 396  86 278 237  82  43 185 420
 171 135 410 286 400 432  81 417 252 121 145   0 461 346 134  64  20 334
  47  68  69 240 327 259 503 271  42 483  33 256  22 307  70 369 214 277
 239 315 478  67  90 101 290 298 243 109 172  55 197 251 344 377 215 409
  13 349 274  32 405 204 139 354 445 205 303 291 490   9  89 471  97 347
 464  99 219 441 158  72 481 200  44 466 416  59 357 245 418 335 337 167
 257 108  40 283 313 102 311 339  53   4 181 435 374 353 452 280 306 191
 209 287 381 195 284  12 395 457 288 270 378  96 211 500 279 192 479  79
 272  31 212 413 321   7 376  61 472 345  94 352 141 456 168 480 380 406
 282 436 351 221 132 453  39 103  51 226 316  49 486 496 358 110 317 493
 299 296 111 383  95 265 468 360 267 198  34 138 308 324 458  11 407 260
 446 363 169 263 473 223  91 415  48 469  73 489 318 292 227  41 137  56
 443 390 382 440 411 268 281 165 246 285 304 258  21 356 365 437  98 207
   1 216 366  88 220 133  18 218 485 106 444 402 105 107 502 244 397 319
  15 276 150 372 375 140 250 213 206 187 474 491 131 289  36 342 477 228
  19]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2010
INFO voc_eval.py: 171: [180 630   1 312 526 527  86 188 153  12 580 206 528 636 804   2 182  31
 488  14 327  92 685 543 791 350   7  87 529 565 447 410 186 175 536 163
 797  10  37 205 787 903 534 324  34 578 870  88 777 542 455 829 635 904
 689 217  85  38 735 664 255 696 670 764 149  44 272 800 546 535 869 396
  56 250 639 513 178 356 508 192 678 484 758 108 552 256 538 457 210 567
 674 768 569 355 803 682 433 317 301 660 216 759 739 864 521 187  72 338
 483 407 506 766 374 503 304 401 279 426 271 703 335 822 874 811   6 719
 247 656 642 533 602 318 587 441 284 194 248 492 726 330 817  40 152 847
 577 371  50 274 412 745 601 246 531 481 109 723 437  22 459 789 877 830
 537 684  28 665 323 911 648 619 675  90 351   5 160 617 270 419 406  63
 326 176 382  62  32 189 539 571 861 444 839 358 779 303 710 328 691 268
 708  30 134 389 672 346 209 765 734 645 615 243 112 316 146 661 799 392
 773 631 454 742 219 363 242 470 775 514 369 145 692 453 411 881 362 860
 147 257 809 184 226 372 652 790 837 850 767 223 458 717 732 701 211  78
 738 237 259 801 448 715 687  81  64  79 751 614 727 315 908 207 606 591
 563 683 340 141 165 467 302 598  11 832 100 807 575 282 796 681 393 400
 360  80 496  46 714 218 540 191 381 477  71 761 841 281 498 741 532 699
 550 493 124 430 858  74  25 429 545 290 215 273 753  65 199 795 729 501
 473 148 177  43 794 241 104 704 375 378  29 712 566 497 150   4 238 844
 418 504  45 814 638 688  39 782 752 793 667 733 865 622 127  61 352 384
 633 711 491 541 643 136 584 402 666 831 125 833 466 851 595 875  58 737
 253 162 204 613 185 813 463 554 676 605 784 469 560 827   9 394 894 119
 783 107 588 298 686 343 232  82 700 287 641 129 465 132 516 285 236  93
 655 517 823 140  33  35 818 583 662 408 698 353 293  70 695 668 398 730
 763 646 568 896 123 909  66 435 570 802 558 500 620 740 416 893 276  73
 604 167 359  95 510 624 428 883 621 495  53 586 308 423 840 590 103 889
 264 573 267 902 654 597 222 305 294 116 336 157 609 197 502 313 231 900
 300 291 403 774 122 169  99 748 728  67  68 659 198  84 111 879 200 792
 438 309 126 651 120 214 697 556 608 320 716 203 386 511 230 647 522 367
 485 756 820 593 380 705 434 227 816 551 891 221 249 475 515 762 555 390
 269 693 234 174 507 871 144 486 805  52 494 509 746 341 781 812  55 557
 863  42 480 376 229 625 585 364 634 399 810 754 329 520 706 404 888 872
 183 690 530 254  47 721 798 524 499  23 525 292 314 788  16 725 427 310
 468 479  26 890 171 709 289 489 482 736 640 130 806 828 718 826  75 322
 596 395 824 110 848 720 133 440 297 853 576  76 383 487 233 637 155 240
  21 260 884 190 743 321  19 476 581 478 361 512 786 368 650 464 296 895
 266 342 319 901  51 117 855 757 825 914 431 213 452 135  13 649 277 912
 505 388 261 244  83 280 750 131 653  89 262 843 663 910 143 747 151 882
 278 629 102 564 838 462 835 461 373 808 179 348 414 387 657 562 142 669
 158 610 265 251 106  54 694 892   3 275 815 201 225 283 235 574 474 339
 771 331 778  15 770  60 354 377 673 450 671 490 471 722  57 544 442 553
 156 561 852 616 370 862 582 113 772 334 594 821 913 885 443 834 854 307
  36 196 731 409 239 422 579 632 168 859 559 417 618 101 887 337 744 680
 878 193 366 105 166 138 549 172 295 519 391 897 306  77 856 181 769 115
 446 202 420  20 623 592 345 208 212  69 421 644 121 905 436 252  97 344
 128 139 170 173  49 755  27  18 195 460 886  17  98 857 397 749 118 154
 899 472 137 679 114 713 627 415 780 846  41 876 349 572   8 258 311 449
 677  24 906 658 379 628 518 819 626 547 603 424 413 724 333 873 589 868
 599 224 836 159 849 880 456 288  59 600 263 245 867 842  94 898 164  48
 611 439 702 607 612 707 432 845 405 220 286 785 299 425  91   0 385 347
 760 445 776 451 523 357 548 325 866 332 907 365 228  96 161]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1918
INFO voc_eval.py: 171: [358 620   4 629   7 633 274 349 307 644 316 355 306 537 815 536 640 726
 405 308 275 452 124 632 744 513 170 812  12 575 638 631 823 662 614 309
 285 311  32 369 545 517 484 691 678 276 723 630 324  66 371 361 315 546
 663  67 635 581 722  21 698 345 318 313  46  19 406 649 602 350 187 639
 278 314 364 653 814 474 142 519 615 317  15 686   8 774 727 227 680 549
 557 664 543 577 248  11 423 681 226 321 268   6 750 514 565 795 303 711
 408 656 359 310 439  28 333 572 688 617 541 539 538 590 652 624 284 567
  24 485 177 207 518 704 339 481 298 520  57 732 363 458 126 783 230 655
 282 816 475 325 540 542 154 720 551 660 219 634 244   0 787 326 637  75
 418 725 179  64 684 175   1 265 352 547  68 295 478 319 288 657 294  48
  70 822 641 115 692  38 544 777 189 396 388  35  34 661 137  18 568 253
 486 409 659 760 682 788 381  52 266 391 434 798 376 267 394 488 120 606
 636 533 453 330 198 515 745 505 238 507 579 382  89 327 180 433  50 580
 424 685 334   5 383 123 144 436 462 407 672  27 552  17 593 377  77 820
 290 707 312 643 561 164 762  72 441 271 203 648 790 280 185 442 182  45
 813 117 558 489  23 574 139 821 460  49 378 454 215  69  51 214 438 322
 670 775   9 459 435 234 807 676 411 562 619 127 755  36  83 140 747 605
 446 743 108 800 804 554 331 302 806 673 695 445 200 729 717 654 765  14
 818 607 761  95  10 380 668 754 392  43 600 463 603 468  60 728 172 173
 763 336 824 611 647 464 277 525 496 272  94 346  86 287 429 304 194 769
 427 589  59 372 428  13 375 601 366 260 204 393  88 731 674 491 597 114
  16 206 286 342 403 553  65  80 512 415 125 210 689 269 384 245  61 503
 509 169 713 151  47 258 791 524 387 548 591  99 104 129 794 569 329 360
 136  73 642  22 235 353  20 742  91 470 645 473 109 338 283 174 752 328
 412 357 528 196 675 708 740 583 437 780 262 162 719 735 487 465 690 121
 609 111 770 351 401 159  92 128 450  82 195 119 522 365 420 759 550 449
 738 217  87 208 768  74 426 753 146 748 443 105 228 193 379 563 291  53
 531 527 201 150 665  90 332 587 167  85 413 197 494 247 608 132 239 143
 736 599 344 721 158  29 455 222  78 693 389 700 263 370 779 320 106 739
 242 425 135 252 588 501 592 236  25 293 784  55 367 482 697 220 564 476
 131 683 510  58 456 506 374 703 390  37 205 621 213 354 178 157 190 156
 799   3 461 256 141 749 781 417 521 225  81 622 585  97 385 809 576 819
 149 666 165 516 523 687 584 616 778 511 356 715 440 797 300 801 500 578
 650 211 626 103 251 710 709 337 257 362 399 130 555 299 826 110 218 612
 566 782 422 448 229  30 160  41 535 737 495 168 243 802 716  33 192 176
 147 340 118  42 292 255 803 343 335 107 573 398 279 718 492   2 477 122
 733 811 447 499 181 466 658 730 254 444 625 402 113 386 705 431 416 772
 101 148 701 756 734 155 224 825 610 702 706 746 145 223 419 776 152 186
  79 623 289 497 817 669 751 467 323 347 796 395 241 786 758 184  93  84
 432 397 792 472 480 259  54 469 138 202 504 582 490 613 773 618 493 604
  71 712 163 789 102 627 810 368 116 341 595 534 421 671 679 764 246 805
 571 699 221 171 628 451  40 741 296 696 216 483 161 530  31 188 598 586
 793  96 212 153 757 281 570 297 594 233 714 199 250 430 134  26 166 404
 808 400 112 767 694 348 559  76 410 596 232 240 249  39 414 305 270  63
 231 498 457 560 724  98 100 556  44 479 508 209 191 373 646 785 526 651
 502 771 301 667  62 264 183 532 529 237 261  56 677 133 471 766 273]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3857
INFO voc_eval.py: 171: [ 65 110 276  37  67 156  59 111 275 196  48 257 113 114 290 185 116  69
  85   8 259 108  47  84 289 100  77  83 287  41 162  49  45 211 246 234
 228 133 273 250  33 232 163  19 277  78 101 131 256  13 136  22  40 285
  86 229 173  46 129 125 213 207 165  70  72 212  93 194 190 153 175 119
 271 260 248  71 181 209 174 292  42  52 265 122 170 171  38 243   2 137
 220  92 150 138 254 160 152  32   6 103  80 191 148  82 105  64 128  54
 132 151 217 199 107 231 224 157  39 263 247 109 189 221  20 270 268 233
  10 127 193  29 195 291 264 206 236 255 123  27  57 238 235  73 168  90
 253  95  75 240 155 244 208  56 279  24 252  51  60   3 274 141  74 227
  87 202  43  88  23 241 172  30  63 210 258 261 183 288 262  36  62 134
 188 251   4 278 184  53   1  12  66 166 142   5  91 205 281 249 143  79
 120  14 102 201 284 154 239 269 242 283 140 177 139 200 192 121 145 218
 115 126   9 197  17  55 219 117 159 161 216 182 167  26 245 186 226 135
  11 118 149 230  81 266 187 104  16 106 180  89 203  68 282 237  94 176
  61  15  21  35  98  99 179   0 267  25  28  96 222 124 130 147 215 280
 198 158 112  76 204  31  58  50  34 146 178 144 272  44 164 169 214  97
 225  18   7 223 286]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4510
INFO voc_eval.py: 171: [368 103 180 343 520 181 477  71  51 350 171 384 438 494 104 380 107 248
 228 186 473 322 501 197 135 243  54 356  47   4 264 270 314 451 523 348
 172  35 342 487 485  83 484 471  52 458  36 363 396 265 376 272  86 345
 106 495  34 359 269 273 130 232  12 478 208 146 525 443 349 460 385 346
  32 453 347  11 405 455 187 285 128 252  84 177 434 136 198  43 498 452
  89 293 397 436 323 245   6   9  75 433 246  74 321 361 158 399  63 316
 496 185 386  24 263 307 440 184 395 132 352 418 230 486 369 400 317 137
 199  76 511  45 328 505 514 226 510 225 430 435 193 287 310 502 161  72
 277 283 242 479 527 398 509 253  49 203 141 366 408 353 439 100  25 468
 254 278 308 459 470 463 409 466 224 480 426 393 291 461  64 284 390 357
  23  17 315 209 147 236 188 282 500  18 256 476 392  73 401 218 194  67
 371 201 139 388 211 318 195  98 535 305 508 220 125 437 111 129 482 442
 271 324 456 337 488 191 108 325 274  95 175  37 275 450 407 475 528 499
 262 286 297  53  19 309 421  99  87 311 329 522 281 370 182   1 404 105
  42 519 358 334  48  44  33 422 122 133  80 221 465  58 335 206 144  68
 238 333 117 109 382  14 301  56 165  61  26 267 391 411 354 431  46 403
 174  78 360 306 419  38 168 212 423  16 413  94 190 217 127 120 233  39
 163 205 143 164  69 196 424 364 151 516 239 326  90 235 255 157 351 515
 377 150 237 142 320 415 204 295 145 207  91 504 170  77 247 115  50 280
 102 428 327 375 288 214 216 417 160 298 123 251 387 432 365 260 339  96
 414 250 294   7 312 266 394 101 507 113 167 234 276  29 149 441 529 341
 372 526 292 300 302 362  22 169  31 162  40 491 124  88 338 319  85 340
 344 474  60 223 412 178 166 378 241 524   2 121 472 290 259 497 330   3
  57 427 202 367 140 131 336 268 261  82 506 447 481   0 420 110  79 154
 118  81  20 219 134 425 448 114 462 189 192   8 374 464 512 222  97 229
 240 153 126 299 449 402 173 503 331  66 533  62 534 483 210 379  10 176
 279 304 373 389 530 410 492 116  41  55  92 156 215 227 536 200 213  28
 513 490 138 289 119 231 429 313 444 446  70 155 489 249 518 532 258  93
 355  21 493  59  27 296 416 406 445  13  15 531 454 517 159 179 244   5
 332 303 467 112 469 381 383  65 457 257  30 152 148 521 183]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2653
INFO voc_eval.py: 171: [14 44  3 15 39 24 45 41 40 17  2  1 46  4 29 49 31 23 47 33 48 30  5 21
 38 11  0 25  6 28 36 43 35 16 19 32 37  9 26 20 27 42 34 13 12 22 18 10
  7 50  8]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0909
INFO voc_eval.py: 171: [ 606 2379 1781 ... 1026  791  491]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3753
INFO voc_eval.py: 171: [11 28 41 58  5  3  4  1  8  6 67  7 29 44 62 42 57 32 64 23 66 10 17 54
 31 33 16 60 25 61 30 55 68 34 27  9 43 24 40 50 15 53 46 45 14 22 38 47
 26 35 59  2 13 51 39 56 12 49 36 20 21 18 52 48 63 19  0 65 37]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0987
INFO voc_eval.py: 171: [343 263 621 704 314 156  63 297 449 767 516  30 750  81  90 422 147 249
  64 626 446 242  67 627 315 211 642  39 616 583 176  57   4  35 313 532
  25 349 122 487 729 412 150 312 724 499 353 677 506 337 252 259 439 639
 148 622 480  43 705 661 201 751 146 440 588 444 431 797 580 611 451 547
  40 240 258 320  66 236 165 481 318  42 420  14 649  33 452 660  27 783
 537 808 662   3 401 421 625 447 213 727 292 130 243  84 636 709 522 149
 578 528 355 518 489 257 296 475 652 711 575 633 572 187  75 157 569 372
 304 336 155  50 402 515 298 209 214 505 477 225 703 712 330 710 295 293
 339 604 413 650 433 733 743 378 345 737  74 124 500 520 756 276 454 442
 695 246 310 550 584 609 453 430 752 655 457 316 543 418  85 254 395 679
 461 234 163 347 640 107 137 277 400 268 167 798 593 718 341 643 706 382
 554 381 456 597  94 103 549 389  99  32 668 256  47 772 766 325 534 427
  11 125 326   8 476 233  44 696 479 517 358 726 776 392 241 145 585 714
 730  48 207 544  86 728 232 111 722 415 308  69 568  51 760 624 688 574
 194  65 416 716 664 274 665 204  87 800 185 121 303  46 656 590 535 458
 141 390 344 113 133 494 503  72 151 333  31 671  49 352 332   1  26 262
   7  93 195 218 411 305 186 428 558 602 723 698 548 564 391 319 210 488
 765 773 171 670 491 445 746 100 753 667  19 799 405 250 278 321 229 394
 691 507 687 492 179 423 342 596 531 777 181  73 425 614 796  36 678 139
 717 180 448 810 747 380 551 203 719 521 460 546 123 647 782 739 594 300
 539 740 119 641 779 619 374 443 658 589  12 757 674 385 198 744 112  92
 692 375 331 519 708 721 190 754 579 438 748  56 441 673 675 618 638 530
 715 361 406 615 485 774 587  41 247 306 630 248 490 216 525 272 464 206
 410 732 648 527 183 299 514 140 237 613 368 684 281 350   9 238 645 364
 129 795 168 253 219 328 581 498 267  45  55 731 208 369 632 429  29   0
 357 327 407 223 182 239 713 804 106  68 592 513 484  95 689 495  96 685
 555 417 770 699 367 261 552 432 273   5  34 104  82  10 221 538 462 793
  37  58 700 651 702 110 524 577 164 170 474 286 366  89 196  23 472 501
 720 143 351 690 290 192 573 806 628 348 742 173 138  78 370 128 631 635
 542 676 523 409 193 607 397 435 693  88  91 324 135 697 131 245 215 682
 533 780  22 466  59 162 654 807 450 764 132 745 363 735   2 184 707 736
 230 541 174 283 189   6 302 289 599 116 301 565  53 383 473  38 205 683
 771 755 467 504 629 620 768 346 317 291 759 623 781 644 109 511  77 775
 354 265 212 118 469 379  79 606 386 414 279  16 786  18 559  15 235 334
 285 436  21 294 482 231 152 471 224 805 287 787 486 788 228  97 144 362
 734 227 169 603 601 669 653 598 741 794  52 508 101 463  98  61 803 605
 340 311 393 483 159 763 600 408 468 566 251 338 502 540 557  24 556 637
 526 493 335 512 142 329 434  13 595 114 175 172 200 701 197 404 612 758
 127 784 561 791 455 161 801 666 115 309 672 191 725 570  28 264 761 120
 384  17 545 792 560  54 510 178 322 377 376 790 388  70 496 680 399 591
 809 686 134 222 275 396 497 646 610 663 323 529 681 282 188 657 117 470
 260  83 387 166 255 373 571 359 199 360 244 465 105  62 269 617 567 582
 789  20 153 365 160 778 634  80 280 177 217 270 419 509 202 608 307  76
 738 403 769 108 271 398 694 220 553 424 802 437 562 785 154 576 762 288
 226 126  71 356 478 659 536 102 371 749 266 459 586 284 158 426 563 136
  60]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3548
INFO voc_eval.py: 171: [32 70 46 19 75 20 10 49 21  1 33 45 65 35 59 74 50 13 23 30 18 47  3 44
 72 66  7 68 17  9 25 37 39 84 73 28 22 12 62 80 55  5 63 57 11 61 34 64
 15 40 53 78 27 58 76 83  8 42 24 51 79 16 38 48 77 31  0 43 60 82 29 36
 56  4 81 54 69 71 26  2 41 52 67 14  6]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0351
INFO voc_eval.py: 171: [79  3 39 15 68 41 40 81 11 63 12 43 13  7 74 19 16 56 66 53  4 67 58 14
 55 84 86 21 87 47  6 46 64 78  8 82 37 42 85 20 48 60 65 69 45 72 90 77
 88 52 23 17  9 32 24 54 73 92 59 80 10 35 76 36  0 50 70 62 22 31 57 61
 27 51 34 18 33 83 71 28  5 30 44 29 49 89 75 38  2 91  1 26 25]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.2341
INFO voc_eval.py: 171: [ 31 169  16  33 168  95  17  96 173 101 122 130  36 100  70  32  37   2
 115  12 129  99 111  61  19  81 138  35 152 166 158  79  98 145 128  60
 132 125   9  84 110  53 156  49  47 135  29  93   3  34  77 102  57  18
  80 141  44 113   1  11   7  48 162 137  65  38  97  88 109  13  10 126
 136 114  94  40 165 143 117  72 116 120 139  58  43  21 172 177 144   0
 170  78 175  68 159  64  28  76  59 119 107  55   4 155 106  71 176 171
 146 154 108 112  20  75  39  87  92   8  89 164  27 127  15 163  41 142
 118  46 160 105  24 133 150 161 148 123  26 174  22  25 147  51  67  56
 140 151  52  42 167 153   6  45 121  30  74  91  62  85  69 124 131  82
 157 103  90   5  54 134  23  66  86  50  73  63 149  14  83 104]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4216
INFO voc_eval.py: 171: [1836 1932 2061 ... 1293  490  435]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.2852
INFO voc_eval.py: 171: [ 320  443  409 ... 1937 1178 1777]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5035
INFO voc_eval.py: 171: [ 4 64 62  1 48 40  2 61 13 91 12 20 88 73  3 56  6 15 66 29 83 43 44 75
 68 59 11 55 35 90 74 31 63 82  8 38 54 14 86 84 45 22 71 58 87 70 52 36
 53 10 32 67 30 65 19  9 77 49 17  0 21 78  7 85 72 16  5 41 80 76 24 18
 46 34 27 26 28 57 37 89 42 60 51 69 33 39 81 47 79 25 23 50]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1995
INFO voc_eval.py: 171: [ 7 13  2 18 29 23 15  6  1  9 33  3 16 10 26 27 28  0  8 21 20  5 11 14
 31 22 24 30  4 19 12 32 25 17]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1591
INFO voc_eval.py: 171: [ 29 149 225  32  30 200  79  31  42 226 116 164  24 229 202  39 173 152
  58 119  33 295 249 110  23  77 170 310 268 244 205 165  81 293  27 155
 123 273 198  25 305 246 201 129 285  35 171 174 190 294  38  87 178 175
 270 254 163  66  12 127 150  65  41 235 242 231 252 250 303 275 124 315
   7 158 145 300 219 288 299 131 132  56 105 224  74 120 267 309 136 179
 245 216 112 151 279  17  73 195 260  86  82 272 144 113 239  80 118 159
 162 203 228  62 193  75 176 317  37  88 312 316  40  68 304  67  52 100
  97  13   3 135  51 248  36 168 207 259 138 160 104 214 210 166 141 154
 269 172  70 241 253 276 208 191  26 306 233  43 261 307  22  95 278 139
 133 223 215  92 181 238  48  96  59 128  90 212 182 148  85   6 186 153
 121 258 101  78 117 157  76  54 209  63 111 125 232 287 243 147  21 183
 107  28 318   5  91 257 130 218 230 262 240 137 283  94  14  69  99  61
 265 280  15 311  49 192  18   1 180 292 320 189  55  47 156  50   9 313
 266 134  44 146 263 302 289  93 222  53  64 187 234 161 188 277 122 281
 194 290 308  98 282 221 169  89 251 271 108 196 291 227 126 197  57 109
 213  72 284  19  11 211 264 301 298  20 236  34  83 296 185 256 106 237
  16  60   4 184 199 167  45 314   8  46 103 115 286   2 274 114 297  71
 140 143 217 204 247  10   0 319 177 142  84 255 206 102 220]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3412
INFO voc_eval.py: 171: [ 91 154  54 112   0  31 155 218 194 160 156   1 165  94  97 224  41 145
  32 168 127 175  33  46 116  72  35 179  55  47  61  43 151  83 141  96
  86   5  23 216  95 164 177 130 174 229  93 121  84 146 203 110  58  34
 228 122 167  82 111 149  17  12 197 125 227 188 226   6  22 124 123 217
 191 234 213  62  81 210  99 153  25 182 136  79 173 220  45 162  42 187
   7 181  85 135 114 201 211 204  29  65  56 118 221 158  30  74  16  36
 128 157  38   4 225 113 230 161  78 178 196 169  52 150 172 147  18 107
 102  24 223   3 144  67  11 131 126  69 132  98 219 212 232  48  15  71
 199 202 185 206 106  59  49  14  39  60 176  27  68 215 103 109 101 100
 183 129  63 159  80  40 200  37 134 138 233 170  13  90 152  88   9 208
 189 142 120 163 231 104 205  26  53  21 148  44 115  76  57 143  20 198
 105  51  92  75   2 137  50 119 214 171  70 209 192  66 108 195 207 180
 190  73  89 133 186  28  10  64 166 222  77 139 184  87 140  19 193   8
 117]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.2384
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.2791
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.242
INFO cross_voc_dataset_evaluator.py: 134: 0.507
INFO cross_voc_dataset_evaluator.py: 134: 0.201
INFO cross_voc_dataset_evaluator.py: 134: 0.192
INFO cross_voc_dataset_evaluator.py: 134: 0.386
INFO cross_voc_dataset_evaluator.py: 134: 0.451
INFO cross_voc_dataset_evaluator.py: 134: 0.265
INFO cross_voc_dataset_evaluator.py: 134: 0.091
INFO cross_voc_dataset_evaluator.py: 134: 0.375
INFO cross_voc_dataset_evaluator.py: 134: 0.099
INFO cross_voc_dataset_evaluator.py: 134: 0.355
INFO cross_voc_dataset_evaluator.py: 134: 0.035
INFO cross_voc_dataset_evaluator.py: 134: 0.234
INFO cross_voc_dataset_evaluator.py: 134: 0.422
INFO cross_voc_dataset_evaluator.py: 134: 0.285
INFO cross_voc_dataset_evaluator.py: 134: 0.504
INFO cross_voc_dataset_evaluator.py: 134: 0.199
INFO cross_voc_dataset_evaluator.py: 134: 0.159
INFO cross_voc_dataset_evaluator.py: 134: 0.341
INFO cross_voc_dataset_evaluator.py: 134: 0.238
INFO cross_voc_dataset_evaluator.py: 135: 0.279
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
