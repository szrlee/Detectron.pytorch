Start testing on iteration 499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.539s + 0.001s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.412s + 0.001s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.397s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.389s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.389s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.388s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.385s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.387s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.385s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.384s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.383s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.384s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.382s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.485s + 0.002s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.369s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.373s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.373s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.378s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.383s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.388s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.383s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.378s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.379s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.376s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.376s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.374s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.421s + 0.001s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.368s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.363s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.370s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.368s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.366s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.365s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.364s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.367s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.367s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.366s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.366s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.365s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.490s + 0.002s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.377s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.368s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.363s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.369s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.366s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.369s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.366s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.369s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.368s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.365s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.366s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.365s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.522s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [166  54 215   5  87 274  48  18 202 116  42  51  47 128 139 281 160 110
 163 292  41 167 169 296 138  76 303   1 108 161  43  46 305 188 127 243
 256  21 112  79  75 118 258 228  30 142  37 246  99 148  96  67 123 269
  74 301  44  91 174  63  29 227 309 288 295 162 213 147  11  27 232  34
  53  81 298 275  55 225  17 247 145  19   3 154  50 249 184 102 266 105
  39 140 164 254 196 153 214 194 124  68  24 302 134 230 119 155 263 183
 267 290 190 144   6 143  56  32  72 272 159 152  36 192 212 260 101 265
 104 312 245 210 293  92 137 282 179  82 235 122 151  66 141 261 173  23
 276 185 199 189 129 207 178  85 239  16 158 109 236 299 311 240 181 257
  95 241 234  40  52  98 278   9  62  59 259 273 277  35 271 200 171  20
 198 226 304 262 182 193 287 157 175  13 297  28 221  69 125 279 314   4
  77 121   2 310  26 222 149 308 111  12  22 107 126 224  78 242  70   0
   7 113  25  86 130 195 206 100 229  61 114 172 233 115 208  31  88 220
 238 252  71  94 270 180 187 191  65  90 106  33  14 197 201 231 186  89
  80 219  57  10 250 209 168 291 280 133 289 146 131  83 135 150 313 217
 218 103 176 244 285 264 204  64  45 286 211 237 117  49 306 136 205 120
 156 177 251 268  38 284 253  15 223 283  73  84  97  60 132 300  93 307
 216 248 165 294  58   8 170 203 255]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3416
INFO voc_eval.py: 171: [186  54 119  24 160 215 224 227 213 125 226 110 138  22  25 214 229 164
  17 172 200  86  91 216 155  78 157 161 218 220 212  90 146 168  61 181
 116 114 149  16  10  52  82   9 235  35  34 203 184  64 141 230 135 223
 199  77  42  75 194  92  97 231  88 166  68  41 183  83 177 134   5 126
   1 163 165 123 208 144  85  94  32 210 139  37 225 127  76 153 237 124
 120 206 143 241 174 195  36  58 228  27  60 207 240  73 131 209   8 234
 198  84 101 197 102 182  47 132 118 162  20 222 176 221  23  51 122 115
 137 170  74 142 178  55 133  99  67 106 190 238 236 232 158 150 117 111
 180  18 171  26  48 108  38 112 173 202  89  65 196 242 129  13 185  81
 205  46 154 147 204  63 233  45 107   4   7 145 219  71  50 104  93 217
 130   6 192  62  59  79 113  98 156 189 211  28  31  11  96  80 151  39
 179 191  40  57  30  72  43 188 175 201  66   0 121 169   3 140 128 105
  19 103  15  33 193  95  69 109 136 159   2  56  12 148 152 239 167  29
 100  44  49  53  70  87  14  21 187]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6216
INFO voc_eval.py: 171: [ 62  26  80  49  68  47  48  76   0 176 161 133 175 163 177  99  58  24
 201   1  59  37 100 164 142  55  50  75  84   9 145 138  70 196 114 187
 193 168 116  39 179 208 156 194  81 149  33  13 181 124 147  72  78   2
 207  57  71 151  60 140  67  22 129  53 174 131  69 143 189  54  42  77
 178 170 146  90   7  32 169 167 153  40  12  15 184  18 182 110 148  95
  11 165  65 115 173 103 120  43 150 139 134 180 198  73 109  36 199  51
   3  46 121 185 127 205   5 172 135 191 188  82 206 117  21 137 141 202
 157  87 130  41 118  96 125  88  34 119  38  45 128 195  98  30  56  91
 126 112  52  17  27 166  25  66 190 144 159  64  86  93 123  63 171  44
  31   4 105 203 154 107   6 113 200  28  74  20 192  61  94 197 104  35
  97 122 132  92 158  10 111 101 108 209 186 136  16  19 152 102  23   8
 155  14 204 106  79  83  85 162 183  29 160  89]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2373
INFO voc_eval.py: 171: [318 371 319   0 107   8 320  52  87 254  27  56 256 348 454 146 119 345
 249 363 253 447 255 398 196 118  10 191 247 142 121 185 321 101 257  29
 172 453 400  91  39 234 295  88 311 391  32 399  21 341 397 477 208 164
 431 102 283 240 195 124 445 372 339  65 108 509 261  64  25 174 393 389
 180 407 456 480 152 362 260 230 380 474 388 314 404  31 417 294 211 482
 324 451 275 438  28 113 266 299 308 425 392 354 390  35 463  89 246 505
 171 462 460 297 173 315  70  13 248 144 186 455 466  61 183 242 244 490
 349 340 280 271   3 159 486 250 411 378 414 428 166 184 381 502 214  14
 269  24 189 233 336 479 210 426 274 270  30  41 473 493 287 285 303  95
 206 496 217 175 130 344 235  49 382 136 286 503  93 325 457  38 190 228
 221 410 252 374 288 147 304 494 441 408 279 268 123 313 481 327 143  77
 220 154 373   4 128  76 229 131 357 129 239 223 353 495  47 421 284 492
 326 435 465 168 508 145 449  54 176 329 416 296 272 258 160 219 109 442
  73 111 412 368  16  45 301 358  42 162  55 199  97 262 141 376 309 170
 335 264 282 134 200 203  92 161 127 418 212 475 444 487 122 476 241 310
 103 227 500   6 366 361 224 222 395 471  81 281 331 182 243 406 401  94
  51  84 387  36 498 423 464 232 316 236 178 125 263  60 277 251 427  17
  66  48 333  53 126 434  46 207 402 469 151 306  69 317 132 117   2  71
 350 323 452 458 501 356 437 461 342 467 231 226  44 419 413 120 338 420
 216 485 110 197  23 369 138 448  79  12 167 188 360 403 307 497 302 245
 330 100  33 429 346 289 179  19 443 156  20 198  75  82 499 450  57  22
 312 383 386 150 432 459 153  80 347 504 405 192 238 105 334 276 278 375
 177 133 106 213 225 489 433 440 328  62 267 424 115  63 365 205 483 193
  78 364 484 439  86 322 139 215 209 204 135  83 394  11 446 293 273 422
 181  37 343  85  40 157 351 472 385 436 112 379  74   5  59  34 155 104
 137 148 201 491  98  72 259 352  58 367 337 384 165 187 488  26 237 202
 292  99 355 396 140 409 415  18  15 430 507 506  50 163 158  90 265 332
 291  67 305  68   9 114  96   7 149 468 218 478   1 194 377 298 300  43
 290 359 470 169 116 370]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3487
INFO voc_eval.py: 171: [419 219 420 421 192 224 195 423   2 191 422 341 223 282 105 193 250 524
 525 343 177   5 324 342 325 178 407 119 194 301 486 340 253 430 431 339
 433 424 345 160 456 477 439 426 365  31 239 443 425 482 436 440  76 428
 447 344 269 451  75 498 444 336 114 391 144 146 230  32 348 167  16 286
 262 374 278 287 453  14  23 458 449 507  54 289 351 226  45 248 136 186
 361 434 236  77 505 506 252 295  73 366 326 369 132 243   6 197 347  44
 280 126 228 162 157 427 448 392 398  43  82 414   3 291 338 279 373 161
 438 354 198 184 150 435 113 165 147 251 310 313 474 206 190 479 378  38
  84 129 168 216 359 452 526 240  17 292 363 399 134  93 471 261  50 227
 508 473 335 328 475 353   4 208 247 454 460   8 487 370  27  40 493 288
  51 459 199 418 375 245 372 108  13 293 234  24 225 148 145 200 170 329
 384 376 352 483 120  36 385 196 258 307 410 169 257 415  33 231 309  56
 112 350 181  15 450 220 142  68 213  62 518 327 268 356 380 153 406 510
   0 358 152 255 393 294 523  92  55 511 100 249 298 383  83 466 189 260
  64 367  26 319 308 256 159 179  34 238 265  11 211 155  87 306  71 316
  69 237  57 362  88 232 462 490 154 416 403 499 296 106 417  49 396 174
 520 204 381 496 457 331 461 173 156  80  63 221 233 263 478 305 166 389
 140 133 270 115 379 182   9  37  70  99 390  20 229 283 302 514  89  60
 210 118 303 404 517 429  94 241 274  86 128 267 492 491 300 355 185 470
 494 481 412  59 188 314 495 122 123 141 349 432  79 217  30 455 135 357
 503 519 382  35  52 205  78 445 388 334 130  10  48 125 346 203  22 387
 202 254 467  21 121 107 463 246 377 497 502   1  53 485  81 317 175 172
  74 364 101 285 411 124 218 522 527 315 337   7 290 437 104 284 368 109
 259  85  46 484  28 235 103  97 402 215  91 273 400 110 127 509 117  47
 138  42 476 111 472 333 299 311 201  67 405 489 409 139 222 151  65 212
 446  72 242 516 465 528 187  12  98 271 277 330  61  66 512 137 158 304
 469 441 386 394 209 183 468  58 395  19 515 244 320  39 171 180 521 488
  95 500  41 264 321 371 207 442 464 102 275 360 131 504 408 332  96 397
 318 322 143 513 401  29  90 413 312 266 214 116 480 149 501 276 163 176
 281 164  25 323  18 272 297]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5640
INFO voc_eval.py: 171: [ 30  16  52  49  78  22 130   3  58 125  65  98  15  84 128 106  80 102
  34  95  94  89  74  29  51 137   2  28  63  79 124  17  61 109  54 105
 126  75   7  97  92 129  57 127 103   8  88 132 104  21 115  48 120 119
  20   0  12 100  87  37  11  39  46  13 118  82  67  42  24 133  64 131
 101  45  36 111  81  86  32 116  55  66  68 121  23 107  71 110  59  77
 122 123  56   4  40  99 117  18  33  90  31   1  19  14   5  26  60  70
 108 113  50  25  91  43 134  73   9 135  69  72  35  62 114  53  41  83
  38 136  10  93 112  47  44  76  27  85   6  96]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5980
INFO voc_eval.py: 171: [297 320 134 394  75 437 474 128 234  11 427  93 158 325  59 438  24  18
 264 211 347 216 192 419 212 418   3 240 439 431 391 351 453 179 230 403
 345 346  92 157 239 260 314 396 401 343 461 367 118 443 298  37 269  19
  83 270 423  89 178  84 326 210 242  60 205 257 181 190 487 279  52 237
 169  28 369 483 149 244  38 400 236 426 265 344 291 414 336 249  40 180
  54 182 168  56 250 390 160  95  64 463 199 115  69 215 248 194 370 373
 241 286 329  50 417 490  31  65 434 294 283 145 243 447 354 137  14 235
 251 299 208 296 174 335 246 113 430 442  44 107 282 402 259 109 469 219
 409  36 191 405   2 203 102 186 176 364 155 422 119 382 252 361 446  41
 384 404  86 416 232 151 448 214  78 184  26 175  27 110 268 307 476 183
 360 289 432 209 154 200 217  55 342 389 324 127 245 284 460 473 309 218
 172 141 220 293 481  91 253  61 412 177  17 425 275 315  58 311 355 372
 478 313  76 125  57 321 121  97 406  10 207 162 213  23 454  98 262 385
 163 152 165 100 436 136  33 484 467  15 397 328  66  25 381   7 144 138
  62 277  47 429 395 161  96 341 187 456 357 197 363  77 334 408 132 278
 410 101 193 366 166 142 129  43  53 365 339 441 280 420 318  49 274 295
 444 202 106  90 433 477 421 379 173 124  42 258 480 238   8 337 475 305
 288 424 225 159 413 350   9   1  94  82  16 375 189 340  71 449  85 415
 150 281 407 206 167  63 290 378  79 471 316 116 272 122 458  22 489 104
 256  51  32  72 135 171 147  70 349 227 435 374 195 470 185 131  39 247
 451 139 198 301  30 333 392  68 322 133 276  80 488 285 376   5 358 323
 457 123 377 482 359 459 112 383 292 146 310 491  99  12 143 126 164 254
 196 304 156 222 263 117 271 140 261  35  67  21 312  88 466 153 452 338
 464 362 462 398  46 231 380 114 352 468 455  20 105 255 111 428 368 223
  74 332 371  34 440  73  29 108 170   0 393 317 302 273 103 356 188 228
 399   4 130 348 120 266 224 287 472 479 445 411 450 221  45 204  13 465
 303 267 233 229   6 486  87 485 148 300 330 331 388 308  48 327 319 306
 226 201 353  81 386 387]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4581
INFO voc_eval.py: 171: [ 4  2 10 46 11 41 21  3  0 35 44 33 43 26 17 23 19 14 45  5  7  9 25 38
 39  6 27 42 47 29 28 13 34 22 24 18 20  1 15 16 32 12  8 31 36 40 30 48
 37]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0045
INFO voc_eval.py: 171: [1543  361  159 ... 1062  114  459]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5217
INFO voc_eval.py: 171: [30  4  2 58 46 21 23 17 62 67 52 19 42 65 36 68 14 29 57 31 47 26 64 38
 63  6 69  5 33 55  3 53 56 49 45 66 51 50 27 20 11 34 24 35 13 22 18 28
 10 60  8 15  7 48 32 41 44 25 61 39 59 12  0  1 37 16 40 54  9 43]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1758
INFO voc_eval.py: 171: [378 173  89 165 328 185 248 250 242 227  47  36 144 289 335 175 406 142
 130 338 363  18 168  17  77 212 141 380 326  46 262 238 314 353   1  60
 334 399 256 350 404 287 172 109 259 428  62 270 313 252  80  95 145 311
 191 365  39 108 367 280 131 282 354  10 267 430 411 398 324 336  30  61
 434  92  25 357 269 217  26  90 161  76 371 274  14 113  28  58 294 407
  68  57 190  22 303 251 123 154 264 414 330 419 374 234 281 387 261 136
 370 426 133 288  51  12   0  16 132 243 118 189 213 436  19  43  35  42
 305 396 237  96 139 138 423 400   5 208 358 110 307 393 279 300 127 366
 202 318 341 129 115 225  11 435  48 410  97 368 295 304 418 348 385 355
 409 349 140 257  55 149 392 388 153 239 382   7 147 375 120 211 157 223
 361 345 291 107  82  94 245 233 351 364  13 218 174  34 431  15  59 128
  66 258  27  20 284 104  70 137 106  79 317 319 114 222 215 315  63 299
 359  84  53 272 266 425  71 424  81 194  93  31 111 183  83 143 346 292
 226 220  91 422 170   6 232 235 263  64 306  56 298 192 352 397 333 343
   4  32 373 408 188  69  99  38 156 372 167 135 103 207 285 283  72 160
 241 325 376   9  85 433 401 116 163 293 164 122 231 278  41 209 273 112
 329 240 162 186   3 421 100 389 199 184 405  75 260 171 221  33 395  23
 198 427 429  67 151  45 331 337  21 219 412 197 312 277 301 196 126 276
 340  88  98 236  87 181 224 322 176 169 356 377 384 390 193 383 309 216
 201 177 323 200  78 228   2 420 321 249 413 310 286   8 117 105 195 339
 402 210 342 180 158 182 297 205 268 369 102 275  49 254 381  37 214 415
 344 244 150 347 125 203 394  50 290  24 247 101 362 360 302 206 327 320
 255 179 403 416 417 148 271 253  44 332 146  73 187 166 134 155 308 229
 316  52  74 124 386 230 379  29  86 152 119 391 296 246 159  54 178  40
  65 121 265 432 204]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4025
INFO voc_eval.py: 171: [ 93  75  46  51  12   1  49  28  52  25 102 103 101   3  17  20  27  68
  80  44   0  16  61  47  43   7  26  40  70  98  33 106  78 104 107  39
  23  15 105   9  14  21  95  32  73 100  42  67  81   6  72  96  86  65
  57  74  62  54  18  55  53  37  60  31  38  77  36  92  50   4   5  99
  88  29  10  66  84  24  11  48  82  91  90  79  30  64  87  63 108  85
   2   8  69  19  41  76  97  56  45  71  59  83  35  13  58  89  22  94
  34]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0522
INFO voc_eval.py: 171: [68  8 57 80 11 56 44 84 82 25 47 43  9 54 55 72 45 59 78 74 10 60 33 61
 69 39  3 75 50 17 28 18 24 77 16 14 22 81 35 67  4 58 15 40  6  2 30 66
 19 20 31 49 13 48 71 29 51 76 27 32 41 62 83 79 34 70  7 21  1 73 37  5
  0 53 52 23 65 63 36 26 42 64 38 46 12]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3918
INFO voc_eval.py: 171: [ 29 220  19 138  98 163 194 144 231  32  11   6 103 170   5 208 145 158
  91 161 225  81  92  49  55 149 207 160 185 134  44 111 139 188  93 173
  12  83  69  43  62  30 127  72  89 162 125 154 237  59 126 113  78 108
 120 117 174  18   1 146 172   4  56  99 228  63  66 164 183  25  80 238
 193 118  23 182 169 132 184 214 110   8 192 191  17  33  71  76  41   2
 210  85 159  36 176 106 187  13 122 230 157  96 232 133  90  54 121  39
  34 105 171 152 202 116  70  74 201 137 112  88  10  60 235 233 195 143
  35 215  15 240  24 180 166 226 101 156 209  58  16 197  97 239 135  50
 114 241  77   9  53  52 206 130 107  82 119  42   0 186 177  45 213 203
 190  84  31 136 212  68 153  28 165  51  38  87 227 140  26  65  94   7
  48 147  37 198 219 115  86  57 211 131  73 168 221  64 150  67 196  61
 124 167 102 141 128 224  47  46 129 223 142 100 199  14 155 109 189  22
 151  20  75 234 178 236  21 200  79 205 175 148   3 204 181 222 123 104
  27 218 217 179 216 229  40  95]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7436
INFO voc_eval.py: 171: [2031 1258 2045 ... 2324  996 1434]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6035
INFO voc_eval.py: 171: [123 146 456 212 740 137 238 689 155 150  75 223 310 631 744  76 299 692
   6  77   5  49  99 445 187 387  78 484 596 225 434 505 242 602  80 416
 239 776  79 658 260 741 273 457 690 662 554 224 545 493 634 170 688 162
 366 168 277 674 507 743 686  86 143  54 506 605 496 604 216 754 165  62
 742 133  89 763 603  63 274 422 356 433 696 337  34 228 120 153 141 227
 389 679 549 105 616 428 312 757  84 519 601 508 262 229 323 440 671 641
 357 186 775 745 307 344 477 677 725 665 694  42 390 338 678 288 110 243
  10 481 553  96 567 691 164 608 801 435 607 124 142 497 218 695 449 635
 795 726 606 101 423 234 358 623 535 388 408 499 748 371  65 246 315 447
 392 666 798 753 458   7 626 473 207 749 552 672 581 612 372 632 475 610
 611 638 442  13 659 514 474 281 431 294  88 341 295  67 761  91  35 485
 359 177  52 219 151 681 167 714   8 362 693  31 498 232 140 317  44  83
  37 640 597 472 802  66 166 257 276  90 321 769 412 636 774 188 526 534
 404 561 525 107 668 751 480 126 130 622 527 355  85 521 643 703  43 267
 245 718 522 265 797 421  14 770 764 367 279 108 723 376 148 781 189 459
 721 230 682 511 326 476 217 483  57   1 395 651 563 192 201 566 587 804
 709 231   4 209 720 437 414  26 738 221 750 536 450 515  92 639 621  60
 513 463 729 436 117 789 314 331 555 593 410 569 699 727 181 226 644 264
 328 311 261 516 342 353 462 787 808 103 248 308 125 460 179 766 190  12
 609 444 275 663 624 290 680 116 266 637 397 620 174 794 378  50 574 136
 560 417 455 570  70 403 796 278 131 698 271  56 469 180 406 664 517 335
 786 284 391 793 772 705 249 588 236  94 286 301 175 106 379 739 325 289
 655 197 270  19 104  64 340 198 132 732 598 550 334  17 780 756 208 244
 600 599 443 193 700 628 176 617 237 169 346 771 806 424 782 370  29  59
 318 565 322  40 330 385 369 656 590 465 528 235 799  20 365 454 381 706
 345 298 375 722 654 577 573 572 327 627 145 291 629 306 160 453 363 687
 613 509 736 396 159 645 419 438 615 783 256 580 158  16 800 735 127 297
 788 701 650 676 349 350 619 135 546 512  45 540 683 779 657 156 803 716
 426 439 712 614 348 704 336 420 710 785 784 697  58   0 280 418 220 667
 446 495 351 731 429 138  15 502 184 551 747 571  33 240 304  27  68 128
 258 466 625 441 583 305 393  22 407 542 524 154 452 533 282 767 669 673
  25 210  32 537 118 768 121 411 755 149 728 489 134 251 470 373 427 584
 778 144 401 520 254 648  69 633 394 119 296 523 652 204 386 332  46 467
 595 329 491 183 758 191 482 653 309 471 547 684 530 746 760 383 737 163
 324 670 112 531 293 402  36 730 589 206 468 503 415 582 425 594 259 200
 487 642  87 211 579  38 711 661 287 448  24 111 313   2  73  28  95  11
 717 500 529 805  71 343 250 558 339 172 213 752 719 430 733 199 399 347
 303 773 647 478 501 129 486 272 660 354 171 316 792   9  81 461  48 685
 203  23 556 765 241 541 368 559 214 562 585  39 762 586 400 178 494 114
 196 268 122  74 618 777 102 253  82  21 504 568 100 185 510 413 194 708
 715 724 646 360 302 538  30 539  98 675 405 374  47  93 548  41 147 490
  51  55 592 269  72  18 759 173 543 380 333 300  53 464 205 195 544 292
 233 113 161 255 591 319 576 109 384  97 222 564 202 734 432 157 575   3
 361 532 488 364 352 152 247 398 630 451 707 409  61 382 713 807 182 479
 377 283 791 790 649 263 252 115 285 702 215 557 320 578 492 139 518]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5871
INFO voc_eval.py: 171: [53  4 49 28  2 37 64 47 46 44 62  6 63 69 26 51  7 11 32 31 58  3 15 21
 24 35 68 27 33 50 57 10 61 60 12 34 29 22 59 30 14 23 42 36 52 41 16 38
  1 39 18 67 65 54  9 20 66 40 17 55 48 43 45 25  0  5 56 13  8 19]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3081
INFO voc_eval.py: 171: [ 61  62  19  95  91  52  60 145  96 112 107  30  72   3   2  70  23  27
  49  83  45 131 102  93  64 117 119  78 126 147 137  80  24 103  46  71
  37 104  26 116  88 100  76  43 133  12 111  40 138  85   8  66  22  86
 121   0  13  16   5  44  18 143  81  29  10 134  54  92  36 128  56  20
  55  50  79  25  34  94 142 127 124  68  28  57  53  89 108   1  65  98
   9   7  77  32  35  84  74  90 144 120  75  41  33  21 105 129  14  67
 135 114  73 146  47 122 141  69 109 118 113   6  97  17  51 110 136  42
 140  31  87 139  48  39 106   4 123  59  11  63  99 132  38  82 130 125
 115 101  15  58]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3674
INFO voc_eval.py: 171: [107  14  62  10  34  50  91  22  75  53 135  29 137  76  71   7 145  32
  30  51  94 149  60  55 123 140  77  48  90  12  66  84  19  26  31  13
 125 124  88   8   4 119 150  95  39  35 100  16  21  98 101  85 128  68
 141  27  54  37 154   6 148 114 111 105  49  64 118  99  47  42 121 130
  93 155  59  23 132  83  65  45  46  67 138  73 152  86  25  57 116 143
  81 103  33 120 142   1 131 133 104  96   0   5 129  52  70  20 108  92
 110 136 127  41 109  44 139 146 115  36 157   3  18 106  38 156 153  15
 151  24  72 147 102  43  40  80  69  97  63  56  78  89  61 134  28 144
  11 117  87 126 122   2   9  79  17 113  82 112  74  58]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3783
INFO voc_eval.py: 171: [134  77 198 212 293   3 159 190  20 267 255 163 121  18  30  72 213 288
  43 274 300 217 104 166  33  23 208  59 245 199 169  64 177 196   7 189
 309 103 200 117  97  74 231 297 221  47  89 204 218  32  44 188 305 259
   2 207 138 105  56 205  51 216 165 289 143 229  71 228  66 192 275 157
  62 312 223  28 224  45 310 276 147 283 146  48 242  90 118  49 253  61
  82 230 129  39 113 222 246  73 123  63 277 162 109 174 290 232 306  75
 252  19 175 164 261 145 102  14 286  22 186 226 272 168 285 101 173 307
  60 234 178 233 136 119  85 155 263 131 236 184  13  31  34 209 120 128
  15 116  46 280  95  76 158 313 125 203  65 179   6 227  26 195 214 268
 150  12  36 278  80  38  67 172  50 130 257 284 262 201  21 127 167 156
 107 249  81 247 308  68 215 260 140 266 303 292 273  94   4  24 197  99
 282 206 311 137 112  69 265 244  37 281  27 161 122 269 251 176 298 211
 187  88 235  42   8  54  10 314 124 170  16  11  29 254 194 153 183  25
  17  86 180 126 250  87 291 108 148 182 139 294 181 152 248 287 299 132
  98 133 304  58 220 142 243  93 171 193  91 279 185 295 238 160 225 301
 239  78 115 264  92 144  96 110 154 149 240  35   9 258  55 135 256   5
   0 210 237 241 141 114 106 270 111 302 271  70  79 296  57  41  52  53
 151  84 202 100  83 191   1  40 219]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4635
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4085
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.342
INFO cross_voc_dataset_evaluator.py: 134: 0.622
INFO cross_voc_dataset_evaluator.py: 134: 0.237
INFO cross_voc_dataset_evaluator.py: 134: 0.349
INFO cross_voc_dataset_evaluator.py: 134: 0.564
INFO cross_voc_dataset_evaluator.py: 134: 0.598
INFO cross_voc_dataset_evaluator.py: 134: 0.458
INFO cross_voc_dataset_evaluator.py: 134: 0.005
INFO cross_voc_dataset_evaluator.py: 134: 0.522
INFO cross_voc_dataset_evaluator.py: 134: 0.176
INFO cross_voc_dataset_evaluator.py: 134: 0.402
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.392
INFO cross_voc_dataset_evaluator.py: 134: 0.744
INFO cross_voc_dataset_evaluator.py: 134: 0.604
INFO cross_voc_dataset_evaluator.py: 134: 0.587
INFO cross_voc_dataset_evaluator.py: 134: 0.308
INFO cross_voc_dataset_evaluator.py: 134: 0.367
INFO cross_voc_dataset_evaluator.py: 134: 0.378
INFO cross_voc_dataset_evaluator.py: 134: 0.463
INFO cross_voc_dataset_evaluator.py: 135: 0.408
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.492s + 0.001s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.360s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.360s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.374s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.377s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.374s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.373s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.374s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.373s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.371s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.370s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.373s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.373s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.541s + 0.002s (eta: 0:01:07)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.394s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.388s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.379s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.376s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.375s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.383s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.383s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.383s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.384s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.383s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.383s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.382s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.434s + 0.001s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.386s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.369s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.364s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.363s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.370s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.369s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.372s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.374s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.374s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.373s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.372s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.371s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.484s + 0.001s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.378s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.374s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.369s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.370s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.366s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.370s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.368s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.365s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.364s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.362s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.363s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.361s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 56.108s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [165 210  53   5  83 263  47  18 200 111  41  49  46 125 136 269 158 107
 161 167 280 135  40  73 169 283 290 103   1  44 159 235  42 186 292 124
 245 112  72  21 115 247 221  28  75 139 258  35 145 120  65 238  96  93
  74  45  87 296  27 220 276 208 144 282 166 219  11 224 104  25  52  32
  79 264  54  20  61 162  17 142 239  99 152 164 255 240 244  50 137   3
 182 151 194 288 209 285 121  66 153 192 223  22  89 188 181 289 131 278
 116  38 256 253   6  55  30 261 140 141 157 150 207  70 190 134 177  80
 133  97 299 205  34 270 281 237 171 138 101 160 250 286  23  64 254 149
 119 187 106  16 197 246 183 298 203 265 228 176 226 232 231 126  15 233
  92 156  39   9  95  57  51  60 267 227 179  33 266 170 262 260  19 218
 248 275 291 191 198  12 215  67 196 108 122 251 301  24  26 173   4   7
 180 118  68 123 155 284   2 102 234 216 146 217 193   0  84  69 268 127
  76 105 109 259  82 295 297 222  78  59 172 202 110  29 184 185 225 113
 178 214 163  63  13  91 199  86 189  31  77  85 243 195 241 168  56 230
 100 204  10 273 279 277  98 213 130 212 148 249 293 143  81 252 300 201
 128 274 174  43  90 236 211  88 117 206 242 114 294  14 287 229  48 129
  62  71 175 154 271 147  94 257 132  37  36 272  58   8]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3436
INFO voc_eval.py: 171: [172  49  24 107 146 198 207 210 196 113 209  99  22 126 211 197  78 150
  17 158 183  25 199 142  70  82 144 147 201 203  26 195 154  81 167  48
 103 136  74 105 131  16  55  32 218  11   8 171  31 187  57 134 127 123
 213 182 206  69 178  39  85  68  88  79 151 214 168  62  38 122  83 152
  75   1 111 115 163 149 132 191  34  67  77  86  30 193   5 114 204  27
 208 130 128 112 220 185 140  45 108  52  54 223 212 190 222  65 119 160
 192 217 120   9  47  93 162  44 169 181  21  33  92 180  23 156 205 110
 125 164  66 104 121  96 175 148 221  90 129  60  50 100 145 219 106 157
  76 166  64  19 137 141  42 101 159  98  53 186  35  43  58  80 188 117
   4 189 216 215 224  73  63  97  14  71  41 176 202   7 133 118 102  46
 179  89  94   6  56 194  36  61  29  72 138  37  10  12 174 143  84  51
  87 116  28 200 170 161  95 173 184   0   3  59  20  18  15 124  40 177
 155  91   2 165 109  13 139 153 135]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6167
INFO voc_eval.py: 171: [ 66  27  83  71  52  50  51   2  82 167 135 183 184 169 185  62  24 100
 210   1 101  40  63 170 147  53  59  79  85  81  10 150  74 206 142 197
 116 203 176 186 118 187  42 204 216 154  34  14 162 189 148   5 152 153
 215  73 157  61  56 145  64 133 130  23  80 119  72 182  58 199  45  93
  76 151 178   9  32 177 159 173  43  18  16  13 192 190 171  69  12  95
 112 104 117 156 181  47 188 144 122 208  54 139 194  77 111 209  48 124
  87 213 198   6 137 211 180 214 128 141 201 120 179 143   4   7 121  21
  88  38  44 127 132  41  89  26  97 115  60  55 129  30 123  49 195  98
  91  35 212  68   3  70 126 149 114 200  67 172  25  99 165 103  31 163
 174  20 160 106  46  94 105 202  36 125 109  65 205 207 113 134  57  17
 217  78  90 158 164  11 191 196  92  96 102  19  84  22  75 168 140 146
 108 166 161 138 131  15  28  29 110   8 175 155 136 107 193  86  37   0
  39  33]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2308
INFO voc_eval.py: 171: [323 374   0 324 109   8 325  51  88 264  27 266  55 355 459 148 121 351
 259 368 263 453 265  10 205 402 120 200 257 123 146 326 103 458  92  29
 267 404  39 180 242 303  89 316 194 395  32 403 346  21 217 481 401 436
 171 104 292 125 344 375 204 248 451  64 511 110 270  63  25 182 412 397
 393 367 251 461 271 188 483 157 269 238 478 390  31 220 319 409 422 302
 469 485  28 329 443 313 276 116 430 396 307 384 489  69 468 341  35  90
 394 360 179 256 258 277 465 507 467  13 149 471 195 181 305 460 190 252
   3 492 356 345 433 223 166 417 260 419 289 174 381 431 385 504 291 192
  14 482 219 198  24 280 241 283  30 477 495  61 226  97 131 498 296 330
  49  41 462 183 294  38 243 140 215  94 295 147 311 160 199 312 378 132
 505 230 386 297 497 262 446 416 151 318 288 124 278  75 332 363 237 176
 130 484 377 413 232  47  76 426   4 359 229 440 293 247 496 494  68 168
 150 184 510 320 268 281 447 331 350 304  71 113  17 228 454 111 167 372
  42 309  45 364 421 274  96 210 114 314  54 134 178 273 209 340 380 145
 290   6  53 212 490 479 221  93  85 423  99 450 129 480 334 236 405 315
 105 475  95  36 370 336 502 233  81 250 287  60 272 449 399 231 428 392
 500 254 321 155  50  52 411 186 127 191 285 244 240  16  48 470 126 338
 439 153 432 216   2 474 362 463  65  70  26 261 366  46 418 347 503 322
 457 473 239 224 442 488 406 112  57 119 133 425 424 357 352 161 235 328
  23 342 343 206  44 122 365 373 466 142  78  83 197 310 175 225 448  12
 139 214 499 408 353 253  33  18 317 335 187 201 501 170  79 456 102 173
 455 193 154  22 135 506 387 410  19 354 284 279 158 491 339 438 376 107
 208 434 163 389 213 246 333 222 234 108 437 464 286  62 369 117 185 202
 487 486 429  77 358 136 143 445  11 218  37 138 282 159  84 327 301 137
 348 398 189 476 152 452   5 415 441 249 141 444  86 391  87 493 300 106
 349  40 427  59  58  80 101 371 382  73 388 414  34  91 115 400 211 420
 165 509 196  72 379 144 299 275 361 100 156  15 203   7 407 227 172 508
  74 255  98 306 298 337  20 207 383  43   1 177   9 164 162 435  56 472
 169 128 245  82  67  66 118 308]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3333
INFO voc_eval.py: 171: [405 215 406 219 189 410 408 192   2 333 220 188 407 276 103 190 509 245
 335 510   5 174 316 334 317 394 191 118 295 175 472 247 332 418 331 417
 420 337 426 159 347 463 442  30 234 412 430 409 411 432 468 427 423  73
 413 336 264 437 431  72 484 112 329 258 377  31 143 144 224  15 165 281
 361 273 283 444  22 435 493 439  53 282 339 221 344  45 243 134 365 184
 421 355 490 248  74  70   7 231 359 491 290 195 238 130 318  44   6 275
 341 223 125 156 162 434 340 383  43  79 414 330   3 285 274 368 400 370
 362 424  96 160 167 182 149 460 422 342 163 146 113 246 196 303 366 187
 306 465  81 292  39 511 348 387 202 353 235 132 128 436 438 194  17 320
 457 242 328 257 461 459 211   4  50 446  26 492 479 222 440 197 207 445
  14 107 473  10 404 228 287 299  40 363 240 198 321  23 379 378 145 364
 494 168 505 374 346 469 371 416 119 166  36 225  11 367 397 179 193  16
 111 401 302 503 254 152 216 263  25  65  61 151 319 352 495  55 252 393
 173  32 141 350 212  90   0 249 381  98  54 345 496 289 244  66 392 256
 288  85 507  80  62  38 301 373 358 251 260 158 452 177 312  56 154 232
 233 209 104  68 226 300 309  13  84 477 153 450 253  49 384 504 171 403
 402 170 482  63 324 296 217 286 155 204 227 230 380  77 447 443 265 464
 139   9  34 178 164 485 131 375 369 269  97 298  67  19 114 267 391  86
  59 147 236 106 478  91  75 206 467 123 343 121 480 453 183 117  83 277
 398 476 419 415 307 349 186  76 176 140 351 386 502 372  58  33 213 338
 127  29 488  12  48 124 250 262 133 481 203 376 470  21 425  51 120 201
  20 327 241 483   1 200  82 441  37 433  71 455  99  52 308 310  78 356
 214 487 122 284 210  60 255  27 278 462 102 279 110  89 497 229 280   8
 448 471 512 126 109 388 389  46 357 268 305  94  47 325 116 137 293 294
  18 323 136 498 266 101 150 475 458 513  69 218 322  64 157 500 205 385
 382 304 313 185  95 135 396 237  57 456  92 501 239 314 474 272 428 454
 489 297 486 129  41 506 508 429 395 449 208 181 161 108  42 169 360 180
 138 142 326 390 115 270 354 105 451  93 315 100 172 261 199  24 311  87
 291 148  88  35 399 271 499  28 259 466]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5632
INFO voc_eval.py: 171: [ 31  15  52  49  78 127  22   3  58 122  65  96  14  83  35 104  80 125
 100  93  92  87  74  30  51 134   2  28  63 121  20  61 107  86  79  54
 103 123  76 126  95  57 101 124   7 102 129  19  21  12 117  45  37 112
 116  90   0  48  98  97 115 130  46  85  38  82  40  67  10  64  43 128
  25  81 109  99  33 113  84   6  55  66  68  24  27 118 105  11 132  23
  59  71  56 108  41 119 120   4  17  77 114  34  32  94   1   5 110  88
  44 106  16  70  50  26 131  69 111   8  18  47  89  73  13   9 133  29
  60  72  53  91  62  42  39  75  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5980
INFO voc_eval.py: 171: [294 315 131 387  73 464 125 426 230  11 416 156  91 427  56 320  25  18
 209 262 344 214 190 409 211 407   3 428 420 384 231 442 395 347 177 339
 228 342 155  90 310 341 241 257 393 388 450 360 432 115  36 266 295 267
  19  87 412 176  81 208  82  57 321 203 233 255 476 188 179 167 237  49
 362  29  37 473 276 392 243 147 235 415 263 246 338 405 329 288  39 178
  52 383  53 247  61 166  93 158 198 216  66 112 452 182 192 213 408 245
 324 366 282  30 479 363  63 280 423 291 242 348 248 141 134  14 232 206
 296 330 172 293 105 279 110 258 431 244  43   2 353 401 419 394 357 457
 217 184 100 189 397 153 106 201 436  35  84 376 116 236 174 249 434  26
 378 396  27  76 340 181 265 437 303 173 149 215 466 180 107 286 421 152
 199 354 207 449 382 337 210 319 281  51 305 462 170 240 218 138  89 290
 471  40 411  48  58 251 403  96  17 161 175  41 467 205 306 273  55 365
 309 311  24  28 349 122 212  98  95 160 163  54 379  10 316 425 443 260
 118 414 398 150 474 133  15  59 456 323 375  32 274 418 389 314  94 135
 159   7 185 143 336 445 356 197 124 129 195 275 126  75 140  99 164  74
 350 399 402 331  44 359  42 334 238 313  88 200 358  50 104 277 460 191
  45 373 171 292 433 468 430 410 470  80 332 422 121 223 271   8 285 256
  47   1 465   9 404 302 346 413  92 157 438  68 335  16 187  69 278 307
 119 165  83  60 400 148 204 368 287  23 113 372  46  70 196 254  67 424
  77 447 132 269 367 102 317  65 297 478 272 345 440 459 312 123 183 144
  31 136 224   5 128 477 130 406  12  79  22 385 472  78 120 328 139 268
 448 162 222 446  97 318  38 351 369 289 377 480 109 220 114 154 300  86
 308  64 194 252  21 352 137 441 169 261 151 370 371 227 355 259 103 111
 374 453 253  62  34 234 390 455 444  20 108 333 283 168 142 361 117  72
 299 221 458 429 469  33 386 127  71 364 451 101 475   0 229 439 270 284
 327 417 343 381 435 225 202   4 186 298 301   6 461 326 391 325 380 322
 145  85 264 250 454 226 193 146 239  13 219 304 463]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4634
INFO voc_eval.py: 171: [ 8  4 15 51 46 16 14 26  6 13  0 40 49 31 38 48 22 28 50 19 11  9 24 44
 30 52 10 45 47  1 32 34  3 33 42 25 18 39 20 29 23 21 27 37 17 12  7 36
 35 41  2 53 43  5]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0043
INFO voc_eval.py: 171: [1470  152 1472 ... 1183 1158  890]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5190
INFO voc_eval.py: 171: [30  5  3 57 45 21 23 17 51 61 66 19 42 63 36 16 67 29 56 31 46 26 38 62
  7  6  4 59 68 54 33 52 55 48 49 44 27 65 50 20 12 34 15 35 10 22 28 18
 24 14  9 58  8 13 32 25 41 43 47 60 64 39  1 37 40 53  0  2 11]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1801
INFO voc_eval.py: 171: [379 170  92 161 328 185 250 251 243 227  47  35 142 290 335 128 406 140
 338 363  17  16 164 139 212  75 263 238  90  59  46 353   1 399 334 257
 325 404 288 350 169 108 260 271  60 429 178 253 313  79 365  96  61 311
 314 281  38 190 367   9 145 129 107 283 397 354 268 158 434 431 324  24
 336  29 270 411 357  13  89 217  57 359 372  25 112  74 254 388 374  67
 407 401  98 275 189 122  56 151 248 295 371  21 380  11 415  26 265 234
 282 289 262 326  51 420 331  15  18 213 134   0 436 368  34  43 131 395
 237 117 137 188 231 136 306  41 318 428 422 424 302 242 341 207 201  48
 383 125 280   4 114 127 301 109 366 225  10 308  97 410 435 110 305 130
 258 409 348 419 147 138 144 393 376 239 296 119 150  95 343 154 349  33
 386  81 355 209 361  54   6 223 345 218 106 171  12 364 292  14 245 432
 100 351  58  65 259 103  77  19 286 126 391 222  62 105 317 358 215  27
 267 135  69 113 300 425  70 273  84 315 220 193 394 141 319  94   5 293
   3 426 233 400  91 168  83 236 396  30 346 352 285 175  53 183 387 187
  80 408  68  31 299 264 191 287  71  99 377 375 307 327  37 153  40 160
  64  55   2  32 294 241 333 232 102 206 186 433 133 274 159 199 111 167
 198 261 373  66  22 208 330 405 329 427 184 195 221 279 157 389 312 430
 197  85   8 115 235 240  20 121 148 303 219 124 423 414 390 278 385 356
 321 181 369 166  45 322 192 378 216 224 412 413 402 337 249 230  88  87
 342 204 244 340 173 384  76  42 104 370 284 155 269 210 182 416 163 298
 200 211 256  49 252 339  36  50 398 180 202 344  73 347   7 360 214  44
 421 323 255 101 132 146 320 152 276 176 162 291  86 277  52 179 304 418
 362 309  93 310 229 116 226  28  78 194 332 272  72 403 205 417 392 118
 177 143 228 297  63 381 382 120 123 196 246 247 156  23  39 174 165 266
 316 149  82 203 172]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4013
INFO voc_eval.py: 171: [ 94  48  76  54  13   2  52  26  29  55 102 103   3  18  21  28  71 101
  34  81  46  17   0  64   1  49  79  45   8  27  72 106  98  42 104  24
 107  16  41 105  96  10  15  22  33  75   7  82 100  97  70  74   6  44
  68  88  65  86  19  57  69  59  39  38  63  32  37  78  93  56  99   5
  53  90  60  11  30  85  12  25  50  14  83  92  67  80 108  89  31  91
  66  58   4  20  77  51   9  43  23  73 109  84  62  47  40  35  87  61
  36  95]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0515
INFO voc_eval.py: 171: [73 62 85  9 12 61 49 87 89 27 52 29 10 59 77 60 51 83 64 79 11 74 66 37
 65 55 44  4 80 32 26 19 20 82 18 14 24 86 63  5 72 50 17 45 40  2  7 34
 71 53 22 35 13 54 33 21 56 76 36 81 67 39 84 31 46 75 88 23  8 30 78  1
 15 16  6 25 58  0 57 69 42 70 68 43 41 28 48 38 47  3]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3969
INFO voc_eval.py: 171: [ 28 216  17 136  95 190 160 141 226  32  12   6 101 166   5 204 142  89
 155 221 158  79  48 203  90  54 132 181 147  43 109 157  13 137 184  91
 170  81  67 125  42  71 151  87 159 123 233 124  77 218 119 106 116 171
 111   1 143 169  56  61   4  97  58  62 161  65  25 179 189  78  53 224
  23 234 167 178 110 117 209 130 108 187  59  70 180  18 112 188  34   2
  40 173  75  14  36  83 205 104 225 121  93 156  31 154 183  88 131 149
 120 227 115 103  39 168 198  69  33 197 135  11  73  24  86 210  35 236
  19 229  15 191 163 133  99  57 153 222 193 235  16  52  94  49 202 140
   9 237 113  76  51 105 176   0 128  80 228 220 118  41   7 208  44 199
 186  82 134  30 207 174  27  50  64   8 223  47  29 194 148 206  38  92
  85  37  68  26 162 200  72 144  66 129  46 138 146  20 195  55 114 122
 165 192 215 100 127  84  60 182 139 164 177 185  45  63 126  98  22 107
 219  10 152 172 175  96 231 230 196  74 217 214  21 201 212 213 145 102
 211   3 232 150]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7362
INFO voc_eval.py: 171: [2200  422 1369 ...  979  370 1598]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6184
INFO voc_eval.py: 171: [117 140 434 200 709 663 146 227 128 143  71 211 295 603 712 284 666  72
   6  74   5  47 176 425  73 369  75 461 482 568 228 213 414 574  77 396
 229  76 249 741 710 630 435 634 664 530 260 212 469 607 521 157 153 350
 261 160 484 660  82 711 137 662 483 472  60 576 577 204  49 724 646  59
 720  84 158 730 575 129 262 401 322 648 339 413  32 670 485 419 135  98
 114 145 525  81 653 371 589 408 299 251 573 497 308 698 217 613 332 340
 175 292 436 713 652 668 539 637 323 104 645 372  40 651 490 273 233 119
 529 580  91  11 457 303 136 415 767 579 112 473 215 761 429 665 207 342
 742 669 370 627 696 578 214 402 453 355 235 513 476 388 596 451 427 722
 194  62 764   7 526 599 373 553 356 605 714 604 582 584 450 583 452 631
  85 569 422 411 326 716 268  33  64 278 667 280  88 493 167  99 341 686
   8 205 218 462 346 655 144  50 223  29 134   9 162  78  42 449 612  35
 118 246 737 159 609 306 768  63  86 392 177 505 263 676 740 512 384  80
 719 338 642 508 614 501 487 595 456 126 255 535 275 252 763 234 502 688
  41 725 693 400 351 265  16 656 178 691 103 488 181 732 221 707 360 222
 745 141 197 417 458 770 189  54 681   4 311 537 172 206 378 560 300 437
 625 540 690 611 474 489   1 594 163 209 111 394 514 542 416 699 301 440
 216 232 170 315 717 495 695 673 565 531 755 389 430 647 327  97 120 718
 313 179 297 439 773 581 559 253 237 424 264 635 250 593 102 654 293 640
 597 534  58 132 106 608 254 762 336  13 446 570 433 266 377 622 543 671
 547 156 169  53 362 636 258 286  67  30 760 561 269 753 383 127 736 759
 678 496 100 708 164 386  18 274 238 491 276  19 310 185 125  22 320 572
 749 628 325 702 423  61 187 363 257  87 224 182 674 772 166 226  27 727
 349 307 330 571 746 367 527 271 161 195 354 601 590 626 765 353 738 563
 317 705 403 312 442 150 364 358 283 624 149 432  95 679 600  20 506 544
 751 329 550 397 245 345 610  15 661 538 406 151 220 522 602 546  48 591
 588 147 587 754  23 426 692 418 675  17 650 629 333 766  12 748  57 552
 321 131 376 586 704 677 361 282 687 319 683 620 331 769 672  56 657 744
 291  43 723 511 379 715 638 443 398 335 639 289 492 517 471 750 747 701
 267 230   0 479 409 399 420 735 528 684 359 428  31 208 374 290 298 247
 598 431 122 421 734 387 130 198 752 139 133 357 519 500  26 515  24 503
 466 486 196 726 101 390 407 110 617 556 240 171 697 142 494 643 555 243
 447 115  94 192 281 375  65  66 621 606  89 173 459 316  44 365 468 680
  68 113 279 728 180 448 444 658 644 623 105  39 368 464  34 314 739 504
 729 121 706 567 404 294 551 562 518 682  36 155  83 382 347 633 193 395
 480 771 270 509 566 309 188 523 507  14 475 248 445 124  28 328  90 199
 554 239 410 272 186 352 191 477 337 499 463 202  21 296 201 302 641 689
 380 288 703 259 700  10 733 478 405 659 615 454 324 107 123 557 116 758
   2 558 242 721 731 685  46 533 545 438 154 231  37  79 481 441 184  96
  55 381 536  93 532 520 632 165 694 256 524 585 592 174 168 516 470 541
  69  70 108 743 393 649 460  45 183 344 366 467 343 391  25 285 287 203
 549 236 564 219  52 616 619 618  38 152 385 348  51 305 244 277 210 138
 510 225 465   3 318 455 334 304 190 412  92 109 498 757 756 241 148 548]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5952
INFO voc_eval.py: 171: [54 27  1 48  3 36 63 46 45 43  6 61 69 62 25 51  8 31 26 11 57  4 14 20
 34 66 22 28 32 56 49 60 59 33  7 12 29 21 24 30 58 23 13 41 15 40 35 50
 37  2 17 38 52 64 67 19 10 65 16 39 53 44  0  5 47 42  9 55 68 18]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3105
INFO voc_eval.py: 171: [ 63  64  21  96  92 143  97  62 111 105  32  73   4   3  25  71  29  54
  51  84 130  47  65  94 101 116  79 119  81 146 136 102  26 125  72  39
  28 115 103  89  15  45  14 100 132  76 110  67  86  20  57 141  42 133
   0 137  10  24 120  87   7  17  46  31  93  82 142 127  22  37  56  59
  49  52  95 144  36   1  27  69  60 126  30  55  78 107 124  99  11  12
   9  90  75  66  34  85  38 117  58  91  68  74  23  35 104  43  77  16
 140 134 128 145  80 122 113 121 108 112  53  48  83   8 109  33  18  70
   6 106  98 135  44 139  40  88 129  50 123 118  61  41   5  13 114   2
 138  19 131]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3314
INFO voc_eval.py: 171: [101  14  59  10  30  48  85  22  70  51 129  29 131  71  68  32   7 138
  49  88 142  53  57 134 118  45  72  12  84  63  78  19  31  26  82 119
 116  13   8 112 143  89   4  16  34  94  38 107  79  92  27  52 135  95
 122 147  65  36   6  58 141  99  21 104  47 124  93 111  40 114  87  56
  61  80 126 148 132  44  43  23 145  77  55  64 109  62  25 137  97  76
 117  33   1 113  90 136  46 127   0  50  67  98 123  73   5  20  86  69
 103  35  39 139 130 102 133 121 150 108  42  18  15  24 128  37 146   3
  96  41 149 144 125 100  91 140  66  60  75  54  83  74  11 110  81 120
  28  17 105 115   9   2 106]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3814
INFO voc_eval.py: 171: [122  72 187 201   2 280 147 180  18 255 244 154  16 109  28  67 202 275
 261  39 207  95 155 197  21  31  54 235 285 188 158 166  59 185  94   5
 179 293 105 190  89 212 223  43  69 193  81  30 283 289  40 242   1 247
 150  96 126 194 153  47  52 131  66 221 276 219  61  57 296 262 181 214
 215  35 294 178  41 136  26  48 263 249 233 137 270  82 206  56  37 117
  76 236 209 222 218 103 100  58  68 164 111 213 264 151 241 152 165  70
 192  17 290 134 277 273  93 203  12  92 157 259 163 176 217 272 196  20
 226 265 295 106 168 173  77 225  44 228  55  11 107 291  87 251 113 119
 124 108  13 104 297  60  29 169 116  71 174 208 198 140  24 267   4 184
 256 146 260  46  42  10 250  36 162 156  19 224  62  63  75  33 220 271
  91 246 118 292 238 115  74 205  98  86 110 243 254 279 269 234 237 248
 127  22 287 268  25 253  64 102 257  34   3 186  45 149 195 167 125 177
  80  78 240   8 112   6 284 191  14  49 200 298  50 160 114 142 183 170
  23 227  38   9 189 172 281 288  79 171  15 141 138 128  99 130 266 278
 239 120  90  53 161 121  83 258 216 286 211 144  27 282 204  85 139 230
  73 159  88 274 175 148 252 182  84 101 231  32 245 145  51 135 210 132
 133   7 129 123 143 199 229   0  97 232  65]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4666
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4071
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.617
INFO cross_voc_dataset_evaluator.py: 134: 0.231
INFO cross_voc_dataset_evaluator.py: 134: 0.333
INFO cross_voc_dataset_evaluator.py: 134: 0.563
INFO cross_voc_dataset_evaluator.py: 134: 0.598
INFO cross_voc_dataset_evaluator.py: 134: 0.463
INFO cross_voc_dataset_evaluator.py: 134: 0.004
INFO cross_voc_dataset_evaluator.py: 134: 0.519
INFO cross_voc_dataset_evaluator.py: 134: 0.180
INFO cross_voc_dataset_evaluator.py: 134: 0.401
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.397
INFO cross_voc_dataset_evaluator.py: 134: 0.736
INFO cross_voc_dataset_evaluator.py: 134: 0.618
INFO cross_voc_dataset_evaluator.py: 134: 0.595
INFO cross_voc_dataset_evaluator.py: 134: 0.310
INFO cross_voc_dataset_evaluator.py: 134: 0.331
INFO cross_voc_dataset_evaluator.py: 134: 0.381
INFO cross_voc_dataset_evaluator.py: 134: 0.467
INFO cross_voc_dataset_evaluator.py: 135: 0.407
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.537s + 0.002s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.369s + 0.003s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.363s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.368s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.373s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.374s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.374s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.377s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.379s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.381s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.380s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.383s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.380s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.390s + 0.001s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.407s + 0.001s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.399s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.389s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.381s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.387s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.397s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.398s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.396s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.394s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.394s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.390s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.383s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.408s + 0.002s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.366s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.360s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.363s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.363s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.357s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.357s + 0.001s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.358s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.354s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.353s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.354s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.355s + 0.001s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.356s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.485s + 0.002s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.381s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.371s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.364s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.360s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.353s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.352s + 0.001s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.349s + 0.001s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.349s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.349s + 0.001s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.349s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.349s + 0.001s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.348s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 56.358s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [157 200  51   5  80 252  46  18 102 190  40  56  45 120 131 258 152 103
 155 159 130  70 266  39 269 160 276  98   1  43  41 176 236 119 224 278
  21  69 110 210  28 133  71 108  35 115  62 140 246 227  91 235  89  72
  44 281  83 139 262 268 209 153  50  99 208 213  25 158  76  59  11  32
  52 253  20  27 137  17 228 156  94 154 151 196 147 233 132 244 146 274
  48 229 199 222 173   3 184 178 116  63 148 212  85 171 182  22 275 264
  54 126  37   6 111 198  30 250 245 272 150 136 242 145 135 129 167 180
 259 195  77 284  67  92 163 175 226 267 134 271  34  23 239 144 243 237
 187  96  16  61 283 101 114 177 215 193 123 172 166 254 217   9 221  88
  55  38  90 220  58 161 169  33  49  53 256 216 255  12 251 204 207 104
  19 249 181 277  36  13  64 219  24 117 183 248 188 127  26  97 286 186
   2 121  66  81 164 113 240 223 206   4 141 270 205 149 211 100  79  73
 279 105 106 170 192 214  14 280 257 162  75 168 107  29 174 203  31  95
  60 189  74  82  87 142 179   0 230 185 260   7 232  57 125  10 263 265
 238  93 128 201 194 241 202 261  86 143 285 138 282 231 225  78 122 112
 165  84 273 197  68 191  42 234  15 247 109  47 124 118  65 218   8]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3442
INFO voc_eval.py: 171: [162  47  21 102 138 188 198 201 186 107 200  94  19 120 202  75 142  15
 148 187 174  22 189 134  79 139 136 191 193 185 145  23  78 156  46  98
 125 129  71 101  14  53 178  29  11 208   8 160 173  28 123 117 203 169
 197  66 196 121  81  84  65  76  55 143  60  36  67 157 204 116   1  72
 109 105 144 153 126  64  31 141  82  74  27 183 182   5 108 194 124  24
 199 176 210 158 195  43 103  52  50 180 132 213 212  80  62 181 207 113
 150 114  88  45 152 122  18 146   9  42 172  20  30 171  87  37 159 104
  63  48  99 154 163 119 211  86 115 166 100  91  58 137  95  73  33 147
 209 133 140  61 179 155  16 170 130  40  96 149   6   4  92  51 177  41
 106  56 206  77  32 111  68 168 205  93 167  97 192 214 112  39 128   7
  44  70  85 127  38  34  89  57 184  13 161  54  26  59  35  12  10 165
  69 110 131  49 164  83  90 135   0  25 190 175 118 151   3  17   2]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6227
INFO voc_eval.py: 171: [ 72  30  89  78  55  56   2  88  54 177 192 193 145 179 194  68  26 108
 219   1 109 156  43  69 180  86  92  63 161  12  81 167 215  59 153 207
 212 125 195 185 196 128  90  45 213 227  60 164  37  16 172 198   5 158
 162  80 226 143 163  67 155 182 159  24 140  87  79 129   3  64 191  83
 160  49 102 187  10 223 186 209 169  35  46 183  20 199 181  18  76  70
  15 203  14 120 112  99 197 166  58 127 217  51 202 190  94 154 131 133
 218 119 221  84 225 147   6 189 210 151 137   4  95 130 188  96 152  23
  47 136  41  44  66   9 106 141  29 126 222  32 124  52  53 138 105 205
 132  77  74  98 208  39  73  27 107 123 173 113 135  22 170 149 114  33
 111  97 101 200 134 211 214  50 122  71 216  19 168  31  62  82 104 144
 117 206 228 174  25 178 157 204 100  21 110  85  13  91 175 171 150 148
  17 116 139 224 201 165 184  61  57  28  36  38  40  42   0 115 118 146
  93 142 103  65  75  34 220   7 176 121  11   8  48]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2311
INFO voc_eval.py: 171: [371 319   0 320 103 321   7  85  49 260  30 262  53 352 348 459 143 118
 256 365 259 453 261   9 201 400 115 196 120 254 322 141  86 458  97  28
 119 265 402 176 239 299  82 190  36 396 312 393  32 401  21 343 214 481
 399 435 168 372  98 340 511 451 245 200 104  25 266  61 275 410 178 364
 267  62 391 264 483 461 248 183 153 235 478  31 382 286 389 472 407 315
  27 298 484 442 420 325 309  65 218 428 110 394 489 303 471  35  83 392
 175 357 255 270 253 508 467 123  13 383 469 473  59 192 144 177 185 316
 460   3 249 301 353 221 492 431 342 417 415 257 223 162 194 288 171 505
 429 378 482 338 216  14 274 381 142  24 188 238 287 279  91 477 126 495
 327  47 291 498  29  38 307 179 464 290  88 135 258 240 127 292 195  40
 211 306 485 227 506 155 445 294 497 314 284 148 384  70 172 272 414  46
 329 360 234 374  94  71 289 229 425 134 116  64 356 244 411 226   4 496
 494 145 276 180 164 263 446 510 345 328 376 107  67 300 269  16  44  41
 225 105 369 310 206 304 455  52 128 163  90 361  51  79 337   6 174 490
 479 140 204 377 450 108 347 208 403 217  89 341 421 311  87 124 367 233
  99  58 331 151 333  76 480 387 317 268  92  17 230 426 454 502 182 397
 247 439 282 437 449 500 468  48 283 228  50 409 419 149 251 237 334 212
   2  66 241 363 359  26 474 475 133 186 463 430 447 236 504 457 416 488
 106 349 441 318 507  23 114 423 210 232 202  45 404 422 362  10 339 117
  43 137 354 323 370  77 193 466 197 305 406 132 501 499 222  12 350 189
  34 313 187 170  55  96 456 166  74 273 250 332  73  18  63 344 280 150
 125 373 159  22 438 209 385 408 203 491 351 101 335 156 355 388 231 366
 487 112  60 167 154 198 330 297 243 219  37  19 462 138  72 129 277 102
 427 184 432 436 278 285  11  78 147 413 131 465   5 181 433 136 324 476
 326 444 130 486 440 493  57 146 121  95 390 395 296 246  39 100  80  56
 452 158 152 424  81 386 443 412 375 161 215  33  75 271 470 199 308  84
 418 336  20 368  68 509  54 157 405 160 173 398 191 122 207 224  69 346
 379   1 139  15 295 252 113 293   8 165 220 434 380 358 448 205 213 169
  93 281  42 242 302 109 503 111]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3339
INFO voc_eval.py: 171: [404 215 405 219 409 190 408 193   2 333 220 406 331 189 274 107 191 245
 507 508   4 178 316 332 393 293 317 192 121 179 470 247 330 416 329 414
 419 335 424 163 461 438  30 233 353 411 427 407 410 429 425 421 466 334
  75 263 412 147 434 428 482  74 116 258 327  31 225  16 146 170 279 272
 441 281 492 432 340  22  54 280 435 337 360 221 342  46 243 365 137 273
 418 186  71 490 248 356 289 491 230   7 196  79 238  45 133 318 128 223
   6 339 166 160  44 431 328   3 283  82 370 361 413 422 399 368  99 164
 458 184 420 172 152 167 117 149 326 367 195 302 338 188 246 509 347 197
 381 463 305  85 291 135  39 235 351 346 131 433 201 459 341 455 320 436
 242  17  51 210 497 477 257 111  26 443 198   5 444 222 457 228 437 207
 363 285 403 471  10 300  40 364 321 239 493 362 148 503 345 173 374 211
 467 366  11 171 122 369  25 182  15  36 115 396 224 194 155 498 301 216
 400 154 262  23 254 499 352 319 385 494  62 177  66 392  37  95 288  56
 286 349 377  89 252 249  55 144 256   0 212 244 102 344  67 295 298  32
 388 505 232  83 162  63  57 251 259 357 108 373 158 448 234 312 226 180
  70 308 156 383  48  88 475 296 380 253 175 449 480  13  64 299 324 176
 227 402 379 284 264 159 401 231 204  78 268 445   8 462 442  34 181 150
 483 142 101 169 134 110 372  68 375  19 389  60 118 476 501 343  21  76
 236 310 126  90  91  87 206 217 124  96 415 417 275 465 266 185 478 450
 240 306 423 143 474  77 397 213 348 350 371 336 250  33 488  59 468  12
  50 130 376 203  52  29 123  20 200 187 241  86 136 481 103 479  72 261
 313   1  61 454  18 125 199  53 309 282 307 460 430 255 114 209 214  27
 127 439  38 452 390 140 265 387 113 106 469 277 325 487 355 202 229 495
 496 384  92 267 391 510 120 278   9  47 276 139 292  49 446 161 105 473
  14 511 323 322  65 132 382 153  97  69 138 456 218 378 129 303 205 314
 472 426 165 486 168 260 500  58 395 141 502 485 145 504 358 506 237  98
  35  42  80 359  81 394 451 440 112 271 453 119 104 183 174 386 447 398
  41 290 297 287 304 484 109 489 354 315  94  24  93 311  28 269 151  73
  43 100 294  84 157 270 464 208]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5623
INFO voc_eval.py: 171: [ 28  14  46  43  69 118  19   2 112  86  57  52  13  74  32  94  71 116
  83  82  78  27 125  45   1  26  55  70 111  20  54  77  97  93  48   0
  67 117  88  81 114  25  51  92  85  65   6 115 107  42  17 102 106 105
  89  11  34  40  87 121  76  35  56  73 120  38  59  72  33  39 119   9
  99  22 110  30 103  90  23  75  91  10  60  58  95  49   5 108  24 123
  21  50  36  63  31  53  98  16 109   3  29 104 113  68  84 101   4  37
 100  62  96  79  61  15  18 122   8  66  44  41   7  47  80  12  64 124]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.5791
INFO voc_eval.py: 171: [292 311 130 384  72 461 424 125 227  11 414 155  90 425  55 315  17 206
 259  24 187 211 339 405 208   3 403 426 417 381 228 391 441 449 343 175
 336 233 225 338 154  89 305 389 254 385 357 430 334 263 114  35 293 264
  86 205 410  80  81 176 200  64 230 252 316 473 185 166 234 177  48 359
  28  36  18 335 470 388 231 239 273 146 337 242 413 260 174 401 324  38
 286 276  50 380 243  52  60 195  92 157 213  67 189 165 111 451 404 319
 210 241 180  29 363 475 279  62 245 344 360 419 238 141 133 289  13 171
 277 203 294 229  83 325 291 103 255 109 429   2  42 396 240 354 350 186
 181 152  99 393 215 455 390 435 246 198 232 333 373 105 115 418  25 172
 407  34 392 432 376  26 212 262 299  75 179  19 436 463 420 170 283 106
 148 151 196 448 351 460 204 278 379 314 207 301 236  51 216  88 178 168
 137 288 160  95 468  56  39 464  16 409 399 202 248 173 149  40  54 362
 310  23 270 302 122 306 307 209  53  10 162 423 345  94 257  97 159  43
  27 442 471  14 182 117  58 132 416 372 271 318 454 412 386 375  93 158
  31 214 353 444 134 332 194 331   7 128 272 327 124  44 192 142  98 112
 163 235 394  74 326 398 356  41 197  87 330 465  73 169 415  45  49 139
 370 104 458 355  37 431 290 408  79 136 274 188 397 467 428 220 462 421
 282   9   8   1 253  47 411 400  68 156 437  91 342  15 121 118 268 303
 275 184 347 201 164  59 284  22 251  46  82 147 365  69 395 193 296 312
 369 131 269  57 422 266  12 446 439 322 244 190 364 120  76 143 402 123
 457 341 256 101 474 265 129  21   5 469 382  78 161  96 138  30 447 119
 287 219 323 374  77 127 217 445 476 113 313 366  65 108  85 135 406  20
 153 224 249 440 452 221 304 285 167 308 102 191  61 352 387 371 116 250
 368 150 367 348 110 443  33 258 349 297 107 466 328  71 126  32 140 453
 472  70 218 237 329 280 358 427 309 100 295 383 438 378 361 456 434 317
   4 340 433   0 267 247 199 281 298 183   6 377 222 144 320  84 226 459
 261 321  66 145  63 346 223 450 300]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4438
INFO voc_eval.py: 171: [ 7  4 15 52 47 16 27 13  6  0 41 50 32 39 22 29 51 49 14 19 11  9 25 30
 45 31 53 46 10 48  1 33 35 34 20 40 26 18 23 21 43 28  3 38 12 17 24 36
 37  5 42 54 44  2  8]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0045
INFO voc_eval.py: 171: [1431 1433  148 ... 1523 1090  202]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5056
INFO voc_eval.py: 171: [32  3  5 59 22 47 24 18 65 52 68 20 43 64 37 16 69 30 58 48 31 27 39 63
  6  7  4 34 70 56 57 50 54 28 51 46 21 67 53 13 35 15 10 23 36 29 19 25
 17 14  8 61 33 26  9 66 60 42 45 49 62 40  1 11 41 38 55  0 12  2 44]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1952
INFO voc_eval.py: 171: [383 168  94 159 331 183 250 251 244 228  46  34 141 291 127 338 413 139
 341 367  17  16 162 138 212 264  76  92  58 406   1 337 357 327 411  44
 261 108 353 167 289 273  61 249 437 239 314 369 312  97 441 252 282   9
 129  36 315 144 175 371 404 188 107 284  60 444 358  23 326 385  45 272
  56  13  28 339 268 363  91 156 218  99 418 376 112  24 408  75 378 393
  67 375  11 414 187 122  55 149 277  40 422  21 296 236 334 283 329  80
 263 290  15  18  25 266 256  50 361 213 372 427 446  33 238   0 401 136
  42 231 135 430 117 320 133 307 303 386 186 199 130 436 106 344 281 207
 114 124 243 126 388 432   4 110  98 226 109 143 370 445 417  96 128 137
 146 380  32 309 351 398 148 426 346 152  82 416 297 365 209   6 348 169
 219  14 224 359 352 368  12 105 258 293  65  53 391 442  57 246 223 362
  78 260 403  62 104 354 287  19 101 302 396 215 269 318 319  10 240  69
 125 221  26 434 301  70 433 399   3 113 134   5  85 191 275 140 286 294
 306 234 166 317  93 185 172 321 237 407  71 189  68 381 288 355  84   2
  31  47  30 330  39  29 300 392  22 100  81 379  35 157 349 265 181 295
  64 232 415 242 184  66 151 333  52 197 276 400 196 200 165 255 262 443
 111 177 158 103 308 193 205 230  54 435 412 208 313 384 182 235 336 132
 332 195 390  77 119 440 222 439 394 356 324 206 395 304 121 377 420 164
 431  20 421 280 360 220 147 155 279 254 178 245 373  86 241 190 253   8
 180 233 409 285 345 203 216 382 225  41 115 323 405 171  95 419 316 423
  37 211 374 299  43  89 123 131 364 271 160 179 257  88  74 322  49 210
 342 343 347 161 145 150 176 201 292 389 387 340 350 173  51  48 153   7
  79 366 198  90 429 214 310 397 428  73 278 425 102 305 192 174 120 204
 274 118 335 298 438 325 227  27  63 194 142 163  38 229 248 410 247 170
 202 217 424  87  59 267  83 259 154 311 328 116  72 402 270]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4037
INFO voc_eval.py: 171: [ 27  95  47  76  53  13  51   2  30  54 103   4  18  21  29  70 102  35
  45  17  82   0  48  63   1 101  44   8  28  72 106  99  41  80  24 104
  16  40  11 107 105  97  34   7  15  75  83  22  98  74  69  67 100  64
  43  89  59  19  52  71  87  39  56  57  38  62  68  37  33  94  79  91
   5  10  86  58  31  55  77  12  14   6  49  25  84  65 108  81  32  90
  66  93  20  78   3  42  50  23  36 109  92  73  88  85  96  26  46   9
  61  60]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1196
INFO voc_eval.py: 171: [73 63 87  9 12 62 48 89 91 27 52 61 29 10 60 78 51 67 65 11 74 84 36 81
 66 43  4 56 80 31 64 19 26 20 14 18 85 39 24 88 49  5 72 17 44  7  2 54
 33 22 34 13 71 55 32 21 50 57 38 77 35 83 86 68 75 30 53 45 90 23 82  8
 79  1 15 25  6  0 41 42 58 70 69 37 28 76 46  3 40 59 47 16]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3884
INFO voc_eval.py: 171: [ 27 206  16 131  94 181 151 135 216  31   5  11  98 157   4 194 136  88
 146 211 149  78  47 148  53  90 140 172 105  42  12 132 176 161  80  66
 192 120  70  41 143 150 118  86 221 119  76 127 208  91 115 162 112 102
  93 160 137   1 107  55   3  95  60  57  61 152  64  24  77 180 170 158
  52  22 213 169 222 106 199 124 113 104 177  69  58  17   2  33 179 108
 164  38 171  39  35  13  82 195  74 100 215 214  99 145 147  87 126 174
 116 111  29  30 142 117 189  68 159  23  10 130 220 178 188  72 200 128
  34 225  18  85 218 154  56  14 144  32 212  96 182 224 184  51 223 193
  15   8  48  75 226 134 109  92  50   0 101 123  40 167   7  79 114 129
 217 198 190  43  81 197  19 196  63 185  28   6  46 186  49  71 166  37
  36  26  45  54 191 139  65  67 138 133  84  25 153 156  89 110 125 205
 175 141 168 122  97 183 155  21  44 210 173 121 165  59  83 103 163 209
  62 219  20   9 202 201  73 207 203 204 187]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7410
INFO voc_eval.py: 171: [ 452 2349 1452 ... 2033 1791 1305]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6212
INFO voc_eval.py: 171: [115 138 420 196 688 144 643 224 126 141  70 584 286 691 207 646 274   5
  72  73   4  45 173  71 411 357  74 447 466 547 222 209 400 553  76 382
  75 241 689 720 610 226 422 614 644 252 510 208 588 454 501 154 150 157
 339 253 467 640 135 642 690 457 105 555 556  47 200 702 626 701  57 155
 699  83 130 554 254  58 388 312 628 399 328  31 651  93 468 133 143 505
 633  81 112 359 395 569 243 289 552 172 480 406 677  77 213 593 299 478
 321 329 649 632 693 283 100 519 617 313 360 625  39 263 472 509 559 294
 134 630 116 211 401 444 558 109 458 415 737 202 721 645 650 358 440 344
 744 557 229 675 331 606 389   9 210 561 493 374 413 438 576 190 506 740
 423   6  60 532 361 585 437 586 439 611 695 563  84  62 317 398  32 692
 647 260 548  88 475  95 268 666 164 330   7 477 201 214 345 219 408 335
 448 119 635 132  28  48  78 460   8 142 595 238  34 421 713 270  41 590
 158 745 156 296 174  85 297 378 694 485  61 608 492  80 719 656 326 255
 697 370 710 488 622 469 575 443 247 265 244 124 739 668 636 515 673  40
 256 716 340 386 175 671 686 218 102 178  15 470 560 217 192 733 747  52
 348 708 403   3 589 704 723 170 517 187 445 660 592 225 300 291 366 539
 203 471 160 574 672 424 521 110 607 402 459 678 161   1 380 205 497 427
 212 168 476  94 479 698 654 674 117 511 350 176 627 696 538 306 327 376
 410 425 514 750 304 257 287 230 529 245 620 428 416 573 544 634 129 293
 577 242  56 615 419 315 738 284 103 246 520 302 652 432 101 522 276 153
 167 258 616  11  29 731 527 264 250  51 125 351 365 540 139 659  65 162
 735 687 123  17  97 371 231 736 473 266 181 409  18 551 583 338 165 393
 681 373 228 727 301 179 316  59 436 715  86 249 183 220  21 310 749  26
 355 724 298 343 221 599 147 742 604 717 602 705 507  90 390 550 542 341
 714 191 146  13 303 684 603 418 582 216 667 571 106 347 308 352 661 383
 580 323 273 237 641  16 524 591 732 342 334 730  46 412 320  19 566 377
 502 148 486  14 319 570 526 609 145 311 565 655 743  22 581 567 128 631
 491 404 385 322 726 619 657 309 531 600  55 364 349 272 368 664 430 279
 618 683 746 722  54 518 653  30 496 456 223 680 725 712 728 637   0 288
 282 414  42 324 483 463 127 362 474 387 396 407 259  96 482 596 452 578
 346 442 120 508 233 204  23 417 137 405 169 394  98 729 703 239 280 194
 193 676 499 108 451  91 494 598 113 236 271  25 741 353 601 363 706 189
 535 718 131 433 140 623 534 269  64 587 307  63 111  99 446 624  43 450
 261 498 663 431 638 648 391  66 685 662  35 177 285 613 541 305 530 484
 605 435  38 748 118 152 171  82 546 545 487 184 464 381 669  20 356  12
 186 240 232 262 198 489 122 336 449 318  27 372 182  89 325 188 670 292
 195 461 568 278 290  33 503 397 682 564 367 711 429 709 441 639 533 104
 621 235  67 114 251  10  79 536 537 462 227 481 700 434 197 281  92 734
 159 248 679 314 392 465 665  36   2 121  53 513 151 525 180 504 658 500
 426 375 354 369 707 333 495 455 562  68 579  44 523 549 597 163 166 629
 107 275 332 185 528 199  24 206  37 234  87 267 572  50 612  69 516 384
 512 337 295 277 594 543 379 215 453 490 149 136  49]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5922
INFO voc_eval.py: 171: [55 27  1 52  2 35 64 47 46 44  7 62 69 63 26 50 40  9 30  4 12 58 21 15
 33 67 24 28 31 57 49 61 60 32 13  8 29 22 25 36 23 59 42 14 16 41 34  5
 51 37 38 65 18 43 53 20 45 11 54 66 17 39  3  0 48  6 10 68 19 56]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3005
INFO voc_eval.py: 171: [ 63  64 141  21  96  54  92  97  62 111  32 105   4  72  24   3  70  26
  51  83  94  46 129 101  78 117  80 115 144  66  55 123 102  71  29  27
  39 114 103  88  14  44  13 130 134  75  67 100  20 139  85  57 110  25
  41  10 118  16   7  86  45  93  31 135 125  81 140  59  22  38  48  58
  56  52  12  95  49  36   1  28  60  74  99 124   9  11 107  68  90  34
  79  30 116 122  73  84  91  15  76 138  35  23  77  37 104  42 143 142
  65 132 131  82  61 112 126 119  53 127 120 108 113   8  33  47 109  17
 106  43 121  18   6 133  69 128   0 137  87  50 136  98  40  89   5   2
  19]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3223
INFO voc_eval.py: 171: [ 96  12  55   8  28  45  81  20  66 123  47  27 125  67  64   6  30 134
 131  49  84  53 127 111  44  68  59  10  80  17  74  78  29 112  24 109
  11 135 106   7  19   3  14  32  89  75 101  87  36  25  48 128 139   5
  90 116  60  34  54  94  69 118  43  88 105 108  38  76  83 126  52  42
 120  57  41 140 137  51  21  92 103  73  31  61  71  23 110  85  58   1
 130 107  97 129  72 121  46   0  63 114  93   4  18  33  37 117  82  98
 132  65 142 102 115  13 124  86  91  40  35 138 136  22   2 122  39 141
 119  56  16  95  70  79 104 113  62  50   9  77  26 100  99  15 133]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3844
INFO voc_eval.py: 171: [119  70 176 189   1 264 136 169  17 240 229 143  13 106  65  27 259 190
 246  37  93 195 144 186  54  30  20 220 269 147 177   5  58 174  92 168
 155 277  86 179 102 199 210  41  67 181  79  29 267 273 227  38   0 232
 139  94 182 142 122 126  64 208  51 261  46  60  55 206 281 202 278  32
 201 167 247 170  39 128 248  25 234 254 152  80 129 194  36 114 197 221
 205  44  53  97 209  74  56 154 100 249 140 108 200  66 141 218 196 127
  68  16 257 274  89 260 191 146 226 153 256  91  11 244 279 250 145 212
 156 185 204  19  75 103 162 214 211  84  10  42  52 110 104  12 280 275
  59 132 236 116  23 101  45 163 158 135 120  69 173 180 113 241   4   9
 235 245 187 251  33  18 151  35 159  62  88  40 165  61  73  72 276 255
 112 219 223 193 231  83 252 115 228 263 239 253  24  95 271  99  63  21
 222 233 242 105 138   2 157  76 109  48 166   7 175  43  78 111 184  22
 183 282  49   3 133   6 121  14 225 149  28 172 268 188  90 272 207 161
  87 217 160 178   8 238  34  57 213 125 265  77 124 224  15  96 130 270
 118 117 243 203 131 123 262  81  71 266 134 192 150 148 198  85  82 215
 216  98 237  26  50 137 230  47 107 258  31 164 171]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4627
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4079
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.623
INFO cross_voc_dataset_evaluator.py: 134: 0.231
INFO cross_voc_dataset_evaluator.py: 134: 0.334
INFO cross_voc_dataset_evaluator.py: 134: 0.562
INFO cross_voc_dataset_evaluator.py: 134: 0.579
INFO cross_voc_dataset_evaluator.py: 134: 0.444
INFO cross_voc_dataset_evaluator.py: 134: 0.005
INFO cross_voc_dataset_evaluator.py: 134: 0.506
INFO cross_voc_dataset_evaluator.py: 134: 0.195
INFO cross_voc_dataset_evaluator.py: 134: 0.404
INFO cross_voc_dataset_evaluator.py: 134: 0.120
INFO cross_voc_dataset_evaluator.py: 134: 0.388
INFO cross_voc_dataset_evaluator.py: 134: 0.741
INFO cross_voc_dataset_evaluator.py: 134: 0.621
INFO cross_voc_dataset_evaluator.py: 134: 0.592
INFO cross_voc_dataset_evaluator.py: 134: 0.301
INFO cross_voc_dataset_evaluator.py: 134: 0.322
INFO cross_voc_dataset_evaluator.py: 134: 0.384
INFO cross_voc_dataset_evaluator.py: 134: 0.463
INFO cross_voc_dataset_evaluator.py: 135: 0.408
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.401s + 0.001s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.358s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.360s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.360s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.365s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.368s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.364s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.363s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.364s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.362s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.364s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.363s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.364s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.385s + 0.001s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.370s + 0.003s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.370s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.373s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.375s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.373s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.383s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.382s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.380s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.380s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.379s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.380s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.378s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.351s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.359s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.361s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.370s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.367s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.366s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.366s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.367s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.369s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.370s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.375s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.376s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.376s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.515s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.385s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.375s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.367s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.367s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.364s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.366s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.365s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.364s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.364s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.361s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.362s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.361s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 54.815s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [150 191  51   6  80  45 241 101  18 183  39  53  44 116 126 246 145 102
 148 152 125  68 254 257  38 153 264  95   2 115  43 168  41 227 215 108
 266  72 202  27 128 107  19  34  61 112 135 236 218 226  11 262  90  69
 269  40 134  83  73 250  49  98  24 256 200 201 205 146  76  57  31 151
  52 242  21  26 132  88 234 219  17 149 147 225  93 188 127 140 144 170
 190 139 220 141  46 204 165 163   5  62 113 176  84 239 252 263 174 189
   7  36  29 260 121 235 143 131 124  15 138 232 130 247 187  77 272 156
 243  66 159  91 154 213 259 167 271 129 217 255 109 172 137 228 118 229
  33  16 185 207 180 169  60  48 111 209  50 100 158  97   9  87 244 164
 212  56  54  37  32  47 161 211  13 208 233 245 103 237 199 173 240  20
  22 195 265  35 238  12  23 114  63 122  81 175  25 117 210  64 181 198
 197 214 274 157   4 196 179  89 258 203 105  99   3  79  71 104 230 267
  67 155 160 142 106 184 206 268 162  94  96  30  28 182  75  59 194 136
 270  74  82   8  86 222 248 221 177 120 171  55  58   0 251 192 231 123
 224  10  92  65 249 166  85 253 273 186  70 223 261 216 133 178 193   1
  42  14 110 119  78]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3469
INFO voc_eval.py: 171: [164  46  17 104 140 190 199 202 188 109 201  95  15 122 203  75 144  12
 151 189 176  18 191 137  79 141 138 193 187 194 148  19  45 158  99 132
 127  71 102  11  52  26 180 209   5   8 162  78 175  24 119 125 204 198
  66 170 197  81  84 123  65  54  76  67  34  59 147 161 118 205   1  72
 111 155 146 107  64  28 128 143 212 171  23 185   3  74 110  20 126 184
 178 200  82 159 213 195 182  49  51 105 131  62 208 135  80 116 183 115
  88  44  41  14 154 153   6 174 103  35  16  86  27 211  87 173 160  73
 100  63 106   2  48 156 121 165  91 168 117  57  96 101 181 139 136 150
  92  61 210 196 172 145   4 157 133  29  50  13  97 152  38  60  31 142
  42  93  77  39 207 179 108  55  98 149 124  30  68 206 113  37 169 114
  36  85  32  56  43  40 130 186 163  70  21  69  53 129  22 214 167 112
   9  58  33 134   7  10  47 166  94 177  90  25  83  89 120 192   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6227
INFO voc_eval.py: 171: [ 82  37  98  88  65  66   2  99  64 193 210 211 158 196 212  33  78 120
   1 238 121 170  51  79  73 195 103  96 176  12  91 183 180 233  69 167
 225 230 137 144 213 101 214 231  54 202  70 246  17  44   5 216 188 173
 155 177  92 245 198 169 174  77 178   3  31  89 153  97  90 141  72 209
 175  58 114  10 205 227  42  86 197 203 185 160  23  80 217 200  20 221
  16 132  55  14 124 235 215 242 111 105  68 182 220  60 140 168 208 148
 244 240 143   6 236  94 207 131 107  13  53 228 165   4  49 151  76  28
 142  56 166 150 206 118 241  36   9 139 154 117  39  62 136 223  87 162
  61 145  46 138  84  83  34 110 125 106 237 108 189 149 135  27 186 119
 199 218  38 232  93 146 126  40 229  22 184 222 134 123  59 179 113  81
  32 234 219 116 156 224 171 190 247 112  26  67 163 129 122 194 226  45
 187 161 191 102  71  19  25  50  95 164 243  43 152 181  15  48  35   0
 201 159 115 128 157 130 127  85 104   8 172   7  74  57  41  47  11 239
  63 192  18 109 133  30  75  24  52 147  29 100 204  21]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2304
INFO voc_eval.py: 171: [367 315   0 316  99 317   7  82  49 253  30 254 347 343  53 452 138 113
 248 360 251 446 252   9 394 196 110 191 246 318 136  84  81 451 114  94
 124 258  28 396 232 297 185 390  36 387 307 171  32 395  21 339 208 473
 393 368 428 503 163 336 444 237 195 414 100  25 267  60 259 405 172 292
 359 257 293  61 385 475 454 240 177 149  95 228  31 470 256 279 377 384
 463  64  27 402 435 311 304 476 321 211 421 388 481 105  35 462 170 386
 261 247 352 378 117 500  13 458 245  87 465  58 187 179 461 496 139 453
   3 241 312 348 122 214 424 295 484 338 411  69 409 249 216 189 157 422
 166 280 334 474 210 137 266  14 374 376  24 120 469 271  38 278 231 301
 183 487  47 121 323 283 490 173 456 282 130 233 284 250  29 152 190 438
  40 219 477 300 498 275 205 287 143 309 167  45 264 489 379  52 325 129
 355 408 310 281  63 227 370 221 111 148  70  91 418 236 268 351 218 488
 140 486   4 255 439 174 262 406 159 372 103 502 341 294  41  44 324  17
  66 201 217 101 413 365  78 123 298 471  51   6 158  88 333 448 482 356
 169 327 134 497 373  86 199 226 104 443 302 342 212 146 118 337 306 225
 358 363  85 329 459 382 260  57 313  75 419 447 176 432  89 472 494 222
 430  50 274  90 330 206 230   2 276  65 354 492  48 442 220 243 466 234
 467  16 440 128 431 412 404 499 239 229 480 344 464 423  23 180 204  76
 197 416 450 102 202  10 350 415 391 434 109 305 397 357 224 132 314 192
 399  43 335 112 319 188  46 366 299 491 401 181 345  22 184 165 161  93
 493  12  34 215 265 410 457  55  73 449 156 349 340 151  72 203 328 286
 119 273 242 145 369  62  18 308 332 403 154 193 162 107  26 223 326 479
 198  97 483 125 383 380 269 346 362 142 331 235 178 270  59  71  37 133
 291 213 115 150 127  11  92 455   5 147  77 436 126 153 468 277 131  19
  98 322  56 420 429 289 175 478 371 426 290 485 320 389 238  39  79 433
 425 445 437 263 116 194  83  96 141 182 407 417  20 303 108 186 244  67
 501 364 381 460 441  80  33   1 207 209 155  74 168 400 144 285 375 288
 200 495   8 398  68  15 272 361 392 135  54 427 353 164 160  42 106 296]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3331
INFO voc_eval.py: 171: [402 216 403 404 220 189 407   3 192 332 221 405 330 188 275 106 190 247
 506 507   4 179 314 331 391 294 315 191 121 178 469 249 329 414 328 412
 417 421 335 162 460 437 351 235 409 425 406 408 427  30 423 465 420 333
 264 410  75 426 433 146 481  74 116 259  31  16 326 227 280 170 145 440
 282 491 339 273 430  22  53 281 336 434 222  44 342 359 238 245 274  71
 136 416 250 186 489 354 369 287 196 490 233  80   8  43 240 132 316 127
 165 340   6 230 157 337  42 284 429  82 327 361 419  99 195 362 411 224
 366 357 457 397 163 184 194 418 151  11 167 356 171 365 117 148 508 349
 377 300 325 248 304 462 187 134 291  85 197 338 350 431 130 458 341 199
 453 318  50  15 109 208 495 476 436 244 441  26 258 231  18 223   5 360
   7 443 207 435 456 285 401 470 319 358 492  39 345 501 147 241 173 363
 372 499  25 210 172 122 295 225 182  83  17 299 394  37 496 193 301 153
 217  36 263 493 352 317  61 386 398  95 390 256  67  23 286  89 375  54
  55 251 257 347 254 246 344   0 107 102 297 143 211 385 290 154 161 504
  56 158  19 253 447  62 370 260 236 502  32 228 311  69 113  46 307 380
 180 381 155 176 288 376 474  63 229 298 232 479  88 177 322 450 378 242
 400 266 269 234  14 159  13  40 218 399   9 149 444  78 442 181 226 482
 461 101  34 133 141 497  59 368 169 371 387 343 475  68 277 237 118 413
 124 422 415  87  96 205  90  76 448  91 305 477 464 142 348  65 308  21
 185 473 252 267 367 214 346  77 395 293 334  33  58  48 137  12 125 324
 126 129 373 467  51 175 213 200  20 166 309 312  86 123 487 480  72  60
  29 243 454   2 478 283 135 201 110 306 206 112  52 114 262 459 255 215
  27 198 265 466 428 139 296 202 278 438 105 452 353 494 382 384 388  92
 111  45  38 389  10 486 472  47 138 510 321 292 268 279 276 204 160  97
 445 104  64 509 320 120 152 379 131  66 374 212 302 128 455  70 313 219
 168 485 498 500 144 471 140 164 484  98  57 203 209  35 323 393 119 355
  79  81 503 424 488 483 364 505 392 432 439 468 289 303 446 108  41 183
 100 174 156 239 272 383 150  24 310 396  93  28 270   1 261  94 449 451
  73  84 103 463 115 271  49]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5584
INFO voc_eval.py: 171: [ 26  12  44  41 118  17  67   2 111  55  85  49  73  11  93  30  70 116
  82  77  81 124  25  43  68  53  24 110   1  15  51  76  96  92  45   0
 117  65  87  22  63  80  91 114  48  84   6 106 115 101  69 104 105  40
  88  10  32  38 108  23  75  72  20 121  71  54  86  33  36  57  31  37
 120  98 119   8   9  28  21 102  64  74  89  58  90  94  56  19   5 123
 107  46  16  29  47  34  18  13  50  97  61 112 103   3  27 113 100 109
  66   4  99  83  14  35  95  52  59  60 122   7  78  62  42  39  79]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6124
INFO voc_eval.py: 171: [286 305 128 376  70 452 417 124 221  10 407  89 151  52 418 309  16 201
 253  23 183 331 205 398 203 419   3 396 411 333 373 384 433 223 441 171
 329 336 217  88 150 299 231 382 248 350 377 423 327 257  34 114 287 291
 258  85 200 403  79 172  78 196  61 246 464 225 310  35 162 181  47 352
 229 173  27  17 142 381 328 461 237 226 233 330 267 254 170 406 394 318
  37 270 280  55 372 238  59  49 191 174 153  91 208 185  64 111 397 161
 442 313 235 206  28 466 356 337 412 240 274 234 139  12 133 166 353 283
 103 271  81 199 319 224 285 249   2 109 422  42 389 347 236 182 178 241
  98 148 343  24 457 386 227 210 115 194 446 326 427 383 366 400 168 147
 105 410 385  33  25 207 425 369 294  73 176 453 413 277  18 428 167 106
 192 272 256 144 451 371 308 202 295 344 211  87 175 282  54 135  94 156
 131 164 454 378  38 340 304 198 458  15 145 392 402 243 169  50 228  22
 355 204 121  39 302 416  43  51 264 296 300 251 158  96  57 434 155 462
  93 409  13 338   9 179  26 325 130 365 265 312 116 445 209  92 368 154
 346 436 405  30 190 324 230 126 266  44 123 387  97   6 159 455 391 188
 112 320  72 132 323 140  41 165  86 193 363 348 177 401 104  77 449 137
  40  48 408 284  66  71 390 424  32  36 268 439 184 421 349  65   1 414
 247   7   8 404 215  46 117 429 101  14 393 152  90 335  53 269 297 278
  21  45 197  58 459 120 306 263 119 189 143  67  11 262 129  56 289  80
 362 388 358 186 415 357 122 438 465 316 260 259 431 395 460 239 448  74
 180 100 220 136 250   5  95  20 127 157 334 440 214 212 374  76 160 118
 281 399 467 218 292 134 317 113 375  29 367  62 301 432 107  19  75  84
 244 279 359 307 149 108 437 345 298 163 380 222 443 273 102 245 456 110
 379 341 364 290 321 361 125  69 146 370 360  68 252 435 463 138 213 430
 232  99 444 322 187 275 242 420 303  31 288 216 342 351   4 332 311  63
 426 195 354  60   0  83 447 339 315 314 141 219 276  82 450 261 293 255]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4714
INFO voc_eval.py: 171: [ 7  4 15 52 47 16 27  6 14  0 32 50 41 39 29 22 51 49 19 12 10 30 25 45
 31 46 53 11 48  1 33  3 35 34 20 40 26 21 38 28 23 18 43 13 24 17 36 37
  5 44 54 42  9  2  8]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0299
INFO voc_eval.py: 171: [1403 1405  144 ...  882  237 1215]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5064
INFO voc_eval.py: 171: [34  6  4 25 62 49 21 68 19 53 71 22 45 67 39 18 72 32 61 50 33 28 41  7
 66  8  5 36 74 60 52 57 59 29 55 23 48 56 70 14 16 11 10 38 31 20 26 15
  9 63 13 17 27 35 64 69 37 44 65 47 43 12 51 42 40  1 24 58 30  0 54 73
  3  2 46]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2133
INFO voc_eval.py: 171: [385 175  99 165 332 188 254 255 339 247 233  41  50 147 133 293 415 145
 342 366  18  17 387 144 168 217 413 268  81  97  62 408   1 265 329 358
 114 173 354  48 276 291  65 253 439 244 370 316 314 102 443 256 135 284
   9 150 180 317 406 286 372  64 193 113  49 275 359  60 328 445  14  38
  96 104 272  23 340 333  30 162 118 377  12 410 378  26  59 222 155 126
 192  39 420  71 395 416  79 380 279  22  43 260 331  19 241 285 267  16
  85 292 218 298 373  54 243  27 447 362 270 112 431 142 403 322 141 238
 305 428   0  45 364 204 122 309 139 191 438 345 283 130 120 132 211 136
 248 390 116 103 101 149  34 231 434 134 143 152 124 382 419 115 446 347
 371 154 400  87 158 352 418 367 427  15 174 213 224 299   6 312  13 349
  78  61 369 262 228 365 295 111   4 229  83  67 360 250 405 444 264  66
 353  57 289 109 273 393  20 304 226 321 220 398 355  73 320 106 245 196
   5   3 131 401 303  74  11  28 288 207 435 277 240  90 146 172 119 296
  98 190 423 308 236 242   2 140  72 177 290 383  40 194 319  51 163 356
 409 335 436 189  36 363 323 266  70  69  31 297 202 201 105  89  32 278
 394 381 302 171 157 205 402 259 198 269 350 388 182  86 239 186 338  33
 417 117 310 237 212 315 164 437 187 209 414  56 392 108 386  37 200 334
 397 326  58 306 128 170 311 125 258 138 227 249 396 422 357 441 337 257
 433 210 100 407  82 374 195  35 153 442 361 225  25 379 234 287 183 281
 221 384 346 185  44 411 246  46 282 424 230   8 219 216 325 318 156 274
 161 166 137  93 376 129 151  77 301 324  53  91  24 294 184 261  55 178
 214 421 348 167  42  76 252  21  94  95 399  84 179 121 181 343 351  52
 430 197 368 313 375 206  68 159 344 203 341   7 426 208 169 280 391 389
 300 307  63 440 127 123 336 235 330 199 107 148  29 263 176 223  47 327
 215 232  75 412 429 425 271  80 160 404  92  88 251 110 432  10]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4037
INFO voc_eval.py: 171: [ 95  54  76  14  52   2  21  27  55  30 105  48   4  19  29  70 103  34
  45  18   0  82  49   1  63 104  47  44   9  28  41  72 100 108  24  80
  35 106  17  40  12 109 107  98   8   7  16  75  83  74  99  22  69  64
  43 101  20  67  90  88  57  62  39  37  68  96  38  78  59  33  79  92
   6  87  11  71  31  53  13  91  56  85  50  65 110  66  81  77  32  94
  42   3  58  15  60  23  10  51  89  36  97  86 111  93  73  26  46   5
  25 102  84  61]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0738
INFO voc_eval.py: 171: [74 88  9 65 12 63 48 90 92 26 53 62 28 10 61 79 52 68 11 75 85 36 82 67
 43  4 57 81 23 17 25 19 14 86 39 64 49  5 89 30 73 18 44  7  8 55 31 33
 13  1 21 56 72 51 66 38 35 58 87 20 84 69 76 78  2 29 22 91 83 45 80  0
 15 24 42 59 70  6 54 27 41 32 37 77 71  3 34 16 60 46 40 47 50]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3941
INFO voc_eval.py: 171: [ 28 197  17 127  89 175 146 131 204  31   6  12  94 152   5 186 132 199
 141  82 144  75  48  54  85 136 101 143 166  43 170 128 156 184  77  14
  64  68 116  42 138 208 145  83 114 115  73 123  86 198 157 111 108  98
 155  88  56 133 103   4  90  60  58   1  61 147  25  63 174 164  74 172
 153 163  23 191 209  35 121 100  67   2 171 107  18  59  33  38 104  40
  13 187 165  96 158 203  71 202  30  95 112 140 122  81 142 137 168  24
 113  10 181 102  66 207 154 201 126 173  32 180  53 124 109  70 193  78
  34 206  19 139 212  57 149  80  15  52 200 178 176  39 185  92  16   9
  49  72 211 210  87 213 105 130   0  51 119  97 125  41   8  20 167 161
 190 110  47 182  44 205  76 188 189   7 179  62  69  50 106  29  37 135
 195  27  46  55 183 160  84 134 129 169  65  26 118 151 162 177  79 148
  93  22 196 120  21 192  11 150 117  45 159   3  36 194  99  91]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7413
INFO voc_eval.py: 171: [2496  473 1540 ...  856 1146 1338]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6177
INFO voc_eval.py: 171: [114 137 417 197 676 145 632 226 125 141  70 574 287 679 636 210   5 275
  71  72   4  44  88 174 407 353  73 440 457 224 537 208 396 543  75 378
 243  74 677 600 211 228 706 416 604 633 502 254 493 578 446 155 151 158
 336 255 458 629 631 134 547 678 448  86 617 545  47 206 691 688 156 687
  56 129  81 544 256  57 312 616 384 330  92 395  31 459 622 497 132  78
 391 355 111 561 245 144 173 290 542 472 402 634 666 470 580 320 299 327
 621 639 100 284 681 432 314 508 607 356 614  39 265 709 465  98 549 501
 294 212 133 548 397 619 437 733 449 115 726 411 105 354 202 640 341 231
 546 385 329 664   9 209 597 216 551 485 498 409 191 565 370 729 584 419
  59 522 575 431 683 430 576 601 481  82 357  61 317 553 637  32 394 262
 680  93 334 467 538 655 165  87   6 270 342 328 219 214 201 404 118 624
 451 131  29  76 240 635  48  34   7 441 418 272 734 159   8 296 175  41
 157  83 682 374 142 478 598 484  60 594 685 366 708 473 700 258 462 645
 286 567 559 249 436 267 246 625 123 325 728 674 661 257 705 337 176 218
 690  40 506 381 463 217 103 179 550  25 221  51  16 692   3 583 579 697
 721 171 406 399 345 507 292 438 711 227 464 362 612 424 566 308 203 398
 109 300 161 529   1 450 213 420 659 162 510 488 205 376 736 168 686 469
 471 347 116 177 663 503 528 615 505 326 642 684 433 519 372 739 128 259
 304 611 288 415 193 232 247 662 534 293 423  55 623 660 244 422 568 509
 605 248 641 277 727 315 412 475 102 554 169 652 425 606 511 260 647 266
 163 124 154 302 517 719 252 187 530  11 101 139 122 186 348  19  50  64
  96 233 675 724 361 541 405 725 367 466 166  17 182 669 180 573 268 230
 716 301 316 428  58 712 138 251  85 738 298 148 656 352 707 147 589  21
 340 310 731 581 592 595 223  89 499 338 703 694  13 532 414  80 593 704
 303 572 349 540 672 106  97 408 630 562 192 344 379 570 274 720 239 649
 389 557 513 307  45 322 386 283 339 582 516 149 333  28  18 515 373 718
 610  23 732 644 560 311 319 643 483  15 318 599 127 556 479 108 220 620
 558 335 590 536 571 279 646 309 380 365 346 225 651 400 321 521 735 715
 608 360  54 273 609 710 280 454 146 410 714 668 586 369 403  94 487 126
 289  26 435 447 343 474 717 490 671   0 476 702 382 235 358 119 626  24
 170  42 261  22 500 445  90 195 136 693 696 461 468 281 392  95 390  84
 204 486 350  53 238 241 588 359 665 112 401 323 653 107 194 491 444 413
 591 426 657 695  27 190 140 271 577 730 443  99  65 613 306 489 524  62
 627 130 564 525  43 439 387 110 531  63 673  66 737 153 263 477 305 603
 638 117 520  79 429  20 442 199 713 650 185 234  38 512  12 188 377 480
 596  30 658 189 452 324 242 291 183 264 172 121 181 104  67 699 628 455
 368 196 237 527 526  33 250  77 363 229 383 113 434 555 670 701 393  91
 453 460  10 456 253 495 143 282 160 689 723 494 654  52 523 648 698 388
 421 569 427 198 351 504 514 313 496 667  35 152 364 236 178 276 722  36
 332 535 587 297 492 539  37  14   2 371  68 207 222 167 164 618 331 120
 563 184 269 150 552 285 295  69  46 518 533 278 602 585 375 482 215  49
 200 135]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5927
INFO voc_eval.py: 171: [55 26  1 52  2 29 47 64 46 44  7 69 62 25 63 50 38  9 30 11 58 15 21  3
 33 67 23 27 31 57 49 61 60 32 13  8 28 20 24 35 59 22 14 42 16 39 40  5
 34 51 65 36 37 19 18 53 45 10 43 54 41 17 66  4  0  6 68 48 56 12]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2727
INFO voc_eval.py: 171: [ 61  62 137  21  93  52  60  89  94  32 107 101   4  70   3  68  29  24
  26  49  81  90  45  97  96  76 112  78 111  64  53  98 118  69  27 110
  39  85  99  13 139  91  43  65 125  12 129  19   0  55  25  83  73 113
 106  40   6   9  15  84  44 124  31 120  57  22  47 130  38  79  56 135
 134  92  54  50  36  11  48 109 126  72   1  66  28   8  10  95  58  71
  34 119  77 103  87  30 117  14  82 133  74  88 100 140  35  80  75  63
 138  41  23 127  37 108  59 114  51 115 122  33 121 104   7 105  42  46
 102 123 116 128  16  17  86 131   5 132  18  67  20 136   2]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3164
INFO voc_eval.py: 171: [ 96  11  55   8  25  44  81  19  66 122  47  28 124  67  64   6  27 134
 131  45  43 110  49  84 126 109  59  68   9  80  16  73  77 111  53  26
  23 135  30  10 117   7 105  18   3  74  31  89 100  24  48  35  88 127
 138   5  13  60  90  54  94  33 119  87  42 136 107  69 104  37  75  83
 125  52  41  57 115  40  92 139 102  51  21  72  71  61  85 129 108  22
 106   1 128  97  58  46  32 120 113   0  29  63  93   4  36  17 116 132
  82 141  15  98 101  65 114  12 137  20  34 123  39 121 130   2  38  56
 118 140  86 112  79  95 103  62  50  76  70  78  99  14  91 133]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3811
INFO voc_eval.py: 171: [115  72 170 182   1 254 134 163  17 230 221 139  14 102  62  26 249 183
 237  35  89 188 179 140  51  28  20 259 212 143 171   6  55 168  88 151
 164 266  99  82 172 191 201  38  64 174  76 263  27 257   0 219  37 223
 135  90 119 138 175 122 199 251  52  57  47  42 270  61 198 194  29 267
 162 193  36 124 240 238 165 245 225 149  25 187 125  77 197 190 110  93
 213 200  49  40  53 152  70  96 136 104 137 189 192 210  63  85 248 123
  65 184  16 218 142 264 150 241 247 141 268 250 239 204 153  71 234  87
  11  19 178  10 196 158 206  80  97 166 203 169 106 269  12 129  56 100
  48 265 227  23  33 112  41 155 167 132 231 159  66 173  98   9 109 116
  44  18 226   4  59 148 107  32 120  30  84 156 242  68 180 211 160  43
 243 215 246  69 111  79  50 229 186  24 108 220 222  95 244  60 253  21
 261  73 127 214 154  45  75 105 232 133 224 101   7  91  22 236 161  39
   2 271 130 177  15   5 145 262  58 176 217 255  86 117   3 258  83 209
 157 181 228 208   8 121  31  54 260 235  74 216 118 126 128 205  34 146
  92 195 233 113 114 103  67  78 131 185 144 147 256 252 207  46  13  81
 202  94]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4624
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4091
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.347
INFO cross_voc_dataset_evaluator.py: 134: 0.623
INFO cross_voc_dataset_evaluator.py: 134: 0.230
INFO cross_voc_dataset_evaluator.py: 134: 0.333
INFO cross_voc_dataset_evaluator.py: 134: 0.558
INFO cross_voc_dataset_evaluator.py: 134: 0.612
INFO cross_voc_dataset_evaluator.py: 134: 0.471
INFO cross_voc_dataset_evaluator.py: 134: 0.030
INFO cross_voc_dataset_evaluator.py: 134: 0.506
INFO cross_voc_dataset_evaluator.py: 134: 0.213
INFO cross_voc_dataset_evaluator.py: 134: 0.404
INFO cross_voc_dataset_evaluator.py: 134: 0.074
INFO cross_voc_dataset_evaluator.py: 134: 0.394
INFO cross_voc_dataset_evaluator.py: 134: 0.741
INFO cross_voc_dataset_evaluator.py: 134: 0.618
INFO cross_voc_dataset_evaluator.py: 134: 0.593
INFO cross_voc_dataset_evaluator.py: 134: 0.273
INFO cross_voc_dataset_evaluator.py: 134: 0.316
INFO cross_voc_dataset_evaluator.py: 134: 0.381
INFO cross_voc_dataset_evaluator.py: 134: 0.462
INFO cross_voc_dataset_evaluator.py: 135: 0.409
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.442s + 0.001s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.355s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.354s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.354s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.357s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.359s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.356s + 0.001s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.359s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.359s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.361s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.361s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.362s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.364s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.378s + 0.001s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.356s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.361s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.353s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.350s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.355s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.369s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.369s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.368s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.368s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.370s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.371s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.370s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.405s + 0.002s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.342s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.361s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.372s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.373s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.375s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.373s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.374s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.375s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.375s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.375s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.374s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.374s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.555s + 0.002s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.406s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.390s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.386s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.384s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.376s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.377s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.376s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.375s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.373s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.370s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.371s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.369s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 54.932s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [144 184  49   6  79 232  99  45  17  38 176  51  42 113 122 238 139 100
 141 146 121  66 244 248 254  37 147  93   2 112  41 162  40 106 256 218
  26 194  73 125 105  18 109  59  67 227 217  33  11 209 252 124 207  88
 259  68 129  39  81  46 182 241  23  96 192 247 197 193  55  30 140  72
  50 145 233  20 128  25  86 143 142 210 216  16 164 123  91 134 183 133
 196 157 211  43  60 110 159  82 230 243   5 181  77 170   7 253 168 251
  35  28 127 120 226 117 138 135 237 132  74 262 180 126 223 153 150 250
 148 205 234  89  64 161 261 208 246 115 107 131 219 166 224 178 220  12
 173  47 163  31  48  15   8 201  58 108  85  54  52 152  32  95  76 235
 158 204  36  14 228 101 155 200  44  21 231 167  19 203 191  34 236 118
  61 255 114  13 111 187  22 189 229 174 202  24 206 263  62 190  98 188
 195  87 199 103 151   4 137  97 102 172 154 149  92 249  78  65  69   3
 198 257  29 136  94 221 177  57 104 225 175 130 258 169  27 186 156  71
   9  70   0 116  80  75 239  56 213 212 260 185  53 165 242  84 119 171
 240 222 160  90 215  63  10 214  83 245 179   1]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3503
INFO voc_eval.py: 171: [ 45 157  15 100 132 183 192 181 103 191 194  91  13 115  10 144  73 182
 133  16 169 184 129  76 136  64 130 180 185 187 141  42  17 151  95 124
  49 120  69  98   9  23   4   7 201 173 155 168  75  22 112 196 118  80
 123 195 163 190  63  78 116  62 140  56  31  74 111 154 197   0 105 138
  70 101 148 204  25 164  61 121  21 176 135 171   3  72 119  18 152 193
 178 205  79  51 175 189  48  59 109 200  46  85 127  77 108 177  12  41
 147  38 117 167 145   5  32  14  99  83  24  96 203 153  84  60   2 149
  44 166 114  87 110 161 174 158 143  92 131 128  54 125  71  88  97 202
 156 165  26 150 188  47 146  93 137 104  11  35  39  57 139  89 199  36
 172 134  94 102 142  52  28  66  27 198  34 107 162 106  29  82  33  40
 186  37  20  53  67 179  50   8  81  68   1  43  58  55  30  90 122 160
 126  19   6  86 159  65 170 113]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5841
INFO voc_eval.py: 171: [ 91  43 111  98  69  71 109   2  70 210 227 228 172 212 229  38 134   1
 256  85 135 185  56  80  86 213 113 106  14 101 200 191 197 252 181  74
 243 249 110 152 250 159 231  59  75 219   7 264  20 232  49 205 189 194
 102 190 184 263 230   4 195  99 100  79  36 107 156 170  88 192 226 128
  12 222  61  95 174  47 214 245  27 202 233 220 217  23  16 239  60 146
  19 260 138 254 115 155  73 126 238 161 216 199 262  65 165  90 154 258
 182 157 255 224 118  58 104 145  15  84 247 178 158  54 235   5  62 215
 132 259  33 180 167 223 131  44  41  10 176 169  51 153  67  97 150 151
 241 160  93 119  66  92 139 234 116  42  31  40 103 123 173   6 203 166
 236 240 251 149 133 237 162  26 248 225  45 137  37 148  72 140 196 204
 130 186 127 206  64  50 253  89 177 171 242  77  30 211 244 265 136  22
 208  55 129  48 201 207 143  29 112 175 179  11 105 124  94 168  39  46
   9 198  53 261  18 188   0 183 142 218 114   8 141 144 108  35  52  82
  63  32  83  13  25  28 257  34  68 209 164 246  21 163 147 117  57 193
 122  87  81  17  96 121 221 120  76  78   3 125  24 187]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2330
INFO voc_eval.py: 171: [369 315 316  99   2 317   7  82  49 256  30 257 347 343  53 454 139 113
 251 362 254 448   9 255 397 197 110 191 249 318  84  81 136 115 453 124
  94 260  28 396 298 186 235 393  36 390 307 173  32  21 398 339 208 278
 370 475 430 337 289 446 164 240 196 416 293 100  25 262 504 407  60 361
 261 294 477 456  61 388 171 243 178 150  95  31 231 259 472  65 379 465
 386  27 437 304 404 478 321 311 423 211 391 483 105  35 464 380 263 502
 353  13 459 250  86 248 467 180 334  59 188 244   3 498 463 193 348 455
  40 214 426 123 312 486  69 297 413 338 217 190 252 424 167 158 137 476
 279 268 210 378 138 401  15 121  24 172 471  38 114 302 120 280 273  47
 234 389 489 142 276 230 323 130 458 492 283 236 184 287 174 153 253 440
 222 301 333  29 309 288 500 168 266 479 129  41 491 376  52  64 381 269
 325 356 310 103 149  54 112 229 410 372  70 270 239 224  91 221 420 352
 264  90 258 282 490 175 488 441   4 102 374  44 160 201 295 324 503 408
  16 341  66 207  78 122 411 220 275 473   6  73 415 367 484 400  17  51
 265 299  87 134 450 147 357 499 159 118 375 170 327 461 199 445 212 360
 313 104 342  57 328 365 384 228 306  85  75 399 434  89 449 421 496 205
  46 432 330   1 225 277 474 355 442  88 233 501 427 444 468  48 242 246
 128 223 469 494 163 232 433 204  76  50 237 414 466 482  23 335 141 406
 418 101  10 177 305 202 425 436 192 351 452 394 227 109 417 132 181 319
 189 359  43 314 185 182 336 349 368 111 493 402 345 284 403 162 300  93
  45 166 267 152  22 157 412 495  12 451  34  72 216 340 245 344 203 119
 332 371 460  26 308  63 405 226 146 238  37 125 292 179  92  55 155 144
 485 385 127 271 382 198 148  97 107 290  58  18 364 326 116  71 133 481
 194 143  56 215   5  11 331 346 154 126 373 322 457 213 131  77 358 422
 470 431 487 291 480  39 117 320 387  98 151 108 435 176  19  79 281 200
 195  20 206 183   0  83 447 241 443 247 439 419 218  96  67 392 409 156
 285 187 428 169 366 140 383  33  80 354 497 363 350  74 438 462   8 303
 286 329 106 377 145 209 429 135  62 274 395  42  14 161 165 296  68 272
 219]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3382
INFO voc_eval.py: 171: [405 220 406 407 224 192   3 194 410 225 408 335 338 191 107 277 193 251
 509 510   5 182 318 334 394 297 319 195 123 181 470 253 333 417 332 414
 420 424 337 165 461 353 441  30 412 238 428 409 411 426 465 423 431 336
 268 413  76 429 437 484 118  75 263  31 232  17 330 282 173 148 342 494
 435 275  22  54 283 147 339 226 438  45  72 344 242 361 284 276 254 249
 418 492 189 371 357 138 199 289 493 237  44   8 244 134  78 320 234 168
 129 343 340   6 162  43   4 364 432  83 286 331 101 422 198 197 228 458
 360 187 415 369 166 421 400 359 154 170 368 511 119 378 174 352 152 303
 136 307 463 252 434 293 329 354 444 459 208  86  39 345 132  51 341 452
 110 201 322 498 476 209 445 235 248  26 440 227 363 262 190   7 210  18
 446 439 287 323 457 367 362  14 495 348  40 504  25  11 365 471 502 376
 149 245 176 212 175 229 124 185 298  16  84 499 397  35 196 302  37 156
 355 392 221 321 496 267  62  96  91 288 393 377  67 260 401  23  55 255
 261 350  56 346 115 250 258 108 301 160 291  57 387 257   0 507  63 164
 104 449 157 145 505 215 240 388  19 231 264  70 314 383 381  32 233 179
  47 158 310 290 304  99  65 482 236 475 300 326  90 183 380 273 270 453
 246 239 180 151 403   9 161 448 222 230 501 404 103  80 184 485 402 135
 462  34 347 425 143  68 373 389  13 279  60 477 372  69 419 416 120 241
 126  88  97 206 370 450 478 351 256 144  92 308 217 349  93 474 379  79
 271 188 311 464 172  21  77  49 295  33 127 218  12  59 178  52 375 128
 328 139  61  20 483 312 490 315 202 454 131  73 467 112 169  87 125 285
 398 460 247 114 203  53 116 480 207   2 309 266 259  29  27 137  15 466
 141 200 219 430 299 497 356 150  94 390 384 386 391 106 456 113  98 513
  46 280  10  48 325 272 163 269 442 140 105  38 281 205 447 278  64 294
 481 489 382 324 317 155 122 512 479  66 433 306  74 167 374 130 133 316
 142  71 500 216 211  36 171 146 472 488 503 473 327 223 100  81 487  58
 121  89  82 491 506 358 486 159 427 396 436 111 305 296 443 292  95 366
  28 204 395 469 153 399 177 102 186   1 313 508 455  42  24  85 385  41
 214 274  50 109 468 243 213 451 265 117]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5574
INFO voc_eval.py: 171: [ 25  12  40  38 113  17  64   2 106  45  51  70  11  80  88  67  29 111
  74  78  77 119  24  39  65  49 105  16   1  15  47  73  91  19  41  87
 112  86  22  62  82  60  76 109   6  44 101  96 110  99  66   0 100   9
  37  83  52  10  31  36 103  23  69  18  68  50 116  72  34  30 117  35
  53  81  93 115 114  61  27  97  20  54  71  84  89   8  85 102   5  42
  28  13  43  32 107  46  95  92   3  26 108  98  57 104   4  63  94  79
  14   7  90  48  33 118  58  21  56  55  75  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6239
INFO voc_eval.py: 171: [281 298 125 371  69 414 448 121 218   9 404  51  88 149 415 302  15 200
 250  22 326 183 203 394 202 416   2 392 408 325 380 369 430 438 220 321
 169 329 214 289  87 148 228 245 378 345 373 420 320  33 254 400 282 112
 286 255 199  84  78 170  77 243 195  59 461 222  34 303  46 181 347 226
  26 172 140  16 377 234 322 457 230 251 223 160 263 168 403 390 311 266
  36 275  53 367 235  57 191  48  91 152 206 185 173  62 393 306 109 439
 232 159  27 204 351 331 409 236 137  11 130 231 164 101 270  81 278 267
 171 348 323 198 312 280 221   1 246 460  73 107 419  41  23 237 341 182
 233 113 177 224  97 384 146 453 260 337 208 319 444 194 424 361 338 396
 166 379 145 381  24 205  56 103 422 287 407 410 364 450  32 174 272 425
  17 104 165 253 447 268 192 201 366 301 120 154 142  93 209 288 297 133
 277 129 449 197  37 162 374 143 454 388  49 239  21 399 167 413 350 225
  54 118 406  42 248 294 156  38 261 458  95 431  12 290 153 318  92 295
 332 360 305  86 128   8 207  25 372 151 443 340  90 114 433 363 227 190
  50 178 334  43 383 451 123 317 132 402  96  40 157  29 387 110 188  85
 316   5 163  71 265 102 358 193 176 398  64  76  47 436 342 135 385 138
  14 446 279  39 386 421 405  70 264  35 184  63 401 343   0 242   6 418
 244   7 213  89 411 426  13 150 328 389  45 273 259  55  20 291  66 196
  44  52 300  10 116 455 189 117 283 127 314 141 115 412 258 186 353 352
  80 256 308 357 428 119 391 217 445 456 435 257  94 155 293 134 180 382
 229 247 212   4  99 210  19 368  72 370 327 124 215 276 437 179 131 395
  75  60 158 111 429 105 126  83  18 462 274 285 362  28 240 310  74 440
 354 339 106 434 344 161 147 292 241 100 219 269  68 284 375 376 452 359
 335  65 122 459 238 313  67  61  30 296 432 427 299 136  98 211 144 356
 304   3 249 271 315 355 417 441 423 324 397 187  58 346 365 307 108  31
 349 262 330 333  82 336 216 442 309 175  79 252 139]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4698
INFO voc_eval.py: 171: [ 7  4 15 53 48 16 27  6 14  0 51 33 42 40 30 22 52 50 19 12 10 31 25 47
 46 32 54 11 49  1 34  3 36 20 35 41 21 39 26 29 23 44 18 13 37 17 24  9
  5 45 55 38  2 43  8 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0301
INFO voc_eval.py: 171: [1375 1377  139 ...  262 1446 1224]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5161
INFO voc_eval.py: 171: [34  5  7 25 63 49 21 69 19 54 72 22 45 68 39 15 73 32 62 50 33  8 28 40
  6 67  9 75 36 60 53 29 58 23 61 56 14 48 57 71 17 12 31 18 16 20 11 38
 26 10 64 13 27 65 70 35 37 43 44 66 47 41  2 52 42 24 59 30 55 74  4  3
  0 51  1 46]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2197
INFO voc_eval.py: 171: [397 177  97 167 344 190 259 261 352 252 238  42  50 150 135 302 428 148
 355 377 147  17 221 426 170 274 399  18  82  62 420 271   1 116 341 371
 175 367 282 258 299  66  49 383 249 325 451 327 104 455 260 137   9 292
 182 153 418 328 294 100  51 281 196  61 115  14 457 340  39 372  99  24
 106  12 278 164 345 389 353  60  31 120 422 158 128 390  40  27 195 459
  72 138 408  80 392 432  55  22 273 160 286 293  16 246  44 386 248 378
  86 300 443 309 375 416 145  63 316 226  28 144 276 114 334 243 208  46
 124 440 320 358 142 291 194 122 450 132 118 103 134  35 215 152 105 403
 253 146   0 139 126 394 155 236 360 157 458 446  15 413 430 431 117 379
 176  79 384 365  13 376   6 229 439 217 233 304 382 362  84 268 310 255
  70 113 417  67 270 323 456 234 297   4 373 111 279  20 315 225 366 333
 411  74  58 250 266 406   5 199 332   3 295 368  75 211 108 314 222 414
 447 174 245 133  11  29 101 240 306 445 149   2 192  41  92 435 231  21
 347 247 395  73 165 121 298 319 272  52  71 205 197 191 369 143  69 204
  37 425 308 331 421 285 181 321 173 201 351 305 265 209 393 244 415 448
  32 343 159 407 107 263 307  91 184 335  33 119 313 242 427 363 189 449
 216 326 275 405 410 317 398  68 166 338  87 429  38 213 188 254 172 434
  57 203 110 346 419 262 385 130  59 370 127 409 232 185 214 296 239 453
 387  36 322 287 402 198 359 374  47  45 141 230 168  26 227 140  95 289
 436 350  25 220 423 187 396 283 156 154 131 179 280  54 454  78 180 251
 337 336 235 224 267 391  83 218 330   8 312 361 183  85 186  56 290 388
 412  77  64 401  98 169  43 380 356 442 324 210 433 163  88 125 223 171
 200 136 303 364 269  96  93 342  53 452 311 129 202  19 241 438   7 444
 349 123 257 348  30 228 212 318 288 151 178  48 354 404 109 219 161  76
 301 357 162  81 237 400  94 256 207 112 329 424 264 437  65 102  23  10
 277  89 284 441  34 206 381  90 193 339]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4105
INFO voc_eval.py: 171: [ 95  55  77  14  53   2  30  22  56  31  50  28 105   4  20 103  70  46
  19  48   0  83   1  63 104  49  45  29   9  42  35 100 108  25  36  18
  80 106  41  11 109  98  72 107   8   7  76  17  84  75  23  44  64  21
 101  69  91  67  99  54   6  62  89  40  58  79  39  96  88  68  34  93
  81  60  71  32  15  61  13  94  12  92  86  65  66  78  38  51  57  73
 110  33  43  82   3  59  24  90  10  37  97  87  52  27  47 111   5  74
 102  16  26  85]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0738
INFO voc_eval.py: 171: [70 84  9 62 12 60 45 86 88 26 50 59 10 27 58 74 49 64 71 11 35 78 80 40
 54  4 76 23 17 19 25 14 82 37 61  5 46 85 29 18 41 69  7 52  8 21 32 30
 13  1 48 53 63 68 36 34 83 55 81 65 72 73 20  2 79 22 28 87 42 75  0 15
 39 24 51 38  6 56 67 66 31  3 33 43 16 47 57 44 77]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3965
INFO voc_eval.py: 171: [187  27  16 120  86 167 140 124 194  31   5  11  90 145   4 179 125 189
 135  79 138  73 177  47 137  53  82 130  96  43 158 162 149  75  63  13
  66 111 132 121  42 198  80 139 109  71 110  83 116 188 150 106 103  55
 148  85 126  94  98  87   3  59  57   1  60 141  24  97 166  62 156  72
 164 146 155 183  22  51  35 199  95  28 114   2 102 163  17  33  58  38
  99 178  12 151  40  92 193  30 192  69 157 107  91 115  78 134  23 136
 131  93   9 108  65 173 160 197 119  14 165 172 191 117  32 147  68 185
 104  34  76  18 202 123  56 195 133 142  52  77 190 170  15  39 176  88
 168  70   8 200 118 100 203 201  19  41   0 182   7  50 113 153 159  48
  46 105 180 174   6 181  67 196  44  74  61 171  49 129 128  84 186  37
  45 101  81  54 175  26 161  29 152 122 127 154 144  21  64 112  25  89
  20 184 169 143  10  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7412
INFO voc_eval.py: 171: [2660  508 1647 ... 2118 1238  146]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6197
INFO voc_eval.py: 171: [113 137 414 198 667 146 623 225 125  69 142 568 670 288 209 627   4 276
  71  70   3 438  44 175  87 355  72 455 404 223 534 207 393 540 378  74
 243  73 594 668 210 694 227 413 597 624 571 489 254 160 499 153 159 339
 255 456 620 622 134 669 544 446 444 608  46  84 542 679 205 682 676 158
 129  55  80 541 607 314 256  56 382 331 392  31 457 613 388 493 132 357
 684  77 110 145 245 557 174 470 291 539 625 468 399 573 658 322 612 100
 630 301 328 285 672 316 430 600 506 358 605  39 265  98 697 463 546 545
 295 211 497 133 394 610 721 435 447 715 408 356 105 114 344 201 631 230
 543 383 330 656   7 208 482 215 494 548 590 406 192 560 577 717 416 371
 520 674 289 428 569  59  82 478 427 319 570 595 628 271  93 550  32 203
 262 335 359 391 465 671 647 166  86 535   5 217 345 214 329 449 401 240
  75 117 131 102 615  34  47  29 415 626 176 273 722   6 673 298 161 591
  81  41   8 157 440 375 475 549 587 481 675  96  61 471  58 143 367 696
 460 689 554 258 562 213 287 249 326 434 246 268 636 616 257 379 216 122
 340 678  25 177 692  40 180 104 461 220 649 576   2 504 686 403 348 572
 172  17 294 226 505 396 462 710 421 561 719 364 699 436 395 603  49 310
 164   1 639 202 212 417 483 501 302 725 448 169 508 527 115 467 349 652
 376 178 469 503 327 655 606 680 526 531 515 498 412 633 128 665 194 259
 431 517 654 306 373 279  52 507 247 419 632 653 248 293 244 614 563 232
 724 644 170 598 420 716 473 165 551 638 599 422 123 317 260 101 252 509
 528 267 338 409 707 121  20 189 233 139 156 538  95 304  10  48 714 666
 402 547  62 182 167 713  18 188 486 183 464 229 363 103 567 581  90 351
 368 704 269 150 700 138 424  57 148  64 303 300 318 251 695 575 354 589
 585 718 727 343  85 648 583 691 495 222 341 708 588 405 312 264 621  88
 107 566 411  21 305 350  79 530  13 193 380  97 553 275 559 641 564 347
 663 239 602 342  45 480 511 280 537 386 574 720 324  27 384 127  23 313
 151 309 693 141 284  16 514 513 556 635 334 374 320 491  19 321 634 593
 584 706 218 552 611 555 476 533 637 366 311 681 565 687 224  94 578 397
 519 601 147 643 723 407 400 126 281 685 433 452 219 323  53 703 290 698
 362 274 124 479 171 472  89 485 702 346 660 278  26  24 196 235   0 381
 118 360  22 487 705 474 459 662  42 370  51 261 596 617 162 361 443 389
 387 352 282 135 466 496 238 241 579 488  83 657 204 108 111 423 484 586
 195 683  99 645 410 272 441 442 558 618  28 191  63 140 398 650 604 308
 522 109 385 445 726  43  60 130 439 523 437 664 200 529 155 701 112 518
 234 582 307  65  78 116 149 186 190  30 377  14 187  38 184 120 629 425
 510 250 263 450 292 325  12 651 106 688  66 237 242 477 592  76 500  91
 709 197 525 642 524 619 181 253 454 453  35  33 369 173 365 661 432 228
 532 458  50   9 712 690 390 353 277 677 492 163 490 640 451 144 179 418
 426 154 236 296 646 336 512 580 283  92 502 521 333 315 199 299  15 711
 659 332 206 609 266  36 119  54  11 297 270 221 136 231  37  67 185 152
  68 286 536 429 516 168 372 337]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5946
INFO voc_eval.py: 171: [53 25  1 49  2 27 47 63 46 44  6 68 61 24 62 51 36  8 28 10 56 22 20 14
  3 31 26 29 64 57 50 60 59 30 12  7 37 19 23 32 21 58 42 13 15 38 39  4
 45 52 18 34 33 17 66  9 16 43 54  5 41 65 35  0 67 48 55 11 40]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2727
INFO voc_eval.py: 171: [ 61  62 137  20  91  52  60  92  68  31 105   3  99  87   2  66  28  23
  25  49  79  88  44  95  74 111  94  64  76 109  53  96 118  67 112  27
 134  83 139 108  97  11  38  65  89 124  42  18  10 129   0  24  55  81
  71   4 104   7  13  43  30 120  39  57  21 130  47  56  90  77 126  82
  50 135 140  54  70  46   1 107   9   6   8  12  75  33  93  58 125 117
 133 119  26  80  85  29  72  48  78  37  98  34  63 127  35 138  40 106
  73  22  36 113  51  59  69 114  32  41 122 101 121   5 100  86 116 103
 102  15  45 123 128  84  19  14 131 136 115  16  17 110 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3147
INFO voc_eval.py: 171: [ 94  11  54   8  25  42  82  18 120  65  45  27 123  66  62   6  26 133
 130  43 108  47  83 125 107  57  78  67   9  15  40  75  71 109  52  24
  22 134  29  10 115 103   7  72  17   3  30  23  98  88  46 126  87  34
   5 137  58  92  53  13  86  32  89 117  41 135  68  73  80  36 102  81
 124  51  39 113  38 105  55  90 100  49 138  69  20  70  84 128 106 104
  59  21  31  44 127   1 111  95  64 118   0  56  28  61   4 114  91  16
 131  35  12  14  79 122 140  99  33  97 112 136  19  63 121  37 129 119
 116   2 110  85 101  50 139  74  77  93  48  60  96  76 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3820
INFO voc_eval.py: 171: [112  69 166 177   1 245 130 159  18 222 214 135  12  99  59  25 241 229
  33 178  86 184 174 136  27  48  19 139 205 167   6 249  52  85 164 147
 160  96 256  79 186 168 196  36 169  61  73 253   0  26 247 212  35 215
 131  87 117 134 170 119 194  49 242  54 260  58  44  39 257 189 158 232
 230 121 188  34 237 183 145  24 233 161 182 122  91  74 191 107 206 195
  50 185 148  37  46  28 193  93  67 132 102 133 217 240  82 120 203 211
 179 187 138  60 146 258 137 239  15 198 254 149  68  77  17 226 173  10
   9 162 243 200 192 154 220  94 197 163 165 103  84 259  11 125  53  22
  97  45 219 151 128 223 109  16 155  32   8  62 218  56 255  95  38 106
 104   4 113 144  31  81  42  65 152  29 231 204 234 235 228 175 208  41
  66  47 108 238 156  23 213 116  43 236 181  76 221 105  92  57  40  70
  72 251 101  20 244 207 150  78 129 224  21   7 124  63 246  98 157 216
  88   2 252 261 141 126 123  83 172   5  13 202 153 210 171 114  80   3
  55 248 118 201  51 250  14 176  30 209 227 142 190 100 115 111 225  64
  75 127  71 199  89 140 110  90 143 180]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4596
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4094
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.350
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 134: 0.233
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.557
INFO cross_voc_dataset_evaluator.py: 134: 0.624
INFO cross_voc_dataset_evaluator.py: 134: 0.470
INFO cross_voc_dataset_evaluator.py: 134: 0.030
INFO cross_voc_dataset_evaluator.py: 134: 0.516
INFO cross_voc_dataset_evaluator.py: 134: 0.220
INFO cross_voc_dataset_evaluator.py: 134: 0.410
INFO cross_voc_dataset_evaluator.py: 134: 0.074
INFO cross_voc_dataset_evaluator.py: 134: 0.396
INFO cross_voc_dataset_evaluator.py: 134: 0.741
INFO cross_voc_dataset_evaluator.py: 134: 0.620
INFO cross_voc_dataset_evaluator.py: 134: 0.595
INFO cross_voc_dataset_evaluator.py: 134: 0.273
INFO cross_voc_dataset_evaluator.py: 134: 0.315
INFO cross_voc_dataset_evaluator.py: 134: 0.382
INFO cross_voc_dataset_evaluator.py: 134: 0.460
INFO cross_voc_dataset_evaluator.py: 135: 0.409
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.472s + 0.002s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.369s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.368s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.363s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.368s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.366s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.362s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.364s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.365s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.364s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.363s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.363s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.364s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.366s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.363s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.366s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.360s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.360s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.359s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.370s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.366s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.365s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.366s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.365s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.365s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.362s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.582s + 0.002s (eta: 0:01:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.390s + 0.003s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.375s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.376s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.379s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.375s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.369s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.370s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.370s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.370s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.369s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.369s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.369s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.420s + 0.001s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.365s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.368s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.362s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.359s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.358s + 0.002s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.356s + 0.002s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.357s + 0.002s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.359s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.357s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.354s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.355s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.355s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 54.228s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [143 182  49   6  78 230  45  98  17  38 175  51  42 112 121 236 138  99
 140 145 120  67  66 242 246 252  37 146  92   2 111  41 161  40 105 254
 216  26  73 192 124 104  18  59 108  68 225  33 215  11 207 250 123 205
  87 257  69 128  39  80  46 180  23 239  95 190 245 195 191  30  55 139
  50 144 231  20 127  25  85 142 208 141 214  16 163 122  90 133 181 132
 134 194 156 209  43  81 109  60 158 228   5 179 241  76 169   7 251 167
 249  28  35 126 119 224 137 116 235 131 260 178  74 221 125 149 152 248
 203 147 232  64 160  88 206 259 114 244 106 165 217 130 222 218  12 177
 172  47 162  31  15  58  48 199 107   8  52 151  54  32 233  84  94 202
  14  36 226 157 100 154 198  44  19 166 229  21 189 201 234  34 117 113
 253  13 185 110  22  61  24 200 187 227 173 188 204  62 197 261 186 102
  97 150 193  96  86   4 148 153 171 101 136  70  77 247  91  65  93  29
 255   3  57 223 103 196 219 135 176 129 256 174  71 184   9  27 168  72
 155   0  79 115  75 258 211  56 237 164 170  53 210 240 183 159  63  83
 118 238  89 220  10 213  82 212 243   1]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3470
INFO voc_eval.py: 171: [ 45 155  15  99 131 181 190 179 102 189 192  90  13 114  10 142 136  72
 180 132  16 167 182 128  75 133  64 129 178 183 185 139  42  17 149  94
 123  49 119  68  97   9  23   4 199   7 171 153 166  74  22 111 194 117
  79 122 193 161 188  63  77 115  62 138  56  73  31 152 110 195   0 135
 104  69 100 146  25 162 202  61  21 120 174   3 169 118  18  71 150  78
 191 176 203  51 173  48 187  59 108  46 198 126  84  76 107  38 145  12
 175  41 143 116 165   5  14  32  98  82  95 201  24  60 151   2  83 147
 113 164  86  44 109 156 172 159 141 127  54  91 130  96  87 154 124 200
  70 163 148  26 186 134 144  47  92  11 103  35  39  57 197  36  88 137
 170  93 101 140  52  28  65  27 196 106  34 160  29  40 105  33 184  81
  20  50 177  53  37  66   1  80   8  89 158 121  58  30  43  67 125  19
  55  85 157   6 112 168]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5841
INFO voc_eval.py: 171: [ 91  43 111  98  71  69 109   2  70 210 227 228 171 212 229  38 133   1
 257  85 134 185  56  80  86 213 113 106 101 200  14 191 197 252 180  74
 243 249 110 151 250 158 231  75 219   7 265  20  59 232  49 205 189 102
 194 190 184 264 230   4 195  99  79  36 100 107 155 169  88 192 226 127
 125  12 173  95  61 222  47 214 245  27 202 220 233 217  23  16 239  60
 145  19 261 137 254 115 154  73 238 216 160  65 199 263 164  90 153 259
 182 156 118 255 224  58 104 247  15  84 144 177 157 235  54  62 131  33
 260   5 215 166 179 130 223  44  41 175 168  51  10 152  97  67 241 149
 119 159  66  92 150 138  93 234  42 116 122 103  40 172  31 236 240   6
 203 251 165 148  26 237 132 225 161 248 147 136  45  37 139  72 196  50
 206 204 253 129 186 176 126  64  89  30  77 170 242 135 244 211 207 266
  22 128  48 201  55 174 208 112  29  94 178 142 105 123  11  53 198  39
 167   9 183 188  46  18 262 114   0 141 218  82 108   8 140  63  32 143
  52  35  83  13  28  25 258  34 163 209  21 246  68 146  87 221  57 162
 193 117 121   3  81 120  96  17  24  78 181  76 124 187 256]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2338
INFO voc_eval.py: 171: [367 313 314  99   2 315   7  82  49 254  29 255 345 341  53 452 114 250
 360 140 253 446   9 395 257 110 197 191 248 316  84  81 136 115 451 124
  94 259  28 296 394 186 234 391  36 388 305 173  32  21 396 337 208 276
 368 473 428 335 287 444 164 239 196 414  25 100 262 503 405  60 359 260
 292 475 454  61 386 171 242 178 150  31  95 230 258  65 470 377 463 290
  27 384 435 302 402 476 309 319 421 211 389 481 105  35 462 378 261 500
 351  13 457 249  86 465 247 180  59 332 188 243   3 496 193 461 346 453
  40 214 424 123 310 484 295  69 411 336 190 216 251 422 167 137 158 474
 267 277 210 399 376 138  15  24 121 469 172 120  38 113 278 300  47 271
 387 233 487 142 274 229 321 281 130 490 184 456 235 174 284 252 153 438
 331 221 129 498  30 168 265 285 299 477 307  41 489 374  52  64 379 323
 268 354 308 103 149 228 112 408  70  54 370 223 269 238  91 418 220 350
 263  90 256 280 488 439 486 175 102   4 372  44 322 160 293 201  66 339
 502  16  78 207 122 406 273 409 471 219   6 413 365  73 482 398  17  51
 297  87 264 134 159 355 448 497 147 118 170 373 325 458 212 358 199  57
 340 443 311 326 304 363 227 382 104  85  75 432 397 447 419 205 494  89
   1 430 328  46 353  88 499 440 224 472 275 232 466 241 245 492 467 128
  48 231 222 442 425 163 431 204  76 236  50  23 480 464 333 404 412 141
 416 177  10 303 101 434 423 349 450 202 192 226 109 392 132 415  43 357
 181 317 312 189 135 334 182 347 185 491 162 298 366 401 343 111 282 400
  45 166  93  22 410 152 157 266 493  34 244 381 215 330 449  12  72 338
 119 342  26 203 459 306 369 225 403 146  63  55 179 291  37 148 125 144
 116 237 155  92 133 194 198 127 479 270 324 483 362 288  71  97 383  58
 380 107   5 344 143 154  56 329  11  18 455 320 371 117 420 213 356 468
 131 126 289 385 478 151  77 318  98 485  39 108 176  19 433 429 279  79
  20   0 441  83  96 246 240 217 390  67 200 206 407 417 183 437 195 445
 187 169 156 283 286 139  33 426 364  74 436 495 460 361 301 352  80   8
 348 375 327 209 393 145 272 427 106  62 501 161 218  14 294  42 165  68]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3380
INFO voc_eval.py: 171: [404 220 405 406 224 192   3 194 409 225 407 334 337 191 107 276 193 251
 508 509   5 182 317 333 393 296 318 195 123 181 469 253 332 416 331 413
 419 423 336 165 460 352 440 411  30 238 427 408 410 425 464 422 430 335
 267 412  76 428 436 483 118  75 263  31 232  17 329 281 173 148 493 341
 434 274  22  54 282 147 338 226 437  45  72 343 242 360 283 275 254 249
 417 491 189 370 356 138 199 288 492 237  44   8 244 134  79 319 168 234
 129 342 339   6 162   4  43 363 431 101  83 285 330 421 198 197 457 228
 359 187 414 368 166 420 399 154 358 367 170 119 510 351 377 174 152 302
 136 462 306 292 353 252 433 443 328 208 458  86  39 344  51 132 110 340
 201 451 321 497 475 209 444 235 248  26 439 227 362   7 190 262 210 445
  18 438 286 456 494 322 361 366 347  14  40  25 503  11 364 470 376 149
 501 176 245 212 175 229 124 185  16 297  84 498 396  35 301 196  37 156
 354 391 266 495 221 320  96 461  62  91 287 392  67 375 260 400 255  23
 261  55  56 349 345 115 258 108 250 300 160 290 257   0 386  57  63 157
 448 145 104 164 504 215 240  19 387 264 231  70 382  32 380  47 233 313
 179 303 309 158 289  65  99 236 481 325 474 299 379 183 272  90 180 452
 239 246 269 151 402   9 161 447 222 230 484 184  80 500 103 424 135  34
 346 403 401  68 143  60 476 388 278  13 372  69 418 415 371 126 241 120
  88  97 206 477 369 449 350 256 144 307 217  93  92 348 473 188 378  78
 270  21 172 463  77  49 310 294 127  33  12  59 178 218  73 128 327 374
  52 139  61  20 202 489 466 482 314 453 131 311 125 169 112  87 284 116
 459 247 479 207 114   2 203  53 259  29  27 265 308 137  15 465 219 141
 397 200 496 429 150 298 383 390 389  94 385 355 106 113  48  98 512  10
 279  46  38 105 140 324 268 163 441 271 280 455 277 323 446 293  64 381
 480 488 122 205 305 155 511  66 316 478 432  74 373 130 133 167  71 315
 142 499 216 211  36 171 146 471 487 502 472 326 223 100  81  82 506  89
  58 486 159 121 395 505 490 426 485 435  95 111 295 291 357 365  24 468
  28 442 177 204 102 304 153 398 394 454 507  85   1 312  41 213 384 186
  42 214 109  50 467 243 450 273 117]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5588
INFO voc_eval.py: 171: [ 25  12  40  38 113  64  17   2 106  45  51  70  11  80  88  67  29 111
  74  78  77 119  24  39  65  49 105  16   1  15  47  91  73  19  41  87
 112  22  86  62  82  60  76 109   6  44 101  96 110  99  66   0 100   9
  37  83  52  31  36  10 103  69  23  18  68  50  72 116  34  30 117  35
  93  53  81 115 114  61  27  97  71  20  54  89   8  85  84 102   5  42
  28  13  43 107  32  46   3  26  95  92  98  57 108 104  63   4  94  14
   7  79  90  33  21  48  55  58  56 118  59  75]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6239
INFO voc_eval.py: 171: [281 298 125 371  68 414 448 121 218   9 404  50 149  87 415 302  15 200
 250  22 326 184 203 394 202 416   2 392 408 380 325 369 430 220 321 170
 329 214 289  86 148 228 245 378 345 373 438 420 320  32 254 400 282 112
 286 255 199  83  77 171  76 243 195  58 461 222 263  33 303  45 182 347
 226  26 140 173  16 377 234 322 457 230 251 223 161 169 403 390 311 266
  35 275  52 367 235  56 191  47  90 152 206 186 174  61 393 109 306 439
 232 160  27 204 331 351 409 236 137  11 130 231 165 101 270  80 267 278
 323 172 198 348 312 221 280   1 246 460  72 107 419  40 237  23 183 341
 233 113 224 178  97 146 384 453 260 337 208 319 194 444 424 338 361 396
 167 145 379 381 205  24  55 287 103 422 407 364 411 450  31 175 272  17
 425 104 166 253 447 192 268 201 301 366 120 142 154  92 288 209 297 277
 133 129 449 197 374 163  36 143 454 388  48 168 239  21 225 399 413 350
  53  41 406 294 248 118  12 261  37  94 290 156 431 458 318 153  91 295
 360 332 128  85 305 207   8  25 372 340  89 151 114 443 433 363 227 179
 190  49  42 334 451 383 123 317 132 402 387  96 158  39  29 110 188 316
  84   5 164  70 265 102 398 193 358 177  63  75 385  46 135 342 436 138
  14 446  38 386 279 421 405  34  69 264 185  62 343 401   0 242 418   6
 244   7 410 426 213  13 389 150 328  88  44 273 259  54 291  20 300  43
  51  65 196  10 116 455 189 127 141 283 117 314 258 115 412 353 352  79
 357 119 391 308 256 428 217 134 445 456 257 435 293 155  93 181 229 382
 247  71 124  99 370  19   4 368 210 212 327 437 276 131  74 395 215 180
  59 159 111 105 429 126  18  82 462 274 362 285 310  28  73 240 344 162
 106 434 354 440 147 339 219 269 241 100 292 284 452 376 375  67 359 459
 122 238 335  64 313  66  60 432 296 427 299  98 211 356 136 249 417   3
 144 304 315 355 157 187 397 365 271 423  95 324  57 441 346 262  30 330
 108 333 216 349 336 307 442  78 252 309 176  81 139]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4712
INFO voc_eval.py: 171: [ 7  4 14 53 48 15 26  6 13  0 51 32 41 39 29 21 52 50 18 11 30  9 24 46
 45 31 54 10 49  1 33  3 35 19 34 40 38 20 28 25 47 22 43 17 12 36 16 23
  5 55 44 37  2 42  8 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0299
INFO voc_eval.py: 171: [1373 1375  141 ...  112 1089  517]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5165
INFO voc_eval.py: 171: [34  5  7 25 63 49 21 69 19 54 72 22 45 68 39 15 73 32 62 50 33  8 28 40
  6 67  9 75 60 29 53 58 36 23 61 56 14 48 57 71 17 12 18 31 16 20 11 38
 26 10 64 13 27 65 70 35 37 43 44 66 41 47  2 52 24 42 59 30 55 74  4  3
  0 51  1 46]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2197
INFO voc_eval.py: 171: [395 176  97 166 342 189 257 259 350 250 236  42  50 149 135 300 426 147
 353 375 146  17 219 424 169 272 397  18  82  62 418 269   1 116 339 369
 174 365 280 256  66 297  49 381 323 247 449 325 104 137 258 453   9 290
 181 152 416 326 292 100  51 279  61 195 115  14 456  39 338 370 106  24
  99  12 163 276 343 387  60 351 120  31 420 157 128 388  40 194 458  72
 406  80 430 390  55  22 271 159  16 384 244  44 291 246 376  86 298 441
  63 307 414 373 144 314 224 143  28 114 274 332 241 207  46 124 438 318
 356 289 141 193 118 132 122 448 103  35 134 151 213 105 284 401 251 145
   0 136 138 126 154 392 234 358 457  15 156 428 377 175 429 444 117  79
 411 382 374 231  13 363   6 227 215 437 302 360  84 380 266 113  70 308
 415 253 268  67 454 321 232 111 295   4  20 371 277 313 364 223 331  74
 409   5 248 264  58 330   3 293 404 198 366  75 209 108 445 312 412 220
 173  29  11 243 133 101 191 304 443 148   2 238  92  41 229 433  21 393
 245 345  73 196 317 121 296 164 204 270 190  71  52 142  37 283 367  69
 203 423 306 419 200 319 349 180 172 329 413 341 391 242 263 303  32 446
 405 158  91 305 107 261 311 183  27 333  33 119 425 188 240 214 361 324
 403 273 447 455 315 165  38 187 408 336 396 211 252  68  87 432 427 171
 130 110 383 202 417 344 260  59  57 368 127 407 294 184 237 400 212 230
 451  36 385 372 285 197 167  47  45 357 320 228  26 434 421 140  95 139
 287 225  25 153 281 348 186 155 218 394  78 179 278 249 452 334 335 131
  54 233 178 410   8 216  83 310 359 222 389 328 265  85 386 399  56 288
  77 182 185  64 378  98 354  43 168 440 431 162  88 208 322 221 129 199
 125  96 170 301 362  93 267 309 201  19 450 340  53 346 226   7 210 436
 316 239  76  30 347 255 442 286 398 150 123 177 109 352 217 160  48 327
 161  65 355  94 299  23 402 235 254 206 205 422  81 112  10 435 262  89
 439 102  34 282 275  90 192 337 379]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4118
INFO voc_eval.py: 171: [ 94  54  76  14  52  30   2  22  55  31  49  28 104   4  20 102  69  46
  19  48   0  82   1  62 103  45  29   9  42  35  99 107  25  36  18  79
 105  41  11 108  97  71   8 106   7  75  17  83  74  44  23  63  21 100
  68  90  66  98  53   6  61  40  88  57  78  39  95  67  87  92  34  80
  58  70  32  15  59  13  93  12  64  91  85  38  65  77  50 109  56  72
  33  43  81   3  24  60  89  10  96  37  51  86  27  47 110   5  73 101
  16  84  26]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0741
INFO voc_eval.py: 171: [69 83  9 61 12 59 44 85 87 26 49 58 10 57 73 48 63 70 11 34 77 79 39 53
  4 75 23 17 19 25 14 81 36 60  5 45 84 28 18 40 68  7 51  8 21 29 31 13
  1 52 47 62 67 35 33 54 82 71 80 64 20  2 78 27 22 86 41 74  0 15 72 38
 24 50 37  6 55 66 65 30  3 32 42 16 46 56 43 76]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.4037
INFO voc_eval.py: 171: [187  27  16 120  86 167 140 124 194  31   5  11  90 145   4 179 125 189
 135  79 138  73 177  47 137  53  82 130  96  43 158 162 149  75  63  13
  66 111 132 121  42 198  80 139 109  71 110  83 116 188 150 106 103  55
 148  85 126  94  98   3  87  59  57   1  60  24 166  97 156  62 165  72
 146 155 183  22 141  51  35 199  95 114  28 102   2 163  17  33  58  38
  99 178 151  40  12  92 193 192  30  69 157 107  91 115  78  23 134 136
   9 108  93 160  65 173 131 197 119 117  32 172 191  14 147  68 185 104
  34  76  18 123 195 133  56 202  52 142  77 190 170  15  39 176 164 168
  88  70   8 200 118 203  19 201 181   0  41  50 182 113   6 159  46 153
 105  48 180 174  67   7  44  74 196 171 128 129  84  61  45  37 186  49
 101  81 175 161  54  29 154 122 127  26 152 100 144 112  21  20  64  25
  89 184 169  10 143  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7412
INFO voc_eval.py: 171: [2672  510 2694 ...  718 2027 2995]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6203
INFO voc_eval.py: 171: [113 137 416 198 670 146 626 225 125  69 142 570 673 288 209 630   4  71
 276  70   3 440  44 175  87 357  72 457 406 223 536 207 395 542 380  74
 243  73 597 671 698 227 210 415 600 627 573 491 160 254 501 153 159 341
 255 458 623 625 134 672 546 448 446 611  84  46 544 685 205 674 680 158
 129  55  80 543 610 256 315  56 333 384 394  31 459 616 390 495 132 359
 687  77 145 110 245 558 174 472 291 541 470 576 628 401 661 615 324 100
 633 301 330 285 676 432 603 316 508 360 608  39 265  98 701 465 547 548
 211 295 499 133 396 613 725 449 437 410 719 358 105 114 346 201 634 545
 230 385 332 659   7 208 215 484 496 593 408 192 561 579 721 418 373 522
 564 678 430 289  59 571 429 598 480  82 320 631 572  93 271  32 551 203
 262 393 337 467 361 650 675 166  86 537   5 217 214 347 451 331 240 403
  75 102 131 117  34 618  47  29 176 417 629 273 726   6 298 677 161 594
  81   8  41 157 377 442 477 550  96 590 483 679  62 473  58 369 700 462
 692 555 258 213 563 287 249 246 436 268 328 619 639 381 257 216 342  25
 177 122 682 696 104 180 463  40 220 578   2 652 506 689 574 350 405 172
 226  17 294 507 398 464 423 562 714 723 366 703 397 438 606  49 311   1
 164 202 485 642 212 419 450 729 169 302 503 529 115 510 655 351 378 178
 469 471 505 658 329 609 683 528 517 533 414 128 500 636 668 194 433 657
 259 519 375 307 509 279  52 635 656 247 248 647 565 232 617 244 421 293
 170 728 422 165 641 601 123 552 475 318 602 424 720 101 260 511 693 252
 121 530 267  20 711 411 188 233 340 156 139   9  95 540 718 304 549 669
 404  48 717  61 187 182 167 488 184  18 466 229 370 365  90 353 103 584
 569 150 704 269 708 138 426 303 148 319  57 300 699 251 577  64 588 592
 722 731  85 356 345 651 695 586 497 712 343 624 591 222 313 264 407 568
 413 107 305 352  88 532  13  21 554  79 644  97 193 382 275 239 605 349
 566 560 666 344  45 482 513 388 539 280 724 127 310  23 580 141 151 326
  27 314 386 557 638 336 516 697  16 322 284 637 323  19 376 596 515 493
 587 710 553 218 556 535 189 478 368 684 567 312 640 690 224  94 581 645
 521 147 604 399 727 219 688 409  53 325 454 402 281 435 126 124 702 290
 274 364 481 487 278 474  24 614 663  26 707 706  89 171 709  22 235 118
 575 196   0 476 348 489 362  51 383 162 372 261 665 363 599  42 389 391
 620 135 498 582 354 445 241 461 238 204 660 425 111 486 468 108 490 272
 648 443 444  83 589 195 412 686 559 143  63 140  99 191 653 400 621 524
  28 607 309 387 447 109 439 531 441 130 200 667 525 306 730  43 112  60
 520 234 585 308 705 155 186 185  78 379 116  14  65  30 149 190 120  38
 632 427 327 263 452 512 292 250 242 691 106  66 654  11 595 237 479 321
 181 646 622 527 197  91  76 526 455 664 253 713 173 228 434 502  35 367
 371 456  50  33 282 460 534 694 716 392 453 643 681 144 277 163 494  10
 355 428 420 335 492 514 179 154 583 236 296 283 649  92 338  15 299 206
 266 662 317 504 523  36 334 612 286 231 199 715  12  67 297  54 183 152
  37 136 270 518 221 374 168 119 339 538  68 431]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5928
INFO voc_eval.py: 171: [53 25  1 49  2 27 47 63 46 44  6 68 61 24 62 51 36  8 28 10 56 22 20 14
  3 31 26 29 64 57 50 60 59 30 12  7 37 19 23 32 21 58 42 13 15 38 39  4
 45 52 18 34 33 17 66  9 16 43 54  5 41 65 35  0 67 48 55 11 40]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2727
INFO voc_eval.py: 171: [ 61  62 137  20  91  52  60  92  68  31 105   3  99  87   2  66  28  23
  25  49  79  88  44  95  74 111  94  64  76 109  53  96 118  67 112  27
 134  83 139 108  11  97  38  65  89 124  42  18  10 129   0  24  55  81
  71   4 104   7  13  43  30  57 120  39  21 130  47  56  90  77 126  82
  50 135 140  54  70  46   1 107   9   6   8  12  75  33  93  58 125 117
 133 119  26  80  85  29  72  48  78  37  98  34  63 127  35 138 106  73
  22 113  40  51  36  59  69 114  32  41 122 101 121   5 100  86 116 103
 102  15  45 123 128  84  19  14 131 136 115  16  17 110 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3147
INFO voc_eval.py: 171: [ 94  11  54   8  25  42  81  18 120  65  45  27 123  66  62   6  26 133
 130  43 108  47  82 125 107  57  67  77   9  15  40  75  71 109  52  24
  22 134  29  10 115 103  72   7  17   3  30  23  98  46  88 126  87  34
   5 137  92  58  53  32  89  86  13 117  41 135  68  73  80 102  36  79
 124  39  51  55 105  38 113 114  90 100  49 138  69  20  70  83 128 106
  31  21 104  59  44   1 127 111  64  95 118   0  56  61  28   4  14  91
  16 131  35  12  78 140 122  99  84  97 136  33  19 112  63 121 129  37
 119 116  85   2 110  50 101 139  74  48  93  76 132  60  96]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3948
INFO voc_eval.py: 171: [110  69 164 175   1 244 128 157  18 220 212 133  12  98  59  25 240  33
 176 227  86 182 172 134  27  48  19 137 203 165   6 248  52  85 162 145
 158  95 255  79 184 166 194 167  36  61  73 252   0  26 246 210  35 213
 129  87 115 132 168 117 192  49  54 241 259  58  44  40 256 187 156 119
 230  34 186 228 236 181  24 159 143 232 120 180 189  90  74 106 204 193
  50 183 146  38  28  46 191  92  67 130 101 215 131  82 239 209 201 177
 118 136 185 135  60 257 238 144  15 147 196 253  68  77  17 224 171 242
  10 160 198   9 190 152 218  93 161 195 163  84 102 258 123  11  53  22
  96 231 149  45 126 217 221 108 153  32  16   8  56 216  62 254  94 105
 103 142  39   4  42  65  81 111  31 150 229  29 202 234 233 206 226  66
 173  41  23 107 237 154 179  76 235 219 114 211  43  47  57 104  70  91
 250  72 243  21  20 100 148  78 122 205 222  63   7 127 245  97 155 214
  88 260 121  37 251   2 124 139 170  83   5 151 200  13 208 169 112 116
  80  55  14   3 249 247 207  30  51 199 174  99 188 225  64 223 113 140
  75 125 197 109 178  71  89 141 138]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4591
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4104
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.347
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 134: 0.234
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.559
INFO cross_voc_dataset_evaluator.py: 134: 0.624
INFO cross_voc_dataset_evaluator.py: 134: 0.471
INFO cross_voc_dataset_evaluator.py: 134: 0.030
INFO cross_voc_dataset_evaluator.py: 134: 0.516
INFO cross_voc_dataset_evaluator.py: 134: 0.220
INFO cross_voc_dataset_evaluator.py: 134: 0.412
INFO cross_voc_dataset_evaluator.py: 134: 0.074
INFO cross_voc_dataset_evaluator.py: 134: 0.404
INFO cross_voc_dataset_evaluator.py: 134: 0.741
INFO cross_voc_dataset_evaluator.py: 134: 0.620
INFO cross_voc_dataset_evaluator.py: 134: 0.593
INFO cross_voc_dataset_evaluator.py: 134: 0.273
INFO cross_voc_dataset_evaluator.py: 134: 0.315
INFO cross_voc_dataset_evaluator.py: 134: 0.395
INFO cross_voc_dataset_evaluator.py: 134: 0.459
INFO cross_voc_dataset_evaluator.py: 135: 0.410
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.414s + 0.001s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.367s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.358s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.358s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.371s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.373s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.374s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.376s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.375s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.377s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.378s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.380s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.381s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.374s + 0.001s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.366s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.363s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.370s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.368s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.370s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.381s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.377s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.375s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.376s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.373s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.374s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.373s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.487s + 0.001s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.377s + 0.003s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.380s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.382s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.383s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.376s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.374s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.372s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.369s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.366s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.369s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.368s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.368s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.439s + 0.001s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.353s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.353s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.347s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.350s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.349s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.351s + 0.001s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.350s + 0.001s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.354s + 0.002s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.356s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.355s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.355s + 0.002s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.355s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.268s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [143 182  49   6  78 231  45  98  38  17 175  51  42 112 121 237 138  99
 140 148 120  67  66 243 246 252  37 145  92   2 111  41 161  40 105 254
 217  26  73 192 124 104  18 108  59  68 226 216  33  11 208 250 123 206
  87 257 128  69  39  80  46 180 240  23  95 190 245 195 191  30  55 139
  50 144 232  20 127  25  85 142 141 209 215  16 163 122  90 133 181 132
 194 134 156 210  43  81  60 158 109  76 242   5 229 179 169   7 251 249
 167  28  35 126 119 137 225 116 236 131 260 178  74 125 222 149 152 248
 204 146 233  88 160  64 207 259 106 244 114 218 165 130 223  12 219 172
  47 177  15 162   8 107 199  58  48  84 234  32  54  52 151  94  14 203
  36 227 157 100 154 198  44 166  19 230 189  21 202 235 117  34 113  13
 185 253  22  61 110  24 228 187 201 173  62 188 205 261 186 193  97   4
  86 102 150  96 197 171 101 136 153  91  77  70  65 247 147  29  93   3
 224  57  31 255 103 196 220 135 176 256 129 174 184   9  27  71  72 168
 155   0 115  75  79 238 212  56 258 170 183 164  53 211 241  83  63 118
 239 159  89 221 213  10 214  82   1 200]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3470
INFO voc_eval.py: 171: [ 46 156  16 100 132 182 191 180 103 190 193  91  14 115  11 143 137  73
 181 133  17 168 183 129  76 134  65 130 179 186 184 140  43  18 150  95
 124  50 120  69  98  10  24   5 200   8 172 154 167  75  23 112 195 118
  80 194 123 162 189  64  78  34  63 116 139  32  57  74 111 153 196   0
  70 105 136 101  26 147 163 203  62  22 121 175 170   3 119  19  72 151
 192 177 204  79  52 174  49 188  60 109  47 199  85  77 127 108  61 176
  39  42 146  13 144   6 166 117  15  99  83  96 202  25 152   2  84 148
  87  45 165 114 110 160 173 157 142  92 131 128  55 155 125  71 201  88
  97 164 149  27 135 145  48  12 187  93 104  36  40  58  89 138 198  37
 141  94 171 102  53 197 107  35  66  28  29  30 161 106  82 185  33 178
  51  41  38  21  54  67  81   1   9 159  44  90  68  59  31 126   4  86
 158 122  20  56 169 113   7]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5841
INFO voc_eval.py: 171: [ 92  44 112  99  70  72 110   2  71 210 227 228 212 172 229  39 134   1
 257  86 135 185  57  81  87 213 114 107 200 102  14 191 197 252 180  75
 243 249 111 250 152 159 231  76 219   7 265  20  60 232  50 205 188 103
 194 189 184 264 230   4 195 100  80 101  37 108 156 170 192  89 128 226
 126  12  96 222  62 214  48 245  27 202 233 220 217  16  23 239  61 146
  19 261 138 116 254 155  74 238 161 216 263 165  91 199  66 259 154 182
 157 255 119 224  59 105  85 247  15 145 177  55 235  63 158   5 260  33
 132 215 179 167 131  45 223  42  52 169  10 175  68 153  98 150 241 120
 139 160  67 151  93  94 234  43 117 104  31 123  41 173 240 236   6 203
 166 251 149 237 133  26 162 225 148 137  46 248  38  73 196 140 204  51
 253 206 186 127 130 176  65  90 190  30  78 242 171 136 211 244 207 266
 129  56  49  22 208  95  29 113 178 201 174 143  11 124   9 106  40  47
  18  54 198   0 218 187 183 142 168   8 262 115 141  36 109  53  83  64
 144  32  84  13 258  28  25  35 209  69 246  21 193 164 121 163 122 147
  58 221 118  97  88   3  79  17 181  77  82 125  24 256  34]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2341
INFO voc_eval.py: 171: [367 313 314  99   2 315   7  82  49 254  29 255 345 341  53 452 114 250
 360 140 253 446   9 395 256 110 197 191 248 316  84  81 136 115 451 124
  94 258  28 296 394 186 234 391  35 388 305 173  32  21 396 337 208 275
 368 473 428 335 444 287 164 239 196 414  25 100 262 503 405  60 359 259
 292 475 454  61 386 171 242 178 150  31  95 230 257  65 470 377 463 290
  27 384 302 435 402 476 309 319 211 421 389  44 481 105 462 378 261 500
 351  13 457 249  86 465 247 180 332  59 188   3 243 496 193 461 346 453
  39 214 424 123 310 484  69 411 295 336 190 251 216 422 137 167 158 474
 277 267 260 210 138 399 376  15 121  24 469 172  37 120 113 278 300  47
 271 142 274 233 487 229 387 130 321 490 281 456 174 235 184 284 153 252
 438 221 331 129 498 307 299 285 265 168  30 477  40 374  52  64 489 323
 268 379 308 354 103 149 112 408  70  54 228 370 223 269 238 220  91 263
 350 418  90 280 175 439 488 486 102   4 372  43 293 201 160  66 339 322
  16 207 406 122 502  78 471 409 273   6 219 365 413  74 482  17  51 398
 264  87 297 134 118 448 159 147 355 497 373 170 325 458 212 358 199  57
 311 443 340 382 104 304 326 363 227  75  85 205 432 397  89 419 447   1
 494 328 353 430  46 499 472  88 440 276 232 224 466  48 128 467 241 245
 492 222  76 442 431 163 231 425 204 236  50 333  23 480 404 464 141  10
 412 101 303 416 450 177 423 349 202 192 434 132 392 415 226 109 189 357
 185 181 317 135 334 182 347 312  42 366 400 401 298 162 491 343 282 111
 166 266  45  93 152 157 410  22 381 244  72  34 215 330 338 493  12 449
 119 203  26 342 369 306 459 403 225  92  63 146 291 144  36 179  55 125
 148 155 116 237  71 198 362  58 194 127   5 483 479 324 107  97 288 380
 383 133  56 329  18  11 344 154 143 455 468 320 131 420 356 117 126 371
 213 289 318  98 478 385  77 485 151  38 429 279 176 108 441 433 195  79
 240 206 217  96  83 200 183 246   0  19  20 417 407 437 445  67 364 139
 156 169  33 187 390 426 286 283 348 460 301 495 436 270  80  73 361 352
   8 209 272 393 106 327  62 145 375 427 501 294 165  14  68 218 161  41]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3381
INFO voc_eval.py: 171: [404 220 405 406 224 192   3 194 409 225 407 334 337 107 191 276 193 251
 508 509   5 182 317 333 393 296 318 195 123 181 253 470 332 416 331 413
 419 423 336 165 461 352 411 439  30 238 427 408 410 425 422 465 430 335
 267 412  76 428 436 483 118  75 263  31 232  17 329 281 173 148 493 340
 434  22 274 282  54 147 338 226 437  45  72 342 242 360 275 283 254 249
 417 491 189 371 356 199 138 288 492 237  44   8 244 134  79 319 234 168
 129 341 162   6   4  43 363 370 431  83 101 285 330 421 197 198 228 458
 359 187 367 166 420 399 358 154 170 368 119 510 351 174 378 152 302 136
 306 463 292 433 353 252 442 328 459 414 211  86 343  39 346  51 132 110
 452 203 339 321 497 475 212 443  26 235 248 447 227 362 262   7 190 213
 444  18 438 286 494 322 347  14 366 457 361  25  40 503 364 471  11 501
 149 377 176 245 229 175 124 185  16  84 297 498  35 396 196 301  37 156
 495 354 391 221 266 320  62  96 462  91 287 392  67 376 400 260 255  55
 200 261  56  23 349 115 108 344 258 250 300 160 290 257   0  57 449 387
 504 157  63 164 145 104 240  19 231 216 386 264  70 179 313  47  32 233
 381 158 303 309 289  65  99 236 481 299 272 325 380 239 183 269  90 453
 246 180 151 402 161   9 446 222 230 500 184 484  80 103 424 135 403  34
 401 345 278  68 143 476 373 388  60  13  69 418 372 415 241 120 126  88
 209  97 477 350 369 450 256 307 144  92  93 348 217 379  78 188 474 270
 172  21 310 464  49  77 127  33  59 218 178  12 294 327  20 139 128  73
 375  61  52 482 467 489 314 204 131 311 454  87 169 112 125 284 116 460
 205 247 114 479  53 210   2  29  27 308 265 259 466 137  15 141 219 397
 202 448 496 429 298 150 512 390 389 385  94 355 383  10 106  98 324 113
  48 279  46 140 440 163 271 455 280 268  38 105 277 293 208  64 432 382
 323 488 480 122 206 445 478 155 511 316 305  66 130 374 133  74 167  71
 502 499  36 326 315 171 146 142 223 487 473 100 472 486 159 121  81 506
  82  58  89 490 485 304 505 395 426  95 435 441 365  24 207 357 291 295
 469   1 102 177 111  28 398 153 394 312 507  42 214 384 186  41  85 456
 243  50 468 109 215 273 201 451 117]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5587
INFO voc_eval.py: 171: [ 25  12  40  38 113  64  17   2 106  45  51  70  11  80  88  67  29 111
  74  78  77 119  24  39  65  49 105  16   1  15  47  91  73  19  41  87
 112  86  62  22  82  60  76 109   6  44 101  96 110  99  66   0 100  37
   9  83  52  10  31  36 103  69  23  18  68 116  50  72  30  34 117  35
  93  53  81 115 114  61  27  97  71  20  54  89   8  85  84 102   5  42
  28  13 107  32  43  46  26  95  92  98   3  57 108 104  63   4  14  94
  79   7  90  55  33  56  21 118  48  58  59  75]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6239
INFO voc_eval.py: 171: [279 296 125 369  68 412 445 121 217   9 402  50  87 148 413 300  15 199
 249  22 324 183 202 392 201 414   2 390 406 378 323 367 428 219 319 169
 327 213 287 147  86 227 244 376 343 371 436 418 318  32 253 398 280 284
 112 254 198  83  77 170  76 242 194  58 457 222 261 221  33 301  45 181
 345 225  26 139 172  16 375 233 320 453 250 229 160 168 401 388 309 264
  35 273  52 365 234  56 190 329  47  90 151 205 185 173  61 391 109 304
 437 231 159  27 203 349 407 235 137  11 130 230 164 101 268  80 265 276
 321 171 197 346 310 278 220   1 245 456  72 417 107  40 236  23 339 182
 232 223 113  97 382 177 145 449 335 258 207 193 317 441 422 336 394 359
 166 144 377 379  24 204  55 285 420 103 409 405 362 447  31 174  17 270
 423 104 165 252 191 266 200 299 153  92 364 120 208 141 286 295 133 275
 129 196 446 162  36 372 444 450 142 386  48  21 411 167 238 348 397 224
  53 404  41 247 292 118  12 454  94 429 155 259 288  91 316  37 152 358
 293 303 128 330 206  85   8  25 370 338  89 150 440 114 431 361 226 189
 178  49  42 332 448 123 381  39 315  96 157 132 400 385  29 110 187   5
 314  84 163  70 263 396 102 356 192  75 434 176  46 340 383  63 138 135
 443  14  38 277 384 419 403 262  34  69 184  62 399 341   0 241   6 416
   7 243 212 408  13 424  88  44 149 271 326 387 289 257  54  20  65 195
 116 298  43  10  51 451 188 117 281 140 127 312 410 115 256  79 355 426
 119 389 351 350 452  73 216 433 134 306 255 442 154 291  93 180 380 246
 228  99 368 366 325 124   4 211 209 393  71  19 274 131 214 435 179  74
 158 427 111 105  59 126 458 272  82 360  18 283 308 239  28 106 438 146
 342 161 352 432 337 240 290 267 100 218 373  67 282 374 455 357 237 333
 122 311  64  66 297 136 294 354 210  98 425  60 430 313 353 302 143   3
 248 415 322 363  95 421 186 269 156 344 439  57 395 175 305 108 328 347
 331 260 215  30  81 334  78 307 251]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4733
INFO voc_eval.py: 171: [ 7  4 14 53 48 15 26  6 13  0 51 32 41 39 29 21 52 50 18 11 30  9 24 45
 46 31 54 10 49  1 33  3 35 19 34 40 38 20 47 25 28 22 43 17 12 36 16 23
  5 44 55 37  2  8 42 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0299
INFO voc_eval.py: 171: [1369 1371  140 ...  139 1320  768]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5159
INFO voc_eval.py: 171: [34  5  7 25 63 49 21 69 19 54 72 22 45 68 39 15 73 32 62 50 33  8 28 40
  6 67  9 75 60 29 53 58 36 23 61 56 14 48 71 57 17 12 18 31 16 20 11 38
 26 10 64 13 27 65 70 35 37 43 44 66 41 47  2 52 24 42 30 59 55  0 74  4
  3 51  1 46]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2197
INFO voc_eval.py: 171: [396 178  97 168 343 191 259 261 351 252 238  42  50 151 137 302 427 149
 354 376 148  17 221 425 171 274 398  18  62  82 419 271   1 117 340 370
 176 282 366 258  66 299  49 382 325 249 450 327 104 139 260 454   9 292
 183 154 417 328 100 294  51 281  61 197 116  14 457  39 339 105 371  24
  99  12 165 278 344 388  60 121 352  31 421 159 129  40 389 196 459  72
  80 407  55 431 391  22 273 246 161 385  44  16 293 248 377  86 300 107
 442  63 146 415 309 374 316 226 145 115  28 276 333 243 209  46 125 439
 320 291 357 195 143 449 134 103 123 119 153 136  35 402 215 286 138 147
 253   0 140 127 156 393 236 359 158 458  15 378  79 429 177 118 445 430
 412 383 229 375  13 233   6 364 438 217 304  84 361 268 381 416 310 114
  70  67 255 270 455 323 297 234 112 279  20 315 372   4 365 225 332 410
  74 266 250   5  58 200 295   3 405 211 367 331  75 175 222 413 446 314
 109 245  11   2  29 240 135  41 306 150 444 101 193 394  92 346 231 434
  73 247  21 166 298 206  71 198 319  52  69 192 272  37 205 122 285 368
 321 202 174 424 420 308 144 330 265 305 182 350 342 392 414 244 263 447
 160  32 406  91 185 307 334  33 120 426 242 313 448 190  27 409 326 216
 404 456 275 362  68 397 337  87 254 167  38 317 173 433 189 213 428 204
 418 131 111 345 384 262 369 452 132 128  59  57 186  36 401 296 239 232
 287 386 408 214 230 106 199 358 373 169 108  47  95 322  45  26 435 141
 422 142 227 289 395 155 188 349 283  25 157 220 133 180 251 181 235 335
  78 336 280  54 411 387 267 312  83 224   8 184 360  56 218 390 453  98
  77 290  64 187 400 170  43 441 355 379  88 223 432 210 269 324 164 126
 130 341 201 363 303  96 172 311  53  19 203 451 257  93 347 348 443  30
  76   7  85 318 437 241 212 228 353 152 288  48 110 124 179 162 399 219
 208 207 113 356  23 329  89 256 163  94 237 423 301 284 403  65  81  10
 102 436 440  34 194  90 264 277 380 338]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4117
INFO voc_eval.py: 171: [ 94  53  75  14  51  29   2  22  54  30  48  27 104   4  20 102  68  45
  19  47   0  81   1 103  61  44  28   9  41  34  99  24 107  35  18  78
 105  40  11 108  97  70   8 106   7  74  17  82  73  43  62  23  21 100
  67  89  65  98  52   6  60  39  56  87  95  77  38  66  91  86  79  33
  57  69  31  15  13  58  63  12  84  90  37  64  76  93  55 109  49  71
  32  42  80   3  59  88  10  96  36  50  85  26  46   5 110  72 101  16
  92  25  83]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0741
INFO voc_eval.py: 171: [69  9 83 61 12 59 44 85 87 26 49 58 10 57 73 48 63 70 11 34 77 79 39 53
  4 75 23 17 19 25 14 81 36 60  5 45 84 18 28 40 68  7 51  8 21 31 29 13
  1 47 52 62 67 35 33 54 82 71 80 64 20  2 78 22 27 86 41 74 15  0 38 72
 24 50  6 55 37 66 65 30  3 32 46 16 56 42 43 76]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.4026
INFO voc_eval.py: 171: [186  27  16 120  86 166 140 124 193  31   5  11  90 144   4 178 125 188
 135  79 138  73 176  47 137  53  82 130  96  43 157 161 148  75  63  13
  66 111 132 121  42 197  80 139 109  71 110  83 116 149 187 106 103  55
 147  85 126  94  98   3  87  59  57   1  60  24 165  97  62 155 164  72
 145 154  22 182 141  51  35 198  95 114  28 102   2  17 162  33  58  38
  99 177 150  12  40  92 192  30 191  69 156 107  91 115  78  23 134 136
   9 108  93 159  65 172 196 131 117 119 171  14  32 190  68 146 184  34
 104  76 123  56 201 133 194 142  52 189  77 169  18  15 175  39 163  88
 167  70   8 199 118 202  19 200 180   0  41 181   6  50  46 113 152  48
 105 179 158 173  44  67   7 195 170  74  61 128 129 185  49  37  45 101
  81  84  54 174  29 160 122 153  26 151 127 143 100 112  21  20  64 183
  89  25  10 168  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7412
INFO voc_eval.py: 171: [ 515 2682 2704 ...  408  403 2059]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6225
INFO voc_eval.py: 171: [115 139 416 200 668 148 625 227 127  70 144 569 671 290 211 629   4  72
 278  71   3 439  44  88 177 358  73 456 406 225 209 535 395 541 380  75
 245  74 596 669 696 229 212 415 599 626 572 490 162 256 500 155 161 342
 257 457 622 624 136 670 545 447 445  48 610  84 543 207 684 294 679 672
 160 131  56  81 542 609 317 258  57 384 335 394  31 458 615 390 494 134
 686 360  78 147 112 247 557 176 471 292 540 469 575 627 659 401 614 326
 100 632 303 332 287 675 431 602 318 507 361  39 607 267  98 699 464 546
 547 297 213 498 135 396 612 448 722 436 716 410 359 105 116 347 203 633
 232 544 385 334 657  91   7 210 483 217 495 592 408 194 560 578 718 418
 374 521  47 563 677 430 570  61 479 429 571 597 322 630  32 273 550 205
 264 339 393 466 362 648 673 168  87 536   5 219 348 216 333 450 242 403
  76 102  34 133 119 617  29 417 178 275 628 723   6  63 300 676 163 593
  41   8 159  82 377 476 441 549 482  96 589 678 472  59 370 461 698 554
 260 215 562 289 251 248 330 270 435 618 638 381  46 259 218  25 681 343
 179 124 694 104 182  40 222 462 577 650   2 688 505 405 351 573 174 296
 228  17 463 398 423 711 561 506 367 437 720 397 702  50 605 166   1 204
 313 641 419 484 214 449 726 171 528 117 304 502 352 509 378 180 653 468
 470 504 331 656 608 682 516 527 130 414 532 499 635 196 655 666 518 432
 261 309 281 375 508  53 654 249 250 634 645 421 295 616 246 564 234 172
 725 474 640 167 551 422 600 125 601 717 320 424 101 510 262 254 691 123
 529 269  20 708 411 190 235 341 158  95 539   9 141 404 715 306 548  49
 169 487  64 714 189 667 184  18 465 186 231 103 366  90 371 583 354 568
 706 703 271 152 140 150 305  58 302 426 321 576 697 728 719  85 253 587
 346 357 591 407 693 649 585 709 496 224 315 344 623 590 266 413 567 108
 307 353 531  21  13 195 382 277 497  97  80 553 559 604 241 345 350 643
  45 565 481 512 664 538 388 282  23 721 579 328 129 143 153 316 312  27
  37 515 338 695 556  16 386 325 286 324 514  19 637 492 636 586 376 595
 701 552 220 555 534 477 191 369 314 683 639 580 689 226 566 644  94 149
 603 399 402 520  54 724 453 283 126 221 687 409 128 434 480 365 276 291
 327 700 473  26 705 704  89 674 486 661 173 280 613  24 574   0 198 237
 349  22 120 475 488 355 707 363  52 373 383 364 164 663 598 389 263 444
  42 460 240 137 619 581 206 485 243 109 467 391 425 113 489 685 588 658
  83 274 646 197 442 443 558 400 651  99 145 412 193 142 620  65  28 311
 606 523 387 111  60 446 530 308 132 665 524 727 440 202 438 157  43  62
 584 236 519 310 114 118 192 188 187 106 151  14  30  86 379 631 451 511
  79 293 329 252 427 122 265  66 690 323  67  11 239 244 107 594 652 525
 183 526 478 621  92  77 454 230 662 372 455 501 199 433  35 175 710  51
  33 713 692 533 392 255 368 459 284 493  10 452 146 279 642 165 356 285
 298 337 582 513 420 428 491 340 156 238 680 181  93 647 299  15 301 319
 503 660 522  36 208 611 336 138  12 185  38 272 201 233  55  68 154 268
 712 121 223 110 537 517 288 170  69]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5914
INFO voc_eval.py: 171: [53 25  1 49  2 27 47 63 46 44  6 68 61 24 62 51 36  8 28 10 56 22 20 14
  3 31 26 29 64 57 50 60 59 30 12  7 37 19 23 32 21 58 42 13 15 38 39  4
 45 52 18 34 33 17 66  9 16 43 54  5 41 65 35  0 67 48 40 55 11]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2727
INFO voc_eval.py: 171: [ 61  62 137  20  91  52  60  92  68  31   3 105  99  87   2  66  28  23
  25  49  79  88  44  95  74 111  94  64  76 109  53  96 118  67 112  27
 134  83 139  11 108  97  38  65  89 124  18  42  10 129   0  24  55  81
   4 104  71   7  13  43  30  57 120  39  21 130  47  56  90  77 126  82
  50 135 140  70  46  54   1   9 107   6   8  12  93  75  33 125 117 133
 119  58  26  80  85  72  29  78  98  37  34  48  63 127 138  35 106  73
  22 113  36  40  51  69  59 114  32  41 122 101 121   5  45 103  86 100
 116  15 123 128  84 102  19  14 131 136 115  16  17 110 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3147
INFO voc_eval.py: 171: [ 94  11  54   8  25  42  81  18 120  65  45  27 123  66  62   6  26 133
 130  43 108  47  82 125 107  57  77  67   9  15  40  75  71 109  52  24
 134  22  29  10 115 103  72   7  17   3  30  23  46  98  88  87 126  34
   5 137  92  58  86  53 117  89  32  41 135  73  68  13  80 102  36  79
 124  39  51  55 105  38 113 114  90 100  49 138  69  20  70  83 128 106
  31  21 104  59  44   1 127 111  64  95 118   0  56 119  61  28   4  14
  16 131  12  91  35 140  78 122  99  84  97 136  33  63  19 112 129 121
  37 116  85   2 110  50 101 139  74  76  48  93 132  60  96]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3939
INFO voc_eval.py: 171: [107  69 160 171   1 240 125 153  18 216 208 130  12  96  59  25 236  36
 172 223  85 178 168 131  27  48  19 134 199 161   6 244  52  84 158 141
 154  93 251  78 180 162 190 163  35  61  72 248   0  26 242 206  34 209
 126  86 112 129 164 114 188  49  54 237 255  44  58  40 252 183 152 226
 224 182 116  33 232 139 177  24 155 228 176 117  88  73 185 104 189 200
  50 179 142  38  46  28 187  90  67 127  99 211 128  81 235 115 173 205
 197 133 181 140 253 132  60 234  15 192 143 249  68  17 167 220 156 238
 194  10   9 186 148 214  91 157 191 159  83 100 254 120  11  53  22  94
 145 227 213  45  75 217 123 106   8  16 149  32  62 250 212  56 101  92
 103  65 108   4  39 138  80  42  31 146  29 225 198 230 229 222 202  66
 150 169  23  41 105 233 207  47  43  76 111 175 215  57 231  70  89 102
  71  98 239  20  21 144 246 119  77 124 201   7 218 241  63 210  95 121
 118  87   2  37 256 151 247 136   5 166  82 147  13 196 109 165 113  79
 243   3  55 245  14  51 195 170  97 184  30 221 203  64 110 137 219 204
 122  74 135 193 174]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4597
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4105
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.347
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 134: 0.234
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.559
INFO cross_voc_dataset_evaluator.py: 134: 0.624
INFO cross_voc_dataset_evaluator.py: 134: 0.473
INFO cross_voc_dataset_evaluator.py: 134: 0.030
INFO cross_voc_dataset_evaluator.py: 134: 0.516
INFO cross_voc_dataset_evaluator.py: 134: 0.220
INFO cross_voc_dataset_evaluator.py: 134: 0.412
INFO cross_voc_dataset_evaluator.py: 134: 0.074
INFO cross_voc_dataset_evaluator.py: 134: 0.403
INFO cross_voc_dataset_evaluator.py: 134: 0.741
INFO cross_voc_dataset_evaluator.py: 134: 0.623
INFO cross_voc_dataset_evaluator.py: 134: 0.591
INFO cross_voc_dataset_evaluator.py: 134: 0.273
INFO cross_voc_dataset_evaluator.py: 134: 0.315
INFO cross_voc_dataset_evaluator.py: 134: 0.394
INFO cross_voc_dataset_evaluator.py: 134: 0.460
INFO cross_voc_dataset_evaluator.py: 135: 0.410
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.494s + 0.002s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.380s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.371s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.371s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.376s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.375s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.368s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.369s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.367s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.369s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.368s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.371s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.371s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.396s + 0.001s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.360s + 0.002s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.367s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.361s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.363s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.367s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.372s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.371s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.368s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.372s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.371s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.369s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.366s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.494s + 0.001s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.388s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.379s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.390s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.383s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.379s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.375s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.375s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.374s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.372s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.374s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.374s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.376s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.622s + 0.002s (eta: 0:01:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.387s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.380s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.377s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.378s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.375s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.377s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.375s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.377s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.377s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.373s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.374s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.372s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.525s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [143 182  48   5  78 231  44  98  37  16 175  50  41 112 121 237 138  99
 140 148 120  66 242  65 246 252  36 145  92   1 111  40 161  39 105 254
 217  25  73 192 124 104  17  58 108  67 226 216  10  32 208 250 123 206
  87 257 128  69  38  80  45 180  22 240  95 190 245 195 191  54  29 139
  49 144 232  70 127  19  24  85 142 209 141 215 163  15 122  90 133 181
 132 134 193 156 210  42  81  59 109 243  76 229 158   4 179 169   6 249
 251 167  27  34 126 119 225 116 137 236 131 178  74 260 125 222 149 248
 152 146 233 204  88  63 259 160 207 218 106 244 114 165 130 223  11 177
  46 219   7 172 162  47 107  57  14 199  53  51  84  31 151  94 234  35
 154 157  13 227 100 203 198  18  43 230  20 202 189 166 117 185  12 253
  33 235 113  21  60 110  23 187 173  61 188 201 228 205 261 186 194  97
  86 102 150   3 101 136 153  96  64 247 171  77 197  68 147  91  28  93
  30 255 196  56   2 135 224 176 220 129 174 256 103  26   8  71 184 168
 155  72 115   0 238 212  79  75  55 258  52 164 241 183 211 170 239 118
  62  83  89 221 159  82 214 213 200   9]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3470
INFO voc_eval.py: 171: [ 45 155  16  99 131 181 190 179 102 189 192  90  14 114  11 142 136  72
 180 132  17 167 182 128  75 133  64 129 178 185 183 139  42  18 149 123
  49 119  68  97  10  24   5 199   8 171 153 166  74  23 111 194 117  79
 193 122 161 188  63  77  34  62 115  92 138  32  56  73 152 110 195   0
  69 135 104 100 146 162  26 202  61  22 120 174   3 118 169  19  71 191
 150 203  78  51 176 173 187  48  59 108  46 198  84 126  76 107  41  60
  39 175 145  13 165 116 143   6  15  98  82 201  25  95   2 151  83 147
 164 113  44 109 159  86 172 156 130  91 141 127  54  70 124 200 163  96
  87 154 148 144  47  27 134  93 186  12 103  57  36  37  88 137 197 170
 101  94 140  52  65  28  29 106  35 196  30 160  81  33 105 184  50  53
  40  66  38  80   1  21 177  89 158  58  67   9  43  31  85 125  20 121
  55   4 168 112 157   7]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5841
INFO voc_eval.py: 171: [ 91  44 111  98  69  71 109   2  70 209 226 227 211 171 228  39 133   1
 256  85 134 184  56  80  86 212 113 106 101 199  14 190 196 251 179  74
 242 248 110 249 158 151 230  75 218   7  20 264  59 231  49 204 187 102
 193 188 183 263 229   4 194  99  79 100  37 107 155 169 191  88 127 225
 125  12  95  61 221  47 213 244  27 201 219 232 216  16  23 238  60 145
 260  19 137 115 253 153 237  73 215 160 262  65  90 198 164 258 152 181
 154 118 156 254 223  58 104 246 144  15  84 234 176 157  54   5  33  62
 214 131 259 178 166 130 222  45  42 174  51  10 168  67  97  92 149 119
 240 150 159 138 233  93  66  43 116 103 122  41 172  31 235 239   6 165
 202 250 236 148 132  26 161 224 247  38 147 136  72  46  50 139 203 195
 175 205 252 129 185 126  64 189  89 170  77  30 241 135  48 128  22 210
 206 243 265  55 173  94 177 207  29 112 200 142  11 105 197  40 186   9
  53 123  18   0 182 217 141 167 140   8 261 114 108  52  32  36  63  82
 143  83  13  25  35 257  28 163  68 245 208 146 192  21 117 120 121  81
  87 162  57  96 220 180  17  76  78   3  24 255 124  34]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2351
INFO voc_eval.py: 171: [367 313 314  99   0 315   7  82  49 254  29 255 345 341  53 452 114 250
 360 140 253 446   9 256 395 110 197 191 248 316  84  81 115 136 451 124
  94 258  28 296 394 186 234 391  35 388 305 173  32  21 396 337 208 275
 368 474 428 335 444 287 164 239 196 414  25 100 262 504 405  60 359 259
 292 476  61 454 386 171 242 178 150  31  95 257 230  65 471 377 463  27
 290 384 302 402 477 435 309 319 211 421 389  44 482 105 462 378 501 261
 351  13 457  86 249 465 247 180 332  59 188   3 243 497 193 461 346  39
 453 424 214 123 310  69 485 295 411 336 190 251 216 422 137 158 167 475
 267 277 260 210 399 138 376 121  15  24 470 172 120 278  37 113 300 271
  47 142 387 233 488 274 229 130 321 235 456 491 281 174 284 184 252 153
 438 129 331 221 168 499 285 265 299 307  30 478  40 374  64  52 489 268
 323 379 308 354 149 103 112 408 269  70 228  54 223 370 238 263 220 350
  91  90 418 439 280 490 175 487 102 372  43   4 201 160 293  16 339 322
  66  78 207 503 122 273 409 472   6 219 483 413 365  74  51  17 398  87
 264 147 134 297 498 355 448 159 118 170 373 458 325 406 212 358 199 311
 382  57 443 104 363 227 304 340 326  75 205 432  85 397   2 495  89 419
 447  46 328 353 430 440 473  88 500 276 466 232 224 493  48 128 241 245
 425 163 442 231 431 222  76  50 204 236  23 333 464 481 141 101 303 404
 416  10 412 177 450 349 423 202 192 434 415 226 392 109 132 185 317 189
 347 357 181  42 135 334 312 182 468 162 401 366 492 400 298 343 111 283
 266  93  45 166 152 157  72  22 410 494 338  34 381 330 449 342  26 467
  12 119 215 244 203 369 306  92 225 291 459 403 146  63 125 237 179 116
  55  36 155 144 148 198 484 324 194 362 288   5 480 127  71  97 380 383
 133 107  58  18 344  56 329 143 154 117 455 131 469 371 126  11 356  77
 320 289 318 385 213 486 151  98 420  38  79  83 479 108 429 195 279 441
 433 246 176  96  67 240 407  20 217 417 206 445   1 200 437 183  19 286
 426 390 139 364 156 169 187 282  33 436  73  80  62 496 460 272 270 301
   8 352 348 361 145 375 106 393 327 427 209 218  41 165 161 294  14 502
  68]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3385
INFO voc_eval.py: 171: [404 220 405 406 224 192   3 194 409 225 407 334 337 107 191 276 193 251
 510 511   5 182 317 333 393 296 318 195 123 181 253 471 332 416 331 413
 419 423 336 165 462 352 411 439  30 238 427 408 410 425 466 422 430 335
 267 412  76 428 436 484 118  75 263  31 232  17 329 281 173 148 494 340
 434  22 274 282  54 147 338 226 437  72  45 342 360 242 283 275 254 249
 417 492 370 189 356 199 138 288 493 237  44   8 244 134  79 319 234 168
 129 341 162   6   4  43 363 371 101 431  83 285 330 421 197 198 228 459
 359 187 166 367 420 399 358 154 170 368 119 512 351 378 174 152 136 302
 306 464 433 353 292 252 460 442 211 328 414  86 343  39  51 110 132 346
 453 203 339 321 498 476 212 443  26 235 248 447 227 362 262   7 190 213
 444  18 438 286 495 322 347 361 458 366  25  14  40 504 364  11 502 149
 377 472 176 245 229 175 124 185  16  84 297  35 499 396 196  37 156 301
 391 496 354 320 266 221  96 463  62 287  91 392 376  67 260 261  55 400
 255 200  56 349  23 344 115 108 250 258 300 160 290 257 450  57 387 505
   0  63 157 145 104 164 216 384  19 231 240 264  70 233 179  47 313 381
  32 158 303 289 309  65  99 236 482 325 299 272 380 454 239  90 246 183
 269 180 151 402   9 161 222 446 230 501 184 485 449  80 103 345 135 401
 424 403 477 143  34  68 278 388 374  60  69 418 415 372  13 120 241 126
  97  88 209 451 307 144 478 256 350 369  92  93 379 217 348  78 270 188
 475  21 172  49  77 465 310 178  33 127  12  61 218  59 294 467 139  73
 128 375  52 327  20 483 112  87 490 204 314 311 455 131 125 169 116 284
 461 480 247 114  53   2 210 205 265 259 308  29  27 468  15 137 219 202
 397 141 448 429 298 150 497  94 383 514 355 386 390 389  10 279 324  46
  48 113  98 106 456  38 140 105 268 271 440 280 163 323 445 481 293  64
 122 206 432 277 382 489 208 513 155 316 479  66 305 130 507 167  74 500
 133 373  36 315 503 326 142 171  71 146 100 488 508 474 473 223  89 121
 487  58  82  81 159  95 506 304 111 491 435 395 486 426  24 457  28 394
 365 102   1 470 357 441 295 177 153 398 291 385 207 214 186 312  85  41
  42 215 469  50 509 109 117 273 201 243 452]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5588
INFO voc_eval.py: 171: [ 25  12  40  38 114  64  17   2 107  45  51  71  11  81  89  68  29 112
  75  79  78 120  24  39  65  49 106  16   1  15  47  92  74  19  41  88
 113  87  22  62  83  60  77 110   6  44 102  97 100 111   0  66 101   9
  37  84  52  10  36  31 104  23  70  69 117  18  73  50 118  30  34  35
  53  82  94 116 115  61  27  98  54  72  90  20  85   8  86 103   5  42
  13  28 108  32  43  26  46  96   3  93  99 109  57  63   4 105  95  14
  80  67  55  91  33   7  48  56  58  21  59 119  76]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6239
INFO voc_eval.py: 171: [279 296 125 369  68 412 445 121 217   9 402  50  87 148 413 300  15 199
 249  22 324 183 202 392 201 414   2 390 406 378 323 367 428 219 319 169
 327 213 287 147  86 227 244 376 343 371 436 418 318  32 253 398 280 284
 112 254 198  83  77 170  76 242 194  58 457 222 261 221  33 301  45 345
 181 225  26 140 172  16 375 233 320 453 250 229 160 168 401 388 309 264
  35  52 273 365 234  56 190 329  47 151  90 185 205 173  62 391 304 109
 437 231  27 159 203 407 349 235  11 137 130 230 101 165 268  80 265 276
 171 321 197 310 346 220 278   1 245 456 107  72 417  23  40 236 182 339
 113 232 223 177 382  97 145 335 258 317 207 193 441 422 336 359 394 166
 144 377 379  24 204 285  55 420 103 409 405 362 448 174  31 270  17 423
 104 252 164 191 266  59 299 200 364 153  92 120 286 208 295 275 129 133
 447 196 162 372  36 444 139 142 450 386  48  21 411 238 397 167 348 224
  53  41 404 292 247 118 259 454  12 429  94 155 152  91  37 316 288 358
 293 370 330 303 206  85 128  25   8 338 150 440  89 114 431 361 226 178
 189 332  42  49 449 381 123  96  39 157 132 385 315 400 110  29 187 314
  84 163  70   5 192 102 396 263 356  75 383 176 434 135  46 340 443  14
 138  38 277 384 446 403 419 262  34 184  69  63 399 341   0 241   6   7
 243 212 416 271 408 424  13  88  44 149 326  10 289 257  54 387  20 116
 195 298  65  43  51 451 188 117 141 281 127 312 410 256  79 115 426 355
 351 119 389 350  73 216 433 134 306 255 442 452 154 291  93 180 380 246
 228  99 131 368 325 124   4 211 209 366 393  19  71 274 214 435 179  74
 158 427 111 105  60 126 458 272  82 360  18 283 308 239  28 106 438 146
 342 161 352 432 337 240 290 267 100 218 373  67 282 374 455 357 237 333
 122 311  64  66 297 136 294 210  98 425 430  61 302 415 313   3 354 353
 143 186 156 363 439 248  57 322 269 421  95 344 260 305 175  30 328 395
 108 331 215 347  81 334  78 307 251]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4733
INFO voc_eval.py: 171: [ 7  4 14 53 48 26 15  6 13  0 51 32 41 39 29 21 52 50 11 18 30  9 24 46
 45 31 54 10 49  1 33  3 35 19 34 40 38 20 28 25 47 22 43 17 12 36 16 23
  5 55 44 37  2 42  8 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0299
INFO voc_eval.py: 171: [1367 1369  138 ...  233  261  227]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5026
INFO voc_eval.py: 171: [34  5  7 25 63 49 21 19 68 54 72 22 45 69 39 15 73 32 50 62 33  8 28 40
  6 67  9 75 60 29 53 58 36 23 56 61 14 48 57 71 17 12 18 31 20 16 38 11
 26 65 10 13 27 70 35 37 64 43 44 41 47 66  2 52 24 42 30 59 55  0 74  4
  3 51  1 46]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2215
INFO voc_eval.py: 171: [395 178  97 168 342 191 259 261 350 252 238  42  50 151 137 302 426 149
 353 375 148  17 221 424 171 274 397  18  62  82 418 271   1 117 339 369
 176 282 365 258  66 299  49 381 325 249 450 327 104 139 260 453   9 292
 183 154 416 328 100 294  51 281  61 197  14 116 456  39 338 105  24 370
  99  12 165 458 343 279 387  60 351 121 159  31 420 129  40 388 196  72
  80 406  55 430 390  22 273 246 384  44  16 293 161 248 376  86 300 107
 441  63 146 414 316 309 373 145 226 115  28 276 333 243 209  46 125 438
 320 291 356 195 143 448 134 103 123 119  35 153 136 401 215 286 138 147
 253   0 127 140 156 392 236 358 158 457  15  79 428 377 118 177 411 374
 382 444 429 233  13 229 363   6 217 304 437  84 380 268 360 310 415 255
 114  70  67 270 454 323 297 234 315 112 278   4 371 225  20 364 332  74
   5 409 250 266 200   3 295  58 404 331 211  75 366 222 314 175 445 412
 245 109  29  11 240 135 150   2 443 101 306  41 193 433 393  92 345 231
  73 247  21 298 166 272 192  71 319 206 198  69  37  52 205 122 367 285
 349 202 419 423 144 321 174 308 330 305 391 265 182 244 341 413 263 446
 160 307  32 405  91 185  33 120 242 313 334 403 447 190 425  27 408 326
 317  68 216 455 361 275 173 254 213 167 337 189  38 432 396 204  87 417
 427 131 111 262 344 383 128  59 400 368  57 451 296 132 214 385 232  36
 407 287 239 199 186  45 230 372 106 169 357 108  47 322 141  95  26 434
  25 394 220 421 142 227 289 188 155 283 157 181 348 133 251 235 335 336
  78 280 180  54  56 386  77 452 312 224 267 184 359  83 218   8 399 410
 290  64 389 187  98 378 170  43 440 354 126 223 431  88 210 164 269 324
  96 311 303 130 172 362 340 201 257 346 449 228  19  93  53 203 241 152
   7 347 212 442  85 318  48 436  76  30 352 219 124 288 113 110 398 179
  94 301  89 402 256 355 422  81 284 207 163  23 237  65 329 162 264  34
 194 435 208 102  90 439  10 379 277]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4115
INFO voc_eval.py: 171: [ 95  53  76  14  51  29   2  22  54  30  48  27 105   4  20 103  69  45
  19  47   0  82   1 104  62  44  28   9  41  34 100 108  24  35  18  79
 106  40  11 109  98  71   8 107   7  75  17  83  74  43  63  23  21 101
  68  90  66  99  52   6  60  39  56  78  88  96  38  92  67  87  80  33
  57  70  15  31  58  13  64  12  91  85  77  65  37  94  55 110  49  42
  72  32  81   3  59  89  10  97  36  86  26  50  46   5 111  73 102  16
  25  93  61  84]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0741
INFO voc_eval.py: 171: [68 82  9 60 12 58 44 84 86 26 49 57 10 56 72 48 62 69 11 78 34 76 39 53
  4 74 23 17 19 14 25 80 36 59  5 45 83 18 28 40 67  7 51  8 21 31 29 13
 47  1 61 52 66 35 33 81 54 63 70 79 20  2 77 22 27 85 41 73 15  0 71 38
 50 24 37 55  6 65 64 30  3 32 16 46 42 43 75]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.4026
INFO voc_eval.py: 171: [185  27  16 119  85 165 139 123 192  31   5  11  89 143   4 177 124 187
 134  78 137  72 175  46 136  52  81  95  42 156 160 128 147  74  62  13
  65 110 131 120  41 196  79 138  70 108 109  82 115 148 186 102 105  54
 146  84 125  93  97  86   3  58  56   1 140  59  24 164 154  96  61 163
  71 144 153 181  22  50  35 197  94 113   2  28 101 161  17  33  57  37
  98 176 149  39  12  91 190 191  30  68 155 106  90 114  23  77 133 135
   9 103  92 107  64 158 171 195 130 118 116  32 170  14 189 145  67 183
  34 200  55 132 122  75 193 141  51  76 188 168  18 174  38  15 162  87
 166  69   8 117 198 201  19 199 179  40   0   6  49 180 112 151  45  47
 104 178 157 172  66  43   7 194 169  73 129  60  48 184  36 100  80  83
 127  44 173  53 159  29 121  26 150 126 152  99  21 111  63  20 142 182
  88  25  10 167]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7412
INFO voc_eval.py: 171: [2689  516 1670 ... 2301  804 2032]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6209
INFO voc_eval.py: 171: [115 138 416 199 667 147 624 226 126  70 143 568 670 289 210 628   4  72
 277  71   3 439  44 176  88 358  73 456 406 224 208 535 395 540 380  75
 244  74 595 668 695 228 211 415 598 625 571 161 490 255 500 154 160 341
 256 457 621 623 135 669 544 447 445  48 609 542  84 683 206 293 678 671
 159 130  56  81 541 608 316 257  57 384 334 394  31 458 614 390 494 133
 360 685 146  78 246 112 556 175 471 291 539 469 574 626 401 658 613 325
 100 631 333 302 286 674 431 601 317 507 361  39 606 266  98 698 464 545
 546 212 296 134 498 396 611 721 448 436 715 410 359 105 346 116 202 632
 543 231 385 332 656  91   7 483 209 495 216 590 408 193 559 577 418 717
 374 521  47 562 676 430 570  61 429 479 596 321 569 629 272  32 549 263
 204 338 466 393 647 362 672 167  87 536   5 218 215 450 347 331 241 403
  76 102  34 132 119 616 177 417  29 274 722 627   6 162 675  63 299 591
  82 377   8 158  41 441 476 548  96 482 588 677 472 370  59 697 461 553
 259 214 561 288 247 250 329 435 269 617 637  46 381 258 217 680  24 178
 342 122 693 104 181 221 462  40 576 649 687   2 572 351 405 505 173 295
 227  17 463 423 560 398 506 397 710 437  50 701 719 367 604   1 165 312
 203 484 419 213 640 725 449 117 170 303 352 509 528 468 652 179 502 378
 470 504 607 330 655 681 129 527 516 532 414 634 499 432 260 518 654 195
 665 375 308 280 508  53 249 633 653 248 615 644 563 421 245 294 171 233
 639 724 124 599 166 550 422 716 474 600 319 424 123 261 510 253 101 690
 267  20 529 707 340 189 234 411 157 140 538  95 404 714   9 547 487 305
 188 713 183 168  49 666  19  64 185 465 366 230 582 371 103 567  90 354
 151 702 705 139 270  58 149 426 304 301 575 320 696 252 592 585 357 718
 727 345 692  85 648 407 496 728 622 589 584 708 413 314 265 223 566 343
 109 306 353 531  13  97 552 276 497 382  21  80 642 558 603 564 240  45
 344 349 194 663 388 281 481 512 152 128 578 142 537 315  27  23 327 720
 515 386  16 311 555 337  37 636 514  18 323 285 694 594 492 324 586 635
 376 219 551 700 554 534 190 369 477 682 688 638 313 565 579 225 602 148
 686 643  94 520  54 282 453 409 220 723 399 125 127 402 434 275 365 480
 290 326 699 673 612 279 473  89 486 172 660  25 704 703 573   0 236  22
  26 120 197 348 475 706  52 163 355 363 383 488 364 662 373 389 239 136
 460  42 597 262 618 580 425 113 391 485 242 467 205 444 443 587 657 557
 645  83 196 684 108 489 442 273 350 650 192 400 412  99  65 619 144 141
  28 310 605 111 446  60 523 387 201 131 726 664 307 530 440  62 524 519
 235 156 309 438 583 114  43 191 186 379 150  30 187 106 118  14  79 511
 451  86 292 264 328 630  66 251 689 427 651 121 107 593  67 238 322 243
  11 478  93 620 526 525  77 182 501 229 661 455 198 174 709 433  35 372
 392 254 283 454  33 691 533  51 368 459 164  10 712 641 452 356 145 278
  92 297 284 493 679 420 581 237 155 180 646 491  36  15 300 298 428 522
 610 339 318 659 207 503 513 335 336  38 271  12 268 232 153  68 711 200
 184  55 137 517 169 287 110 222  69]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5916
INFO voc_eval.py: 171: [53 25 49  1  2 27 47 63 46 44  6 68 61 24 62 51 36  8 28 10 56 22 20 14
  3 31 26 29 64 57 50 60 59 30 12  7 37 19 23 32 21 58 42 13 15 38 39  4
 45 52 18 34 17 33 66  9 54 16 43  5 41 65  0 35 67 48 11 40 55]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2727
INFO voc_eval.py: 171: [ 61  62 136  20  91  52  60  92  68  31   3 105  99  87   2  66  28  23
  25  49  79  44  88  95  74 111  64  94  76 109  53  96 118  67 112  27
 133  83 138  11 108  97  65  89  38 124  18  42  10  24   0 129  55  81
 104   4   7  71  13  43  30  57 120  39  21  47 130  56 126  90  77  82
  50 139  70  46 134  54   1 107  12   9   6   8  75  93  33 117 125 132
 119  58  80  85  26  29  72  98  37  78  63  48  34 127 137  35  22 113
 106  73  51  40  36  59 114  69  32  41 122   5 101 121  45 100  15 116
 103 102 128  86  84 123 131  16  19  14 115 135  17 110]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3147
INFO voc_eval.py: 171: [ 94  11  54   8  25  42  78  18 120  65  45  27 123  66  62   6  26 133
 130  43 108  47  82 125 107  57  77  67   9  15  40  75  71 109  52  24
 134  22  29  10 115 103  72   7  17   3  30  23  46  98  88  87 126   5
  34 137  92  58  53  86 117  89  32  41 135  68  81  73  13  36 102  80
 124  39  51 113  38 105  90 114  55 100 138  49  69  20  70  83 128 106
  31 104  21  59  44   1 127 111  64  95 118   0 119  28  56  61   4  14
  16 131  12  91  35 140  79 122  99  84  97  63 136  19 112  33 129 121
  37 116  85   2 110  50 101 139  74  76  48  93 132  60  96]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3939
INFO voc_eval.py: 171: [107  69 159 170   1 237 125 152  18 213 205 130  12  96  59  25 233  36
 171 220  85 176 167 131  27  48  19 134 196 160   6 241  52  84 157 140
 153  92 248  78 178 161 188 162  35 239  61  72   0 245  26 203  34 206
 126  86 112 129 163 114 186  49  54 234 252  44  58  40 249 181 151 223
 116 180  33 221 229 175 138  24 154 225 117 174  88 183  73  93 104  50
 197 187 177 141  38  28  46 185  90  67 127  99 208 128 232  81 172 202
 115 194 133 179 139 132 231 250  60  15 190 246 142  68  17 166 217 191
  10 155 235   8 184 147 211 156 189 158 100  83 251 120  11  53  22  94
 224 144 214  75 210 123  45  16   9 106 148  32  62  56 209 247 103  91
  39 101 137   4  80  31 108  65 145  42 195 222  29 227 226 219 199 168
  66  23  41  43 204 111 105  47 149 230 212 173  76  57 228  89 102  70
 236  98 243 143  21  71  20   7 215  77 124 198 238  63   2 253  95 207
 150 121  87 244 118 135  37  82 146   5  13 165 193 164  79 119 113  14
   3 242 109 240  55 192  97  51 200  30 182 169 218 136  64 122 216 110
 201  74]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4593
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4099
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.347
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 134: 0.235
INFO cross_voc_dataset_evaluator.py: 134: 0.339
INFO cross_voc_dataset_evaluator.py: 134: 0.559
INFO cross_voc_dataset_evaluator.py: 134: 0.624
INFO cross_voc_dataset_evaluator.py: 134: 0.473
INFO cross_voc_dataset_evaluator.py: 134: 0.030
INFO cross_voc_dataset_evaluator.py: 134: 0.503
INFO cross_voc_dataset_evaluator.py: 134: 0.221
INFO cross_voc_dataset_evaluator.py: 134: 0.412
INFO cross_voc_dataset_evaluator.py: 134: 0.074
INFO cross_voc_dataset_evaluator.py: 134: 0.403
INFO cross_voc_dataset_evaluator.py: 134: 0.741
INFO cross_voc_dataset_evaluator.py: 134: 0.621
INFO cross_voc_dataset_evaluator.py: 134: 0.592
INFO cross_voc_dataset_evaluator.py: 134: 0.273
INFO cross_voc_dataset_evaluator.py: 134: 0.315
INFO cross_voc_dataset_evaluator.py: 134: 0.394
INFO cross_voc_dataset_evaluator.py: 134: 0.459
INFO cross_voc_dataset_evaluator.py: 135: 0.410
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.608s + 0.001s (eta: 0:01:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.385s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.376s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.363s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.366s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.366s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.366s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.364s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.364s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.366s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.368s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.371s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.372s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.426s + 0.001s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.365s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.377s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.370s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.372s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.376s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.386s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.386s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.382s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.385s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.385s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.387s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.385s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.530s + 0.001s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.417s + 0.003s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.389s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.391s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.389s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.382s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.382s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.383s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.381s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.378s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.377s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.377s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.377s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.485s + 0.002s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.392s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.379s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.378s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.372s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.374s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.374s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.371s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.372s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.371s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.369s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.369s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.370s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 56.028s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [143 182  48   5  78 231  44  98  37  16 175  50  41 112 121 237 138  99
 140 148 120  66 242  65 246 252  36 145  92   1 111  40 161  39 105 254
 217  25  73 192 124 104  17  58 108  67 226 216  10  32 208 250 123 206
  87 257 128  69  38  80  45 180  22 240  95 190 245 195 191  54  29 139
  49 144 232  70 127  19  24  85 142 209 141 215 163  15 122  90 133 181
 132 134 193 156 210  42  81  59 109 243  76 229 158   4 179   6 169 249
 251 167  27  34 126 119 225 116 137 236 131 178  74 260 125 222 149 248
 152 146 233 204  88 259 160  63 207 218 106 244 114 165 130 223  11 177
  46 219   7 172 162  47 107  57  14 199  53  51  84  31 151  94 234  35
 154 157  13 227 100 203 198  18  43 230  20 202 189 166 117 185  12 253
  33 235 113  21  60 110  23 187 173  61 201 228 205 194 261 186 188  97
  86 102 150   3 101 136 153  96  68  64 171  91  77 147 197  93 247  28
  30 255 196  56   2 135 176 224 220 129 256 174 103  71  26   8 155  72
 168 184 115   0  79  75 212  55  52 258 164 170 238 211 183 241  62 118
  83 239  89 159 221 214 213  82 200   9]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3470
INFO voc_eval.py: 171: [ 46 156  16 100 132 182 191 180 103 190 193  91  14 115  11 143 137  73
 181 133  17 168 183 129  76 134  65 130 179 186 184 140  43  18 150 124
  50 120  69  98  10  24   5 200   8 172 154 167  75  23 112 195 118  80
 194 123 162 189  64  78  34  63 116  93 139  57  32  74 111 153 196   0
 105 136  70 101 203 147  26 163  62  22 121 175 119   3 170  19  72 192
 151 204  79  52 177 174 188  49 109  60  47 199  85 127 108  77  42  39
  61  13 146 176 166 117 144   6  15  99 202  25  83  96  84 152   2 148
 165  45 114 160 110  87 173 157 142  92 131 128  55 125  71 201 164 155
  88  97 149  27 135 145  48  12  94 187  58  36 104 138  37  89 198 141
  40  95 171 102  53  35 197  66  28  29 107 161  30 106 185  33  82  41
  54  51  67  38  21 178  81   1  59  90   9  44 159  68  31   4  86 122
  56  20 126   7 169 113 158]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5841
INFO voc_eval.py: 171: [ 91  44 111  98  69  71 109   2  70 209 226 227 211 171 228  39 133   1
 256  85 134 184  56  86 212 113 106  79 199 101  14 190 196 251 179  74
 242 248 110 249 158 151 230  75 218   7  20 264 231  59  49 204 187 102
 193 188 183 263 229   4 194  99  80 100  37 107 155 169 191  88 127 225
 125  12  95  61 221  47 213 244  27 201 216 219 232  16  23 238  60 145
 260  19 137 115 253 153 237  73 215 160 262  65  90 198 164 258 152 181
 154 118 156 223 254  58 104  84 246 144  15 234 176 157  54   5  33  62
 214 131 259 178 166 130 222  45  42 174  51  10 168  67  97 119  92 149
 240 150 159 138 233  93  66  43 116 103 122 239  41 172  31 235   6 165
 202 250 236 148 132  26 161 224 247  38 147 136  72  46  50 139 203 195
 175 205 252 129 185 126  64 189  89 170  77  30 241 135  48 128  22 210
 206 243 265  55 173  11 177 207  29 112  94 200   9 142 105 197  40 186
  53 123  18   0 182 217 141 167 140   8 261 114 108  52  32  36  63  82
 143  83  13  25  35 257  28 163  68 245 208 146 192  21 117 120 121  81
  87 162  57  96 220 180  17  76  78   3  24 255 124  34]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2351
INFO voc_eval.py: 171: [366 312 313  98   0 314   7  81  48 253  28 254 344 340  52 451 113 249
 359 139 252 445   9 255 394 109 196 190 247 315  83  80 114 135 123 450
  93 257  27 295 393 185 233 390  34 387 304 172  31  20 395 336 207 274
 367 473 427 334 443 286 163 238 195 413  24  99 261 503 404  59 358 258
 291 475  60 453 385 170 241 177 149  30  94 256 229  64 470 376 462  26
 289 383 301 401 476 434 318 308 210 420 388  43 481 104 461 377 500 260
  12 350 456  85 248 464 246 179 331  58 187 242   3 496 192 460 345 452
  38 423 213 122 309 484  68 294 410 335 250 189 215 421 136 166 157 276
 266 474 259 209 375 137 398 120  14  23 469 112 171  36 277 119 299  46
 270 141 487 273 386 232 228 129 320 455 173 490 280 234 283 183 251 152
 437 128 330 220 306 167 298 284 264 498  29 477  39 373  51  63 488 378
 322 267 307 102 353 148 111 407  53 227 268  69 222 237 369 262  90 349
 219  89 417 279 438 174 489 486  42 371 101   4 159 292 200 321  77 338
  15  65 272 206 121 502 408 471 218   6  73 364 412 482 397  16  50 263
  86 146 296 133 158 447 497 117 354 372 169 457 324 405 357 198 211 381
 442 310  56 103 339 362 303 325 226  74 431 396 204  84 418 446 494   2
  88 327  45 439 429 352 499 275  87 472 231 223 465 492 240  47 127 244
  75 424 430 221 441 230 162 203  49 235 480  22 463 140 332 415 302 100
 411 403 348 176 201 422 449 433 191 414 108 391 225 131 188 356 316 180
 346 184 467 134 181 333 311  41 491 365 161 110 400 297 342 399 282  92
 165 265  44 156  71 151  21 409 448  33 493 380 337 329  11 466 341  25
 243 118 214 202 305 368 458 224 402  62 290  91 124 145 287 178  35 154
 236  54 143 115 147  57 126 379  96 193   5 382 323 479 197  70 132 361
 483 106 142 153  55  17 328 343 370 125 130 468 454 355 116  10 419 384
 288 212  37 150 317  97 319  76 485  82 478 194  78 107 440 278 428 432
 444 416 245   1  66 406  19 216  18 436 205 199 175 182 239  95 168 389
 363 186 285 425 138 155 281  32 271 269 495 360 435  79 351  72   8  61
 300 459 347 426 326 105 144 208 392 374  13  40 217  67 293 160 501 164]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3385
INFO voc_eval.py: 171: [404 220 405 406 224 192   3 194 409 225 407 334 337 107 191 276 193 251
 510 511   5 182 317 333 393 296 318 195 123 181 253 471 332 416 331 413
 419 423 336 165 462 352 411 439  30 238 427 408 410 425 466 422 430 335
 267 412  76 428 436 484 118  75 263  31 232  17 329 281 173 148 494 340
 434  22 274  54 282 147 338 226 437  72  45 342 242 360 283 275 254 249
 417 492 370 189 356 199 138 288 493 237  44   8 244 134  79 319 234 168
 129 341 162   6   4  43 363 371 101 431  83 285 330 421 197 198 228 459
 359 187 166 367 420 399 358 154 170 368 119 512 351 378 174 152 136 302
 306 464 433 353 292 252 460 442 211 328 414  86 343  39  51 110 132 346
 453 203 339 321 498 476 212 443  26 235 248 447 227 362 262   7 190 213
 444  18 438 286 495 322 347 366 458 361  14  25  40 504 364  11 149 472
 502 377 176 245 175 229 124 185  16 297  84  35 499 396 196  37 156 301
 496 354 391 221 320 266 463  96  62 287  91 392 376  67 255 260 261  55
 400 200  56 349  23 344 115 108 250 258 300 160 290 257 450  57 387 505
   0  63 157 145 104 164 216 384  19 231 240 264  70 233 179  47 313 381
  32 158 303 289 309  65  99 236 482 325 299 272 380 454 239  90 246 183
 269 180 151 402   9 161 222 446 230 501 184 424 449  80 103 485 345 135
 401 403 477 143  34  68 278 388 374  60  69 418 415 372  13 120 241 126
  97  88 209 451 307 144 478 256 350 369  92  93 379 217 348  78 270 188
 475  21 172  49 465 310  77 178  33 127  12  61 218  59 294 467 139  73
 128 375  52 327  20 483 112  87 490 204 314 311 455 131 125 169 116 284
 461 480 247 114  53   2 210 205 265 259 308  29  27 468  15 137 219 202
 397 141 448 429 298 150 497  94 383 514 355 386 390 389  10 279 324  46
  48 113  98 106 456  38 140 105 268 271 440 280 163 323 445 481 293  64
 122 206 432 277 382 489 208 513 155  74 479  66 305 316 130 507 167 500
 133 373  36 315 503 326 142 171  71 146 100 488 508 474 473 223  89 121
 487  58  82  81 159  95 506 304 111 491 435 395 486 426  24 457  28 394
 365 102   1 470 357 441 295 177 153 398 291 385 207 214 186 312  85  41
  42 215 469  50 509 109 117 273 201 243 452]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5589
INFO voc_eval.py: 171: [ 25  12  40  38 114  64  17   2 107  45  51  71  11  81  89  68  29 112
  75  79  78 120  24  39  65  49 106  16   1  15  47  92  74  19  41  88
 113  87  22  62  83  60  77 110   6  44 102  97 100 111   0  66 101   9
  37  84  52  10  36  31 104  23  70  69 117  18  73  50 118  30  34  35
  53  82  94 116 115  61  27  98  54  72  90  20  85   8  86 103   5  42
  13  28 108  32  43  26  46  96   3  93  99 109  57  63   4 105  95  14
  80  67  55  91  33   7  48  56  58  21  59 119  76]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6239
INFO voc_eval.py: 171: [279 296 125 369  68 412 445 121 217   9 402  50  87 148 413 300  15 199
 249  22 324 183 202 392 201 414   2 390 406 378 323 367 428 219 319 169
 327 213 287 147  86 227 244 376 343 371 436 418 318  32 253 398 280 284
 112 254 198  83  77 170  76 242 194  58 457 222 261 221  33 301  45 345
 181 225  26 140 172  16 375 233 320 453 250 229 160 168 401 388 309 264
  35  52 273 365 234  56 190 329  47 151  90 185 205 173  62 391 304 109
 437 231  27 159 203 407 349 235  11 137 130 230 101 165 268  80 265 276
 171 321 197 310 346 220 278   1 245 456 107  72 417  23  40 236 182 339
 113 232 223 177 382  97 145 335 258 317 207 193 441 422 336 359 394 166
 144 377 379  24 204 285  55 420 103 409 405 362 448 174  31 270  17 423
 104 252 164 191 266  59 299 200 364 153  92 120 286 208 295 275 129 133
 447 196 162 372  36 444 139 142 450 386  48  21 411 238 397 167 348 224
  53  41 404 292 247 118 259 454  12 429  94 155 152  91  37 316 288 358
 293 370 330 303 206  85 128  25   8 338 150 440  89 114 431 361 226 178
 189 332  42  49 449 381 123  96  39 157 132 385 315 400 110  29 187 314
  84 163  70   5 192 102 396 263 356  75 383 176 434 135  46 340 443  14
 138 277  38 384 446 403 419 262  34 184  69  63 399 341   0 241   6   7
 243 212 416 271 408 424  13  88  44 149 326  10 289 257  54 387  20 116
 195 298  43  65  51 451 188 281 117 141 312 127 410  79 115 256 355 351
 350 426 389 119 306 134 452 216 442 255 433  73 291  93 154 180 368 380
 228 246 325 211   4 131 209  99 124 214  71 179 435 393 274 366  19  60
 427 158  74 105 111 360 272 283  18  82 126 458  28 308 239 161 352 432
 342 438 337 106 146 218 290 267 240 100 374 282  67 373 455 333 122 237
 357  64  66 311 297 136 294  98 425 430 210  61 415 354   3 353 313 302
 143 186 363 439 344 322 269 421  95 156 248  57 395 347 108 305 331 328
  30 260 215 175 251  78 307  81 334]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4735
INFO voc_eval.py: 171: [ 7  4 14 53 48 26 15  6 13  0 51 32 41 39 29 21 52 50 11 18 30  9 24 46
 45 31 54 10 49  1 33  3 35 19 34 40 38 20 28 25 47 22 43 17 12 36 16 23
  5 55 44 37  2 42  8 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0299
INFO voc_eval.py: 171: [1368 1370  138 ...  227  944 1254]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5026
INFO voc_eval.py: 171: [34  5  7 25 63 49 21 19 68 54 72 22 45 69 39 15 73 32 50 62 33  8 28 40
  6 67  9 75 60 29 53 58 36 23 56 61 14 48 57 71 17 12 18 31 20 16 38 11
 26 65 10 13 27 70 35 37 64 43 44 41 47 66  2 52 24 42 30 59 55  0 74  4
  3 51  1 46]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2215
INFO voc_eval.py: 171: [395 178  97 168 342 191 259 261 350 252 238  42  50 151 137 302 426 149
 353 375 148 222  17 424 171 274 397  18  62  82 418 271   1 117 339 369
 176 365 282 258  66 299  49 381 325 249 450 327 104 139 260 453   9 292
 183 154 416 328 100 294  51 281  61 197 116  14 456  39 338 105  24 370
  99  12 165 458 343 279 387  60 351 121  31 159 420 129  40 388 196  72
  80 406  55 430 390  22 273 384 246  44 161 293  16 248 376  86 300 107
 441 146 414  63 309 316 373 145 226 333 115  28 276 243 209  46 125 320
 438 356 291 143 195 448 123 134 119 103 136  35 295 153 401 215 286 253
 138 147   0 127 140 156 392 236 358 221  15 158 457  79 428 377 118 177
 411 374 382 444 429 229  13 233 363 217   6 304 437  84 268 380 360 310
 415 255 114  70  67 270 454 323 297 234 315 112 278 225   4  20 371 364
 332   5  74 409 250 266  58 200   3 404 211 331 366  75 412 445 175 314
 245 109   2 240  29  11 150 135 443 306 193 101  41  92 345 433 231 393
  73  21 247 166 298 272  71 192 198 319 206 205  52  37  69 122 285 367
 321 419 308 174 423 349 202 144 182 391 330 265 305 244 413 341 263  32
 307 160 446 185 405  91  33 120 242 313 447 334  27 190 403 425 361 455
 326 317  68 275 408 216  38 213 254 337 173 167 396 189 432 204 131 417
 427  87 344 262 111 383  57 400  59 132 368 451 128 296 199 407 239 186
 287 214  36 232 385 372 169  45 357 230 106 322 108 141  47  95 421  26
 434 394  25 142 227 220 289 348 157 188 181 155 283 251 133 280 180 335
 336 235  54  78  83 224 386   8 399 218 267 184 410 312 359 452  77  56
  64 389 187 290  98 378 170 354  43 440  88 269 210 223 164 126 431 324
 172 130 362 311 340 303  96 201 228 203  19 449 346 257  93  53  85  76
 442 152 212  30 318 436   7 347  48 241 124 219 179 110 288 352 398 113
  94 355 301 207  65 329 402 256 162 284  81 163 422  89 237  23 439  90
 264 194 102 277  10 435  34 208 379]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4117
INFO voc_eval.py: 171: [ 95  53  76  14  51  29   2  22  54  30  48  27 105   4  20 103  69  45
  19  47   0  82   1 104  62  44  28   9  41  34 100 108  24  35  18  79
 106  40  11 109  98  71   8 107   7  75  17  83  74  43  63  23  21 101
  68  90  66  99  52   6  60  39  56  78  88  96  38  92  67  87  80  33
  57  70  15  31  58  13  64  12  91  85  77  65  37  94  55 110  49  42
  72  32  81   3  59  89  10  36  97  86  26  50  46   5 111  73 102  16
  25  93  61  84]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0741
INFO voc_eval.py: 171: [68 82  9 60 12 58 44 84 86 26 49 57 10 56 72 48 62 69 11 78 34 76 39 53
  4 74 23 17 19 14 25 80 36 59  5 45 83 18 28 40 67  7 51  8 21 31 29 13
 47  1 61 52 66 35 33 81 54 63 70 79 20  2 77 22 27 85 41 73 15  0 71 38
 50 24 37 55  6  3 64 65 30 32 16 46 42 43 75]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.4026
INFO voc_eval.py: 171: [184  26  15 118  84 164 138 122 191  30   5  10  88 142   4 176 123 186
 133  77 136  71 174  45 135  51  80  94  41 155 159 127 146  73  61  12
  64 109 130 119  40 195  78 137 107  69 108  81 114 185 147 104 101  53
 145  83 124  92  96   3  85  57  55   1 139  58  23 153  95 163  60  70
 162 143 152 180  21  49  34 196  93   2 112 100  27 160  16  32  56  36
  97 175 148  38  11  90  29 190 189  67 154 105  89 113  76  22 132 134
 102   9  91 106 170  63 194 157 129 117 169 115  31 188  13 144  66  33
 182  54 121 131 199  74 140  50 192  75 187 167  17  37  14 173 161  86
  68 165   8 116 197 200  18 198  39 178   0   6 179 111  48  44 177 150
 156 103  46 171   7  65  42 193 168  72 128  59  47 183  43  99  35  79
  82 126  28  52 158 172  25 120 125 151 149  98  20 110  19  62 141 181
  87  24 166]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7412
INFO voc_eval.py: 171: [ 517 2693 2716 ...  291 1731 2375]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6233
INFO voc_eval.py: 171: [115 138 416 199 667 147 624 226 126  70 143 568 670 289 210 628   4  72
 277  71   3 439  44  88 176 358  73 456 406 224 208 535 395 540 380  75
 244  74 595 668 695 228 211 415 598 625 571 490 161 255 500 154 160 341
 256 457 621 623 135 669 544 447 445  48 609 542  84 206 683 293 678 671
 159 130  56  81 541 608 316 257  57 334 384 394  31 458 614 390 494 146
 133 685 360  78 112 246 556 175 471 291 539 469 574 626 658 401 613 325
 100 631 333 286 302 674 431 601 317 507 361  39 606 266  98 698 464 545
 546 212 296 498 134 396 611 721 448 436 410 715 359 346 105 116 202 632
 543 231 385 332 656  91   7 495 483 209 216 590 408 193 559 577 418 717
 374 521  47 562 676 430  61 570 429 479 321 596 569 629 272  32 549 204
 263 466 338 393 647 362 672 167  87 536   5 218 215 450 347 331 241 403
 102  76  34 132 119 616 177 417  29 722 274 627  63 675   6 162 299 591
  82  41   8 158 377 476 441 548  96 588 677 482 472 370  59 461 697 553
 259 214 561 288 250 247 329 435 617 269 637  46 258 381 217  24 680 342
 178 693 122 181 104 221  40 462 576 649   2 687 505 572 405 351 173 295
 227 423  17 463 398 560 506 397 710 367  50 701 437 719   1 604 165 312
 203 213 419 484 640 725 117 170 449 303 528 352 509 652 179 468 502 378
 470 504 330 607 655 681 527 129 516 532 414 634 499 654 665 518 195 260
 432 375 280 508 308  53 633 249 248 653 644 563 421 294 245 615 233 171
 724 639 550 422 124 166 599 716 600 474 319 424 510 123 253 101 261 690
 529 267  20 707 411 234 189 340 538 404 140  95 157   9 714 547 188 168
 305 666 487 183 713  49  64  19 185 465 582 371 366 230 567  90 103 354
 151 705 702 270 139 426 301  58 149 304 696 320 575 718 252 357 592 727
 345 585  85 407 692 648 584 728 622 708 589 496 343 109 314 566 223 413
 265 531 353 306 497 276  21 382 552  97  13  80 194 344  45 240 558 603
 564 349 642 663 281 481 512 388 327 537 128  23 315 152 578 720  27 142
 337 515 386  16 555 311  37 694 285 636 514 323  18 594 492 586 635 376
 324 700 551 219 190 534 554 477 682 369 688 579 638 225 313 565 148  94
 643 686 602 409 127 125 399 282 402  54 434 723 220 453 520 365 699 275
 480 326 290 660 172 612 673 279 704  89 486  25 573 703   0 348  22  26
 236 197 120 473 475 364 355 363 383 488 163  52 706  42 597 373 239 136
 662 262 618 389 460 391 113 467 242 580 444 485 205 425 108 587 196 489
 645 443 557  83 684 273 657 442 350 619 650 400 412  99  65 192 310  28
 144 141 605 523 387 446  60 111 726 664 201 131 307 530 440 519 583 524
  43 156 309 438 114  62 235  14 379 150 106 186 187 191  30 118 451 511
  86  79  66 251 264 292 328 630 593 651 243 427 322 121  11 107 689 238
  67 478 620  93  77 526 182 525 174 709 455 661 229  35 433 198 501 372
 283 459 254 454 691 368 533  51 392  33 145 278 452  10 164 712 641 356
 297 493  92 180 284 679 646 155 581 420 491 237  36 513 522  15 503 300
 298 610 428 335 659 336 339 318 207 153 271 268 232 200  38 711 184 137
  55  68  12 287  69 517 110 169 222]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5913
INFO voc_eval.py: 171: [53 25 49  1  2 27 47 63 46 44  6 68 61 24 62 51 36  8 28 10 56 22 20 14
  3 31 26 29 64 57 50 60 59 30 12  7 37 19 23 32 21 58 42 13 15 38 39 45
  4 52 18 34 17 33 66  9 54 16 43  5 41 65  0 35 67 48 11 40 55]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2727
INFO voc_eval.py: 171: [ 61  62 136  20  91  52  60  92  68  31   3 105  99  87   2  66  28  23
  25  79  49  44  88  95  74 111  64  94  76 109  53  96 118  67 112  27
 133  83 138  11 108  97  65  89  38 124  18  42  10  24   0 129  55  81
 104   4   7  71  13  43  30  57 120  39  21  47 130  56 126  90  77  82
  50 139  70  46 134  54   1 107  12   9   6   8  75  93  33 117 125 132
 119  58  80  85  26  29  72  98  37  78  63  48  34 127 137  35  22 113
 106  73  51  40  36  59 114  69  32  41 122   5 101 121  45 100  15 116
 103 102 128  86  84 123 131  16  19  14 115 135  17 110]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3147
INFO voc_eval.py: 171: [ 94  11  54   8  25  42  78  18 120  65  45  27 123  66  62   6  26 133
 130  43 108  47  82 125 107  57  77  67   9  15  40  75  71 109  52  24
 134  22  29  10 115 103  72   7  17   3  30  23  46  98  88  87 126   5
  34 137  92  58  53  86 117  89  32  41 135  68  81  73  13  36 102  80
 124  39  51 113  38 105  90 114  55 100 138  49  69  20  70  83 128 106
  31 104  21  59  44   1 127 111  64  95 118   0 119  28  56  61   4  14
  16 131  12  91  35 140  79 122  99  84  97 136  19 112  33  63 129 121
  37 116  85   2 110  50 101 139  74  76  48  93 132  60  96]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3939
INFO voc_eval.py: 171: [107  69 159 170   1 237 125 152  18 213 205 130  12  96  59  25 233  36
 171 220  85 176 167 131  27  48  19 134 196 160   6 241  52  84 157 140
 153  92 248  78 178 161 188 162  35 239  61  72   0 245  26 203  34 206
 126  86 112 129 163 114 186  49  54 252 234  44  58  40 249 181 151 223
 116 180  33 221 229 175 138  24 154 225 117 174  88 183  73  93 104  50
 197 187 177 141  38  28  46 185  90  67 127  99 208 128 232  81 172 202
 115 194 133 179 139 132 231 250  60  15 190 142 246  68  17 166 217 191
  10 155 235   8 184 147 211 156 189 158 100  83 251 120  11  53  22  94
 224 144 214  75 210 123  45  16   9 106 148  32  62  56 209 247 103  91
 101 137   4  80  39  65 108 145  42  31 195 222  29 227 226 219 199 168
  66  23  41  43 204 111 105  47 149 230 212 173  76  57 228  89 102  70
 236  98 243 143  21  71  20   7 215  77 124 198 238  63   2 253  95 207
 150 121  87 244 118 135  37  82 146   5  13 165 193 164  79 119 113  14
   3 242 109 240  55 192  97  51 200  30 182 169 218 136  64 122 216 110
 201  74]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4593
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4100
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.347
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 134: 0.235
INFO cross_voc_dataset_evaluator.py: 134: 0.339
INFO cross_voc_dataset_evaluator.py: 134: 0.559
INFO cross_voc_dataset_evaluator.py: 134: 0.624
INFO cross_voc_dataset_evaluator.py: 134: 0.474
INFO cross_voc_dataset_evaluator.py: 134: 0.030
INFO cross_voc_dataset_evaluator.py: 134: 0.503
INFO cross_voc_dataset_evaluator.py: 134: 0.221
INFO cross_voc_dataset_evaluator.py: 134: 0.412
INFO cross_voc_dataset_evaluator.py: 134: 0.074
INFO cross_voc_dataset_evaluator.py: 134: 0.403
INFO cross_voc_dataset_evaluator.py: 134: 0.741
INFO cross_voc_dataset_evaluator.py: 134: 0.623
INFO cross_voc_dataset_evaluator.py: 134: 0.591
INFO cross_voc_dataset_evaluator.py: 134: 0.273
INFO cross_voc_dataset_evaluator.py: 134: 0.315
INFO cross_voc_dataset_evaluator.py: 134: 0.394
INFO cross_voc_dataset_evaluator.py: 134: 0.459
INFO cross_voc_dataset_evaluator.py: 135: 0.410
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.522s + 0.001s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.379s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.377s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.376s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.379s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.377s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.372s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.373s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.377s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.375s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.372s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.375s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.373s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.600s + 0.002s (eta: 0:01:14)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.397s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.386s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.381s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.381s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.378s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.391s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.387s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.387s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.384s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.384s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.383s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.378s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.406s + 0.001s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.380s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.369s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.371s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.367s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.364s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.364s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.360s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.358s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.359s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.361s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.360s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.361s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 8,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 2e-05,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': True,
           'FREEZE_CONV_BODY': True,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.454s + 0.002s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.363s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.365s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.367s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.360s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.361s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.360s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.363s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.362s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.361s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.358s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.357s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.355s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_1stream_noreg_freeze-boxhead-conv/Nov13-18-57-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 55.545s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [143 182  48   5  78 231  44  98  37  16 175  50  41 112 121 237 138  99
 140 148 120  66 242  65 246 252  36 145  92   1 111  40 161  39 105 254
 217  25 192  73 124 104  17 108  58  67 226 216  10  32 208 250 123 206
  87 257 128  69  38  80  45 180  22 240  95 190 245 195 191  29  54 139
  49 144 232  70 127  19  24  85 142 215 209 141  15 163 122  90 133 181
 134 193 132 156 210  42  81 109  59 158  76 243 229   4 179   6 169 251
 249  27 167  34 126 119 225 116 137 236 131 178 260  74 125 149 222 248
 152 146 204 233  88 259 160  63 207 114 106 218 244 165 130 223  11 177
 219  46   7 172 162  14  47 199  57 107 151  51  53  31  84  94 234 203
 154 227  35 100  13 157 198  18  43 230 202 189  20 166 117  33 185 235
 253 113  21  12  60 110  23 187 173  61 201 228 205 261  97 186 188  86
 102 150 194  96 101 136   3 153 147 197  77  91  68 171  64  28 247  93
 255  56  30   2 196 224 103 176 135 256 174 220 129  26   8  71  72 184
 155 168   0 115  75 212  79  55 170 258 164 183 211  52 241 238  62  83
 239 118 221 159  89 214 213  82 200   9]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3470
INFO voc_eval.py: 171: [ 46 156  16 100 132 182 191 180 103 190 193  91  14 115  11 143 137  73
 181 133  17 168 183 129  76 134  65 130 179 186 184 140  43  18 150 124
  50 120  69  98  10  24   5 200   8 172 154 167  75  23 112 195 118  80
 194 123 162 189  64  78  34  63 116  93 139  57  32  74 111 153 196   0
 105 136  70 101 203 147  26 163  62  22 121 175 119   3 170  19  72 192
 151 204  79  52 177 174 188  49 109  60  47 199  85 127 108  77  39  61
  42  13 146 176 166 117 144   6  15  99 202  25  83  96  84 152   2 148
 165  45 114 160 110  87 173 157 142  92 131 128  55 125  71 201 164 155
  88  97 149  27 135 145  48  12  94 187  58  36 104 138  37  89 198 141
  40  95 171 102  53  35 197  66  28  29 107 161  30 106 185  33  82  41
  54  51  67  38  21 178  81   1  59  90   9  44 159  68  31   4  86 122
  56  20 126   7 169 113 158]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5841
INFO voc_eval.py: 171: [ 91  44 111  98  69  71 109   2  70 209 226 227 211 171 228  39 133   1
 256  85 134 184  56  86 212 113 106  79 199 101  14 190 196 251 179  74
 242 248 110 249 158 151 230  75 217   7 264  20 231  59  49 204 187 102
 193 188 183 263 229   4 194  99  80 100  37 107 155 169 191  88 127 225
 125  12  95  61 221  47 213 244  27 232 201 216 219  16  23 238  60 145
 260  19 137 115 253 153 237  73 215 160 262  65  90 198 164 258 152 181
 154 118 156 223 254  58 104  84 246 144  15 234 176 157  54   5  33  62
 214 131 259 178 166 130 222  45  42  51 168 174  10  97  67 119  92 240
 159 150 138 149  43 233  93  66 116 103 122 239  41 172  31 235   6 165
 202 250 236 148 132  26 161 224 247  38 147 136  72  46  50 139 203 195
 175 205 252 129 185 126  64 189  89 170  77  30 241 243 135  48 128  22
 210 206 265  55 173  11 177 207  29 112  94 200   9 142 105 197  40 186
  53 123  18   0 218 182 141 167   8 261 114 140 108  36  63  52  32  82
 143  83  13  28 257  35  25 163 245  68 208 192 117 146  21 120 121  81
  87 162  57  96 220 180  17  76  78   3  24 255 124  34]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2341
INFO voc_eval.py: 171: [366 312 313  98   0 314   7  81  48 253  28 254 344 340  52 451 113 249
 359 139 252 445   9 255 394 109 196 190 247 315  83  80 114 135 123 450
  93 257  27 295 185 393 233 390  34 387 304 172  31  20 395 336 207 274
 367 473 427 334 443 286 163 238 195 413  24  99 261 503 404  59 358 258
 291 475  60 453 385 170 241 177 149  30  94 256 229  64 470 462 376  26
 289 383 301 401 476 434 420 318 308 210 388  43 481 104 461 377 500 260
  12 350 456  85 248 464 246 179 331  58 187 242   3 496 192 460 345 423
 452  38 213 122 309 484  68 294 410 335 250 189 215 421 136 166 157 276
 266 474 259 209 375 137 398 120  14  23 469 112 171  36 277 119 299  46
 270 141 487 273 386 232 228 129 320 455 173 490 280 234 283 183 251 152
 437 128 330 220 306 167 298 284 264 498  29 477  39 373  51  63 488 378
 322 267 307 102 353 148 111 407  53 227 268  69 222 237 369 262  90 349
 219  89 417 279 438 174 489 486  42 371 101   4 159 292 200 321  77 338
 121  15  65 272 206 502 408 471 218   6  73 364 412 482 397  16  50 263
  86 146 296 133 158 447 497 117 372 169 354 457 324 405 357 198 211 381
 442 310  56 103 339 362 303 325 226  74 431 396 204  84 418 446 494   2
  88 327  45 439 429 352 499 275  87 472 231 223 465 492 240  47 127 244
  75 424 430 221 441 230 162 203  49 235 480  22 463 140 332 415 302 100
 411 403 348 176 201 422 449 433 191 414 108 391 225 131 188 356 316 180
 346 184 467 134 181 333 311  41 491 365 161 110 400 297 342 399 282  92
 165 265  44 156  71 151  21 409 448  33 493 380 337 329  11 466 341  25
 243 118 214 202 305 368 458 224 402 290  91 124 145  35 154  62  54 236
 178 115 143 287 147 361   5  57 126 379  96 193 106 483 479 197 132  70
 382 323 153 142  55  17 328 370 125 130 468 454 355 343 116  10 419 384
 288 212  37 150 317  97 319  76 485  82 478 194  78 107 440 278 428 432
 444 416 245   1  66 406  19 216  18 436 205 199 175 182 239  95 168 389
 363 186 285 425 138 155 281  32 271 269 495 360 435  79 351  72   8  61
 300 459 347 426 326 105 144 208 392 374  13  40 217  67 293 160 501 164]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.3397
INFO voc_eval.py: 171: [404 220 405 406 224 192   3 194 409 225 407 334 337 107 191 276 193 251
 510 511   5 182 317 333 393 296 318 195 123 181 253 471 332 416 331 413
 419 423 336 165 462 352 411 439  30 238 427 408 410 425 466 422 430 335
 267 412  76 428 484 436 118  75 263  31 232  17 329 281 173 148 494 340
 434  22 274  54 282 147 338 226 437  72  45 342 242 360 283 275 254 249
 417 492 370 189 356 199 138 288 493 237  44   8 244 134  79 319 234 168
 129 341 162   6   4  43 363 371 101 431  83 285 330 421 197 198 228 459
 359 187 166 367 420 399 358 154 170 368 119 512 351 378 174 152 136 302
 306 464 433 353 292 252 460 442 211 328 414  86 343  39  51 110 132 346
 453 203 339 321 498 476 212 443  26 235 248 227 447 362 262   7 190 213
 444  18 438 286 495 322 366 347 458 361  14  25  40 504 364  11 149 472
 502 377 176 245 175 229 124 185  16 297  84  35 499 396 196  37 156 301
 496 354 391 221 320 266 463  96  62 287  91 392 376  67 255 260 261  55
 400 200  56 349  23 344 115 108 250 258 300 160 290 257 450  57 387 505
   0  63 157 145 104 164 216 384  19 231 240 264  70 233 179  47 313 381
  32 158 303 289 309  65  99 236 482 325 299 272 380 454 239  90 246 183
 269 180 151 402   9 161 222 446 230 501 184 424 449  80 103 485 345 135
 401 403 477 143  34  68 278 388 374  60  69 418 415 372  13 120 241 126
  97  88 209 451 307 144 478 256 350 369  92  93 379 217 348  78 270 188
 475  21 172  49 465 310  77 178  33 127  12  61 218  59 294 467 139  73
 128 375  52 327  20 483 112  87 490 204 314 311 455 131 125 461 169 116
 284 480 247 114  53   2 210 205 265 259 308  29  27 468  15 137 219 202
 397 141 448 429 298 150 497  94 383 514 355 386 390 389  10 279 324  46
  48 113  98 106 456 163  38 140 105 268 271 440 280 445 481 293 206 122
 323 382 432 277  64 208 489  66 513 316  74 479 155 305 507 130 500 133
 373 167 142 315 503  36 326 146 171  71 473 100 508 488 223 474  89  82
 121  81 487  58 159 491 435 304  95 111 506 395 486  24 426 102 365   1
  28 457 357 441 295 394 398 153 470 177 291  42  41  85 385 207 312 214
 186 117 215 509 109 469  50 273 452 243 201]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.5589
INFO voc_eval.py: 171: [ 25  12  40  38 114  64  17   2 107  45  51  71  11  81  89  68  29 112
  75  79  78 120  24  39  65  49 106  16   1  15  47  92  74  41  19  88
 113  87  22  62  83  77  60 110   6  44 102  97 100 111   0  66 101   9
  37  84  52  10  36  31 104  23  70  69 117  18  73  50  30  34  35 118
  53  82  94 116 115  61  27  98  54  72  90  20  85   8  86 103   5  42
  13  28 108  32  43  46  96  26   3  93  99 109  57  63   4 105  14  95
  80  67  91  33   7  55  58  59  48  56  21 119  76]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.6239
INFO voc_eval.py: 171: [279 296 125 369  68 412 445 121 217   9 402  50  87 148 413 300  15 199
 249  22 324 183 202 392 201 414   2 390 406 378 323 367 428 219 319 169
 327 213 287 147  86 227 244 376 343 371 436 418 318  32 253 398 280 284
 112 254 198  83  77 170  76 242 194  58 457 222 261 221  33 301  45 345
 181 225  26 140 172  16 375 233 320 453 250 229 160 168 401 388 309 264
  35  52 273 365 234  56 190 329  47 151  90 185 205 173  62 391 304 109
 437 231  27 159 203 407 349 235  11 137 130 230 101 165 268  80 265 276
 171 321 197 310 346 220 278   1 245 456 107  72 417  23  40 236 182 339
 113 232 223 177 382  97 145 335 258 317 207 193 441 422 336 359 394 166
 144 377 379  24 204 285  55 420 103 409 405 362 448 174  31 270  17 423
 104 252 164 191 266  59 299 200 364 153  92 120 286 295 208 275 129 133
 447 196 162 372  36 444 139 142 450 386  48  21 411 238 397 167 348 224
  53  41 404 292 118 247 259 454  12 429  94 155 152  91  37 316 288 358
 293 370 330 303 206  85 128  25   8 338 150 440  89 431 114 361 226 178
 189 332  42  49 449 381 123  96  39 157 132 385 315 400 110  29 187 314
  84 163  70   5 192 102 396 263 356  75 383 176 434 135  46 340 443  14
 138 277  38 384 446 403 419 262  34 184  69  63 399 341   0 241   6   7
 243 212 416 271 408 424  13  88  44 149 326  10 289 257  54 387  20 116
 195 298  43  65  51 451 188 281 117 141 312 127 410  79 115 256 355 351
 350 426 389 119 306 134 452 216 442 255 433  73 291  93 154 180 368 380
 228 246 211   4 131 209  99 124 325 214  71 179 435 393 274 366  19  60
 427 158  74 105 111 360 272 283  18  82 126 458  28 239 308 438 432 342
 161 337 352 106 146 290 218 267 100 240 374 282 373  67 237 122 357 333
 455  64  66 311 297 430  98 294 136 425  61 210 313 354 353 302 415   3
 143 156 269 248 322  57 186 421  95 439 363 305 344 215 175 260 328 331
 108 347 395  30  78 307  81 251 334]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.4735
INFO voc_eval.py: 171: [ 7  4 14 53 48 26 15  6 13  0 51 32 41 39 29 21 50 11 18 30  9 24 46 52
 45 31 54 10 49  1 33  3 35 19 34 40 20 38 28 25 47 22 43 17 12 36 16 23
  5 55 44 37  2 42  8 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0301
INFO voc_eval.py: 171: [1369 1371  138 ...  227 1139  137]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5026
INFO voc_eval.py: 171: [34  5  7 25 63 49 21 19 68 54 72 22 45 69 39 15 73 32 50 62 33  8 28 40
  6 67  9 75 60 29 53 58 36 23 56 61 14 48 57 71 17 12 18 31 20 16 38 11
 26 65 10 13 27 70 35 37 64 43 44 41 47 66  2 52 24 42 30 59 55  0 74  4
  3 51  1 46]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2215
INFO voc_eval.py: 171: [395 178  97 168 342 191 259 261 350 252 238  42  50 151 137 302 426 149
 353 375 148 222  17 424 171 274 397  18  62  82 418 271   1 117 339 369
 176 365 282 258  66 299  49 381 325 249 450 327 104 139 260 453   9 292
 183 154 416 328 100 294  51 281  61 197 116  14 456  39 338 105  24 370
  99  12 165 458 343 279 387  60 351 121  31 159 420 129  40 388 196  72
  80 406  55 430 390  22 273 384 246  44 161 293  16 248 376  86 300 107
 441 146 414  63 309 316 373 145 226 333 115  28 276 243 209  46 125 320
 438 356 291 143 195 134 119 123 103 448 136  35 153 295 401 215 286 253
 138 147   0 127 140 156 392 236 358 221  15 158 457  79 428 377 118 177
 411 374 382 444 429 229  13 233 363 217   6 304 437  84 268 380 360 310
 415 255 114  70  67 270 454 323 297 234 315 112 278 225   4  20 371 364
 332   5  74 409 250 266  58 200   3 404 211 331 366  75 412 445 175 314
 245 109   2 240  29  11 150 135 443 345 306 193 101  41  92 433  21 231
 393  73 247 166 298 272  71 192 198 319 206 205  52  37  69 122 285 367
 321 419 308 174 423 349 202 144 182 391 330 265 305 244 413 341 263  32
 307 160 446 185 405  91 120 242 313 447  33 334  27 190 403 425 361 455
 326 317  68 275 408 216  38 213 254 337 173 167 396 189 432 204 131 417
 427  87 344 262 111 383  57 400  59 132 368 385 451 128 296 287 407 239
 186 199 214 232  36 357 106 230 372  45 169 108 322 141  47  95 434 394
  26 421  25 142 227 220 289 348 157 188 181 155 283  78 251 133 280 180
 335 336 235  54 224 386   8 399 218 267  77 184 410  56 312 452 359  64
  98 290 187  83 389 354 440 170 378  43 324 164 269 431 126  88 210 223
 201 130 311 340 362  96 172 303 228 203 449  19 346 257  93  53 318  76
 442 152  30 212  85   7 436 241  48 347 124 398 113 110 352 288 179 219
  94 329 207 301  65 402 422 256 162 284  81 102 163  89 237  23  90 264
 194 355 435 208  10  34 439 277 379]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4117
INFO voc_eval.py: 171: [ 95  53  76  14  51  29   2  22  54  30  48  27 105   4  20 103  69  45
  19  47   0  82   1 104  62  44  28   9  41  34 100 108  24  35  18  79
 106  40  11 109  98  71   8 107   7  75  17  83  74  43  63  23  21 101
  68  90  66  99  52   6  60  39  56  78  88  96  38  92  67  87  80  33
  57  70  15  31  58  13  64  12  91  85  77  65  37  94  55 110  49  42
  72  32  81   3  59  89  10  36  97  86  26  50  46   5 111  73 102  16
  25  93  61  84]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0741
INFO voc_eval.py: 171: [68 82  9 60 12 58 44 84 86 26 49 57 10 56 72 48 62 69 11 78 34 76 39 53
  4 74 23 17 19 14 25 80 36 59  5 45 83 18 28 40 67  7 51  8 21 31 29 13
 47  1 61 52 66 35 33 81 54 63 70 79 20  2 77 22 27 85 41 73 15  0 71 38
 50 24 37 55  6  3 64 65 30 32 16 46 42 43 75]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.4026
INFO voc_eval.py: 171: [183  26  15 118  84 164 138 122 190  30   5  10  88 142   4 175 123 185
 133  77 136  71 173  45 135  51  80  94  41 155 159 127 146  73  61  12
  64 109 130 119  40 194  78 137 107  69 108  81 114 184 147 101 104  53
 145 124  83  92  96   3  85  57  55   1 139  23  58 163  95 153  60  70
 162 143 152 179  21  49  34 195  93 100   2 112  27 160  16  32  56  36
  97 174 148  38  11  90 189  29 188  67 154 105  89 113  76  22 132 134
 102   9  91 106 169  63 157 193 129 117 115 168  31 187  13  66 144 181
  33  74 198 121  54 131 140 191  50  75 186 166  17  14  37 172 161  86
 165  68   8 116  18 196 199 197  39 177   0 178  48   6 111 176  44 150
 156 103  46 170  42   7  65 192 167  72 128  59  43  47 182  35  99  79
  82 126  28 171  52 158 120  25 149 125 151  98 110  20  19 180 141  62
  87  24]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.7412
INFO voc_eval.py: 171: [2693  517 1673 ... 2034 2049  386]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.6211
INFO voc_eval.py: 171: [115 138 415 199 665 147 622 225 126  70 143 566 668 288 210 626   4  72
 276  71   3 438  44 176  88 357  73 455 405 223 533 208 394 538 379  75
 243  74 593 666 693 227 212 414 596 623 569 161 489 254 499 154 160 340
 255 456 619 621 135 667 542 446 444  48 607  84 540 681 206 292 676 669
 159 130  56  81 539 606 315 256  57 333 383 393  31 457 612 389 493 683
 146 359 133  78 112 245 554 175 470 290 537 468 572 624 400 656 611 324
 100 629 332 285 301 672 430 599 316 506 360 604  39 265  98 696 463 543
 544 295 134 497 395 609 213 719 447 435 409 713 358 345 105 116 202 630
 541 230 384 331 654  91   7 209 482 494 217 588 407 193 557 575 417 715
 373 520  47 560 674 429 568  61 428 594 478 320 567 627  32 271 547 204
 262 465 337 392 645 361 670 167  87 211 534   5 218 346 449 216 330 240
 402  76 102  34 132 119 614 416 177 720  29 273 625  63 673   6 162 298
 589  82 158   8  41 376 475 440 546  96 586 481 675 471 369  59 460 695
 551 258 215 559 249 287 246 328 434 268 615 635  46 257 380  24 678 341
 178 691 122 181 104 220 461  40 574 647   2 685 570 504 404 350 173 226
 294  17 462 422 505 397 558 396 708 717 366 699 436  50 602   1 165 311
 203 214 418 483 638 723 170 448 117 351 508 526 302 467 179 650 501 377
 469 503 653 329 605 679 525 129 413 515 530 632 498 663 195 431 259 652
 517 507  53 374 307 279 651 247 248 631 642 420 613 293 561 244 171 722
 637 232 548 421 124 597 166 714 473 598 423 318 123 260 101 252 509 688
  20 266 527 189 410 339 233 705  95 536 157 403 140 712   9 545 168 711
 183 304 664  49 486 188  19  64 185 464 365 229 580 370 565 353 103  90
 151 700 703 269 139 425 149 300 303  58 319 573 694 344 716 356 583 725
 590 251 406  85 646 690 495 726 620 706 582 587 109 412 222 264 313 564
 342 352 305 529 275 550  97  80  13  21 496 239 556  45 348 640 194 601
 381 562 343 387 661 511 280 480 128 576 535  23 718 152 314  27 142 326
 553 385  16 336  37 310 514 322 284 692  18 513 634 633 592 584 375 323
 491 549 698 190 532 552 476 680 368 686 636 224 312 563 577 148 641  94
 684  54 721 600 452 398 401 519 433 281 408 127 125 219 325 289 274 697
 479 364 658 172 485 671 278  25 610  89 702 571 701   0 235  22  26 120
 197 347 472 474 704  52 163 354 362 382 363 487 616 261 136 238 595 459
  42 372 660 388 484 578 205 241 424 113 390 466 443 682 272 441 488 555
 643 196 655 108 585  83 617 349 648  65 399  99 442 411 144 309 192 141
  28 111 386 522  60 603 445 131 724 662 306 201 528 439 234  43 156 308
 523 581 518 114 437  62 150 118 186 191 106  30 187  14 378  79 450 628
  86 327 263  66 250 291 510 321 426  11 107 687 121 649 242  67 237 591
 477  93 182 524  77 618  35 228 371 174 454 659 500 198 432 707 391 282
  33  51 253 458 689 367 531 453  10 451 355 277 145 164 639 710 677 490
 180  92 236 492 296 644 419 283 155 579 521  15 299 297 427 608 317 657
 512 334 502 335 207 338  38  12 270 267 231 153  36  68 709 200 184  55
 137 169 516 286 110 221  69]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5915
INFO voc_eval.py: 171: [53 25 49  1  2 27 47 63 46 44  6 68 61 24 62 51 36  8 28 10 56 21 20 14
  3 31 26 29 64 57 50 60 59 30 12  7 37 19 23 32 22 58 42 13 15 38 39 45
  4 52 18 34 17 33 66  9 54 16 43  5 41 65  0 35 67 48 11 40 55]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2727
INFO voc_eval.py: 171: [ 61  62 136  20  91  52  60  92  68  31   3 105  99  87   2  66  28  23
  25  79  49  44  88  95  74 111  64  94  76 109  53  96 118  67 112  27
 133  83 138  11 108  97  65  89  38 124  18  42  10  24   0 129  55  81
 104   4   7  71  13  43  30  57 120  39  21  47 130  56 126  90  77  82
  50 139  70  46 134  54   1 107  12   9   6   8  75  93  33 117 125 132
 119  58  80  85  26  29  72  98  37  78  63  48  34 127 137  35  22 113
 106  73  51  40  36  59 114  69  32  41 122   5 101 121  45 100  15 116
 103 102 128  86  84 123 131  16  19  14 115 135  17 110]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3147
INFO voc_eval.py: 171: [ 94  11  54   8  25  42  78  18 120  65  45  27 123  66  62   6  26 133
 130  43 108  47  82 125 107  57  77  67   9  15  40  75  71 109  52  24
 134  22  29  10 115 103  72   7  17   3  30  23  46  98  88  87 126   5
  34 137  92  58  53  86 117  89  32  41 135  68  81  73  13  36 102  80
 124  39  51 113  38 105  90 114  55 100 138  49  69  20  70  83 128 106
  31 104  21  59  44   1 127 111  64  95 118   0 119  28  56  61   4  14
  16 131  12  91  35 140  79 122  99  84  97 136  19 112  33  63 129 121
  37 116  85   2 110  50 101 139  74  76  48  93 132  60  96]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3939
INFO voc_eval.py: 171: [107  69 159 170   1 237 125 152  18 213 205 130  12  96  59  25 233  36
 220 171  85 176 167 131  27  48  19 134 196 160   6 241  52  84 157 140
 153  92 248  78 178 161 188 162  35 239  61  72   0 245  26 203  34 206
 126  86 112 129 163 114 186  49  54 252 234  44  58  40 249 181 151 116
 223  33 221 180 229 175 138 154 225  24 117 174  88 183  73  93 104  50
 197 187 177 141  38  28  46 185  90 127  67  99 208 128 232  81 172 202
 115 194 133 139 179 250 132 231  60  15 190 142 246  68  17 166 217 191
  10 155   8 235 184 147 211 189 156 158 100 251  83 120  53  11  22 224
  94 144 214 123 210  45  75 148   9  16 106  56  32  62 247 209 103 101
  91 137  65  80  39 108   4  31  42 145 195 222  29 227 219 226 199  66
  41  23  43  47 105 204 230 111 168 149 173 212  76  57  89 228  70 102
  98  71 243  21 236 143  20 198 215   7  77 238 124  63 253  95 150 207
 135   2  37 244  87 121 118  82 146  13 193 165   5 113  79 119 164 242
  14 240 109  55  97 192   3  51 169 200  30 182 218  64 136 122 216 110
 201  74]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4595
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.4099
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.347
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 134: 0.234
INFO cross_voc_dataset_evaluator.py: 134: 0.340
INFO cross_voc_dataset_evaluator.py: 134: 0.559
INFO cross_voc_dataset_evaluator.py: 134: 0.624
INFO cross_voc_dataset_evaluator.py: 134: 0.473
INFO cross_voc_dataset_evaluator.py: 134: 0.030
INFO cross_voc_dataset_evaluator.py: 134: 0.503
INFO cross_voc_dataset_evaluator.py: 134: 0.221
INFO cross_voc_dataset_evaluator.py: 134: 0.412
INFO cross_voc_dataset_evaluator.py: 134: 0.074
INFO cross_voc_dataset_evaluator.py: 134: 0.403
INFO cross_voc_dataset_evaluator.py: 134: 0.741
INFO cross_voc_dataset_evaluator.py: 134: 0.621
INFO cross_voc_dataset_evaluator.py: 134: 0.592
INFO cross_voc_dataset_evaluator.py: 134: 0.273
INFO cross_voc_dataset_evaluator.py: 134: 0.315
INFO cross_voc_dataset_evaluator.py: 134: 0.394
INFO cross_voc_dataset_evaluator.py: 134: 0.459
INFO cross_voc_dataset_evaluator.py: 135: 0.410
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
