Start testing on iteration 499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.445s + 0.029s (eta: 0:00:58)
person 0.90781283
person 0.98314685
person 0.9914363
person 0.92633325
person 0.975906
person 0.96703833
person 0.9191997
person 0.9387622
person 0.90369046
person 0.9822124
bottle 0.9846125
bottle 0.98217493
bottle 0.9404052
bird 0.9804208
boat 0.9504778
person 0.9785663
person 0.9333795
chair 0.9349965
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.313s + 0.037s (eta: 0:00:39)
aeroplane 0.9433185
pottedplant 0.9706716
pottedplant 0.9757513
pottedplant 0.93535554
person 0.9114452
person 0.95319384
person 0.92249596
person 0.95196295
person 0.9884644
person 0.9614475
person 0.92511266
bird 0.952961
cat 0.92415464
person 0.9951389
tvmonitor 0.9686839
tvmonitor 0.90130144
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.319s + 0.042s (eta: 0:00:37)
person 0.9712852
sheep 0.9529824
sheep 0.94248164
person 0.92045337
chair 0.95479256
pottedplant 0.94289196
aeroplane 0.92911875
chair 0.91417164
chair 0.98908573
person 0.9917019
person 0.9943039
person 0.983266
person 0.9905964
person 0.99431926
person 0.9210105
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.316s + 0.041s (eta: 0:00:33)
person 0.9607357
person 0.9961998
person 0.9909677
person 0.98959935
person 0.98423034
person 0.91798544
person 0.93054444
person 0.9279673
person 0.97130156
person 0.98487574
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.317s + 0.040s (eta: 0:00:29)
boat 0.9652771
boat 0.9297974
diningtable 0.93994755
chair 0.989781
chair 0.98293346
chair 0.98369664
chair 0.90152186
pottedplant 0.9662585
pottedplant 0.90368825
pottedplant 0.9478169
pottedplant 0.92152125
pottedplant 0.94234306
pottedplant 0.97613543
pottedplant 0.98312324
pottedplant 0.9644666
pottedplant 0.9645247
pottedplant 0.9757697
pottedplant 0.96889997
pottedplant 0.97478557
pottedplant 0.9487154
pottedplant 0.9234328
car 0.96660924
person 0.98137486
person 0.9895996
person 0.95829314
person 0.9908929
person 0.9029473
person 0.951258
person 0.9546547
person 0.924585
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.317s + 0.040s (eta: 0:00:26)
pottedplant 0.9408224
person 0.9462824
train 0.97817945
person 0.9863323
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.319s + 0.039s (eta: 0:00:22)
pottedplant 0.98172784
motorbike 0.9621025
diningtable 0.9354828
pottedplant 0.93198544
pottedplant 0.9159418
person 0.96599466
aeroplane 0.9211783
aeroplane 0.9825174
aeroplane 0.93027586
aeroplane 0.97798836
aeroplane 0.96330583
aeroplane 0.9386408
chair 0.9940165
chair 0.99418676
pottedplant 0.9771926
tvmonitor 0.94951767
bird 0.9955871
bird 0.90271586
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.319s + 0.039s (eta: 0:00:19)
bus 0.90077204
car 0.9671794
person 0.9952147
pottedplant 0.9855025
pottedplant 0.91357267
pottedplant 0.92276585
person 0.98331636
person 0.92749697
horse 0.9413787
person 0.9735093
person 0.9087591
person 0.96194446
car 0.98905396
chair 0.9306039
bicycle 0.9812134
bicycle 0.99338806
person 0.9627618
person 0.9694605
chair 0.9682272
chair 0.9341888
chair 0.9642776
pottedplant 0.9459467
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.317s + 0.038s (eta: 0:00:15)
chair 0.9409802
chair 0.95794636
person 0.9822871
person 0.92294466
person 0.9743071
person 0.9185249
person 0.97760963
person 0.97193843
person 0.9026394
person 0.988617
person 0.9348075
person 0.9882442
person 0.9473254
person 0.98867106
pottedplant 0.90198165
pottedplant 0.9862968
pottedplant 0.9211115
person 0.9940837
bus 0.9048687
bus 0.9853018
person 0.90066797
chair 0.9024486
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.320s + 0.039s (eta: 0:00:12)
motorbike 0.99623567
boat 0.92408967
cat 0.9420125
bird 0.94196296
person 0.97584945
pottedplant 0.94573396
chair 0.9441759
pottedplant 0.90385115
chair 0.92309374
chair 0.962234
chair 0.9758936
chair 0.9184714
chair 0.9491315
chair 0.9544002
chair 0.9568455
chair 0.94588363
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.321s + 0.039s (eta: 0:00:08)
boat 0.9878413
boat 0.94827974
boat 0.923106
person 0.95593566
person 0.9959209
person 0.98947597
person 0.94640005
person 0.99056983
person 0.9873826
person 0.9252258
person 0.9187231
person 0.9119187
car 0.9648881
person 0.9876155
person 0.98850864
pottedplant 0.9154594
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.320s + 0.039s (eta: 0:00:05)
diningtable 0.94284576
chair 0.91506904
chair 0.99115807
chair 0.9239323
chair 0.90803766
chair 0.9407153
chair 0.9963438
chair 0.9838733
pottedplant 0.9755103
chair 0.9889331
chair 0.9192996
tvmonitor 0.96183586
chair 0.9296065
boat 0.932234
person 0.9467767
person 0.97688544
chair 0.936953
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.314s + 0.038s (eta: 0:00:01)
person 0.9829214
person 0.9159301
person 0.97202885
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.348s + 0.026s (eta: 0:00:46)
person 0.95227253
person 0.98617864
person 0.9811102
bicycle 0.9937261
person 0.9292019
bicycle 0.9363409
bicycle 0.9352245
bird 0.9755702
bird 0.9623891
bird 0.91581047
bird 0.95198345
pottedplant 0.98932236
person 0.9695905
person 0.90733933
person 0.96575546
person 0.93770784
person 0.9872651
person 0.904752
person 0.9610083
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.315s + 0.038s (eta: 0:00:40)
person 0.97574496
person 0.9800694
person 0.97850925
pottedplant 0.9325663
pottedplant 0.96806777
bottle 0.91258645
chair 0.9391914
chair 0.9222657
pottedplant 0.9736833
pottedplant 0.970602
pottedplant 0.93546194
pottedplant 0.91891253
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.318s + 0.041s (eta: 0:00:37)
car 0.9530507
car 0.9413033
person 0.9866346
person 0.91851777
person 0.9277494
person 0.9533583
car 0.9803648
car 0.99377173
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.314s + 0.041s (eta: 0:00:33)
person 0.9327086
person 0.9877165
chair 0.949744
chair 0.9237888
person 0.949584
person 0.9311755
chair 0.9471613
person 0.902441
person 0.9401179
person 0.957035
person 0.9883633
pottedplant 0.91611624
pottedplant 0.9533822
person 0.9852447
person 0.96875775
person 0.97587955
person 0.9137488
bird 0.99606484
person 0.91921914
bird 0.9770796
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.315s + 0.040s (eta: 0:00:29)
person 0.92118144
aeroplane 0.9677586
person 0.9442812
person 0.98412466
pottedplant 0.92119974
car 0.9648881
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.317s + 0.040s (eta: 0:00:26)
bird 0.99478334
bird 0.979711
bird 0.96591425
aeroplane 0.9318294
person 0.90527827
chair 0.96420926
person 0.9465147
diningtable 0.9522655
chair 0.95768327
chair 0.92005694
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.325s + 0.040s (eta: 0:00:23)
person 0.9849712
pottedplant 0.9662434
bus 0.98512036
diningtable 0.95848083
chair 0.94013506
chair 0.9944718
chair 0.98095816
chair 0.940647
chair 0.92681086
pottedplant 0.9704494
person 0.91126984
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.323s + 0.039s (eta: 0:00:19)
diningtable 0.9594485
chair 0.93922615
chair 0.918573
person 0.9255246
person 0.9055516
person 0.9489102
chair 0.9471795
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.320s + 0.038s (eta: 0:00:15)
person 0.94618195
train 0.96287847
person 0.95738465
person 0.96178883
person 0.98716927
person 0.9052419
car 0.9326942
person 0.9061941
person 0.92606723
person 0.92603487
person 0.9658111
person 0.9933154
person 0.9037916
person 0.9857356
person 0.9596117
person 0.9322494
tvmonitor 0.9721457
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.320s + 0.038s (eta: 0:00:12)
person 0.934507
person 0.90010667
horse 0.9194301
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.318s + 0.038s (eta: 0:00:08)
person 0.9894683
person 0.9868552
person 0.9259964
person 0.9626811
chair 0.9853271
car 0.9637849
car 0.9458766
car 0.9597362
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.319s + 0.038s (eta: 0:00:04)
person 0.9844242
person 0.95911777
person 0.9640745
person 0.9256401
person 0.90712035
person 0.94554204
person 0.9421657
person 0.9174309
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.316s + 0.037s (eta: 0:00:01)
diningtable 0.9497743
person 0.91010106
person 0.9853511
person 0.9901017
person 0.9326598
person 0.9795304
person 0.9646123
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.373s + 0.028s (eta: 0:00:49)
person 0.98846215
diningtable 0.9037345
person 0.95631504
bottle 0.96565163
bottle 0.98331565
person 0.990954
person 0.92658114
tvmonitor 0.9025642
tvmonitor 0.9756541
pottedplant 0.96730655
person 0.96206915
chair 0.9363953
chair 0.9353277
chair 0.98447716
chair 0.9851437
chair 0.9162337
person 0.90579146
person 0.9963319
person 0.98636496
person 0.9536083
person 0.9797585
person 0.97249585
person 0.9455379
person 0.91083944
person 0.9434975
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.338s + 0.033s (eta: 0:00:42)
person 0.95112926
person 0.9891838
person 0.9502306
person 0.9448892
bottle 0.9607598
bottle 0.9797451
bottle 0.9782144
bottle 0.96015924
bottle 0.9622456
chair 0.9540866
pottedplant 0.91339195
pottedplant 0.90773463
chair 0.93449277
chair 0.9433286
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.339s + 0.033s (eta: 0:00:38)
chair 0.9855919
chair 0.9803273
bottle 0.9818871
person 0.9539783
person 0.90196246
person 0.91931343
bottle 0.97500277
bottle 0.98952395
person 0.98668045
person 0.9584577
sheep 0.90564054
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.337s + 0.035s (eta: 0:00:34)
person 0.9146371
person 0.97435737
person 0.9257819
person 0.94063705
person 0.9017612
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.335s + 0.036s (eta: 0:00:31)
person 0.9847447
person 0.95342016
person 0.960638
pottedplant 0.9211489
person 0.95597476
person 0.94105273
person 0.90142393
person 0.97770506
person 0.9320617
person 0.97685516
person 0.97675735
bottle 0.98233217
chair 0.91374505
chair 0.9579516
chair 0.9948179
chair 0.99029714
chair 0.92884743
chair 0.90643764
chair 0.9745881
pottedplant 0.9657268
pottedplant 0.9350615
boat 0.97021466
boat 0.97321665
person 0.9022043
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.330s + 0.035s (eta: 0:00:27)
car 0.9896068
person 0.9706881
person 0.90487593
person 0.9728335
person 0.9523481
person 0.95597094
pottedplant 0.9891073
pottedplant 0.9744889
car 0.99459237
person 0.9154786
person 0.9875572
person 0.98953515
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.326s + 0.035s (eta: 0:00:23)
person 0.9842419
car 0.9556792
person 0.94989496
pottedplant 0.9141553
pottedplant 0.9171759
tvmonitor 0.97537
aeroplane 0.9630904
person 0.91156584
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.325s + 0.035s (eta: 0:00:19)
chair 0.97383296
tvmonitor 0.9245308
person 0.9165135
train 0.92088443
train 0.9569744
bird 0.95310944
bird 0.98781556
person 0.9357794
bicycle 0.9081656
bicycle 0.9899974
pottedplant 0.90711296
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.324s + 0.035s (eta: 0:00:15)
person 0.9473502
person 0.9315887
person 0.96874356
person 0.99203753
person 0.96608514
person 0.992292
person 0.93134403
person 0.9885594
person 0.9216848
pottedplant 0.943379
person 0.96087843
pottedplant 0.9073671
person 0.96538234
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.322s + 0.036s (eta: 0:00:12)
person 0.90297794
chair 0.938941
chair 0.99192375
chair 0.97152406
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.323s + 0.035s (eta: 0:00:08)
pottedplant 0.9033622
person 0.9333515
person 0.97168857
person 0.941429
person 0.9176649
person 0.9088558
person 0.9859678
pottedplant 0.9853328
pottedplant 0.94967353
boat 0.97899103
boat 0.9121973
person 0.95483726
bottle 0.9693028
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.322s + 0.036s (eta: 0:00:05)
chair 0.94234467
person 0.9500843
person 0.93674576
person 0.91534793
person 0.95634097
person 0.93724257
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.323s + 0.036s (eta: 0:00:01)
person 0.97128963
bird 0.95176333
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.382s + 0.029s (eta: 0:00:50)
person 0.91169816
chair 0.92387664
chair 0.9813954
chair 0.95504075
person 0.9284067
person 0.927243
chair 0.9262454
diningtable 0.9672435
person 0.90191036
person 0.9420235
chair 0.90839607
chair 0.9710277
person 0.9961669
person 0.9797212
person 0.9929716
person 0.903424
person 0.99349445
person 0.9831192
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.303s + 0.043s (eta: 0:00:39)
tvmonitor 0.9304647
pottedplant 0.9758129
bottle 0.93730074
bird 0.9302238
bird 0.97971237
bird 0.9680605
person 0.96939653
person 0.9000754
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.312s + 0.039s (eta: 0:00:36)
car 0.9089028
person 0.9639729
person 0.95889896
person 0.96273816
bottle 0.97720057
bottle 0.9149349
bottle 0.9678972
pottedplant 0.93305963
bottle 0.98166496
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.313s + 0.040s (eta: 0:00:33)
car 0.9413122
pottedplant 0.97966146
pottedplant 0.918238
chair 0.9814191
chair 0.9149823
chair 0.95948076
person 0.97742426
bottle 0.9843211
bottle 0.904504
bottle 0.9853699
bottle 0.99230766
bottle 0.9750115
bottle 0.9003728
bottle 0.9485812
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.318s + 0.042s (eta: 0:00:30)
person 0.9670447
sheep 0.92024636
sheep 0.93778574
car 0.9206153
person 0.97066057
person 0.99451655
person 0.9874049
person 0.98862433
bicycle 0.9506918
chair 0.9525368
pottedplant 0.94036996
person 0.91168016
person 0.94344723
person 0.9973568
person 0.9164397
person 0.9849123
chair 0.9201561
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.317s + 0.041s (eta: 0:00:26)
person 0.97239166
diningtable 0.96458447
chair 0.9681994
chair 0.9965082
chair 0.9917528
chair 0.92389613
chair 0.9053608
aeroplane 0.91919816
car 0.9419418
person 0.98109597
person 0.96094435
person 0.93863016
person 0.94124514
person 0.9703582
car 0.9272045
car 0.9837933
car 0.9863642
car 0.96014637
bird 0.9374157
bird 0.9405309
bird 0.9175178
bird 0.95450544
chair 0.95347506
chair 0.9555008
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.321s + 0.041s (eta: 0:00:23)
person 0.9881814
person 0.9686022
person 0.961395
person 0.9957908
person 0.9492263
person 0.9471164
person 0.9908303
person 0.9347837
person 0.9107798
horse 0.9431214
person 0.9686317
bottle 0.9719659
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.318s + 0.040s (eta: 0:00:19)
person 0.9158141
pottedplant 0.91433257
bottle 0.9430687
chair 0.9017112
chair 0.9912436
chair 0.9305783
chair 0.98753417
chair 0.99405646
pottedplant 0.97947216
pottedplant 0.92558295
pottedplant 0.99079543
person 0.9467129
person 0.91765565
person 0.9676108
person 0.9354825
person 0.91971767
bicycle 0.9856287
bicycle 0.94714546
person 0.98604506
person 0.969184
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.316s + 0.040s (eta: 0:00:15)
person 0.9969614
person 0.92673606
person 0.9433768
person 0.9893022
person 0.98385173
person 0.9213413
person 0.9097846
person 0.98062325
person 0.92876446
bus 0.91106766
person 0.9285208
person 0.91523886
person 0.93318766
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.316s + 0.041s (eta: 0:00:12)
person 0.9866101
person 0.9069945
chair 0.9434651
chair 0.98828775
chair 0.97960794
tvmonitor 0.94588953
tvmonitor 0.9077785
chair 0.9873984
chair 0.9775474
chair 0.93894935
chair 0.9860656
pottedplant 0.9490376
pottedplant 0.9306548
chair 0.91843635
pottedplant 0.969022
person 0.9233162
pottedplant 0.95642745
person 0.979219
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.314s + 0.040s (eta: 0:00:08)
motorbike 0.9865172
motorbike 0.9283563
person 0.9007288
person 0.9067699
car 0.93430656
bicycle 0.95930237
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.314s + 0.040s (eta: 0:00:04)
person 0.958552
person 0.96625274
person 0.9097732
bicycle 0.9083952
person 0.9088157
person 0.9131504
bird 0.94132614
chair 0.9885811
chair 0.90997726
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.314s + 0.040s (eta: 0:00:01)
person 0.94808066
chair 0.91055024
bottle 0.92984575
bottle 0.95642394
bottle 0.94100404
person 0.9873755
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 80.957s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [390 401 808 ... 272 268 986]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2714
INFO voc_eval.py: 171: [ 440  241 1043 ...  181  953  197]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.4841
INFO voc_eval.py: 171: [ 939  381 1067 ... 2490  369  370]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2197
INFO voc_eval.py: 171: [ 767 2831 2380 ... 2643  397  512]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2260
INFO voc_eval.py: 171: [1591  955   25 ...  219  614  218]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4104
INFO voc_eval.py: 171: [270 134 739 135 108 137 279 112 139 109 738 397 119 485 115 263 114 656
 591 467 403 398 251 668 482 140  33 146 216 422 667  31  53 267 488 653
 157 779 741  99 224 508  38  59 191 353 316 121 174 592 306 136 464 712
 162 708  90 414 556 506 539 525 572 371 124 722 655 593 122 262 427 479
 110 507 706 113 622 781 772  27 349 361 342 176 554 238 171 661 277 736
 688 560 159 186 726 120 169 236 769 228 138 299 475 509 559 483 348 477
 620 411 259 222 100 328 235 369 168 603 151 190 366 395 107 750 375 773
 536 421 648 172 644 290  28 623 161 749 436 715   4 144 434  30  43 574
  37 308 408 330 562 192 206 292 504 481 546 234 167 484 416 117  45  80
  54 288 250 550 433 380 439  51 419 695 272 389 202 390 106 651 762 148
 396 561 478 165 577 564 223 675 782 182 746 123 694 538 314  41 515 116
 633 264 617 767 754 445 118 639 204  81 352 737 596 770 751 111 660 336
 253 164 405 768 573 604 325 400 502 671  97 686 650 526 490 197  42 188
 221 734  74 252  25 447 689  50  26 210 304 130 552 156 583  32 474 409
 376 227 563   6 181  72 331  35 735 590 511 449 613 774 709 544 170 149
 237 248 636  68 725 166 327 153 744   8 365 446 226 476 654 743 189 585
 500 557 199 142 579 244 548 555 670 558 505  92 415 359 729 205 383 132
 575 635 225 363 289 520 522 780 468 265 160 152 362 322 231 732 581 313
 301 364 394 690 354 542 193 158 472 687 401 266 649 514 297 432 480 337
 537 230 760 459 641 618 155 307 261 145 271 566 185 637 412 543 294 776
 471  56 351 198  87 634 540 452 410 180 413 429 716 207 512 605  24   2
 125 243 335  86 233  21 465 451 541 638 710 673 384 305 645 713  75   7
 358 628 340 360 321 184 693 291 454 516 549 463  84  47  76  36 777 201
 624 163 368 565 370  82 696  83  39 103 183 691 630 752 178 187 219 553
 274 521 700 444 196  60 607 257 666 470  85 399 312 778 424 669 697  73
 215 665 298 588 287 570 707 775 584   3 147 615  58 619 247 466 355 385
  10 730  34 379 347 229 315 254 510 711 317 195 407 503  79 258 343   5
 217 582 194 756  11 218 740 529 460 296 771 300 260 733 131 491 220 718
 765 320  15  55 310 311 431   1 333 551 255 367  40 175 450 647 728 702
 242 211 232 200 699  17 425 443 606  22 587 642 133  62 430 602 141 143
 104 469 547 658 303 495 513 382 719 759 534  67 309 747  66 256 652 519
 748 386 126 657 714 692 629   0 535  61 518 608 643 318 499 154 246 377
 473 727 626 440 621 457  12 701 378 674 662 501 426 423 293 357 393 381
 753 646  69 302 295 212  44  29 517 323 373  18 356 324  96 177 731 326
  20 640 245 105 329 428 659 437 627 387 402 388 173 276 208 523 209  23
 698 101 213 417 334 685 420 616 269 703   9 569 576 580 404 418 319 595
  77 498 150 493 435 571 614 600 601 533 406 680 672 532 545 723 586 456
 589 129 442 766 578 286  48 704 280 489 338 374  57 598 597 594 764  63
 339 663 742 664 268 757  19 705 763 438  65  94  64 350 341 494 496 273
 721 128 717 127 684 632 372 531 102 625 448 345  78 720 275  52 344 758
 455 179 724 678 461  16  88 462 497  49 441 755 332 487 599 492 761 683
 392 610 631 453 568 611  13 458 612  71 609 567  14 486 682 679 681 527
  93  95 346  89 677 676 524  98 745 528  91 391 203 285 239 241 282 249
 284 214 240 281  46 530 283  70 278]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4505
INFO voc_eval.py: 171: [1398  651 1340 ...  239  866  171]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.3376
INFO voc_eval.py: 171: [ 85  27 410  25 352 413 164 351 172  88  23 353 128  98 102 123 230 118
  28 125 260 171 157 412 185 401   1  18 169 178  66 403 132 337  40 165
 135  19 213  24  75 318 411 179  29 134 191 382 415 384 162 402 399 207
 397 183  86 117  26 293  78 263 310 160  96 182  76 295 206  87 151   0
 319   3 138 112 395 168 152 323 130 344   4 383  80  22 253 255 173 196
  21  37 324  33  89 159 221 311 120  38  50 129 325  97 416 145 405 144
 214 404 161 336 193 268  17  72  67 315  10 391 266 122 215 262 304 328
  12 308 105   5 385 376 224 209 142 150 225 204 219 270  81 106 170 356
 348 133 222 155 103 269 127 146 281 390 121 210 274 418 261 163 277  62
 340 320  99 187  32 110 393 216 158   2 368 303 299 392  64  31  61 188
 267 363 326 211 423 177   6 139 273  68 398   7 140 119 116 175 174   8
  20 297 272 377 208 301 236 166 305 257 107  63 143 137 195  34 361 366
 136 189  39 104 264  73 232  79 131  30 317 329 314 124 349 201 231  35
 149 228  77 300 100 422  36 167 302  84  82 316 141 378  41 296 312 421
 227 190 239 307  71 186 379 406 306 309 153 381  47 218 154 371  94 370
 111  45 126   9 271 419 280  56 394  91 233 343 194 327 321 369 331 226
 313 367 338 234  93 396  16 245 197 285 265  48 279  49 259 420 202 203
 335 354 359 240 278 217 229 380 180 345  11  69  15  44  46 372 101 148
  90 223  74 147 251 250 374 212  43 409 347 346 339 192 184 322 256  70
 109 283 333  83 244 291 199  13 237 341 330 362 298 238 342 375 181 358
 235 205 364 198  92 350 334 156  60 249 108 254 294 292 115  51 365  58
 414  57  53 248  14 332 357 220 373  95 282 387 114  52 407 284 200 408
 400  54  55 113 246 241 360 247 275  42 386 286 258 417 389 355 176 243
 252 388 276 242 287 288 289 290  59  65]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0759
INFO voc_eval.py: 171: [7811 1860 5553 ... 8509  883  884]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.4009
INFO voc_eval.py: 171: [110 105  98 176 403 306 398 112  27 320 377  96 304  22  57 181  97  95
 184 385 328 178 396  26 391 126 101 307 107  20 399 405 380 292  34 402
  25  64  30 305 106 179 301  33 325 311 393 104 395 388 125 116 379 141
 113 368 171 109 121 386 326 341 226 102 392 381 118 383 100 108 382 156
 401 254  99 127 134 404 319 143 255 376  56 291 369 312 322 111 131 397
 173 182  59 293 137 186 151 207 187  19 103 115 308 185 155  83 309  62
 400  66 206 238  61 358 378 114 260 227 152 120 387 355  65  12  32 119
 316 117 356  13 157  86 142 323 394 315 183 384 365 340 283 375 332  85
 264  24 214 166 299   0 249 212  60 180 177 390 331 411  84  29 410 149
 192 336  37 327 158 190 350 413 354 409 252 216  63 159 145  87 124 204
 389 202 263 201 239 300 259 334  58 298  28 210   9 353   6  80 175 366
 287 240 205 129 174 286   7 313 165 367 284  40 147 351  31  14 329 122
 285 352 288 412 188 237  74 343 335  39  49 236 266 342  36 344 274 144
 261 128 359 318  47  35  38  52 269 148 203 314   5  23 146 333  77  21
 211  76 150 408  92  45 248 330 199 208 217 247 130  78 324 251 337 234
 267 321 213  42  73 231 232 296 215  17  16 170  79  51 229  15 153 257
 265   8 195  75 139  18 297 276 167 347 273 230 406 407 364  46  70 295
 223 372 218  44 362 162 224 253  71 172  94  89 154 219 140 338 189 363
 339  53  90  50 225 258 198  10 250 138 275 361 349 222 270  82 303 278
 279  54 371 168 360 346  11 169 243  81 123 194  88 228 233 209 268  55
 256 241 348 310 136  91 191 197 277 242 289 290 135 132 221   1 235 262
  72 271 160 281 133 357 370  67 317 294 374 373 282   4 280 193 302  93
 200   2   3 220 164 196 161 345  43  41 246 163 244  68 245  69 272  48]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1231
INFO voc_eval.py: 171: [2934 3252 1535 ...  408  435  434]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3359
INFO voc_eval.py: 171: [267 284 149  18 545 220 287  87 541  42 248 554 180 139 219  21  86 111
 274 542 143  16 142 112 152 233 146 186 591 461 552 245 573 463 338 160
 145  99 228 297 555 593 288 176 575 561 278  98 550 156 527 140  84 221
 275 154 329 239 266 581 368 566   8  19 157 543 273 206 560 184 498 579
  23 234 127 119  36 222 462 246 411 460 203 120 177 570 113 187 214 110
 427  81 241 585 522 428  31 125 589 582 443  43 102 271 165  49 426 361
 289 226 268  44 567 153  82 109  38 391  37 569 559 357 188 240  58  52
 580 358 285 340  17 334 548 404 371 269 182 202  72 574 291 150 563 299
  45 571 151 147 425 359 468 141  75 339 412 286 148 252 162 529  39  22
 557 523 247 471 181 223   0 547 372 189 144 108 272 366 551 516  40 175
 440 356 576 346 270 553 212 437 333 572 549 229  30 577 282 364  41 544
 568 509 407 341 578 429 528 491 507  29 205 451 418 505 537 352 365 243
 100 210 244 594 122 439 330 183 508  47  33 556 414   7 397 336 401  35
 217 123 353 179 185 435 419 377 155 583  88  64 345 546 342 466 444  50
 511  74 159 124 525 564 584 454 126 565 298  25  73 129 178 398 504  97
 524 390 535 195 590 164 242 420 215 538 493   6 596 103 510  51 410 409
  92 280 445 360 392 227 400 158 201 416 386 408 500 540   1 526   4 592
 121 293 379 262 283 225 457 194  78 279  77 237  32 250 452 104 232   2
 107 204 344 595 375  93 399  56 174 402 197  54 413  55   9  85 533 367
 114  48  46 403 136  76 312 347 534 265 501 588 161 490 587 536 255 438
 319 396 513 327  80   3  83 101 474 343   5 465 310 117 393  89 213 198
 190 191 335 494 192 332 314 308 200 331 473 480  53 384 106 381 417 163
  57 309 128 209 519 171 395 196 455 495 231 486 487 259 277 433  91 450
 362 464 276 216  70 261 193 479 369 422 305  71 317 492 208 230 405 118
 363  65 301 207 446 211 199  15 432 456 387  95 224 258 458 105 116 472
 388 385 313 294 382 449 349  13 355 497 520 328 453  96 134 236 281 503
 264 322  69 256 459 337 394 415 302 448 292  12  14 515 354 421  67 348
 350 380 447 304 290 138  34 512 475 257  79 373 489 521  26 325 430 253
 586 467 481 306 311  10 318 488 477 168  20 263 167 115 321 300 496 434
 502  62 558  59 130  90  94 172 383 376 166 483 307 470 315 478 482 485
 303 406 436 506  68  63 260  27 135 499 562 517 131 423 137 431 238 320
 518 170 132 484 476  66 370 469 133  24 514 173 235 374  11 169 251  28
 249 351 389 324 254 424 531 218 442 441 539 326 296 378  60 316 323 295
 532  61 530]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1419
INFO voc_eval.py: 171: [470  73 200 411  97  76 201 205 318  74 412 406 103 317  79 334 529 415
  84 391  80 100  75 405  81 550 410 319 152 478 102 321  98 346 414 324
  91 230 482 427 408 316 418 531 229 202 453  87 454 228 340  78 392 198
 511 135 189  15 472 515 407 512 174 210 161 396 421 348 476 115 416  22
 163 332  77 191 298 534 527 106 530 145 536 473 538 480 132 133 417 518
 131 154  53 137 483  99 413   6 101 393 186 143 517 508 134 142 320 187
 199 533 481  48   7 354 164 490  85 516 234 237 339 247 509 330 522 429
 409 272 225 282 138 552 112 420 207 128 514 428 136  95 180 519 129 301
   8  24  52 107 108 226 188 232 110 190 149 151  10 471 448 537 329 549
 322 521 261  40 419 326 398 235 130  19 520  89 394  92  25 374 464 439
 535 510 169 513 440 109 162  61 288 312 182 306 328 302 123 240 181 256
 475   9  90 241  12 390 345  31 233 499 500 462 166 280 104  32 236 327
 436  82 458 540 325 372 246 457  28 294 371 125 497 153 167 238 323 300
 111 113 239 498  29 197 353 496 494 331 487 231 248 173 165 311 541   3
 389 463 486 278   2 551 400 378 363  49  60 528 222 220 116 114 250  51
   4 144  50 299 242  20 175 449 365 450  62 435 434 350 285 183 553 469
 337  11 105 336  56  58   1 438 159  17 204 160 295  26 296 366 387 432
 283 539  69  30 155 397 338 281 532 185 192 265 168 344 364 399 150 373
 461  27 488 139 495 117  86 485 184  43 156 452 120 506 342  94 307 467
  83 193 343 404 369 426 489  38 258 468 443 333 219 466 341 284  16 493
 376 442   0 217  23  65 544 542 479 194   5 313 223 349 289 279 362  57
 388 335 460  93 401 437 433 441 377  18 352 456 251 491 148 430  59 218
  54 309 446 252 243 267 505 347  46 447  14 249  21 455 263 459 384 269
 423 361 118  71  68  34  39 431  41 260  47  44 402 215 303 474  67 203
 119 368 304 375  36 126  66 523 127 492 176 179 216 257 403 385 305 355
 178 290 297 547 445 381 227  13 245 465 140 158 484  42 271  35  37 262
 451 264 244 124 379 444 422 543 383  96 221 268 308 206 212  88 425 259
 195 157  70 121 424  55 386 507 146 286 277 525 351 122 266 524 255 526
 314 287 477 315 395 196 382 367 380 502 370 141 546 310 209  72 504 356
 211 213 270 292 503 358 273 177 214 224 548  33 545 291  63 254 208 275
 253 170 276 147 172 293 359 357 171  45 274 501  64 360]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.2855
INFO voc_eval.py: 171: [159 806  99 804 529 160 382 127 179 181 564 798 866 161 546 863 536 825
 108 101 178 166  59 801 400  42 366 656 813 725 542 545 755 619 310 705
 543 879  79 600 110 790 873  63 180 875 531 634 878 732 874 246 872 371
 870 871 671 808 302 570 424 450  95 548 698 665 272 211 744 480 692 509
 464 422 533 846 655 114 797 521 378 849 624 377 452 612 523 758 409 117
 186 730 864 742   4 649 194 799 641 661 566 789 403 703 173 383 289  61
 183  88 628 326 134 482 150 439 571  72 467 779 136 386 271  62 292 576
  44 369 167  46 631 474  89 701 506 214 700 275 238 503   3 233 537 468
  90 284 632 124 618 840 520 191 868 421 240 603 423  78 251 273 237 212
 349 594 596  51 218 540 487 379 303 298 223 502 483  49 845 704  92 621
 290 247 763 171 743 153 629 756 229 375  96 505 304 190 664 810 401 712
 780 248 222 381 263 373  97 590 658 372 702 697 602 592 754 128 807 106
  32 353 327 384 217 733 835 203 129 317 185 286 567 306 176 320  37 800
 699 456 299 869 137 437 472 865 867 391 534 380 485 374 486 236 827 415
 135 370 262 151 300 291 583   2 477 462 102   8 112 750 683  64 216 839
  94 640 156 376  47 764 694 555 877 252 672  35 568 524 200 723 783 419
 771 231 572 219 350  73 481 834 100 689 234 653 354 843 130 368 585 579
 168 330 387 207 215 884 152 574  75 601 811 749 508 554 221   5 213 274
 569 449 778 558  10 430 469 821 762  34 504 883 109 249 720 581 772 677
 876 360 367 322 711 858 484 761 714 591 802 294 296 427 463 131  98 630
 499 768 250 650   7  70 253  71 615 528 465 318 301  45 507 225 385 325
 195 196 206  60 580 812 881   0  48 269 264 232 210 132 662 489 857  33
 659 757 695 314 599 270 266 113 446  14 735 539  11  38 690 199 850 165
 235 122 182 676  74 713 197 578  87 842 209  24 359 405 645 365 140 332
 455 337 470 201 149 230 577 518 551 155 228 174 311 717 312 490 777 440
   1 696 512   6 670 625 776 562 184 356 788 582   9 475 336  91 331 513
 340 193 208 527 158 338  16 716 254 668 355 633 175 226 669 666 660 586
 751 678  54 295 293 397 880  93 556 841 256 420 239  57 803 837 198 447
 205  81 752 170 838 142 357 241 553 297 855 305 111 737 679 848 541  28
 267 728 364 882 448  17  76 408 681 329 667 691 426 597 530 164 187 259
 188 495 192 120  83  40 162  68  39 614 636 265 316 407 339 321 781 243
 133 687 847 715 856 657 413  23  80 126 526 478 389 532 392 710  84 308
 315 309 148  82  65 816 471 154 361 285 726 748 307 324 121 435 559 745
 261 348 511 351 851 760 287  26 268 708 335 638 277 753 605 157 283 328
  36 769 685 258 169 244 573 319 334 460 396 345 473 706 739 563 552  77
 476 390 775  86 721 693 255 611  25 404 428 759 675 844 491  41 815  67
 784 139 510 773 729 747 805  66 786 635 347 610 595 220 343 362 313  30
 740 443 731 852 418  15 146 538 680 565 663 177 727 242 388 414 767 393
 138 402 500 774 517 412 823 411 125 746 830 461 736 479 535  29 674 785
  13 575 741 718 860 245 724 627 105 416 103  31 709 445  69 673 202 609
 438 765 604 417 436 544 654 434 227 707 766 853 684  22 613 257 410 119
 123 363 323 488 492 431 626 646 341 639 143 606 441 608 115 344 549 620
 854 333 466 770 346 607 809 260 547  58 406 429 458 719 557 358 525 204
 623 622  85 587 224 398 814 550 118 352 637 796 519  52 163 144 561 593
 433 826 787 643 861 189  19 688 560 588 651 172 817  12 686 147  50 425
 145  56 276 107  27 782 589 584 432 342 832 280 514 399 652 829 828 501
 822 282 279 116 598  55 722 617 616 141  53 642  43 515 493 522 792 647
 497 738 644 288 682 104 278 442 496 494 831 648 395 444 836  18 795 734
 859 862 824 820 818 459 516 281 794 833  21 819 453 451  20 394 498 793
 791 457 454]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5606
INFO voc_eval.py: 171: [9116 8380 8704 ... 9268 1231 1228]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.4792
INFO voc_eval.py: 171: [4602 1332 3103 ...  670  653  682]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5090
INFO voc_eval.py: 171: [ 24  17 299 296 198 181  34  22  41 298  23 317 183 195  25 358  18 129
 351 293 346 172 316 200 194 197 270 148 313 301 352  72  21 315  26 179
 309 174  76 300 355 145 205 363 177  73 147 130 271 171 173 132 230 146
 297 308 149 359 314 349 304 189  29 343 187 157 285 303  38 345 117 232
 265 160  70  78 311 131 175 203 196 215  31 190 231 360 302  28 233 206
 268 348 188 191 324 204  74  92 350 178 342  79  86 341 192 180  75 133
  33 176 186 184 310 182 290  96 185 193  71 356 153 347 294 199  80 354
 357 277 291 307 353 137 135 362 269 334 158 134 306 272  95 136  20 257
 264 207  77  27 127 280 273  89 333  16 361  82  39 120 161  36 295  19
  91 104 275  40  35 339 110 328  13 282 281 289 329 208 279  37 151  98
 369  55  90 260 305 288  94 128 220 373 256 266 162  97 312 103  87 169
 284 100 274 150 107 251 267 108 246 115  88  56 319 320 262 375  57 126
 332   6  64  32 141 170  15 249 125 222  99  60 276 152 159 261 344 325
 287 374  50 112 121 155 226 255 254  93  46 102 113 326 278  54 263 109
 111 236 210 165  49  30 143   1 247 235 321  14 201 164  65 340 118 156
 114 106 163 216 122 283 318 119 286  12 229 364 237 370  52  48  61  62
 124  51   8 253   7 372 105   9 138  69  66 327 238 234  59 244 245 371
 101 209 144 322   2  81 331 330  68 139  45   4 123 223 154  42 248  67
  47 211  63 239 250 212 259  11  10  85 252 218 214 217 227 142 243 365
  84   0  83 323 221 225   5 258 368   3 241 366 367 140 116 292 166 242
 202 219 228 213 167 240 224 338 337  53 336 168  43  44 335  58]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2276
INFO voc_eval.py: 171: [230 244  78 241 323 229  80 232 372 231 536 236 184 443 247 203 233 240
 316 269 375 476 397  57 202 425 242 325 234 197 326  58 243 402 438  77
 349 117  17  75 462 193 214 179 272 534 533  73 448  83 324 394 318 206
  19 374  59 380  14 238 447 180 267 424 148 161 357 222  81 140  16 516
  89 454 376  87 201 113  40  51 373 151  52 200 204 491 153 185  49  82
 162 301 111 420 265 261 317 208 493  39 401 532 112  42 360  79 528 207
  76 283 523 488 239 205 517 262  91 531 245 535 506 124 475 327 348 537
 209 164 526  31 199 270 178  74 172 195 141 400 271 300 320 399 381 512
 437 423   1 268 435  50 294 328 114 121 513 445 107 482 264 368 392 110
 527 419  55  54 398 455 186 266 246 166 174 127 477 452 226 137  15 149
 122 480 181 427 296 275 276 150 263 428 505  18 390 105 395 515 486 479
 396 182 306 393  41 363 336   0 142  20   2   3 346 123 369 163  47 235
 456 504 173 331 108  94 198 391 403 322 152 278 170 109 248 106 302 298
 160 171 453 483 421 492 194 487 100 290 101 258 440  13 176 507  70 332
 307 158 155 183 509 463 538 468 221  38 358 177 128 422  66 139 196  99
 389 146 192  56 125 220 299 514 356 355  92 351 411 154  98 508 259 104
 103  88  97 237 497 377 225 338 478 490 249 367 102  86 439  90  53 350
 495 485 210  26  95 500 366  62 260 529 295  44 337  93  67 354 175 341
 343 251 433 442 520 501 291 311 297 519 467  27 145 339 472 494 382 496
 304  43 359 361 484 364  96   8 284 330  36 457  85  12  37 410 404   9
 116 211 436 254   7 347 525 446  24 469 481 441 143 342 138 147 165 329
 255 340 303 387 465  23 405 417 191 333 334  84 444 187 335  11 426 409
 345 518 470 169 216 498  48 224 308 156  45 502 344 313 144 459 321 167
 362  60  68 218 292 385 132 524 386 464 309 319 474 406 288 253 189 510
   5  72 449 159 130 384 473 466  33 293 289 503 383 305 379 215 408 157
 413 416 274  69 432 118 412 415 227 287 219 522 460 223 352 190  10 388
  32 213 257 414 217  22  46 212  71 277 430 273 120 431   4 458 133 489
 131  61 188 126  63 450 310 314  29 365  64  30 129 471 135   6 521 315
 252 418 115 256 250 407  65 119 378 451  28 461 429 281 312 228 282 279
 499  34 434 285 353 530 370 168 511 280 136 134  35  21 286  25 371]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.2216
INFO voc_eval.py: 171: [123 445 738 ... 647 650 100]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3325
INFO voc_eval.py: 171: [626 796 527 ... 421 119 146]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3479
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3221
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.271
INFO cross_voc_dataset_evaluator.py: 134: 0.484
INFO cross_voc_dataset_evaluator.py: 134: 0.220
INFO cross_voc_dataset_evaluator.py: 134: 0.226
INFO cross_voc_dataset_evaluator.py: 134: 0.410
INFO cross_voc_dataset_evaluator.py: 134: 0.451
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.076
INFO cross_voc_dataset_evaluator.py: 134: 0.401
INFO cross_voc_dataset_evaluator.py: 134: 0.123
INFO cross_voc_dataset_evaluator.py: 134: 0.336
INFO cross_voc_dataset_evaluator.py: 134: 0.142
INFO cross_voc_dataset_evaluator.py: 134: 0.286
INFO cross_voc_dataset_evaluator.py: 134: 0.561
INFO cross_voc_dataset_evaluator.py: 134: 0.479
INFO cross_voc_dataset_evaluator.py: 134: 0.509
INFO cross_voc_dataset_evaluator.py: 134: 0.228
INFO cross_voc_dataset_evaluator.py: 134: 0.222
INFO cross_voc_dataset_evaluator.py: 134: 0.332
INFO cross_voc_dataset_evaluator.py: 134: 0.348
INFO cross_voc_dataset_evaluator.py: 135: 0.322
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.525s + 0.077s (eta: 0:01:14)
person 0.9415921
person 0.9727666
person 0.98894185
person 0.97081876
person 0.95374477
person 0.92565084
person 0.98107475
bottle 0.96754295
bottle 0.9647818
bottle 0.94716537
bird 0.9687228
person 0.9019201
person 0.97880256
person 0.93037355
person 0.9424659
chair 0.9178054
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.346s + 0.047s (eta: 0:00:44)
person 0.91439223
pottedplant 0.9347825
pottedplant 0.9441109
person 0.96709955
person 0.91358644
person 0.96571577
person 0.9677241
person 0.95821
person 0.9854719
person 0.9157868
person 0.9252905
person 0.93971896
bird 0.9408939
person 0.9802602
person 0.98837644
tvmonitor 0.9541337
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.337s + 0.043s (eta: 0:00:39)
person 0.944757
sheep 0.93558955
sheep 0.92851865
person 0.93974674
chair 0.9446831
person 0.93933636
aeroplane 0.90010077
chair 0.979159
person 0.9060991
person 0.9827503
person 0.99343115
person 0.9131695
person 0.97661126
person 0.9889115
person 0.9928135
person 0.9313899
person 0.9267372
person 0.90457976
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.328s + 0.042s (eta: 0:00:34)
person 0.93638396
person 0.99501497
person 0.98381627
person 0.9866204
person 0.97706085
person 0.9329434
person 0.90475005
person 0.92780995
person 0.9612821
person 0.9694493
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.325s + 0.040s (eta: 0:00:30)
boat 0.95223254
diningtable 0.9390721
chair 0.97001284
chair 0.9775277
chair 0.9040325
chair 0.9297796
pottedplant 0.91902226
pottedplant 0.9595478
pottedplant 0.9292669
pottedplant 0.957456
pottedplant 0.97214377
pottedplant 0.94700575
pottedplant 0.9650079
pottedplant 0.9656255
pottedplant 0.91697127
pottedplant 0.9348338
pottedplant 0.96161973
car 0.9254359
person 0.91539824
person 0.97632444
person 0.9851799
person 0.9541198
person 0.98767006
person 0.9054893
person 0.9006788
person 0.94680905
person 0.92424536
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.324s + 0.039s (eta: 0:00:26)
person 0.93797797
person 0.90307456
person 0.91071564
person 0.91261226
train 0.95349234
person 0.9851126
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.322s + 0.039s (eta: 0:00:23)
pottedplant 0.9601963
person 0.92943364
person 0.9307458
motorbike 0.9596344
diningtable 0.90011275
pottedplant 0.9013069
person 0.9641555
aeroplane 0.9670407
aeroplane 0.9647491
aeroplane 0.9314959
aeroplane 0.9411577
chair 0.9878257
chair 0.9896588
pottedplant 0.95897883
pottedplant 0.9286381
tvmonitor 0.9298005
bird 0.99445444
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.321s + 0.039s (eta: 0:00:19)
car 0.9448479
car 0.93463886
person 0.90067065
person 0.99413294
pottedplant 0.96273154
person 0.9633718
person 0.91363466
person 0.98625135
horse 0.9478414
person 0.9330584
person 0.9638155
person 0.9587146
car 0.9808836
bicycle 0.9399804
bicycle 0.9035322
bicycle 0.98817444
person 0.97133654
person 0.9652885
person 0.9176908
chair 0.9143992
chair 0.9350119
pottedplant 0.90162235
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.319s + 0.039s (eta: 0:00:15)
chair 0.90408134
chair 0.9184046
person 0.97694415
person 0.946878
person 0.9650181
person 0.9823485
person 0.9788616
person 0.90587735
person 0.964493
person 0.9642751
person 0.9062511
person 0.9826498
person 0.93134755
person 0.9915697
person 0.98756
person 0.92756915
person 0.91643393
pottedplant 0.967946
person 0.9926743
bus 0.934655
bus 0.9653285
person 0.91940236
chair 0.9095461
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.320s + 0.038s (eta: 0:00:12)
motorbike 0.9920197
cat 0.9222567
bird 0.93666583
person 0.97872365
person 0.92680657
person 0.9277356
pottedplant 0.9021998
chair 0.9144694
chair 0.9373546
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.321s + 0.038s (eta: 0:00:08)
boat 0.9850042
boat 0.94372135
boat 0.92189366
person 0.9497471
person 0.99410444
person 0.99069464
person 0.94904214
person 0.98684406
person 0.94678426
person 0.98216873
person 0.93184394
person 0.92239046
car 0.9607877
person 0.9243092
person 0.9843255
person 0.98104113
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.321s + 0.038s (eta: 0:00:05)
diningtable 0.9351764
chair 0.9064113
chair 0.9821409
chair 0.9462696
chair 0.97535783
chair 0.99214643
chair 0.9307834
pottedplant 0.95004773
chair 0.97679293
tvmonitor 0.9284612
chair 0.9012886
boat 0.92993003
person 0.947202
person 0.9445333
person 0.98338205
person 0.90387803
person 0.9205365
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.318s + 0.038s (eta: 0:00:01)
person 0.9785649
person 0.92841065
person 0.95482117
person 0.909879
person 0.9267626
boat 0.9037448
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.469s + 0.043s (eta: 0:01:03)
person 0.9645973
person 0.9703151
person 0.98243666
bicycle 0.9631802
person 0.9236621
person 0.9010772
bicycle 0.98917526
bird 0.94461125
bird 0.9096904
bird 0.910046
pottedplant 0.9780938
pottedplant 0.9175842
person 0.901568
person 0.9688658
person 0.94756615
person 0.9571431
person 0.93681586
person 0.98112595
person 0.93764496
person 0.953557
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.323s + 0.036s (eta: 0:00:41)
person 0.95154464
person 0.971826
person 0.9796824
person 0.96992403
pottedplant 0.934993
chair 0.908323
chair 0.92569506
pottedplant 0.9429204
pottedplant 0.9134693
pottedplant 0.905833
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.328s + 0.037s (eta: 0:00:37)
car 0.93486434
person 0.9775004
person 0.9651608
person 0.918414
person 0.9184493
car 0.98193824
person 0.9159568
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.327s + 0.038s (eta: 0:00:34)
person 0.9776943
chair 0.9214512
chair 0.9254797
person 0.9172836
person 0.9566869
person 0.9227918
person 0.92422664
person 0.93175894
person 0.98099524
pottedplant 0.90610015
person 0.9570711
person 0.9924078
person 0.9249756
person 0.9747843
bird 0.9926086
bird 0.9774941
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.329s + 0.038s (eta: 0:00:30)
aeroplane 0.92481595
aeroplane 0.90285724
person 0.9556461
person 0.98188454
car 0.9607877
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.328s + 0.037s (eta: 0:00:27)
bird 0.9558274
bird 0.98955286
bird 0.9484185
chair 0.9311311
person 0.96452403
person 0.98107994
diningtable 0.9587307
person 0.9007021
chair 0.9480161
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.335s + 0.037s (eta: 0:00:23)
person 0.97924614
person 0.9140273
pottedplant 0.9305108
bus 0.9753802
diningtable 0.9533653
chair 0.9055938
chair 0.99011326
chair 0.90779275
chair 0.9678159
chair 0.9207003
pottedplant 0.9532858
person 0.93570995
person 0.9481207
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.333s + 0.037s (eta: 0:00:19)
diningtable 0.9552953
chair 0.9131392
person 0.9271226
person 0.9412053
bird 0.9446317
person 0.9448647
person 0.93627214
person 0.9732676
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.328s + 0.037s (eta: 0:00:16)
person 0.97132856
train 0.9494744
person 0.9500945
person 0.9489805
person 0.97033316
person 0.9077522
car 0.9106906
person 0.9087645
person 0.9157786
person 0.902529
person 0.90465313
person 0.953388
person 0.9909163
person 0.9840057
person 0.9507621
person 0.91430986
tvmonitor 0.94642395
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.329s + 0.037s (eta: 0:00:12)
person 0.9444645
person 0.9451476
horse 0.9126543
person 0.9587673
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.328s + 0.037s (eta: 0:00:08)
person 0.94200003
person 0.98263437
person 0.97978413
person 0.9206648
person 0.9047515
person 0.9694262
person 0.9005991
chair 0.9707009
person 0.94442195
person 0.9115688
car 0.94071347
car 0.96693134
car 0.90323013
car 0.94257635
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.329s + 0.037s (eta: 0:00:05)
person 0.9799555
person 0.9143977
person 0.95685446
person 0.9515824
person 0.9401893
person 0.98844737
person 0.95911205
person 0.97960585
person 0.9733444
person 0.90419453
person 0.93909204
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.324s + 0.037s (eta: 0:00:01)
diningtable 0.9354101
person 0.939996
person 0.99199706
person 0.98273796
person 0.9313684
person 0.95780766
person 0.9715461
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.409s + 0.044s (eta: 0:00:56)
person 0.9827397
diningtable 0.93672067
person 0.9587176
person 0.9275741
person 0.9005752
bottle 0.92813814
bottle 0.95862716
person 0.9225632
person 0.9831062
tvmonitor 0.9508169
pottedplant 0.95355844
person 0.95644045
chair 0.90501124
chair 0.9437786
chair 0.96833974
chair 0.97763616
person 0.9945826
person 0.98482436
person 0.97270244
person 0.95785254
person 0.9622909
person 0.93776613
person 0.9700345
person 0.9225652
person 0.9169825
person 0.9517983
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.335s + 0.033s (eta: 0:00:41)
person 0.9411276
person 0.9883307
person 0.9517276
bottle 0.97171146
bottle 0.9207086
bottle 0.9005685
bottle 0.9512127
bottle 0.91392094
chair 0.9349486
person 0.9144144
chair 0.9057489
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.327s + 0.037s (eta: 0:00:37)
chair 0.9727486
chair 0.92712426
bottle 0.9629884
person 0.92834795
person 0.9592525
person 0.9051454
person 0.9154783
bottle 0.95161456
bottle 0.97658396
person 0.9820477
person 0.953065
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.332s + 0.037s (eta: 0:00:34)
person 0.93022436
person 0.9530707
person 0.9360451
person 0.95879614
person 0.9442445
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.329s + 0.038s (eta: 0:00:30)
person 0.979074
person 0.9572115
person 0.9022877
person 0.9657142
person 0.92083794
person 0.97871304
person 0.93946743
person 0.9695858
person 0.9711159
person 0.9048963
person 0.90991396
person 0.96635735
bottle 0.972867
bottle 0.90122104
chair 0.9857314
chair 0.94683117
chair 0.9617739
pottedplant 0.9310177
boat 0.93036467
boat 0.9546724
person 0.91768396
person 0.94572765
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.325s + 0.037s (eta: 0:00:26)
car 0.98198307
person 0.9709456
person 0.9034068
person 0.9015708
person 0.941862
person 0.9725618
pottedplant 0.97844386
pottedplant 0.9445078
car 0.9918012
person 0.9197605
person 0.98728263
person 0.9431154
person 0.97675735
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.322s + 0.037s (eta: 0:00:22)
person 0.97691286
car 0.94772255
person 0.941983
tvmonitor 0.9532925
aeroplane 0.93836564
person 0.90793025
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.323s + 0.037s (eta: 0:00:19)
chair 0.95858294
person 0.9667673
train 0.94640136
bird 0.98117125
person 0.9381621
bicycle 0.9780611
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.321s + 0.037s (eta: 0:00:15)
person 0.9343696
person 0.92904437
person 0.93879634
person 0.9545693
person 0.96099985
person 0.97600454
person 0.9916516
person 0.96589714
person 0.991771
person 0.9364659
car 0.91920066
person 0.98746836
person 0.9566772
person 0.91123956
person 0.96583587
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.319s + 0.037s (eta: 0:00:12)
person 0.9057857
person 0.9031756
person 0.9116437
chair 0.9851451
chair 0.94477284
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.320s + 0.037s (eta: 0:00:08)
person 0.95807165
person 0.9691235
person 0.9261268
person 0.9786648
pottedplant 0.9697091
pottedplant 0.9164167
boat 0.95556587
person 0.92015964
person 0.952489
person 0.9329771
bottle 0.92615926
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.320s + 0.037s (eta: 0:00:04)
chair 0.9140782
person 0.9559305
person 0.9182884
person 0.96749675
person 0.91110784
person 0.9225911
person 0.9082366
person 0.90448564
person 0.9543555
person 0.94630677
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.322s + 0.037s (eta: 0:00:01)
person 0.904751
person 0.96759886
bird 0.95449585
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.391s + 0.041s (eta: 0:00:53)
chair 0.9102724
chair 0.9628405
chair 0.914074
person 0.9533358
person 0.9469651
diningtable 0.9471255
person 0.9436337
person 0.92209995
person 0.96204334
chair 0.9753361
person 0.9929085
person 0.97326547
person 0.98873967
person 0.9880592
person 0.9772679
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.317s + 0.036s (eta: 0:00:40)
pottedplant 0.95957494
bird 0.91264546
bird 0.95908904
bird 0.9717217
bird 0.90285546
bird 0.9112627
person 0.97444314
person 0.9249661
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.315s + 0.035s (eta: 0:00:36)
car 0.9016822
person 0.9090457
person 0.9633358
person 0.9501773
person 0.9655862
person 0.90287244
bottle 0.9638329
bottle 0.9311662
pottedplant 0.92237353
bottle 0.96263254
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.317s + 0.036s (eta: 0:00:33)
car 0.90707827
pottedplant 0.9661999
chair 0.9567829
chair 0.9319368
person 0.97954637
person 0.9151426
bottle 0.95303607
bottle 0.9767238
bottle 0.9326879
bottle 0.9622234
bottle 0.9313622
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.318s + 0.037s (eta: 0:00:29)
person 0.9326345
person 0.95606214
person 0.9357512
sheep 0.9223591
person 0.9933202
person 0.9060705
bicycle 0.93073195
person 0.97983825
chair 0.913938
person 0.9318048
person 0.93430656
person 0.9955035
person 0.9332474
person 0.9870547
person 0.9300487
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.314s + 0.037s (eta: 0:00:25)
person 0.9667418
person 0.9448029
diningtable 0.9635266
chair 0.99122846
chair 0.9706509
chair 0.9809832
chair 0.9021565
aeroplane 0.91865903
car 0.91653174
person 0.9767974
person 0.96759504
person 0.9361296
person 0.9375402
person 0.96502644
car 0.90913254
car 0.9792341
car 0.9557013
car 0.97266126
bird 0.9264231
bird 0.9261232
bird 0.9039782
bird 0.9538668
chair 0.9150338
chair 0.91921437
chair 0.9387626
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.319s + 0.037s (eta: 0:00:22)
person 0.9542256
person 0.9842764
person 0.97581744
person 0.99344844
person 0.94424844
person 0.91100574
person 0.98816866
person 0.94815576
horse 0.9427725
person 0.9670021
bottle 0.94286025
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.315s + 0.036s (eta: 0:00:18)
person 0.9254372
chair 0.97930324
chair 0.93344533
chair 0.9738599
chair 0.98409593
pottedplant 0.9642223
pottedplant 0.9815538
person 0.9326766
person 0.9136707
person 0.9579419
person 0.94244134
person 0.9253134
bicycle 0.97180825
bicycle 0.92216384
person 0.98544586
person 0.9553181
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.315s + 0.036s (eta: 0:00:15)
person 0.994879
person 0.9395936
person 0.9062588
person 0.9841233
person 0.9786568
person 0.9642856
person 0.9782949
bus 0.90375745
person 0.92038286
person 0.94528973
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.316s + 0.036s (eta: 0:00:11)
person 0.98392254
chair 0.94996285
chair 0.97293186
tvmonitor 0.93291783
chair 0.9759552
chair 0.97736496
chair 0.96026105
pottedplant 0.9034678
pottedplant 0.9419707
person 0.9327083
person 0.9283543
pottedplant 0.9145026
person 0.97543836
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.314s + 0.037s (eta: 0:00:08)
motorbike 0.97603923
motorbike 0.9011962
person 0.9031878
person 0.966674
horse 0.95125455
bicycle 0.9544975
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.315s + 0.037s (eta: 0:00:04)
person 0.95606196
person 0.97697693
person 0.948119
person 0.94535154
person 0.9130125
bird 0.93277836
chair 0.9767378
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.315s + 0.037s (eta: 0:00:01)
person 0.9763026
bottle 0.90586
person 0.93773377
person 0.98528767
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 80.633s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 387  368  367 ... 1470 1290 1903]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2889
INFO voc_eval.py: 171: [ 288  156  651 ...  646 1088  127]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.4835
INFO voc_eval.py: 171: [ 479 1200 1366 ... 2247  464  463]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2083
INFO voc_eval.py: 171: [ 699 2600 2176 ... 2439 2437 3425]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2397
INFO voc_eval.py: 171: [1220  726  791 ... 1407 1411 1410]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4014
INFO voc_eval.py: 171: [238 119 121 618  96 120 104  94 617 248  99 348 110 193 150 350 568 107
 423 550 126 124 230 349 505 218  45 621 254 402 598 315 506 554 122 382
  52 106 123 592 362 427 111 328 191 127 651 450 471 196 278 199 131 372
 595 492 299 103 580 420 615 102 553 286 444  24 171 269 144 268 192 160
 211  72 112 443 645 607 543 132 417 484 416 108 130 583  93 376  76 422
 197 231  92 162 459  25 100 630 185 447  18 101 184 438 148 403 561  28
   1 421 566 526 541 545 468  17 418 263 329 275  98 496 648 225 163 336
 352 237 187 393 170 347 562  95 486 525 324  97  20 239 181  73 285 109
 602 419 105 439 567  69 209 158 314 316 226 646 616 369 493 446 134 641
 359 276 540 515 485 411 370 149 548 591 143 412  33 544 643 653 507  30
 384  22 190 282 157 346 118 556 476  21 487 339  44 273 363 156 623 383
 233 612 509 637 321 140 628 200 338 345 341 288 330 325  46 202 147  65
 335 582 552 478 173 260 606 221 494 435 527 290 501 460 514 152 517 481
 495 159 210 169 319  57  35 473 584 186  31 270  53 644 642 361 532 272
 625 650 351 178 581 629 138 198 271 223 220 585 647 622 441 141 258 386
  82 551 194 154 297 303 116 518 294 180 334 153 167 469 619 575 353 565
 176 139 320 480  32 601 608 151 609 474  64  70 172 175 232 470 472  16
 207   0 203 274 404 354 500 479 475 360 542 327 415 579  67 626  58  47
 569 539 610 168  66 483 318 309  68  23 546 649 597 267 205 146 183 368
 195  59 234 451 445  56 593 512  26 201 632 405 408 594  60 259 482 498
  19 401 437 266 227 381 179 578 534 508 340 516 245 558 277 283 588  11
 599 627 293 457 533 177 262 182 428 161 449 547   2 652 113 458 455 503
 448 174  13 257  71 291 224 576 586 456 600 587 261 477  74 136 375 406
 378 155 292 333   6 453 252 425 519 442 564 125 208 317 264 164  27 145
 613 524 488 436 596  29 409 614 549   4 326 337 242 219 434 563 358 204
 142 400  80 165  14 206  61 513 364 129 557 374 388 529 216   9 128 255
  55 356 394 590 454  49  62 249 228 366 188 555 135 589 491 322 377 611
 265 414 379 452 215 521 133 605 373 497 440 222 410 430 389  63 523  37
 302 560 229  42 537 117 137 432 371 235 289  75 407 355 413 504 530   5
 396 531  43  54 365 280 279 357 639  10 631 236 391 306 559   3   7 528
 380 465 387 332 367 499 398 323 287  12  51 395 426 241 399 604 464 344
  50 281  15 284 343 240 301 577 502 522 114  48 310 385 331 603 633 298
 392 538 214 461 313 115 308 574 397 510 640 635 304 296 390 429 536 311
 166 520 535  83  78 305 573  81 431 570 295 424 511 307 572 300 312 189
 342 253 634  79 624 638 463 636 490 433 213 571 462   8  88 244 212 466
  41 247 489  77 217  85 467  87 256 246  39 243  34  38 251  40  91 250
  86  36  90  89  84 620]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4125
INFO voc_eval.py: 171: [1422  675 1364 ... 1516 1334 1337]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.3305
INFO voc_eval.py: 171: [ 70  27 344  31  25 291 290 142 148  72 341 101 198  71 342 110  75 100
 334 166  29 242  86 317  26  20 237 281 152  99 343 146  21 140  24 337
   3 275 158 164 120 165  73 150 339 224 119 138 316  77 177 105   8   6
  65 276 274 312   4 318  89 132 265  49  90 106 319 151  94  79  28 185
  30 323 125 161 121 273   5 102 200  32  81 336  53 255 302 304 346 228
 263 122 226 191 145 338   9 340 113 257  23 127 287 328 266 277 223 354
   2  67 124 310 134 246 117 239 225 126 227  85 278  35 118 335 251 159
 128 167   1 293 111  52 195  64 107 214 201 183  22 131 139  91  87 305
 248 301 143 289 184  62  11 324  40 194 261 135 315 352 262 256  56   0
 296  36 104 314 313  55 115 205   7  93 116 267 311  50 108 123 103 238
 252 149 271 245  10  51  74 137 264 329 348 114 160 243 240 309 349  37
 253 179 282 300 141 181 178 112 244 136 326 272 345 249 229 154 331 351
 247 295 220  58  54  63 182  88  18 147 250 307 144  34 130 188  38 180
  82 189 176  45  48 254  39 197 187 109 133 173 218 129  69 268  33 163
  12 353  78 279  80 206 202 190 157 303 308 199 196 330  19 241  59 259
 260 306  61 332  57 222 212 283 286 211  95 193 208 325  96  68 270 204
 172 288 186  66 350 203 299 327 192 294 174 175 207 155 231  76  84  47
 215 285  13 162 258 234 168  92 213  83 230 292 284 221  98 235 232  14
 297 233 156  15  60  46 298  44  16 153  41 217 280 216  43  17  42 171
  97 347 333 169 210 219 209 269 170 236 322 321 320]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0387
INFO voc_eval.py: 171: [1690 7117 3113 ... 5025 7752 7759]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3995
INFO voc_eval.py: 171: [151 148 243 471  24  76 430 375 240 467 464 454  31 137 391  37 136 153
  34  28  78  25 372 456 245  41 139 249 184 247 359 469 463 250 175  30
 217 373 145  83  16 371 451 361 453 394 123 380 138 466 440 194 360 206
 143 163 460 318 246 429  35 241 473 199 191 459 150 160 185  53 314 317
 465 414 472 242 146  79 236 393 142  73 210 174 389 234 457 248 161 149
  33 152 141 439 154 140  22 286 448 382 376  81 200 374  46 108  51 267
 180 470 111 298 432 381 449 450 215 198 378 398 315 461 176 103 230 221
  82 377 197 219  14 158 362 156 229  94 368  38 274 162 272  17 159  50
 228 216 452 415  54  86 468 433 379 395  63 220 455 316 144  84 147  75
 352  47 401  49  85 385 474 431 109 458 407 326 441  45 319 104 239 332
  89 128 370 287 110  27 324 130  43  77 244 412  20  87 428 411 155 399
 129 356  40 299 277 403 296   0  68 127 386  95 105 190 308 165  32  18
 425 157 297 192 195   8 107 337 366  66 435  52 351 355  80  26  64   6
 478 420 330 331 255 223 211  65 261  48  42 218 113 462   1 173 125 208
 421 369 413 402 294 106 224 400  67  74 202  36 187 477  88 295 434 213
 276 408 313 121 238 212 422 112 131 363 124 196 283 406  13 388 188 114
 419 264 387 417 222 179 263 309 207 384 262  98 327 135 177 251  15  44
 475 436 235 168 270 311 424 189 266 265  62 423 344 334 269 416 126 310
 201  21 349 203  23  39  60 132  97 275 392  12 345 271 281 350 273 390
 343 312 178 333 383  10 232 258  70 300 354  99 405 396  19 325 364 259
 409  29 119 346 335  69   2 182  72 367 169 134 307 289 321 418 284   7
 282 438 225 115 117 233 404 397 410 237 365 285  61 118 181 353 231 186
   9  96 320 329 171 133 116  71 288 268 101 100   5 437   3 323  56 193
 357  59 167 102 427 293 292 447 358 328 252 120 183 301 342 322 254 226
  11 476 166 290 257 306 209 347 170 278 253 445  90   4 164 172 122 280
 291 340 214 426  58 260 256 304 204 305 336 339 205 303 279 446 443 341
 348 227 444 442  92  91  93 338  57  55 302]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1295
INFO voc_eval.py: 171: [3026 1310 1436 ...  399 3266  402]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3603
INFO voc_eval.py: 171: [390 413 348 415 826 133 761 386 210  72 235  22 766 429 410 498 792 430
 777 502 350 333 762 135  59 832 827 201 205  57 768  19  60 267 760 264
 208 396 213 275 204 765 239 137 720 785 246 331 364 328 782 332 784 203
 739 164 394 230  20 128 801 162 665 664 781 273 773 207 336 351  61 833
 152 175 308 262 293 831 434 767 278 772  75 824 190 805  73 244 206 412
 813 789 645  26 469  64 200 395  30 397 138 125  49 571  69 590 770 322
 234 482 393 271 808 783 500 551 316 373 803 822 587 811 663 303 134 305
 526 525 270 491 800 226 277 723  74 216 566 245 432 620 408 595 222 180
 148 398 618 668 359 709  10 388 778 644 586 797 621 292 385 732 176 131
 387 828 389 549 718  76  47  77 130 202 776 391 136 580 231 670 740 775
 279 212 400  62 758 211 370  63 261 181  71 240 268 529 392 806 358 554
 349 807 338 523 360 192 715 834  65 486 266 157 754 600 117 272 786 319
 799 427 509 619 472  66 127 276 306 578 209 217 193 285 669  46 256 357
 589 763  28 242  25 752  92 572 493 464 640 534 524 516 829  70 356 154
 633 735 320 812 651 219 119 225 489 165  55 795 771 317 446 593 769 229
 788 228 790 503 132  67 622 774 750 416 588 497 113 304 362 299 191 223
 574 804 538 149 418 731 798 174 654 417 796 237 346 105 583 499  23 705
 118 627   8 598 142 674 594 687 764 579 794 565 816 838 623 495 582 419
 221   9 363 274 335 599 312 496  68 215 734 155 232 742 440 753  51 802
 810 793 163 224 584   1 522 561 641 713 407 787 609 710 342 823 711 227
 737 533 438 150 825 647 329 197 457  34  56 597 809 330 738 257 830 504
 539 791   2   7 263 233 369 214 568 596 751 759  48 310 315 733 284 243
 741 506 121 435 672  38 650 501 492 511  33 309 282 327 366 220 749 286
 608 528 722 626  18 114 218 295 536 745  21 631 607 642 301 178 345 409
  84  52 643 169 471  53 411 698   0 473 820 116 269 546 182   4 708 405
 605 530  80 591 521 821 321 365 404  88 179   3 613 581 281 575 347 401
 260 577 736 686 291 671 259 494 383 436 115 555 168  93 184 183 314 287
 653  86 414 297  83 441 141 337  85  89 283 156 324 126 611 468 703 648
 675 241 403 564 453  17 445 639 488 726 124 311 677 454 519  58 289 724
 120  13  78 122 352 296 280 585 512 444 727 110 714 717 716 247 676 688
 728   6 318 381 300 339 658  54 406 461 177 746 719 657 450  91 660 819
 562 426 508 556 437 505 480 353 323 109  16 382 507 443 697 814 139 700
 646 531 515  50 313 449 517 690 474 567 490 265 144 294 288 510 679 667
 729 458 542 129 576 258 552 106 707 343  15 573 361 520   5 543 151 198
 702 367 652 681 372 298 399 447 484 680 629 704 420 290 625 462 307 463
 421 238 725 682 334 302 194  81 250 483 532 167 673  79 557 537 439 236
 606 604 487  99 485 694 695 535 637 628 103 451 448 442 649 706  82 455
 123 341 384 730 563 185  90 422 638 431 712 744 696  87 602 559 601 656
 655 102 550 553 632 107 425 701  11 172 251 518 466  40 699 470 423 143
 748 603  14 254 475 145  37 196 159 721 835 527 459 634 815 836 624 662
  98 837 630 344 424 252 160 111 173  45  96 428 189 779 433 541 689 661
 659 171 452 481 456 513 101 548  97 612 100 560  35 544 146 140 377 756
 153 780 570  29 402 691 186 108 465 248 467 161 368 617 170 558 636 747
 685 616 166 477 354 592 340 683 479 249 355  41  27 112 610 325 374 614
 147  44 545 199  39 158 375 195  42 693 692 666 818 255 514  31  32 187
 376  24 188 371 678 615 460 253  43 379  12 684 104 476  36  94 326 635
 540 817 757 755 547 378 569 380  95 478 743]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1826
INFO voc_eval.py: 171: [607  84 547 237 238 112  87 619 487 616 609  91  85 485 394 460 566 119
 379 479 466 115  86 104 376 488 372 551 492  90 168 171 338 615 114  88
 483  95 243 489 613 374 463  94 484 413 265 491 375 559 371 501 481 610
 525  92 593 267 213 395 133 220  66 276  89 130 412 240 391 188 382 611
 570  14 169 468 602 217 345 549 272 181 132 215 262 234 591 493 480 160
   4 157 326 225 123  65 370 177 565 467 337  22 604 190 199 116 526 219
 122 553 385 596  55 639 621 218 260 378 216 356 209 271   5 470 161 592
 125  64 155 241 416 175 159 397 496 208 562 390 599  25  50 557 620 624
 622 597 558 322 494 600 165 163 482 377 346 603 176 384 414 573 572 158
 273 403 207 457 383 129 357 113 393  33 399 350 445 490 252 472 264 590
 430 172 204 340 486 640 502  63 561 595 189 162 301 625 277 579  48 214
 180 278 381 197 143 126  26 344  18 354 594  97 398 166 618 495 118   9
 182 268  40 458 544 117 283  69 156 555 351 236 339 556 598 154 282 247
 387 601 380 447 191 137 211 623 124 121 315 629 192 396 200 286  10 366
 605 358 266  29 404 173 244 127 270 402 563 388 524  32 424 389 365 518
 548 520 235 210 120  49 128 392 507 148  27 334 442 294 513 196 153  62
  51 511 411 335 386 638 510 454 233  52  67 373 284 576 406 102 134 359
 568 319 462  54 250  17 506 614 281 353  68 149 420 202   8 578 452 212
 183 230 201  59 170 581  44 336  58 433 167 438 150 279 464 459 434 536
 187  98 178 275 560 471 328  12 531 582 405  53 352 193 194  93 103 517
 325 522   0 432 444 641 174 131  16  19 617 324 186  28 451 418 109 612
 257 203   2 258 221 540 608 341 443 142 630  46 455 263 327 436 164  70
 106 504 410 546 274 580 342 231 529 606 550 269  77  30 179 368 313 355
 347 429 367 195 532 569 626 152 145  38 147 571 628  35 505 552 146 144
 222 303 407 627 305 554 545 249 631 519 440  71  37  20 408 300  31  21
 333 435 141 450   3 446 348 514 509 362 512 108 400 309  79 508 635 523
 297 361 206 503  96 111  45 296  57 574  74 421 285 304  47 537 329 533
  24 586 521 320 360 465 254 476 323 105 369 409 228 567 425 140  23 449
 431 312  80 439 293 498 248 401 224 543 497 107 564 139 295  41 256 477
 239 287 136 437 317 589 343 331 515  36  73 349 110 534 499  78 299 311
 255 516 292 448 302 298 575  61 307  60 259 308  76 226 253 422 474 535
 539 291  99  82 280 306  56 456 100 246 321 577 310 138 417 588  15 475
 542 184 289 415   7 500 135 245 223 101  43 363 541 364  13 185 419 205
 151  75 232 290  39 316 229 318 261 538 453  42 530  11 461   6 473 633
 527 441 469 478 227  81   1 528 423 314 637 632 288  83 587 636 634 251
 428 242 198 330  72 427 584 585 332  34 583 426]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.2882
INFO voc_eval.py: 171: [110 622  65 611 384 395 633 411 116 114 656 283 113 132 605 316 402 119
  72 285 122 606  67 139  41 665 135  71 273  83 470 465 387  40 400 669
 572 111 615 454 488 657 229 453 220 637 554 663 620  27 415 668  29  30
 401 399 530 307 640 121 639 118 333 456 426 412 198 157 315 305 506 487
 313  43 282 500 570 600  45 617 129 664  46 373 140 469 392 458  63 284
 393 123 126 349  78 608 335 275 660 226 417 222 524 655 125 382 558 311
  68 279 635 348 391 418 191 160 201 532  77 176 624  55 128 658 555 250
 662 423  82 574 171 141  81 394 461 420  66 599 468 424 493 390 577 234
 233 193 642 362 614 363 179 164 475 397 661 565 276  42  98 372 535 346
 200 274 159 218  75 580  24 528 670 277  69 356  13 666 117 124 364 352
 379  28 448 533  99 165 385  36 318 559 192 299 619 457 455 667 237 512
 175  73 484 353 581 146 133 163 644 646 659 170 414  64 413 359 278 345
 539 354 612 162  60  59 361 425  25 252 371 168 107 214 189 199 158 322
 317 336 442 360 466  85  54 497 485 590 607 281 386 236 609 144 429 398
 449  84 217 328 131 227 522 280 216 239 286 321 289 641 312 526 476 186
 323 181 213 671  93 374  44 137 601 325  61 138 531 149 534   3 174 104
 357 184  34 416 509 185 585 152 556 634 142 358 303 557 571 560 231 106
 130 166 350  90 224 173  53 190 230 248 495 260 568 212 441 241 474 180
 143  62 389  20 232 527  17  56 492 643 343 378  95   2 501 244 219  48
 439 516 499 647 223 161  39  37 109 431 187 112 177 215 221 267 478  52
 172 243 197 211 578 256 547 433 115 396 594 525 127 381 569 308 182 545
   0 452 338  35  79  94   4 460  96 327 446 251 645 210 183 430  16 587
 432 494 573 472  31 105 351  51  19 421 228   6 271 552   5 529 589 355
 225 523 597 156 108 151 404 255  18 582 584 242  26 428 520 245 195 537
 503 515 511 388 489 147 196  97  15 366 438 498 434  57 636 292 536 103
 583  88 405 100 505 188 368   1 459 254 576 508 269  14 473 575 365  21
 491 444 464 579 502 253 329  22 145 593 383 467 403 550 566  33 419 649
 347 610 504 562 410 407 268 632 408 586 136 309 150  47 510 270 249 265
 247  80 544 553 257 306 422  89 314 427 178 477 437 496 272 542  50 650
  32 540  49 538 246 155 238 507 204 369 591 240 483 595 235 259 486  58
 101 490 451  38 436 344 310 261 302 266 288 167  91 618 648 546 120 148
 588 294 567 564 440 406 471  23 134 463 330 301 450 296 375  76 409 209
 380 541 543 447 320 319 621 370 445 300 596 298 295 482 479 598  87 258
 263 264 604 443 326 592 519  92 297 304  74 480 376  86 561 435   7 169
 629 462 331 514 206 631 625 563 521 208 651 153 332 481 551 638 548  70
 627 102 518 652 517 154 367   9 342  10 262 513 613 623 549 207 616 630
   8 291 202 293 626 654 653 628 205 337 324 287 290 203  11 334 194 340
  12 339 341 377 602 603]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5555
INFO voc_eval.py: 171: [11083  6757   848 ...  1559  1558 12233]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5263
INFO voc_eval.py: 171: [3777 1090 2528 ...  552 3979  539]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4925
INFO voc_eval.py: 171: [ 28  24 384 375 244 227  21 242  37 228  41 239 376 371 230 425 373 367
  47 372 240 161  51 215 331  25 444 185  44 430 217 388 245 253 386  38
 284 434  42 433 183 188  93 224 336 378  49 209 387  22  98  36 369 216
 186 251 221 420 255  46 163 379 101 418 423 196  40 202 214 370 166 287
 424 169 335 158 232 118 254 439 285 124 377 219 220 187 184 382 167 147
 252 349 236 168 272 288 437  43 428 235  48 243 417  94 383 368 246 204
  95 380 171 283 334 438 111 435 222 148 233 198 443 218  97 108 429 329
 396 241  91 226 165 318 431  34 223 261 120 350  26 229 366 440 129  99
  92 364 206  65 195 234 436  35 453 302 170 237 360 122 197 419 442  32
 172 225 104 212 432 461 365  23 338 319 422 231 238 162  53  15 119 389
 426 427 127 410 128 355  27 102 275 356 114 159 142  54 441 115  96 157
 256 100 421 398  73  71 160 317 138 312 286 408 164 205 323 337 126 117
 107 352 190  52 400   8   2 452 320  62 390 200  50 393 463 325 112 144
 293 363  66 308 342  74 391 116 125 406 332 153 136 178 341 402  80 412
  16 345 143 397 351 213 123 454 385  19 191  70  45 353  29 121  68 304
  31 333 315 278 248  75 207 328  72 316 155  67  18 193 462 130  30 305
   0 208 150  78 179 359 339 354 445 464 140 176 189 395 314 247 374 257
 344 276  17 135  39 394 381 194 192 327  77 399 139 313 141 326 357 263
   9  79 282 414 459 266  83 210 137 358  59 455 343 330  69 404 307 182
 199 306 201 113 460 340 403 152  90 133   5 416 348  33 456 446 291 362
 294 303  82 321 181 203 260  20 361 347 415 289 290 292  81 156 151 392
 180  13 103   3 346 132 295  11 301 457 458 211 280 259 413 324 322  58
 264 173  84  57  64 269 149 401 300  14   1 409 249  61  63  87 134   7
  60 146  10 407 131 279 405   6 297 105 448  85 175   4 447 273 277  89
 110 449 106  12  86 451 281 154 309  88 177 296 268 258 270 109 450 274
 265 311 267 262 310 271 298  55 174 145 250  76  56 299 411]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2433
INFO voc_eval.py: 171: [195  70 198 189 203 278  68 463 325  66 193 191 275  10 201 231 380 161
 274 327 192 228  14 404 206 197 384 459 279 163 102 343 152  52  72  11
 349 273 272 235 340  51 385 348 460  81 376 328 159  47 366 368  53 229
 124 154 394 204 461 151 130  96 221 277 342 222 194 324 150 280 166  97
 326 308 345 232  65  69 233 101 344  71  67 321  73 207 234 347 168 171
 375  79 158  74 182 295 225 200 226 313 255 466  37 456  26 165 403 464
 142 174  35 128 169 257  95 367 121 185  98 296 332 377 462 373 230  48
 208  78  44 125  76 127 333 465 134 160 455 188 450 116 440 429 414  94
  92 337 386 162 388 444 170 104 164  46 183 129 363 256 306 153 458 289
 131 420 106 209 167 434 406 435  39  30  54 190  89 293 346 443 196 320
  38  77 341 381 393 417 223 383 118 175 413 457 205 408 259 469 407  28
 452  82  88 202 405 396 141 283 105 103 252 355 251 382  91 442 451 236
  93 288 199 220 353 146 422 260 468 247 470 147  86   3 109 110 135   0
  80  36 454  13 305 467 389   5  27 218 285 307  85 282  50  49 365   9
 441 120 334   1 145 290   4 315 126  63 184 354 372  61 224 423 390 426
   2 227 424 357  12 358  15 339 374 181 254 132 143 294 291 425  45 156
 219 329  90 292 419  75 213 421 399 266 144 172 310 318 133 298  43 284
  33  84 155 140  32 397 148 149 119  87 117 409 387 336  34 108   8 314
 265  83  42 240 276 311 432 309 312 446 416 123 253 437  16 453 242  22
 300 356 122 297  31 281 287 428 433 418 245 359  64 410 427 350 268 338
 431 237 400 246 262  41 248 430 369 316  40 364 211 286 395 322 371 449
 352 392 370 360 263 239 250 448 299 379 261   6 186 173 178 412 411 243
 378 317 415 331 249 214 351 401 176 447  23 217 330 112 258 271 445 398
 269 238  99 212 361  58  62 244 216 107 113 362 267 100  56 270 179  57
  25   7 180  60 402 137 391 111  55 210 439  17  59 215 177 319 115 187
  24 335 136  21 438 241 114 264 304 139 138 302  18  20 301  29 323 303
  19 157 436]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.2160
INFO voc_eval.py: 171: [ 96 376 627 628 124 109  98  95 635 207 390 419 380 125 178 432 313 293
 128 206 115  94 517 383 417 375 855 312 298 780 106 464 210 631 630 439
 386 437 864 706 523  89 215 389  77 427 297 275 871 420 314 295 490 211
 185 775 857 434  29 213 315 800 842 100 720 187 441 209 111 498  56 872
 711 860 890 898 414 431 520 415 533 385 653 229 515 686 504 247 108 487
 692 214 616 862 428 188 126 806 379 340 655 105  30 216 465  31  90 804
 179 827 756 467 869  13 287 652 556 578 688 205 611 197 572 524 710 381
 690 460 204 863  17 659 781 194 180 789  63 279 193 518 102 135  91 851
 165 202 335 175  67 296 801 687 294 704 365 843  81 217 502 122 881 462
 262 208 530 609 413 544 647 676 831 212 734 861 117 237 231 282  57 157
  33 184 220 703 489 508  62 359 239 766 416 870 878 170 230  84 868 337
 325 339 321   4 301 440 176 154 133 384 722 201 629 278 603 829 311 168
 225 897 568 341 171 854  75 867 367  43 567 236  41   8 805 852 265 242
 662 169 813 421 244 754 522 276 443 678 875 576 451 356 306 494 442 304
 602 134 436 382  70 198 461 807 651 526 412 233 430 182 177 146  72 740
  92 614 418 395 283 649 664 661 173 565 348 570 802 713 689 353 606  73
 896 536  22 903 433 850  88 123  76 269 156 305 174 707 721 718 121 459
 591 240 798 605 199 503 364 388 377 588 844 423  19 183 882 712  35 226
 163 351 816 373 303 791 435 685 590 553 219 879 669 172 803   7 200 232
 360 411 186 601 654 470 788 650 227 716 717 249 856  83  93 604 848 746
 114 899  14 438 167 677 675 280   3  15 767  59 366 858 144 865 560 866
 694 763 369 691 764 299 268 902   5 719 715 656 765 873 527 372 558 714
 783 708 319 345 192 849  99 742 253 308  10 657 607 750 671 491 302  24
 277 424 478 569 118 228 307 289 735 245  26 142 495 547 181 799 254  50
 559 620 739 901 723 463 370 859 531 545 152 797 286 584 107  20 469 284
 264 900 658 895 516  42 266 363  74 904 248 757 251 334 738 589 267 392
 883 499 634 129 795 577 624 333 422 476 830 191 447 697 819 352 397 324
 116 158 252 619 846 839 705 452 387 189 218 166 263 309 310  71 403 195
 361  16  21 110 327  68  97 679 639 794  86  38  23 768 145 466 357 281
 853  66 506 642 429 241 876 256 288 285 741  25   1 528 190 539 773 525
 646   6 769  40 543 770 238 445  55 613 743 260 371 566 702 450 472 501
 507 833 546 538 815 510 667 196 471 826 811   0 785 358   2 426 779 893
  39 378 243 709 519 222 737 316 593 246 674 473 594 368  54 151  28 347
 745 727 683 101 261 610 790 492 474 592 453 290 500 663 147 778 374 329
 259 762 497 112 120 529 521 203 481 149 393 555 300 349 782 822 744  69
 668 880 615 391 877   9 725 562  32 571 580  80 621 752 552 407 874 575
 482 557 682  11 587 684 673 796 541 150 493 771 787 235  79 793 759 250
  34 600 751 792 774 537 103 148 612 825 847 404 534 513 512 292 153 270
  87 670 119 561  18 608 448 425 273 104 761  49 574  85 648 532 477 155
 224 234 130 699 772 483 271 162  82 840 579 828 274 563 810 564 884 823
 818 777 159  51 362 113  78 599  60  53  52 496 821 136 784 809 753 638
 400 622 409 328 257 730 776  12 760 350 326 255  64 817 258 405 582 729
 446 161 726 660 140 643 585 160 749  58 291  37 617 731 475 583 581 680
 468 672 841  61 665 396 346  36 886 573 479 511 505 845 223 681 758 509
 736 586 786 535 755 836  65 887 598 514 700 444 323 812 733 480 457 484
 814 336 636 488 331 820 808 449 701 542 732 272 885 458 632 666 633 835
 834 221 406 338 623 637  47 748 891 892 698 399 401 410 728 402 394 322
 137 618 330  46 164 889 408 551 824 696 695 693 540 724 747 888 320 132
 455 625 894 837 626 318 486 548 131 485 139 838 456 550 554 596 398 143
 344 342 343 355 317 597  27 454 354 644 332 141  45 127 138 595  44  48
 641 640 645 832 549]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3114
INFO voc_eval.py: 171: [  9 671 528 ... 743 978 980]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4112
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3260
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.289
INFO cross_voc_dataset_evaluator.py: 134: 0.483
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.240
INFO cross_voc_dataset_evaluator.py: 134: 0.401
INFO cross_voc_dataset_evaluator.py: 134: 0.412
INFO cross_voc_dataset_evaluator.py: 134: 0.330
INFO cross_voc_dataset_evaluator.py: 134: 0.039
INFO cross_voc_dataset_evaluator.py: 134: 0.399
INFO cross_voc_dataset_evaluator.py: 134: 0.130
INFO cross_voc_dataset_evaluator.py: 134: 0.360
INFO cross_voc_dataset_evaluator.py: 134: 0.183
INFO cross_voc_dataset_evaluator.py: 134: 0.288
INFO cross_voc_dataset_evaluator.py: 134: 0.555
INFO cross_voc_dataset_evaluator.py: 134: 0.526
INFO cross_voc_dataset_evaluator.py: 134: 0.492
INFO cross_voc_dataset_evaluator.py: 134: 0.243
INFO cross_voc_dataset_evaluator.py: 134: 0.216
INFO cross_voc_dataset_evaluator.py: 134: 0.311
INFO cross_voc_dataset_evaluator.py: 134: 0.411
INFO cross_voc_dataset_evaluator.py: 135: 0.326
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.556s + 0.027s (eta: 0:01:12)
person 0.9488788
person 0.9006456
person 0.96331424
person 0.9878221
person 0.9534793
person 0.97032654
person 0.9205432
person 0.97933584
bottle 0.9516217
bottle 0.94656694
bottle 0.9291676
bird 0.9619264
person 0.90107274
person 0.9778632
person 0.9342591
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.329s + 0.035s (eta: 0:00:41)
person 0.9103016
pottedplant 0.9132086
pottedplant 0.91396755
person 0.90073353
person 0.9067988
person 0.9214273
person 0.96491
person 0.9156207
person 0.9096438
person 0.9695948
person 0.986172
person 0.92253196
person 0.9289839
person 0.9203402
person 0.9482487
bird 0.93001235
person 0.98216945
tvmonitor 0.9048284
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.324s + 0.039s (eta: 0:00:37)
person 0.9366393
sheep 0.91978496
sheep 0.9190071
person 0.93042904
person 0.9128366
chair 0.934437
person 0.9578116
person 0.9038333
person 0.9180317
chair 0.97253597
person 0.98240733
person 0.98845774
person 0.96779144
person 0.9919647
person 0.9873018
person 0.9344922
person 0.9197764
person 0.9052558
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.319s + 0.037s (eta: 0:00:33)
person 0.92096514
person 0.99408686
person 0.90900487
person 0.9817067
person 0.90055823
person 0.9865476
person 0.966294
person 0.9331441
person 0.90126675
person 0.90837145
person 0.959736
person 0.92126954
person 0.96210444
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.319s + 0.037s (eta: 0:00:29)
boat 0.923648
diningtable 0.93847513
chair 0.94761187
chair 0.9672117
chair 0.9449809
chair 0.90659827
pottedplant 0.907475
pottedplant 0.9377821
pottedplant 0.91084737
pottedplant 0.91173863
pottedplant 0.9294363
pottedplant 0.96252394
pottedplant 0.9524115
pottedplant 0.9476248
pottedplant 0.9297099
pottedplant 0.9436397
pottedplant 0.94424987
pottedplant 0.94393003
pottedplant 0.9390942
pottedplant 0.9058891
person 0.96431965
person 0.98346186
person 0.9839189
person 0.9860648
person 0.90108347
person 0.91010344
person 0.9469636
person 0.930231
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.320s + 0.036s (eta: 0:00:26)
person 0.90171814
person 0.9033674
person 0.90171874
person 0.94424796
person 0.90009624
person 0.9091645
train 0.92654186
person 0.9848721
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.319s + 0.037s (eta: 0:00:22)
pottedplant 0.9409191
person 0.93280894
person 0.9502983
motorbike 0.93982655
person 0.9607325
aeroplane 0.9497603
aeroplane 0.9168744
aeroplane 0.94343364
aeroplane 0.9182664
chair 0.98418427
chair 0.9848214
pottedplant 0.93854445
tvmonitor 0.9134712
bird 0.9936287
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.321s + 0.036s (eta: 0:00:19)
car 0.93472916
car 0.928889
person 0.9915441
pottedplant 0.94240564
person 0.9407849
person 0.9784252
person 0.9155872
horse 0.95313793
person 0.9403159
person 0.9588875
person 0.91684586
person 0.9665296
car 0.9708317
bicycle 0.9027845
bicycle 0.9840225
person 0.96631426
person 0.9622738
person 0.91785026
chair 0.90997756
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.322s + 0.036s (eta: 0:00:15)
person 0.97260714
person 0.94755524
person 0.96169615
person 0.9623234
person 0.9831336
person 0.9150734
person 0.97758627
person 0.9790962
person 0.98407114
person 0.9485626
person 0.98759276
person 0.96517396
person 0.9859766
pottedplant 0.9516188
person 0.9918031
bus 0.9174705
bus 0.9490824
person 0.9178854
person 0.9288232
chair 0.90839756
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.323s + 0.036s (eta: 0:00:12)
motorbike 0.9872248
cat 0.90596646
bird 0.9247852
person 0.91517085
person 0.9768803
person 0.93779236
person 0.9202885
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.323s + 0.036s (eta: 0:00:08)
boat 0.9830389
boat 0.9265655
boat 0.9342611
person 0.9408871
person 0.9920094
person 0.9791403
person 0.9567401
person 0.9879297
person 0.9849081
person 0.90525717
person 0.90720767
person 0.9345135
person 0.9160661
person 0.9344926
person 0.9453426
car 0.9522615
person 0.9069555
person 0.9822886
person 0.9757224
person 0.9383137
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.320s + 0.036s (eta: 0:00:04)
diningtable 0.9349373
chair 0.90327275
chair 0.97702926
chair 0.9329247
chair 0.9683468
chair 0.9896402
chair 0.9222313
pottedplant 0.93088603
chair 0.97078204
person 0.91446126
tvmonitor 0.90241385
boat 0.9249825
person 0.9451254
person 0.98039484
person 0.9052812
person 0.9431097
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.314s + 0.035s (eta: 0:00:01)
person 0.9798278
person 0.9384991
person 0.95184195
person 0.93963736
person 0.94173086
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.393s + 0.038s (eta: 0:00:53)
person 0.96281904
person 0.9693527
person 0.98004353
bicycle 0.9209906
person 0.9188841
bicycle 0.9856258
bird 0.93065906
bird 0.9118955
pottedplant 0.968214
person 0.9563049
person 0.9534107
person 0.9462805
person 0.9323563
person 0.9792078
person 0.92946774
person 0.9520728
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.308s + 0.035s (eta: 0:00:39)
person 0.9602572
person 0.9682673
person 0.97688454
person 0.96659786
person 0.9367114
chair 0.9041118
pottedplant 0.9047511
person 0.9042166
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.311s + 0.036s (eta: 0:00:36)
car 0.92657894
person 0.97105676
person 0.9179472
person 0.97002864
person 0.92795837
person 0.9088052
person 0.91161466
car 0.94402766
car 0.97537154
person 0.91442835
person 0.9155962
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.308s + 0.037s (eta: 0:00:32)
person 0.9734442
chair 0.9046971
person 0.91276383
person 0.92404306
person 0.95996743
person 0.9177652
person 0.90284514
person 0.9269199
person 0.9806989
person 0.9563791
person 0.98316634
person 0.97519016
person 0.90584046
bird 0.9905668
bird 0.9784952
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.312s + 0.037s (eta: 0:00:29)
person 0.95825934
person 0.9803853
car 0.9522615
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.316s + 0.037s (eta: 0:00:26)
bird 0.9363898
bird 0.9826926
bird 0.93267506
person 0.904824
chair 0.90120965
person 0.9224327
person 0.98344547
diningtable 0.9606106
person 0.93307465
chair 0.9279274
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.320s + 0.037s (eta: 0:00:22)
person 0.9782884
bus 0.98299074
diningtable 0.95921475
chair 0.943999
chair 0.98380697
chair 0.9615936
chair 0.9157899
pottedplant 0.90308446
pottedplant 0.9368954
person 0.937454
person 0.95611686
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.320s + 0.037s (eta: 0:00:19)
diningtable 0.9166269
person 0.9159765
person 0.93913865
person 0.9065721
person 0.90444314
person 0.9028599
bird 0.9579698
person 0.9393175
person 0.94464475
person 0.9075381
person 0.9653468
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.317s + 0.036s (eta: 0:00:15)
person 0.95696527
train 0.94329095
person 0.96009606
person 0.9005085
person 0.94634366
person 0.96477246
person 0.9144494
person 0.9078885
person 0.93133706
person 0.9411198
person 0.9892771
person 0.98117954
person 0.946555
tvmonitor 0.9244751
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.318s + 0.036s (eta: 0:00:12)
person 0.9439449
person 0.9374157
horse 0.9249313
person 0.95984
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.318s + 0.036s (eta: 0:00:08)
person 0.9759429
person 0.97452164
person 0.9088884
person 0.9031467
person 0.9256019
person 0.9821231
person 0.9014932
chair 0.9627365
person 0.9544858
person 0.9257756
person 0.9150342
person 0.9158058
car 0.9349443
car 0.96223325
car 0.9395766
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.320s + 0.036s (eta: 0:00:04)
person 0.97766393
person 0.9230448
person 0.9492885
person 0.94783854
person 0.94934404
person 0.9887783
person 0.9802807
person 0.9745419
person 0.9072086
person 0.9393522
diningtable 0.90343034
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.320s + 0.035s (eta: 0:00:01)
person 0.909226
diningtable 0.9506821
person 0.9498512
person 0.913063
person 0.9908221
person 0.9025015
person 0.9816861
person 0.9323593
person 0.953223
person 0.966075
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.352s + 0.039s (eta: 0:00:48)
person 0.9776388
person 0.9582456
diningtable 0.946274
bottle 0.9219536
person 0.9767428
person 0.93226355
tvmonitor 0.9253089
pottedplant 0.94323653
person 0.95635766
chair 0.92014515
chair 0.96825856
chair 0.96432686
person 0.99464095
person 0.9822694
person 0.9043442
person 0.96931386
person 0.9152597
person 0.95757663
person 0.90907836
person 0.9169575
person 0.95230824
person 0.9369786
person 0.9296672
person 0.95486736
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.314s + 0.034s (eta: 0:00:39)
person 0.9186108
person 0.9862917
person 0.95454603
bottle 0.96358746
bottle 0.9248029
chair 0.91748303
person 0.9180817
chair 0.9065963
person 0.90044916
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.308s + 0.035s (eta: 0:00:35)
chair 0.96508616
bottle 0.94702345
person 0.9220549
person 0.95736617
person 0.91783863
person 0.90093076
person 0.9003519
person 0.91502124
bottle 0.93013394
bottle 0.96202403
person 0.97997034
person 0.95616305
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.319s + 0.035s (eta: 0:00:33)
person 0.905508
person 0.93892306
person 0.94875735
person 0.9409384
person 0.95501554
person 0.94514394
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.321s + 0.036s (eta: 0:00:29)
person 0.97516453
person 0.95273393
person 0.9036738
person 0.9641891
person 0.9198514
person 0.9779338
person 0.92072767
person 0.95656204
person 0.913552
person 0.96801883
person 0.9089323
person 0.9539101
bottle 0.96331394
bottle 0.9052559
chair 0.97434694
chair 0.9465248
chair 0.90724623
pottedplant 0.9074677
boat 0.94031835
boat 0.9240373
person 0.95445853
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.320s + 0.036s (eta: 0:00:26)
car 0.97897685
person 0.97034836
person 0.9009158
person 0.93117964
person 0.96835405
pottedplant 0.9641325
pottedplant 0.9300533
pottedplant 0.9114672
car 0.9891138
person 0.92406416
person 0.9881417
person 0.97445434
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.320s + 0.035s (eta: 0:00:22)
person 0.9808594
car 0.94540596
person 0.94012475
tvmonitor 0.94055367
aeroplane 0.91105807
person 0.9104275
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.325s + 0.036s (eta: 0:00:19)
chair 0.9413444
person 0.96927434
train 0.9244747
train 0.9410841
bird 0.9791987
person 0.9316025
person 0.9033054
bicycle 0.9680719
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.322s + 0.035s (eta: 0:00:15)
person 0.9484848
person 0.9311386
person 0.9343363
person 0.95118886
person 0.9569804
person 0.9782689
person 0.98763216
person 0.95676726
person 0.9912662
person 0.90549874
person 0.92886925
person 0.9378259
person 0.9183733
car 0.92329633
person 0.97858787
person 0.950554
person 0.914883
person 0.96218437
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.318s + 0.036s (eta: 0:00:12)
person 0.91704684
person 0.92722243
person 0.912192
chair 0.98064274
chair 0.92246187
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.320s + 0.036s (eta: 0:00:08)
person 0.96252483
person 0.9217341
person 0.96035963
person 0.9106827
person 0.9758163
pottedplant 0.90787685
boat 0.9343329
person 0.94171715
person 0.92182314
person 0.9344752
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.320s + 0.035s (eta: 0:00:04)
person 0.95713097
person 0.93269026
person 0.9610174
person 0.9623492
person 0.9170727
person 0.94811654
person 0.9520176
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.321s + 0.035s (eta: 0:00:01)
person 0.91031754
person 0.96814996
bird 0.9495247
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.385s + 0.029s (eta: 0:00:51)
chair 0.90338594
chair 0.9543127
person 0.9496082
person 0.94881153
person 0.9069783
diningtable 0.93290126
person 0.9377443
person 0.95132536
person 0.96113265
person 0.9415727
diningtable 0.9212209
chair 0.97492677
person 0.9014185
person 0.9021285
person 0.99179596
person 0.97537255
person 0.9885876
person 0.9461441
person 0.924126
person 0.98529804
person 0.9723036
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.310s + 0.035s (eta: 0:00:39)
person 0.93943006
pottedplant 0.9484994
bird 0.955182
bird 0.97140616
bird 0.93496794
person 0.9686132
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.326s + 0.036s (eta: 0:00:37)
person 0.9007609
person 0.95763075
person 0.9531229
person 0.9395198
person 0.96642804
person 0.91047287
person 0.9078941
bottle 0.9535815
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.324s + 0.035s (eta: 0:00:33)
pottedplant 0.9558752
chair 0.94391376
chair 0.91291887
person 0.97974634
person 0.91615295
bottle 0.90187055
bottle 0.9592209
bottle 0.93798363
bottle 0.91361576
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.325s + 0.035s (eta: 0:00:30)
person 0.93989396
person 0.9497147
person 0.94478244
sheep 0.9104597
car 0.9180175
person 0.9939084
bicycle 0.91439724
person 0.97776186
person 0.9222559
person 0.9451563
person 0.9051313
person 0.9330067
person 0.99405056
person 0.9458421
person 0.98533833
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.322s + 0.036s (eta: 0:00:26)
person 0.9596246
person 0.953867
diningtable 0.9647868
chair 0.9418478
chair 0.9012035
chair 0.9338333
chair 0.9777555
chair 0.98629016
aeroplane 0.91859305
person 0.97304374
person 0.9684861
person 0.9352082
person 0.93415266
person 0.96088153
car 0.959228
car 0.9044729
car 0.97222805
car 0.96786135
bird 0.915235
bird 0.9171334
bird 0.95106554
chair 0.90500957
chair 0.9337307
person 0.9058946
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.324s + 0.036s (eta: 0:00:23)
person 0.9822527
person 0.9643344
person 0.9640363
person 0.9920105
person 0.9004084
person 0.9392154
person 0.9033299
person 0.9874822
person 0.9299533
horse 0.94303465
person 0.9612508
bottle 0.90852916
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.323s + 0.036s (eta: 0:00:19)
person 0.91759074
chair 0.9198957
chair 0.9807763
chair 0.97518516
pottedplant 0.9515455
pottedplant 0.93603486
pottedplant 0.9730819
person 0.9268758
person 0.9100525
person 0.95275897
person 0.94423795
person 0.92570996
person 0.907868
bicycle 0.95899487
person 0.9832677
person 0.9504864
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.319s + 0.037s (eta: 0:00:15)
person 0.99306524
person 0.931917
person 0.98014575
person 0.9773736
person 0.9670001
person 0.97638863
person 0.9424932
bus 0.9175874
person 0.90761036
person 0.9474234
person 0.90147656
person 0.92711747
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.318s + 0.038s (eta: 0:00:12)
person 0.9796331
chair 0.92939514
chair 0.9622774
tvmonitor 0.93333286
chair 0.96932554
chair 0.9669162
chair 0.94263226
pottedplant 0.92767483
person 0.954612
person 0.9279671
person 0.97287875
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.317s + 0.038s (eta: 0:00:08)
motorbike 0.9325133
motorbike 0.9675492
person 0.9190455
person 0.9611593
horse 0.9721409
bicycle 0.944961
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.316s + 0.037s (eta: 0:00:04)
person 0.9506206
person 0.9786188
person 0.9535672
person 0.95975345
person 0.914874
bird 0.9278715
chair 0.9262194
chair 0.9676501
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.315s + 0.037s (eta: 0:00:01)
person 0.9761854
person 0.92921084
person 0.98313934
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 80.473s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 356  364 1630 ... 1365 1193  102]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2779
INFO voc_eval.py: 171: [237 133 552 ... 925 926 928]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5137
INFO voc_eval.py: 171: [ 531 1362 1556 ... 3489 2304 2306]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.2005
INFO voc_eval.py: 171: [ 642 2066 2454 ... 2297 3223 3226]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2442
INFO voc_eval.py: 171: [ 506  626  573 ...   67  632 1143]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3904
INFO voc_eval.py: 171: [209 114 543 116 118  92 101 115 542  91 222  96 169 302 137 486  94 490
 120 547 303 500 119  95 203 527  42 445 372 102 211 444 277 194 173  48
 315  93 354 304 177 521 335 508 419 117 238 244 365 178 434  99 481 167
 473 253 266 316 393 398 524 108 170 492 366 579 369 286 513 429 103  67
 388 174 104 132 136 236 560 106 155 122 576  98 123 463 301  97 475 287
 124 327  15 437 540 532 242 479 498 210 417 395  19 373 234 162 356 347
  72 263 407 328 204  22 100 306 575 386  90 280 105 164 525 452 168 186
 466 146   1 367  12 577 133  69 147 239 256 107 152 139 489 276 476 368
 208 318 144 512 495 572 435  13 154 392 387  28 431 160 462 230  45 574
 297 202 252 361 406 320 488 371 401 138  27 176 538 294 240 450 453  17
 477 416 148 296 336 237 130 268 278 471 291 499 135 530 389 281 248 145
 518 447 362 140 179 231 198 298  66 196 551 284  68 442  14  61 531  21
 569 432  77 573 322 290 127 143 128  33 153 423 421 339 305 509 156  41
 359 163 550  80 430 491  53 158 485 418 424 504 510 580  63  65 207 517
 436 425  40 554  20 571 180 112  16 223 428 556 241 422 157 171 330 337
 383 493 193 552 151 364 259 184 456 555 289 314 229 534 427 522 454 285
 199 111 457 257 394 181 482 141  62   8 478 261   9  26 496  23 292 142
 377 201 557 455 219 216 420 228  43 391  70 461  55 183 172 233 558 526
 134 109  51 311 353  25 562 396 469  64 243 440 426 472 279 511 175 310
 274 467  76  58 357 159 474  44 507 578 295   0 192 334 344 385 536 390
  56 214 400 523 206  74 345 528 149 182  31 232  49 258 200  54 249 559
 205 480 255 226 514 494 515 221 235 449 293 516 505 319 355 541 195 441
 329 307 121 506 533  18  24 110 497 350 568 399 403   2 360 535 269 264
 332  52 265   5 185 272 404 520 402 443 483 250  57 384 539  71 113 487
 129   4 465 375  59  10 503 363 125 519 374 397 484 346  11   7 379  50
 300 468 267 131 273 161  46 537 246 288 408 126 460 358 348   3 262  39
 283 333 405 197 331 439  84 254 378 308  47 271 323 321 309 313 227 325
 245 438 324 326 275 529 413 212 376 317 548 370 343 282 270 561 251 312
 213 352 351 260  60  75 380 563 448 464 409 412 451 220 247 470 446 459
  83 189 166 225 565 458 190 349 224 570 564 381 501  78 150 566  81 338
  36 299 382 191 553 502 567 341 165  73 433  38 188 217   6 218 342  89
 187  30  85 340  34 415  32 545  37 410  88  82  35 544  79  87 215  86
 414 411  29 549 546]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4306
INFO voc_eval.py: 171: [1460 1397  678 ... 2169 2159 2168]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.3353
INFO voc_eval.py: 171: [ 69  24  22  28 310 262 308 261 142 162  71 135 110 309 182 101  82 105
 285 299  26 161 252  23 100 215  18 116  29 141 147 160  72  30 173 240
 287  88 284 140  19 249  81 288  27 230   1  52 212 143  74  49 115 286
  93   6 151 303 120  80 124 108 244  54   9 200 144  53 281 304 241 245
 307 255 133 201 271 231 163 234 239   2 243 122 125  25  83 131 204 129
   4 238  12  64 148  67   5 270  85 233  73 121 317 112 229 220 300 302
 218 273 232 158  89  21  87 295 242 289 279 213 119 184 202 301 306 145
 217  75 172 250 222 118 104 174  31 102 111 106  86 130  13 113 193 126
 296 305  95 282 127 264  84 228  99 177 136 274  61  51 269  16 227 203
 103   3   0 316 138  56  70 283 211  35 107 216 251  48 267 109 117 123
 280 150 146 134 223 225 219 260  20  46 114 290  14 199 254  91 195 175
  55 221   8  11   7 128  77 137 139 180 278  76 311 224 226 179 265 277
 197 253  33 167 132 237  66  36  90  92  10 178 293 246 266 247  62 291
 297 156  34 313 314  43 214  40  63 205 294 315 272  98  42 183  50 259
 292 186  94 275 187 176 170 168 154 236  32  68 181 169 155 171  17 258
 149 185 276  59  44 198  39 152 196  60  47  37 194  78  97 256  79 298
 248 191 257 153  45 190  38 235  65  57 159 192 263 157  15 312 268 208
  41  58 209 207 164  96 206 188 166 165 189 210]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0593
INFO voc_eval.py: 171: [1610 6711  804 ... 7352 7348 7342]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3803
INFO voc_eval.py: 171: [297 180 481  33 530 298 296 183 506 523  91 427  45  32 147  49  95 172
  43 175 438  37 525 171 293 261 187 411 227 515 176 301 424 410  19 249
 522  38 302 304  97  92 426 222 526  46 532 216 425 366 243 442 422 423
  42  67  41 431 413 535 198 228 253 305 235 173 200 299 524 487 505 509
 185 441 179 300 244 365 521 513 531 182 161 440 264  48  96  39  62 132
 287  89 498  44 294  54 111  27 192 130 292  18 100 464 177 246 167  52
 217 512 367 178 520 131 197 510 432 445 149 102 186 133 188  64 414 160
 270 279 103 257 245 123 428 336  77 496 113  60 429 236 174 194 349 368
 239  20  56 265 412 165 517 193 529  98 485 446 289 242  68 184 281  34
 259 127 518 516 125 327 421 450 199 381 507 369 195 196 189  36 491 508
  63 453 124 295  99 325 380 104  53 233 527 105 108 533 479 163 211 467
 528 534 223  17 191 434 492 128  57 480 158  29 350  78 416 454 430 148
 231 221 382 375  31  58 403  47 383 166 456 181 254 483 212 203  51 136
 511 362 266 497 303 514 280 463 152 337 461 271 230 263  66 112   8  84
 190 251 482 358 444 408   6 215 322 255 122   2  21  80 351 448 406   0
 126 156 135 465 267 256 224 155 260  93 435 402 168 272  69 159 328 316
 484 542 258 285 129 347 331 451 345 359 206 268 150 493 475 291 333 343
 457 306 472 470 232 107 389 346 164 134  79 344 106  55 478 144 262 241
 269 407 384 101 364 486 250 153 363  40 488  65  26 154   9 541  90  94
  76 313  83 433 146 240 319 417 419 443 121 151 321 220  35 310 141  61
 473 116  14   1  30 466  59 437 361 536 318 400 157 519 360 348 460 114
 162  22   3 540 377 489  75 371 317 324  88 320 226 378  24 471 396 474
 385 117  11  15 395 376  16 118  28   7 312 138 326 286 229 490  87  25
 449 282 439 139  50  81  23  10 290 405 356 213 284 283 143  86 404 469
 504 447 386 218 137 210 169 329 307 332 315 219 458 372 205  85 455 207
  82 140 338 494 357 335 209 420   4 409 401 274 170 452 462 503 495 275
 323 353 418 119 120 334 459 115 225 277 436 387  12 477 238 379 341   5
 352 374 355 311 145 370 373 342 252 201  74 288 415 202 234  72 309 208
 214  13 204 538 314 237 142 468 354 388 476 248 247 397 308 537 394 109
 539 339 330 391 273 398 399 340 502 393 276 278 392 110  71 499 500 390
 501  70  73]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0943
INFO voc_eval.py: 171: [2923 1263 1319 ...  181  387 3170]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3485
INFO voc_eval.py: 171: [443 469 397 369 919 152 470 465 484 437  85 266 853  71 400 570 229 466
 156 439 854 886  74 212 923 485  28 920 866  64 806 535 927 473 855 672
 398 880 380 852 145 929 226 442 230  26 646 295 275 374 309 300 234 232
 448 487 161  88 151 922 878  24  73 328 268 889 231 360  65  70  86 915
 875 261 732 828 256 379 158 248 486 228 199 154  25 868 811 491 755 542
 296 623 897 426 150 601 668 468 329 644 157 188  89 445 859  84 377  75
 489 870 531 215 883 280 381  79  23 756 344 577  63 307 444 858  55 901
 392 237 225 571 343 378 279 799 483 805 447 305 857 764 461 631 302 844
 917 918 411  51  14 627 354 244 293 548 271 603 842  72 187 342 262 449
 440  76 258 754 218 640 441 904 930 143 311 849 888 260 233  67 874 358
 703 357 677 864  33 200 899 648  30 701 712 399 931 657 303 239 252 410
 472 245 463 409 832 259 202 684 704 438 669 155 471 464 492 146 649 765
 879 566 181 128 891 729  78 423  83 558 679 681 726 843 310 317 895 796
 831 301 670 833 666 446 238  80 353 543 802  87 898 253 176  77 663 821
 561 884 451 159 724 935 892 600 680 650 319 474  54 890 160 384 370 541
 132 496 573 263 297  35 287 269 763 407 304 822 594 494 554 816 769 250
 924 153 671 173 809 766 728 839 227 856 217 406 637 546 376 740 678 910
 267 545 454 767 221 499 655 702 575 607 682 393  69 508 198 900 614 288
 249 793 450 255 251 235 204  90 736 568 458 934  52 887 807 242 850 933
 538 861 308 321  19 584 862 794 851 213 572 578 130 641 564 405 509  81
 140 355 830 667 147 265 349 777 581 687 730 417 792 705 101 863 149 164
 731 356 183 536 576 216 214 206 394 725 345 562 797  53 909  56 916 823
 257  82 272 928 651 820 467 179 894 604 190 457 346 643 338 180 412 719
 860 205 896 664 236   5 547 247 510 893 585 746 264   6 320 390 493 692
 738 653 134 903 475 865 760 574 375 801 902 533 579 580 885 829 372 368
 826 596 602   4 804   1 174 827 727  68 408 673 133 582 921 246 688 330
 322 814 118 241 383 560 207 761 240 762  11 926 841 642 693 869 371 913
 462  43 569 373 175 615 610 733 914 456 144 925 750 192 326 867 243 656
 290 665 721 685 735  91 314  32 459 521 126 386 294 292 748 334 825 824
 658 838 488 490 537 694 131 129 810  62 532 497 775  27 299 127 148   9
 167 201 589  17 674 708 636 632 270 706 340   0 759 630 742 359 834 361
 364  12 422  42   2  66 524 396 757 138 313 110 565  13 645 647 778 771
 163 505 135 514  95 367 583 142 324 141 453 306 385 222 337 567 413 523
 517 652 455 803 298 606 189 539 339  60 689 871 363 436 104 737 779 316
 203 593 401 877 429 817 219 276 318 661 362  31 734 556 625 506 776 683
 598  10 616 112 836   7 660 352 325 184  21 722 787 348 312 460 690 749
  96 798  58 137 586 800 289 638 629 511 102 815 882 331 512 518 772 476
 595 421 519 819 291 350 710  97 818 121 881 277  92 278 608 906 707  99
 336 452 273 274 495 323 114 747 633 549 210 335 662   3 781 347 195 563
 500 808 743 315 162  94 503 477   8 480 908 434 177 905 327 332 333 654
 634 597 166 770  18 659 185 872 109 526 117 715 795 611 618 741 846 626
 907  61 840 282 555 791 639 609 540 599 482 720 182 402 591 550 617 507
 435 223 873 341 635 388 387 211 713 111 105 686 113 414 544 481 788  15
 745 116 479 522 139 520 108 382 115 780  57 612 559 768 714 932  20 178
 624 716 515 418 478  93 123 691  59  22  50 786 498 136 675 208 516 785
 416 186 124 613 557 628 501 790 106  98 837 433 739 605 752 431 284 695
 107  39 534 513 430 193 812 621 784 835  49 552 122 711 717 100 432 419
 165 744 700 502 789 813 197 753 191 224 391 751 194 196 168 254 220 283
 504 424 697  46 389 170  48 281 403 587 395 525 527  29 698 425 365 286
 592 171 709 723  45  36  34 699 404  47 120 774 696  41 773  16 551 125
 420 912 553 783 848 590 285  38  44  40 351 209 172 718 758 676 415 876
 847 119 620  37 782 169 845 588 366 427 911 619 529 428 528 530 103 622]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1873
INFO voc_eval.py: 171: [703  95 635 288 289 708 132 722  97  96 103 565 566 156 460 471 654 538
 541 295 138  98 725 136 559 113 453 713 100 392 442  99 637 117 202 568
 718 204 134 726 646 104 569 540 573 562 110 112 101 711 196 111 447 313
 469 542 217 683 266 263 290 203 608 153 548 399 564  75 445 570 492 578
 269 647 446 472   2 661 271  74 415 572 328 695 207 318 560 391 653 214
 226 154 421 416 270 638 209 476 468 323 254 482 186 479 380 714 274 143
 224  13 700 145 394 563 691 312 493 641 420  72 441 649 287 681 525  27
  39 237 645 332 686 205 741 115 660 457   9  71 320 268 657 692 643 142
 311 208 395 398 305 183 188 470  61  24 602 455 152 299 419 567 502 609
 452 480 694 557 400 475 187 449 687 730 466 690 561 133   6 218 490 712
 158 571 474 393 688 604 648 135 451 481 253  55  57 280 426 454 240 487
 668 459 265  76 210 242 192 729 508 448 685 697 252 699 534 684 325 429
 319 640 574 151 680 131 190 336 150 189   5 120 373 264 707  29 170 322
 491 139 105 535 229 123  54 728 650 706 512 141 148  40 334 267 140 236
  81 225  38 389 463  28  70 247 197 330 484 182 427 682 206 397 351 244
 302 417 191 631 425 696 211 185  47 537 473  79 199 462  77 255 174 444
 119 734  73 335 670 424 230 144 235 483 526 592 294 555 213 435 106 693
 149 241 238 539  83 358 375 331 256 159 731 219 157 715 651 679 488 527
 617 721 422 458 184 315 339 198 724 658 284 163 740 689 176 698 388 361
 378  23  32 588 181 732 443 401 642 489 258 636 727 396 231 485 450 585
  18  82 515 276 524   3  21 316 285 656 130 655 610 291 381  56 723 414
 126 379 317 272 390 456 497 411 367 245  30   8  80 257 543 716 477 710
 719 352 519 329 486  59 717 659 644 581 195 418 215 465 464  87 618 137
 705 591 201 423 146 594 545 232 168  78 440  20 124 536 179 667 246 666
 155 118 248 169 671 356 147 652 405 410 412 461 704  22 227 672 599 297
 547 326 742  37 662 178 498  64 409 212 500 702 606 216 523 343 579 372
   4 504 709  52 514 171 243 554 239 467 413 309 669 223 273 193 663  65
  19 544   0 327  69 177 376 520 589 172  58  60 234 720 622 321 173 310
 614 516 431 175 296 733 222 262 277 513 439 233 200 282 619 228 108 634
 348 600 360 342 601  88   1  89 109  36  26  25 122 639 403 314 407 303
 324 127 735 664 558 107 530 517 584  45 607 349 357 428 528 510 362 633
 627 518 363 620 521 387 590 260 259 430 509  44 125 355 251  42  67 549
 603 511 374 577 529 261 350  34 597 596   7  90 162  51  68  33 116 354
 353 478 292  15  84 402 630 438 436 404 408 382  66 437 347 165 587 605
  53 624  31 701  16 307 621 102  43 586  12  10  91 333 406 580 675 359
 501  85 308 739 583  35 281 674 665 114 593 595 346 300  14 582 496 129
  48 121 301 576 279 632  50 338 298 364 505 556  62 673  11 337  92 283
  63 434 286 278 575 344 161 598 166  86 345 550 128 678 531 164  46 553
 432 384 677 552 293  93 623 433 377 167 275 533 546 249 532 495 625 737
  17 180 551 611 503 499 221 626 160 628 629  49 220 736 371 676 368  94
 370 366 616 613 341 615 194 494 306 522 340 365 612 250 369 738 304 385
 386  41 507 383 506]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.2894
INFO voc_eval.py: 171: [ 85 491  48 477 297 307  90  91  87  59 320 213 515  54 494 112 234 215
  28  96 305 473 315 365  50  95  56 116 495 298 206  71 452 514 367 119
 474 326 300  27 482  92  86 438  32 383  94 170 109 520 314  36 352 171
 449  16 322 253 480  14 496 353 175 101 357  64 299 329 355 440 369 518
 371 100 336 216 356 412  97 392 256 237  99 519 470 158 321 131 308 302
 229  34 284  30  65 395 208 523 214 354 103 264 406  58 457 525 366 230
  61 240 212 328 141 242 111 306 385 492 293 265 268 521 476 453 485 135
 270 388 187 210 233 117 132 159 516  68  63 278 441 304  49 331 527 483
 425 113 120 136 413 323  25 486 146 472 188 110 107 524 526 455 334 460
 469 124 374  46 414 373 478  13 346 522 303 239 207 209 475 140  21   4
 276 122 178 105 275 415  55 154  98  82 316 335 517 174  42 458 183 283
 291  88  35 166  41  62  15  60 151 168 274 169 427 266 503 133 421 416
 167 211 181 260 226 263 439 180  51  37 411 313 430 506 267 309 342 484
  31 152 507 273 451  43 147 408 409 379  81 186 292 144 243 149 389 324
 236 255 139 134 327 155  29 508 285 165 363 143 282 443 310  17 397 509
 176 351 191 325 271  33  24 501 390  39 333 402 345 179 114  74 277  47
 125 142 417 199 358 123 192 172 360 442 398 204 150 262 177 394 129 468
 332 250 138 450   9 364 244 393 436 410 400   0 218 311 459  52  75 359
 344 173 145 349 317 205 189 384 448  89 106   3 261 338 269 272 337 418
 227 232  73 505  84  20 153  44 137  26 254 382  19 387 182  69 391 431
 437 201 157 184  22  53 493 104 380  76 454   8  38 467 420 446 330 121
 456 126  18   6 312 219 497 185 463 301 251 424 108 196 202 289 195 407
 462  83 479 115 348 428 396 102 339 464 461  11 190 200 378 405  10 203
 197   7 193 296 429  67 224 228 423 290 118 488 343   5 347 279  70 223
 198 419  12  40 245  45 241 422 222 148 377 249  93 128 238 319 386 362
  23 465 466 318 220 295 130 399 426  77 444 231  66  57 162 500 372  80
 225 404 341 280 281 163 350 370 487 498 252 248 471 447 368 235 376  72
  78 381 510 499 481 160 340 490 247 375 489 127 435 164 403 401 361 504
 502 433 217  79 513 445 432 286 287 434 221 294 161 512 194 511   2 259
 156 257 258 246   1 288]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5671
INFO voc_eval.py: 171: [ 7604 12307   952 ... 13705 13703  9589]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5267
INFO voc_eval.py: 171: [3336  960 2215 ...  486  499 3500]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4669
INFO voc_eval.py: 171: [ 32  27 454 270 443  24 285  40 274 290 444 276 439 434 451 279 497 192
 442 394  54 438 261 216  26 292  55 343 440 306 262  28 455 503 519 112
 294  60 278  51 415 504  43 215 233 219 277  98 445 244 309 291 101 263
 195 492 436 304 495 133  47 305 198  35 199 490 107 498 242  44 346 437
 402 138 283  46 265 288 284 515 407 260 308  99 345 522 296 201 203 264
 403 446 282 113 496 435 350 217  57 135 289 508 514 100  50 383 396 413
 512  41 342 243 382 450 190  73 108  97 174 501 460 280 231  38 232 494
 189 511 295 125 266 173 143 447 281 499 202 516 102 229 509 466 393 247
 139 506 517 148 134 457 197 130  45  56 114 355 465 315 275 267 422 268
 136 544 427 193  31 103 230 116 532 491 533 420  53 256 521 432 431 505
 518 424 332 124 513 164 209 131 344 428 401  23  74 273 507  37 287 188
 489  78 146 106 272 399 200 245  17 117  75 502  61 115 475 480 381 269
 500 271  49   2 286 307 196 191 293 175 458 186 406 510 520 347 171 453
 147 493 547 140 129  25 194 425 123 353 220 377 246  70 482 105 298  10
 310 463 238 397 236 400 398 172 104 144 459 388  20 162 141 299  77   0
 163 416  33 478 185 137 536 211 142 132 234 248  42  81 429 523  52 456
 469 409 224 128 369 165  79 155 354  86 365 464 534 470 370 218 539 235
 228 317 311  30 258  80 419 421 336  76 227 468 546  39 166 223 338  21
 545 473 408  64 150 222 390 392 301 208 250 297 467  68  11 159 180 417
 225  22 441  58 300 391 179  18  84 122 253 156 418 452 430 449 379  95
 462 241 366 252 226  89 389 378 481 214 537 404 221  88 182  67 145 386
 157 149 109 259 538 524 327  36  83 367  87 405 426 319   5 535 210 487
 212 330 237 448 542 461 528  85  59 411  34  19 484 410  66 395 541 543
 356 339 540 348 249 240  12 368 187 351 168 357 255 360 479 349  65 257
 154  16 183 412 239 423 352   9 341 167 329   1 312 320   3 170 176  29
 485 384 359 474 380  63 472  48 313 314 213 483 414 254  69  15  96 184
 127 525   8   6 120 337 324  14 302 153 358 486 151 110  93 476 206 477
 322 385 333 111 387 375 373   4 204 181 362  90  92 118 321  71  94 178
 119 326 323 471 335   7 205  13 152  72  91 361 364 363 177 433 488 126
 303 121 160 169 334 161 328 318 372 529 316 340 331 530 531 325 527 371
 251 526 158  82 374 207 376  62]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2706
INFO voc_eval.py: 171: [160  56 162 168 243  57   7 159  62 161 244 286 197 242 202 240 165 287
  58 333 304 393 329  15 241 327  83   9 200 296 136 245 275 303 351 288
 204 334  43 239 198  28  42 394 196 285 298 158 129 391 163 386 122  61
 190  76 199 300  44 140 340 101 135 195  77 289 128 194 396 318  82 201
 112 301 176 397 284 392 319 105 130 139 320 226 167  60  95 278  14 175
 126 146 299  27 371 385  40 246 228  70  25 302 138  54 141 389 203  99
  41  59  55 164 395 270 171 291 324 227 107  63  65 111 106 127 373 177
 225 144 151  38 305 143 294  96 142 262 352 133 170 315  78 193  22 326
  37 102 113   8 137 173 247  10  75  64 153 192 398 297 260 376 166 172
  92 363 134  30 377 295 366  23 132 335  86  39 342 360 308 256 290  73
  68 103 251 283 229  45 292 311  31  84 380 125 191 379 381 223  11  87
 252  13 346 152 390 121 312 400  12 353 108 399 341  67 174  72 330 388
 169 350 356 114 383  66 355 349 358 257 182  36 189 266  88 109 309 310
 272 269   2 314  74  29 183 280  98 367 277 205 104   5 217 370 281  24
 233 259 100 332 187 273 325 124 231 365   1 150  26 248 344  19  97 145
 258  52 338 369 384 263 331 337 354 206 154 155 279  85   0 261  69  71
 316 156 110 321 345 382 271 336   3 276   4 188 215   6 123 317 359 374
 274 216 147  34 184 222 232 237 224 253  35 306 293 209 368 120 323 364
 313 221 234 264 343  53 328 362 254 255 249 210  50 208 372 220 357  33
 375 267 186 265  32 307 213 181 361 230 250 378 282 322 218 207 211 214
 219 347 212 149  91 178 235 115  90 238 185  46 180 236  81  49  17 348
 179 339  51 116  93  21 148  48  47  80  79 117  94  20 118 119 157 387
 268  89 131  16  18]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1722
INFO voc_eval.py: 171: [345 579  84 578  92 112 350 113 584 287 349 580 188 166 127 399 267 193
  89  81 387 189 356  87 483 344 286 581 765 384 106 268 190 453 406 705
 195 431  86 192 357 271 288 783  79 593 776 117  96  66 403 170 648 289
  25 108 654 174 401 407 492  98 601 378 585 784 460 754 714  45 726 662
 722 634  91 408 100 768 124 280 479 115 298 380 199 451 184 264 270 347
 114 183 484 653 632 721 274 803 352 566 388 605 397 191 703 163 561  65
 197  24 348 792 226 766 129 355 482  51 812 308 379  77 637 269 261 101
 763 456 602 743 194 164 775 631 402 598 208 623 607  14 531 273 425 433
 782  71  54 300 181 481 465 314 789 779 121 424 489 755 510 329 769 282
  44 147  26 658 254 391 426 196 159  75  76 210 337 278 647  49 491 160
 689 745 172 524 772 257 187 475 276  28  69 177 161 428 737 224 146 557
 723 502 351 219 720 550 761 389 781 552 218 162 392 272 669 725 466 810
 610 686 215 154 201 294 804 473 744 528 732 688 404  62 685 710 583 167
  50  60 521 313 646 291 277 376 377 715 692 165 259  12 198 284 660 171
 635 599 217 630 706 762 604  53 110  83 780  33 306 724 684 221 186 360
  99 400 222 383 416 564 409 315  80 617 467 645 656 169  93 309 144 452
 663 785 200  85 673 718 229 316 214 655 486 609 248 156 182   2 809  19
 790 611 649 354 185 811 624 542 518 693 258 145 405  63 325 427 520 238
 213 633 211  78 773 398 157 175 429 504 346 786   5 793 652 225 554 324
 390  70 138 597  64 555 661 487 116   9 563 322 603  16 651 435 553 241
 385  58 691 251 468 606 622 556 131   3 333 262 813 674 339 549 375  18
 111 704  30 795 279 571 328   7 699 342 340  95 283 788  15  29 619 228
 103 778 153 469 680 230 514  56 719 178 690 255 457 759 760 212 777  59
 173 432 519 529 537  22 657 695 791 209 105  20 326 343 608 275 532  82
 796 310 445 151 476 511 694 717 430   6 142  88 574 659 363 496  32 500
 436 168 158 232 462 758 335 179 616 243 814  61 204 731 733 770 794 123
 615 490 503 629 815 155  67 240 109 672 413 570 513 334 263 764 353 338
  74 223 237 260 730 650  55 472 477 508 227 180 628 756 242 774 330 787
 752 382 620 381 176 301 371 220 417 488   0 671 539 239 441 411 523 253
 205 440 636 250 509 711 757 104 507  73 573 713 712  97 414  68 252 341
 336 501 459 682 437  10 767 233 234 332 203 613 626  46  13 546 141 107
 771 600 735 202 742 395 558 716 551  42 135   1 516 455 285 393 415 464
 729 281 458 527 676  38 140 543 727 236 119 797 540 256  43  47 738 548
 358  21 734 418  11  94 560 102  72 368 474 708 677 369  57 572 559 736
 696 495 249 207 640 741 246 143 443 150  90 471 373 292  31 295 681   4
 266 448 582  48  27 216 575 244 517 439 530 139 321 701 235 231 627 525
 675 290 569 748 753 740 485 480 394 589 739 245 526 319 454  17 538 698
 697 512 687  41 666 515 700 625 533 664   8 463 331  52 265 152 307 434
 534 447  40 621 366 545 446 396 372 547 361 136 567 386  39 562 148 320
 370 639 679 642 323 362 493 293 478 412 410 618 522 461 707 577 588 683
 612 442 494 444 568 751 438 670 709 728 702 668 420 299 643 122 799 498
 591 614 374 470 497 499 800 296 644 367 678 806 750 317 449 118 422 576
 638 318 364 798 149 535 641 801 665 206 802  36 423 247 536 565 805  35
 596 311 746 312 421 667 365 541 304 587 128 808 297 505 749 595 359 419
 807 305 450 132 544 126 506 125 134 303 302 133  23 327 130  34 120 586
 592 137 594  37 590 747]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2895
INFO voc_eval.py: 171: [597 870 466 398 107   9 238 595 387 662 778 401 402 624  71 485 701 567
 630 625 223 482 474 821 882 105 846 672  54  68 615 604 666 335 155 872
  69 738 671  70 145 609 522 467 516  10 275 400 225  89 628 517  18 334
 303 663 230 144 670 674 404 257 724 106  66 480  35 312 737 361 150 195
 484 619 207  56 241 664 215 475 202 871 192 906 679 359 566 844 887 471
 381  15 707  62 705 680 775 456 353  13 758 568 270 572 464 709 523 420
 530 907 873 458 148 682 890 254 809 760 340 793 891 500 169 403 240 834
 276 433 648 409 224 622 481 824 845 428 365 222 781 652 491  59 273 194
 263  40 833 427 898 750 673 596 348 118 399  26 356 536 388 776 847  37
 665 205 476 473 313 434 901 152 617 310 146 631 479 424 858 860 445 459
 636 884 311 783 358 669 602 539 681 295 831 667 173 807 444 599 763 678
  75 759 677 147  27 899 457 255 175 697 654 174 634 825 432 676 431 703
 199 271  61 468 520 191 739 462 822 905 823  72 876 653 368 749 532 576
 394 746 573 857 296 336 502 116 355 203 584 790 910 875 668 453  67 320
  97 896 613  52 237 122 366 893 212 538 699 189 886 115 762 436 117 540
 234 439 338 343 773 710 806 570 874 443 892 675  88 741 641 269 352 308
 341 251 537 756 357 109 832 561 904 761 319 569  50 469 111 413 531 503
 524 787 632 229 913 912 266 351 440 533 747  73 211 528  65 881 908 718
 797  29 181 461 278 450 614 698 216 885 210 700 693  38 407 649 808 419
 378 460 841 564 317 552 627 655 376 331 277 102 272 696 134 112 723  14
 843 911 521 478 177 751 611 853  41  43 172 902 695 178 382 598  23 534
 176  28 247 328 810  57 221 610 815 104 437 704  39 519 470 126 414 307
  92 859 541 706  74 132 164 640 435 395  36 180 658 232 227 477 289 253
 608 158 442 301 888 606 772 745 487 441 740 429 200 249 626 100 438 297
 542 198 162 472 553 757 735 803 430 179 837 422 883 239  42 455 744 492
 124 345 412 903 463 415 612 708 816 702 406  51 720 506 862 220 574 753
 722 526 742 465 580  55 518 309 529 555 897 208 228 496 344 185 282 372
 817 113 909  96 389 128 895 153 514 861 543 556 318 743 774 488 103 143
 264 347 900 262  25 719 170 151 618 659 527 446 550 423 129 370 525 914
 426 226 163 258 571 616 788 713 582 786 692 755 731 183 206 603 483 646
 721 274 201  81  93 851 789 236 168 217 752 242 509 683 259 554 256 587
 801 448 396 535 167 193 780 196 629 294 421 748 838 889 157 863 600 820
 393 588  53 818 306 864 243 219 791 842 233   8 826 375 101 293 645 186
 350 726 451 454 894  86  12 497 110  85 326 330  95 291 354 813 880 495
  82 248 121 812 245 515  17  19 447 590 727 795 135 635 390 346 314  90
 204 425 647 717 184 316  16 290 605  11 252 161 114 782 452 754 855 339
 878 408  63 802 159 279 688 623 714 828 156 231 123   7 620 716 302 397
 380 250 364 337 342 836  87 182 209 108  33 650 575 784 154   1 796 349
 546 300 136 218  58 798 235 499 299 770 292 138 160 850 621 213 489 418
 687 166  94 260 214 560 501 327 140 119 305 149 852  20 849 197 405  91
 267 268 373 377 819 265  60  21 736 363 734 507 684 559 579   4 165 637
 768 505 792 839 794 187  45   0 392 785 581 374 391 246 711 486 800 315
 131 362 504 805 766 607 360 283 651 779 691 547 329 732 281 601 325 730
 498 171 371 261  98 133 633 449 369 244 512 715 585 835  84 379  46 286
 510 656 771 280   2  64 642 830 188 725 799 298 304 190 557 513 508 137
 563 367  22  24 558 139 804 511 142   3 657 848 565 814 811  49 578  34
 767 493 577 877 386 694 583 544 728 586 288 549 729 384 733 777  83 494
 120 411 416 856 712 490 643 323 385  31 548 765 545 854  78 285  48  44
 644 130 125 591 562 127 589 769 829 764 685 551 417  47 827 638  76 141
 840 689 332 594 865 410 287  80 639 321 592 879 284 593 383  77  79   5
 690 333 322 324   6 686  99 869 661 866 868 660  30  32 867]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4079
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3226
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.278
INFO cross_voc_dataset_evaluator.py: 134: 0.514
INFO cross_voc_dataset_evaluator.py: 134: 0.200
INFO cross_voc_dataset_evaluator.py: 134: 0.244
INFO cross_voc_dataset_evaluator.py: 134: 0.390
INFO cross_voc_dataset_evaluator.py: 134: 0.431
INFO cross_voc_dataset_evaluator.py: 134: 0.335
INFO cross_voc_dataset_evaluator.py: 134: 0.059
INFO cross_voc_dataset_evaluator.py: 134: 0.380
INFO cross_voc_dataset_evaluator.py: 134: 0.094
INFO cross_voc_dataset_evaluator.py: 134: 0.349
INFO cross_voc_dataset_evaluator.py: 134: 0.187
INFO cross_voc_dataset_evaluator.py: 134: 0.289
INFO cross_voc_dataset_evaluator.py: 134: 0.567
INFO cross_voc_dataset_evaluator.py: 134: 0.527
INFO cross_voc_dataset_evaluator.py: 134: 0.467
INFO cross_voc_dataset_evaluator.py: 134: 0.271
INFO cross_voc_dataset_evaluator.py: 134: 0.172
INFO cross_voc_dataset_evaluator.py: 134: 0.289
INFO cross_voc_dataset_evaluator.py: 134: 0.408
INFO cross_voc_dataset_evaluator.py: 135: 0.323
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.368s + 0.038s (eta: 0:00:50)
person 0.95211834
person 0.9018398
person 0.9044206
person 0.95763654
person 0.987593
person 0.96031576
person 0.9706918
person 0.92188656
person 0.97887236
bottle 0.95573133
bottle 0.9316396
bird 0.9566342
person 0.97805655
person 0.93410164
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.305s + 0.037s (eta: 0:00:39)
person 0.9113525
person 0.9128807
person 0.95475256
person 0.93196774
person 0.9799396
person 0.9687089
person 0.9112312
person 0.9074686
person 0.92342603
person 0.9184582
person 0.9051223
person 0.9073257
person 0.95161057
bird 0.9200344
person 0.9802069
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.299s + 0.037s (eta: 0:00:34)
person 0.9280108
sheep 0.91070473
person 0.9291104
chair 0.93214065
person 0.96307546
person 0.92513275
chair 0.9682615
person 0.9807411
person 0.9691804
person 0.9867923
person 0.9654227
person 0.9914852
person 0.9854748
person 0.9170558
person 0.9271673
person 0.9104283
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.308s + 0.038s (eta: 0:00:32)
person 0.9142341
person 0.9932411
person 0.9133766
person 0.9798707
person 0.9848632
person 0.96103495
person 0.9536061
person 0.969766
person 0.9253148
person 0.9581409
person 0.9040834
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.319s + 0.038s (eta: 0:00:30)
boat 0.92664075
diningtable 0.93793046
chair 0.93760264
chair 0.96118957
chair 0.9398352
pottedplant 0.9022139
pottedplant 0.9528516
pottedplant 0.9088691
pottedplant 0.9190612
pottedplant 0.94467115
pottedplant 0.9325642
pottedplant 0.9247775
pottedplant 0.94123083
pottedplant 0.9298233
pottedplant 0.9353935
pottedplant 0.9253421
pottedplant 0.94898254
person 0.9628031
person 0.98128533
person 0.9820493
person 0.9128984
person 0.9852915
person 0.90471333
person 0.95147187
person 0.9337402
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.323s + 0.038s (eta: 0:00:26)
person 0.9110299
person 0.9128682
person 0.90893584
person 0.90345895
person 0.94585997
person 0.9036456
person 0.9849759
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.323s + 0.037s (eta: 0:00:23)
pottedplant 0.9247469
person 0.9375902
person 0.9511577
motorbike 0.9162031
person 0.95788467
aeroplane 0.93417555
aeroplane 0.90050024
aeroplane 0.93256587
chair 0.98224765
diningtable 0.9149054
chair 0.9808841
pottedplant 0.9240441
bird 0.99312395
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.324s + 0.037s (eta: 0:00:19)
car 0.9263601
car 0.929981
person 0.9901416
pottedplant 0.9259834
person 0.9746007
person 0.9143442
horse 0.95776653
person 0.9385632
person 0.9562184
person 0.9600452
car 0.96329945
bicycle 0.9857007
bicycle 0.93657166
person 0.960857
person 0.9657736
chair 0.9106747
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.322s + 0.038s (eta: 0:00:15)
person 0.966275
person 0.9485032
person 0.9607191
person 0.9667702
person 0.982694
person 0.925625
person 0.9771996
person 0.9775793
person 0.9875036
person 0.9526184
person 0.98620045
person 0.9555149
person 0.9839086
pottedplant 0.93990797
person 0.9911123
bus 0.9087747
bus 0.93649447
person 0.93202394
person 0.93270147
chair 0.9052654
chair 0.9033734
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.322s + 0.038s (eta: 0:00:12)
motorbike 0.98237914
bird 0.92184037
person 0.90590256
person 0.975117
person 0.94125926
person 0.9349579
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.321s + 0.038s (eta: 0:00:08)
boat 0.9813339
boat 0.924363
boat 0.9129152
person 0.93528086
person 0.9381955
person 0.9906343
person 0.9769414
person 0.95148546
person 0.98601156
person 0.985075
person 0.90299004
person 0.9228835
person 0.97033924
person 0.9457027
car 0.9478861
person 0.9704661
person 0.9830461
person 0.9050201
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.322s + 0.038s (eta: 0:00:05)
diningtable 0.9092444
chair 0.97373486
chair 0.94086695
chair 0.9251146
chair 0.9404094
chair 0.9831686
chair 0.97055763
pottedplant 0.9174667
chair 0.9137954
chair 0.9662687
person 0.91928804
boat 0.9214763
person 0.94646883
person 0.90714616
person 0.9816551
person 0.90122104
person 0.942119
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.319s + 0.038s (eta: 0:00:01)
person 0.97564834
person 0.94363827
person 0.9511209
person 0.93784434
person 0.93866146
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.390s + 0.026s (eta: 0:00:51)
person 0.96810985
person 0.96994376
person 0.97811
bicycle 0.936131
bicycle 0.9833482
bird 0.92036873
pottedplant 0.95994896
person 0.9565915
person 0.95160526
person 0.94686836
person 0.9332982
person 0.9790041
person 0.95388037
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.349s + 0.035s (eta: 0:00:43)
person 0.96000606
person 0.9655506
person 0.9034884
person 0.977101
person 0.9651568
person 0.91255915
chair 0.94672805
pottedplant 0.9124511
person 0.9005289
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.346s + 0.036s (eta: 0:00:39)
car 0.92463076
person 0.97020817
person 0.9722477
person 0.90540224
person 0.9401529
person 0.9019922
car 0.98191154
person 0.90518224
person 0.92966455
person 0.929325
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.337s + 0.037s (eta: 0:00:35)
person 0.96993756
chair 0.9045157
person 0.9122126
person 0.9252
person 0.9544837
person 0.90825504
person 0.9245807
person 0.9786306
person 0.9739111
person 0.91899014
person 0.9840464
person 0.97393835
person 0.90761095
bird 0.9896842
bird 0.97461885
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.336s + 0.036s (eta: 0:00:31)
person 0.9003634
aeroplane 0.9083094
person 0.9663521
person 0.9800205
car 0.9478861
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.338s + 0.037s (eta: 0:00:27)
bird 0.9183748
bird 0.92326677
bird 0.9734708
bird 0.92177147
person 0.91208977
person 0.9083586
person 0.9840732
diningtable 0.96251553
person 0.9640549
person 0.90294164
chair 0.9182448
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.339s + 0.037s (eta: 0:00:24)
person 0.9794199
bus 0.9785014
diningtable 0.96097696
chair 0.9816804
chair 0.90282595
chair 0.9585312
pottedplant 0.92388827
person 0.9339185
person 0.95842606
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.336s + 0.037s (eta: 0:00:20)
diningtable 0.9037564
person 0.9226211
person 0.93598247
person 0.91082597
person 0.9097387
person 0.9206724
bird 0.9608319
person 0.9365228
person 0.95093745
person 0.96973366
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.333s + 0.037s (eta: 0:00:16)
person 0.9550442
train 0.94239736
person 0.9576846
person 0.94369876
person 0.957797
person 0.915927
person 0.9027606
person 0.93841875
person 0.93118274
person 0.98866445
person 0.97826284
person 0.94213235
tvmonitor 0.9240258
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.335s + 0.037s (eta: 0:00:12)
person 0.9427667
person 0.93232477
horse 0.9315261
person 0.9550525
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.333s + 0.037s (eta: 0:00:08)
person 0.9747001
person 0.96817654
person 0.90056777
person 0.90727085
person 0.92573684
person 0.9822848
person 0.9037115
chair 0.9604636
person 0.9544897
person 0.930421
person 0.92546517
person 0.91130966
car 0.92147064
car 0.95566785
car 0.93905425
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.333s + 0.037s (eta: 0:00:05)
person 0.9754431
person 0.950198
person 0.94621557
person 0.9039462
person 0.9505026
person 0.9744616
person 0.98716277
person 0.978969
person 0.9770611
person 0.9341617
diningtable 0.90853566
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.331s + 0.037s (eta: 0:00:01)
person 0.9162116
diningtable 0.94404083
person 0.95209855
person 0.98722315
person 0.9050519
person 0.9612756
person 0.9829396
person 0.93078536
person 0.9632439
person 0.95071447
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.469s + 0.039s (eta: 0:01:03)
person 0.9737611
person 0.92910683
diningtable 0.9511839
person 0.96164995
person 0.9723779
tvmonitor 0.9323117
pottedplant 0.94086856
person 0.9579957
diningtable 0.9061609
chair 0.9012623
chair 0.9640372
chair 0.9549268
person 0.9952058
person 0.98018414
person 0.96471393
person 0.903424
person 0.95594656
person 0.9011374
person 0.95603865
person 0.942329
person 0.932642
person 0.9568157
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.322s + 0.037s (eta: 0:00:40)
person 0.921045
person 0.980963
person 0.91452533
person 0.95731264
bottle 0.90355915
bottle 0.9552689
chair 0.90464854
person 0.9179794
person 0.91709036
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.330s + 0.040s (eta: 0:00:38)
chair 0.96023774
bottle 0.93138105
person 0.9266483
person 0.957622
person 0.9089899
person 0.9040984
bottle 0.91284907
bottle 0.95223
person 0.98093075
person 0.91245115
person 0.9515988
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.325s + 0.041s (eta: 0:00:34)
person 0.9086198
person 0.9435449
person 0.95917994
person 0.9450924
person 0.9560555
person 0.9427646
person 0.9270465
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.327s + 0.040s (eta: 0:00:30)
person 0.9733986
person 0.94961935
person 0.96130896
person 0.9159788
person 0.9757563
person 0.9087481
person 0.9536031
person 0.91203946
person 0.94674474
person 0.9662901
person 0.9115262
person 0.90740234
bottle 0.95763284
bottle 0.903023
chair 0.9606684
chair 0.9220445
boat 0.92710346
boat 0.9110001
person 0.9638781
person 0.92789316
person 0.91956234
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.327s + 0.040s (eta: 0:00:27)
car 0.9777304
car 0.9324712
person 0.9706072
person 0.9240711
person 0.91871214
person 0.95742625
pottedplant 0.9536674
pottedplant 0.9112074
car 0.9872907
person 0.9268543
person 0.9884403
person 0.90794
person 0.94187194
person 0.9208342
person 0.9769701
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.327s + 0.039s (eta: 0:00:23)
person 0.9798496
car 0.92071444
person 0.93994606
tvmonitor 0.9310558
person 0.91253436
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.323s + 0.039s (eta: 0:00:19)
chair 0.9271753
person 0.96892905
train 0.92914444
train 0.9320839
bird 0.97795326
person 0.92285025
person 0.9101552
bicycle 0.95942324
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.322s + 0.038s (eta: 0:00:15)
person 0.9529548
person 0.9333017
person 0.9318879
person 0.93280935
person 0.90866363
person 0.9870062
person 0.9528317
person 0.9742087
person 0.9907823
person 0.9037393
person 0.9353276
person 0.9462948
person 0.91351825
car 0.92650044
person 0.97326195
person 0.94082284
person 0.9002282
person 0.96114564
person 0.9285916
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.322s + 0.038s (eta: 0:00:12)
person 0.9119716
person 0.909698
person 0.9106912
chair 0.9773301
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.323s + 0.038s (eta: 0:00:08)
person 0.9670533
person 0.9030619
person 0.9206737
person 0.9537225
person 0.97514707
pottedplant 0.9173544
boat 0.91876405
person 0.95534015
person 0.9119704
person 0.9072098
person 0.94241303
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.324s + 0.039s (eta: 0:00:05)
person 0.96764183
person 0.94435275
person 0.96647495
person 0.9230189
person 0.9289421
person 0.91333956
person 0.9265076
person 0.94826335
person 0.9515784
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.323s + 0.038s (eta: 0:00:01)
person 0.9127354
person 0.9669093
bird 0.9415987
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.371s + 0.041s (eta: 0:00:51)
person 0.9001285
chair 0.90037614
chair 0.9491208
person 0.9496675
person 0.9155923
person 0.947084
person 0.9149345
diningtable 0.92384464
person 0.94347787
person 0.95711005
person 0.90269226
person 0.95381993
person 0.9205182
person 0.95364505
diningtable 0.93093574
chair 0.97536236
person 0.9051052
person 0.987943
person 0.9075997
person 0.9743663
person 0.90632695
person 0.9866498
person 0.93163705
person 0.9054778
person 0.98353726
person 0.969184
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.333s + 0.037s (eta: 0:00:42)
person 0.92896914
pottedplant 0.9431186
person 0.9045014
bird 0.9451143
bird 0.9211707
bird 0.9698178
bird 0.9154702
person 0.96655166
person 0.9505955
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.335s + 0.039s (eta: 0:00:38)
person 0.9562685
person 0.9560661
person 0.9669552
person 0.91210014
person 0.9202318
bottle 0.94693476
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.338s + 0.038s (eta: 0:00:35)
pottedplant 0.94715005
chair 0.9363437
person 0.9351538
person 0.979627
bottle 0.94104207
bottle 0.915786
bottle 0.9010502
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.339s + 0.038s (eta: 0:00:31)
person 0.9463508
person 0.9479754
person 0.94603986
car 0.90421987
person 0.9941812
person 0.97646433
person 0.93199795
person 0.9521796
person 0.9371238
person 0.9357227
person 0.9930489
person 0.9567629
person 0.95198584
person 0.9473911
person 0.98214036
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.332s + 0.038s (eta: 0:00:27)
person 0.9547589
person 0.95729715
diningtable 0.96698284
chair 0.93742764
chair 0.97100204
chair 0.98469234
aeroplane 0.91506076
person 0.969934
person 0.9697361
person 0.9313401
person 0.9152387
person 0.93460053
person 0.95767975
car 0.93479604
car 0.93301487
car 0.96808696
car 0.9516887
car 0.9627585
bird 0.9087727
bird 0.91751164
bird 0.949329
chair 0.92798156
person 0.9163649
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.330s + 0.037s (eta: 0:00:23)
person 0.9504849
person 0.94256586
person 0.9812251
person 0.9622309
person 0.99041325
person 0.9135881
person 0.93589246
person 0.9041308
person 0.98731333
person 0.93015814
horse 0.9260918
person 0.9562739
person 0.9129865
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.326s + 0.037s (eta: 0:00:19)
person 0.92545867
person 0.90562326
chair 0.97795767
chair 0.9688997
pottedplant 0.9419928
pottedplant 0.9229894
pottedplant 0.9657155
person 0.92401856
person 0.9210375
person 0.95137614
person 0.9435563
person 0.9257736
person 0.9178734
bicycle 0.9473573
person 0.98229903
person 0.949639
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.328s + 0.037s (eta: 0:00:16)
person 0.99193656
person 0.94143575
person 0.92279166
person 0.9777478
person 0.9740692
person 0.9692733
person 0.9738255
person 0.939982
bus 0.9262358
person 0.9473075
person 0.91117907
person 0.9263882
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.325s + 0.037s (eta: 0:00:12)
person 0.9767601
chair 0.9559459
chair 0.9116163
tvmonitor 0.92171615
chair 0.9656749
chair 0.9583721
chair 0.94458646
pottedplant 0.9140768
person 0.9595677
person 0.9310284
person 0.97327965
person 0.91330564
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.323s + 0.036s (eta: 0:00:08)
motorbike 0.9701975
person 0.9742353
horse 0.97506374
bicycle 0.946815
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.321s + 0.036s (eta: 0:00:04)
person 0.9583038
person 0.9783507
person 0.9563959
person 0.9729115
person 0.9156789
bird 0.9082614
chair 0.9152123
chair 0.96185845
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.320s + 0.036s (eta: 0:00:01)
person 0.9786211
person 0.9824442
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 77.829s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 349  354 1536 ... 1144 1301  101]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2696
INFO voc_eval.py: 171: [ 97 184 426 803 725 103 188 613 834  99 859 485 860 433 749 453 812 192
 195 193 855 614 619 863 203 865 804 191 728 746 489 861 102 802 871 835
 877 107 658 201 101 872 616 126 120 190 495 499 628 880 454 429 727 491
 830 729 442 621 828 115 118 451 112 100 423 368 623 110  98 833 856 700
 198 683 646 556 706 630 200 679 730 864 852 104 132 650 652 881 189 617
  65 656 857 647 467 547 839 824 468 550 704 886 785 311 702 424 441 497
 291 663 502 133 853 805 742 808 551 490 666 640 305 196 428 806 504 624
 241  50 325 571 631  44 639 832 296 452 634 829 242 758 328 488 874 194
 879 737  72 686 299 814 738 186 438 664 740 220 456 626 645 618 637 108
 716 509  36 320 109 295 139 743 256  39 553 374 722 432  42 185 503 457
 174 277 625 436 425 533 455 377 119 854 170 511 649 540  51   8  53 809
 308   2 726 680 744 736  16 838 638 810 493 465 681 318 437 869 478 848
 240 557 734 543 434  73 842 435 710 129 873  70  21 555 578 219 306 883
 121 684 167 687 389 202 281 654 113 515 884 733  64 400 342  55 713 532
 283 720 568 276  91 819  63 807  48 463 510 866 678 534 721 868 701 243
 535  41 709 867 501 858 309 784 850 763 870 622 231 131 876 862 259 711
 362 130 820 293   5 298  40 885 554 643 538 204 633 172 270 302 843 778
  37  93 430 205 548 840 620 137   7 718 326 748 635 707 443 144 875 600
 766 662 171 405 317   4 269 307 826 169 708 206  89 266 513  18  60 818
 445 125 719 141  76 827 324 741 705 496  75 127 128 187 518 632 111 800
 229 237 197 823  58 332  56 300 431 371 629 114 213 449 199 519 294 825
 786 297 732 688 236 849 756 851   9 173 731 282 292 473 498 168 481 105
 163 487  79 462 260 841 760 322 150 177 480 271 670 218 221 249  84 505
 450 210 482  20 301 791 636 695 682 583  83 398 712 303 792 845 765 777
 214 811 439 369 717 346 582 353 667 779 179 759  45 584  86 661  94 757
 160 471 446 323 837 878 226  71 232 585 304 657 703 790 254 372  10 847
  59 122 539 319  43 469 349 821  96 408 209 517 685 781 723 580 367 176
 570 280 327 466 401 604  14 228 644 607 224  33   6 358 134 813 273 475
 248 671 750  66 521 250  29 290 343 161 572 796  54 665 762 387 310 149
 145 330 264 483 799 157 831 479 520 215  57 340 313 523 767 415 247 761
 591  34 641 386 798 817 409 416 124 227  25 675 586 272 472 331  46 142
 822 795 801 797 474 699 836 615 677 399 162  95 676 576 780 407 794 526
 223 793  92  77 329  35 581 648 278 470 106 138 464 406 597 388 123 610
 659 815 494 262  32 577 477 263 522 211 397  67  49 782 816  61 312 447
 564 235 609 592  78 337 714 286 698 692 151  28 158 289   0 560 565 642
 653 764 739 558 265 579 724 788 715 588 484 257 567 542 225 476 844 559
 506 207 314 288  82 153  30 745  12 691 261 135 182 552 355  11 274 222
 178 769 537 768 516 508 507 444 612 268 514 146 245 735 512 697 500 117
 255 574 156 267 140 155 344 573 655 599 246  69 251 285 361 208 419 164
 136 486 152 562  52 333 345  15 316 440 348 352 181 598  80  88  19 411
  90 279 448 595 569 258 212 373 252 846 770  22 601 529 275 747  47 627
 660 696 180 611 594 357  81 338 651 492 608 287 380 404 244 563 339 789
 414 165 787 359 154 350 253  17 392 234 544 284 427  26 315 347 351  38
 546  13  87 561 233 116 148  85 147 336 143 530 541 183  74  68 384 379
 545 382 385 356  23 690 370  24 378 755 410 752 238 693 689 525 524  31
 753 776 354 527 376  27 321 360 590 754 751 596 589 694 593 566 587 403
 394   1 606 549 461 528 381 383 602 603 166 882 402 575 341 390 365 673
 364 413 363 531 783 771 175 536 159 674 672 375 418 458 422 420 217 391
 417 335 334 669 668 239  62 605 421 775 230 396 412 460 459 216 366   3
 774 773 393 772 395]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5493
INFO voc_eval.py: 171: [ 555 1446 2689 ... 2687 2459 2444]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1919
INFO voc_eval.py: 171: [ 618  261 1982 ...  126 2217 2223]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2468
INFO voc_eval.py: 171: [541  12 431 ... 550 549  58]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3597
INFO voc_eval.py: 171: [194  96 504  98 101  78 503  85  97  77 206 152 119  76 451  81 102 290
 464 414  35 353  80 196 291 103 189  99 264 157  88 470  87 413 481 439
 333 447  40 220  51 344 454  79 301 391 403 272 183 233 371  91 162 349
 248 367 484 281 302 410 317 226 100 153 159 525 541 363 150 117  86 474
 442 457  89 195 273 309 224 399 107  83 289 416  90 221 120 500 242 389
 218 429 538 269  84 455 508 431 172  82 128 456 136 327 368 485 446 475
  63 105 335 463 217 293 250 284  59 147 213 145 361 106  48 127 171 263
  11 499 375 190 151  54 443 116 305 348 436 346 441 345 125 492 161 298
 154 540 539 135 286   1 422 366  52 141 278 379 219  24 112 320  15 514
 181  22 214 404 390 129 121 535 223 156 338 497 400  49 427 405 362 205
  17 234 180 461 236 118 453 444 282 306 124 285 513 537 421 536  36 466
 126 401 142 415 489 520 265 519 134 393 164 398 472 480 411 321 440  12
 350 276  21  42  27 166 486 388 203 342 184 395 137  41 515  53 146 394
 307 130 158 279 229 408 534  66 531 396 542 179 370 392 448 296 192 212
 244 193  16 491 208 122 222 300 473 275 420 238 449 397 423 241 271 165
 123 110 460  50  20 303  14  44   9 334 482 169  37 445 249 476   8 132
 133 313 251 167 438 200 155 341 277 369 216 170 139 138 521  58  47 433
 452 299 160  19 424 364 419  56  18 426 437 185 359  13  92 471  25 215
 168 493  95  60 487 261 527 131 311 347 188  94 468  39 543 270 330 140
 310 237 204 383 104 502 469 488  93 316 252 187 178 186 259 522 332  29
 479 360 428  23 409 325   0 501 114 268 210 182 483 365   3 239 340 266
 111 163 336 374 283 143 477  65 406 295  57 230 498 245  45 376 430 462
 467 337  10 339 297 280 432 372  43   2 113 267 274 294 530 211 407 108
 109 197 292 287 260 478 385 459   7 191 144 373 450 326 523 314   5 435
 254 228 378 356 417 352  38 495   4 198 343 227 318 115 351 315 494 524
 243 412 258 509 328 240 312 253 354 323 246 490 255 496 262 458 247  34
  46 308 257 304 256 232 225 235 331 355 386 425 231 329 418  75 526 207
 381 528 357 434 288 518 209 322 380 149 533  74 176 177 199 148 175 377
 358  68 382  62  32 529 507 324  26 202  70  69 319 402 465  67  55  30
 532 173   6  33 387  73 174  72 201  61 516  71  64  31 506 512 517 505
 510 511 384  28]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4225
INFO voc_eval.py: 171: [1480  684 1418 ...   97   99 2188]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.3168
INFO voc_eval.py: 171: [ 67  28  22 286 242  26 241 284 145 125  32 101 123 285  69  93 146  68
 261 232  23  25  29 107  70  97  24 260  96 277  18 193 135 264 144  52
  31 265 229  74 106 128 190 220 158 207  20  47 127 114  83 236 262 223
  53 219  78  11 159 166  72 111  55 221  89 155  66   1 204   7 278 206
 181 255 217 205 212 283 139 280 103 182 113  73  19 281 183 149 249 224
 136 282 121 216  30 131 120  13 186  80 115 150  27 208 211 109  54 279
 195 148 248 292 112  71 110   2 147 130 104 209 143 222  79 157 102  82
  84 230   4 191 118 197  64 257 263 184 218  98 268 185 133 116 175  21
 259 225 272 153 196  49 215 244 108 100 160 247 201  95  33 256 250  15
 172  44  99  75 273 210  46 194  37 234 202 291  92 124 198 258   3  10
  12 119  94   0 251 189  56 105 162  61 237 132  90  17 246 254 274  88
 126 117  36 173 203  81  65 240 269 122   9 199 129  51 138 227 161 200
   6 267  86 226 156   8  85  50 214 233  57 180 252 170  87  48  35   5
 178 235 154 245 192 168 152  42  16 253 231 164 289  34  40  43  62 167
 271 288 176  39 165 290  45 169 270 187 163 142  63 141  38 275  60 266
 238  76  77 228 174 239 179 177  41 140 137 188  14  91  59 276 243  58
 287 134 151 213 171]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0589
INFO voc_eval.py: 171: [6542 1570  770 ... 4940 7152 4571]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3696
INFO voc_eval.py: 171: [515 197 322 199  33 565 196 324 164 201  47 558  96 538 456  32  52 191
 321 287 242 560 203 468  37 190 440 277 250 323 195  18 551 327  46  51
 557 454 564 441 330 168  69 328 106 233 455 270 329  49 567 181 281 393
 390 193 522 444  41 451 472 569 317 252 460  48 452 198 537 234 549 291
 145 255 192 213 453 542 214 471 561 102 443 559 325 121 326 109 562  67
 163 566 235 470 545 273 539 265 146  57 306 387 311  59 100 202 297 309
  97  27  94  17 209 391 105 180 143  54 162 101 205 107 144 258  75 232
  34 392 320 474  71 533 556 194 442 296 267 457 541 136 405 499 461 208
 206  43 182  83 243 137 244 104 134 362 513 404 140  63 274 123  65 458
 210 173 124 520 240 186 394  36 103 184 543 544 373 207  68 300 310 286
 445 486  19 165 111  84 406 211  50 534 262 292 271  16 450 516 114 225
 511 257 290 386 517 529 215 527 153 116 305 254 371  62 308 142 245  76
 204 288 381 552 266 540 487  58 518 475 568 112 151 389  45 548 212   8
 563 353 282  29 289 152 307 400 170  56 294 465 547 528 117 335 536 409
 127 283 293 179 449 407 501  98 550  66 284 408 279 227 200 350 148 285
  85 546 139 150  72  40 272 246  95 555 147 500 174 410 382 429 167 298
 339 316   2 299   9 480 497 218 496  14 363 348  89 521 141 138 357 368
 149 479 355   0 118 157  44 438 295 485 172 512  70 530 108 435 231 388
 489 490 356 484 343 256  61 220  30 553 241 171 525 177 380 370  12 464
  20  99 554 189 238 574  25 369 478   3 436 110  64 502 115 125 166 113
 508 185 367 482 268 374 344 159 509 366 506 161 129 178  38 384 463  82
  60 133  86 239 446 278 523  73 570 437 264  28 573 396 428 383 352 473
  74 154   1 514 169 347 462 183  93 519 488 346  88  35 413  21 507 372
 375 176 345 493 359 122 175 420 126 535  10 130 467 269 418 385 263 236
 411  81 495 403 351 224  24 526 155  55 342 249 510 131  23 338 340 253
 401 419 477 318  26   7  92 379 331 447 237 156 223  91 187 399 476 492
 228 469 226  42  22  13 494  31 397 439 354 434 491 158  15 358 459  53
 505  39 315 361 135 301  11  87 481 466 302  90 132 431 304 128 524   4
 532 498 483 360 402 433 221 432 376 261 430 334 260   6 216 188 248 395
 349 313 319 229 448 259 398 230 217 312   5 336 314 412 503 571 572 377
 160 341 219 365 531 251 280 421 364 425 222 275 276 378 247 504 303 332
 333 337 424 422  78 423 417 427 119 415 120 426 414  77  79 416  80]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1140
INFO voc_eval.py: 171: [2999 1295 1353 ... 3230 2329  212]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3487
INFO voc_eval.py: 171: [384 460 490 933 416 156 486  79 506 491 454  65 277 595 867 388 552 163
 685 217 938  66 456 869 493  61 815 238 601 932 457 893  59 899 508 155
 278 146 943 394 510 868 164  67 935 683 289 871 391 162 865 870 236  28
 658  82 892 234 304 558 395 385 562 248 417 240  26 154 324  64 927 659
 465 890 282  81 813 265 315 160 902 255 280 223 462 158  22 415 413 244
  68 567 747 242  84 343 887 513 483 909  83 365 645 204 851 682 504 393
  80 627  60 241 646 931 250 279 224 463 305 344 484 854 837 551 861 239
 896 603 247 877 396 937 930 269 285 284 770 779 814 237 907 358  73  24
 572 489 481 252 913 655 442 410 874 321  69 159 714  47 626 879 656 414
  51 319 882 694 647 191 357 872 317 743 519 811 274 723 906 143 458 944
 392 332 459 771 372 912 268 359 901 597 891 940 625 679 853 291 212 697
 806 678 948 264  36 665 128 886  71 881 464 318 367 266 669 326 482 428
 692 165 516 841 166 455 235 469 600 226 936 157 742 471 713  72 661 205
 429 273 852 461 485 684 824  76 675 251 780 207 430 260  86 681  74 688
 262 243 783 325 560 599  35 423 189 898 950 784 664 731 383 211 323  23
 842 222 689 225 477  41 832 272  63 134 400 929 570  78 473 487 466 623
 218 341 336 316 270 261 470 729 652 840 598 873 908 230  46 246 900 161
 131 897 334 579 220 829   7  49 529 732 290 752 514 148 862 876 208 424
  18 509 569 556 370 740 864 735 219 425 520  85 182 848  32 298 947 615
 259 757 283 586 389 387 778 928 921 821 753 616 701 522   3 476 209 276
 203  50 712 800 584 911 314 187 322 866 271 863 602 150  70 745 803 441
 188 919 878 553 666 467 802 839 512 253 591 210 227  75 137 741   5  77
 408 604 880 563 651 566 914 488  37 554 831 739 657 667 781 693 257 168
 777 905 221 374 267 910 690 561 715 422 812 307 939 571 426 746 468 256
 202 138 127 335 141 245 596 691 945 695 550 833 275 834 677 606 920 147
 390 427 126 368  27 934 792 371 764 818 925 378 631 180 766 337 310 830
 258 895 702 249 903 628 311 411 152 589 838  52 398 342 750 382 705 668
 521 149 846 153 347 478 585 178  88 915 330 699 609 904 386 530 644 698
 926 475 102  31 151 196 206 361 608  34 607 618 287 875 254 494   6 751
  11 670 759 300 850 193 480 559 348   2 772  57 183 942 145 309 946 605
 306 676 949 181 409 379 810 331 822   8 369 340 231 431 776 624 171 360
 719 492 941 820 541 301  15 629 592 320 836 696 373 804 734 354 312 133
 352 761 114 847 328  39 706 108 375  87 568 748 738 787 177 835 648 686
 167 308 722 583 144 338 412 129 786 355 142 228 733 660 793 119   0  20
 435  56 794 765 633 449 101 472 327 401 791 448 894 532 883 130 495 333
 356 302 737 844 707  29 816 303 313 140 643  98 377 619 581  62 805 749
 539 526   1  10 703 662 402 828 135 132 671 362 826 100 329 725 286 376
 366 136 801 536 397 103 718 339 527 754 288 474  12  40 184 505 116 281
 139 544 346 593 418 565  92 858  58 617 437 613 642 577 496 112 200  48
 590 353 790 299 588 674 104 186 809 533 511 594 479 531 105 744 653 918
 673   9 351 350 349 884 113 672 807 634 170 580  95 654 635 889 717 573
 808  99 185 917 497 916 517 727 610 649 436 663 502 535 700 843 827  93
 192 438 817  54 575 115 650  21 399  96 538 885 109 785 498 515 345 453
  19 767 111   4 849 630 796 233 500 216 169 762 179 621  55 118  91  97
 620 774  17 622 452 125 728 421 540 576 405 557 782 823  13 545 433 582
 795  45 195  94 632 758 213 756 720 295 499 106  90 587 503 755 528 534
 292 190 537 451 121 214 117 798 107 518 403 446 763 775 110 381 760 232
 447  25 641  53 197 611 380 819 736 450 501  89 445 799 711 637 507 721
 768 229 543 194 724 845 726 614 709 769  33 825 525 555 716 524 523 263
 363 406 122 407 201 797 730  38 123  44  43  30 124 710 215 198 708 199
 680 789 120 888 704 574 859 443 296 294 687  16  42 420 172 293 439 612
 404 542 176 639  14 175 174 173 432 860 924 857 419 564 444 297 788 855
 636 546 773 638 856 364 434 578 440 922 923 547 548 549 640]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1878
INFO voc_eval.py: 171: [735 101 304 668 305 739 103 745 139 108 307 102 669 468 498 578 600 573
 688 602 163 310 209 111 749 476 752 104 143 407 106 105 671 593 199 680
 124 610 214 141 148 437 576 606 604 599 109 228 605 472 210 750 120 583
 219 306 477 596 114 417 466 224 722 435 113 145 471 158 584 716 334 275
 762 751 283   4 278 505 646 681 482 409 684 598 693 615 467  77 286 485
 440 528 264 726 743 347 672 338  74 675 410 753 237 474 746  42 509 676
  32 594 152 718 608 436 215 724 189 285  41 415 470 484 212  11 411 679
 755 332 561 727 429 220 733 298 324 497 396  15 609 591 495 223 438 732
 408 511 714 188 350 697 721 483 229 695 161 499 597 607 249 263 340 331
 303 200 281 501 213  79 424 140 162  73 519 542 154 510 514 502 253  66
 674 153 107 779 715 159 717 742 713 236 319 144 342 480 741 333 595 157
 444 478  76 725  29 637 190 701 320 581 280 414   8 125  84  87 276 138
 454 448 201 192 492  34 490 768 405 590 686  70 112 516 433 146 121 525
 639 432 582 682 150 611 221 570 156  63 116 729 147 119  85 720 723  33
 339 255  65 603 491 767 240 677 252 442 284 341 187 571 151 546 738 389
 211   7 450 488 216 239 728 522 282 731 354 265 277 736 518  83 291 279
 447 191  60 117 274 704 756 377 256 601 543 740 300 685 198 562 248  88
  81 764 445 469 529  82 164 572 225 165 205 356 335 748 475 261 155 420
 481 177 664 527 766 355 371 515 353 487 268 486 431 394  75 132  61 166
 149 218 247  10 687 690 461 719 651 670 412 186 765 128 398 629 250 267
 434 336 772 195 494  46 288 194 744 375  80 178 479 760 135 770 587 523
 430 425 230 689 579  89 203 207 204 123 761 269 395 769 503 206 574 292
 293 266 345 526 512 730 182 692 242  19 185 563 678 758 580 349 357 624
 449 160 360 473  86 202 771 763  26 133 506 294 489 222 380 217 260 142
 548 372 406 404  78 622 413 620  36 254 559 702 696 757 290 308 694 451
 174  69 441 301 259 778 531 231  93 176 734 706 241   5 346 258 318 535
  28 270  62 496 118 585  96 243 625 703 369 683  54 122 673 167 589  27
 379 446 422 183 337 246  25 381 650  64 616 251 257 647 700 208 493 314
 387 127 428 558  57 227 110 747   6 552  40  35 465 500 547 640 691 328
 115 504 388 699 773 549 175  67 759 737 577  17 667 652 560 524 295 184
 134 397 754 245 374  68 630 628  24 343 344 634 348 521 321 235 627 626
 439 361 312 586 244 179  37   0 550 621 196 130 443  38 426   9 452  30
 464 705 370 564 181 575 399 642 551 238 507 649 273 226 180 455 545 366
 382 423 544 419 655 272 364 592 329 287   1 665  31 513 517 566 775 508
 633 774 418 554 384 392 234  43  14  71 368  12 289   3 653 557 657 643
 636 296 373 520 777 453 618 129  45 351 631 403 536 632 661 376  94 638
 271 316 556 299  91 171 326 378 386  23 126  55 654  90 416  56  53 623
 421  95 173 427 136  72  98 666  39 565 555 588 131 315 352 537 644 462
 612 619 463 358 708 707 137 663  92 330  58 635 170 363 359 567  99  59
 645 641 698 367 317 709 169  16   2 327  22 613 614  97 362 365  13 538
 193 569 172 456 711 656  49 297 383 100 617  50 459 391  20 393  18 302
  21 168  51 533  44  52 309  48 539 568 532 658 325 648  47 390 776 457
 262 659 197 662 402 323 660 460 710 534 530 458 232 233 712 385 313 311
 322 401 553 400 540 541]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3004
INFO voc_eval.py: 171: [ 68 410  36 244  67 414  77 260 249  73  70  49 264 175  94 436 176  79
  75 197 245 418  76  51 408 425  38  46 396  25 301  42 100 246 421 171
  29 174 266 268  74  72 379 364 412 439 406 138 315 375  53 304  30  81
  91 435 303 437 397 265 209  23 291 255 443 365 177 143 271  11 367 293
 273   8  26 416 248 274 295  87 269 312 323 252  82  93  37 294 302  22
 381 348 337 422 251 140  20 213  83 292  80  48 270 326 198 193 190 109
 132  78 377 237 224  41 119 438 305  45 250 332 149 434 393 258 173   3
 242 221 172 103  84  85 225 442 202  90 232 385 350  89 356 256 307 399
 441 287 114 152 113 102 261 201  24  50 278 196 319 336 440 247 153 279
 383 338  54 120 195 392 276 415 139 134   6 223 111 308 215  44 366  56
 133 417 110  34 145 228 231 124 121  92 101 253 126 335  98  28 403 238
 254 395  12 340 277   9  43 289 398 229 115 341  13 127 226 125 189 199
  15 339 222 342 370 382 384 141  33 257 262 137 147  64 129 280 122 220
 112  97 368 155  27 162  19 296 211  96 236 128 148 328 241 290  69 378
 316 321 431  88 320  39 299  31 239 322 183 376 334 430 142 163 151  17
 144 118  65 285 117 170 259 156 104 123 363 352  86 230  95  58 267  52
  16 154 354 380   0 288   5  71 329  59 330 217 345 333 240 146 349 298
 409 362 300 210 203 351 411 314 297 313 325 282 423 284 150 179 131 355
 263 343 204 373 391  32 402 272 283 369 275 182 206  60  66 165 227 219
 105 191  55 200 233 317 169 390 187 160 157  47 166   4  40   7 159 207
 311 235 194 164 167 116 358 331 387 218 347  21 106  99 318  18 324 386
 309  63 158 344 401 161 426 281 388 168 107 186  14  35 389 192 404 353
 135 346 413  10 188  57 327 310 306 419 432 400 205 429 234 286 208 428
 394 405 371 420  61 374  62 427 108 407 361 424 185 360 181 433 372 180
 178 184 136 359 357 130   2 243 216 212   1 214]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5718
INFO voc_eval.py: 171: [ 7971 12962 13114 ...  9172   732   726]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5339
INFO voc_eval.py: 171: [3084  888 2056 ... 3214  207 2242]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4405
INFO voc_eval.py: 171: [ 28  29 513 298 497 313  44  40 296  64 318 499  30 496 450 490 127 557
 211  58 503 292 239  54 494 382 323 319 341 495 322 510 238 459 325 565
 588  62  55 584 255 308 553 149 266 241 344 512 114 498 309 117 293  42
 214 493 558 592  66 155 314 339 212 217 385  57 340  51 125  37 240 327
 492 511 312 381 454 305 304 301  43 580 384 566 491 115 397 221 129 343
 116 193 220 460 290 587 219 124 152 462 392 563  59 295 317 307 311 447
 521 253 573 427 517 380  82 153 132 505 564 303 570 254 389 134 267 577
 569 194  52 326 428 157 150 548 568 574  35 324 113 297 251 123 275 560
 160 146 271 583 140 131 310 608 145 572  83 133 624 444 252 501 590 167
  50 411 514 581 121 306 476 299 449 609 229 351 571 130 478 589 576 567
 550 506 260 218 484  27 549 184 328 209 256 578  36 300  90 128 216 487
 585 118 302 426 551 386 383 291 294 515 158 434 147 207 210  65 139 368
 532 537 331 628 164 208 488 289 321 520 215 316  32 453   5 119 561 122
 451 195 315  31 562 320 579  38  84 612 329 586 575 177  60 269 504 141
  47 582 547  20  61 554 342 519 222 353 522  49 213  24 591  85 259 120
 509  76 610 455 390 159 422 539 552 559 182 461 345 166 467 525 446 156
 481  41 437 445 516 183 442 204 186 595  89 231 482 615 242  48 248 410
 620 346 257 611 394 282 523 627 250 188 463 175   8 524 464 268 148  88
  92 246 613  98 555  91 626  75 228 173 151 536 594   0 154 332 330 471
 249   2 247  39 287 185 355  69 230 110  25 187 412 258 233 161 407  14
 126 507 406 475 170 144 243 473 232 362 237 244 179 421 279 625 500 162
 197 472 189 101 508 623 619 477 286 526 456 202  21 347 200 111 425 280
 174  63 448 443 203 245 441 530 168  56 433 593 439 556  95 457  26 281
 102 538  33 486  94  15 596 483 616 199 423  99  46 436 169 264  68 272
  53 176 288 502  22 409   1 544  10  74 534 261 438 192 617 600 599  23
 518 466 479 359 236 485 465  13 440 375 172 540 366 387 388 480 622 265
 469 542  77 273 263 546 262 621 604   9 374 391  34 398 408 474  96 395
  72  12  70  71 424 350  18 405 618 468   4 543 614 335 371 602  45 541
 533 458 274 379 234 270 276 285  73 452  86 283  17  87 393   6 235 136
 190  97 431  19 363 470 531 400  11 349 432 358 112 284 535 360 396 435
 100  16 373  78 107 348 205 334 137 356 545 180  80  79 430 527 361   7
 528 142 135 198 171  81 489 108 333 109 357 191 416 201 206   3 399 181
 196 223 372 105 529 377 106 104 369 165 103 277 364 163 605 138 401 278
 378 598 354 224 370 603 429 365 352 376 367 606 420  93 601 404 227 597
 414 336 607 225 178 413 419 226 415 337 143 338 418 417  67 403 402]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2729
INFO voc_eval.py: 171: [130  50   5 208  51 135 138  54 140 166 209  48 128 206 250 299 203 129
 205 172 251  12 357 270  69 296 263 254 255   6 163 171 202 300 293 239
 127 141 167 173 110   7 168 315  67 350  36 102 358 360  29  66  49 159
 249 158 107 169 269 267 193 253 271 207 307 252 361  85 105 106  96  88
 356 113 170 204 145 294 192 212 247 354 131 336  52 162  71 283  11 132
 243  79  78 264 285 235 272 134 266 104 111 164  53 115 116  25 284 112
 259 120  90 117 341 347  33 281 268 191 290 338 292  57  34 147  47 332
  83 359 211 217 210 256 286 363 343  19 265 114  28  89 213 261  91 262
 275 362 122 143 349  31 224 316 278 124  17 258 329 303  56  76 144 241
 165  68  86 136  64  16 309  32 353 160 319  37 142 161 194   9  95 326
 344 137 248  72 342   8 226 317 188  60  87 324 274  27 146 150 302 242
 237 298 276  70  94 308 246  65 297  92 234 133 277 231 157  59 320 139
  63  10  61 257 223  82 175 346 355 295  26 196 279 335 103 289 330  62
 218 221 301 238 214   1 331 155 310 240 291   0 174  84  35 125 318 236
 312 222 197 119 232  20 184 245  58  80 333 195 118 148 348 123 151 126
   4  45 334 339  81 244 328   3 190 282 288 225 154 345 227 198 323 178
 108 220 201 156 189 179  30  93 311  55 304 152 327 273  18 121  39  21
  22 322 219   2 280  24  46 321 314 182 187 185  23 176 228 186 325 230
 216 177  44 337 229 181 183 215 287 180 149  97 340 200 199  38  41 153
  77  75 313  74  43 305 306  40  42  99 351 100  15 352 101 109  98 233
  73 260  13  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.2455
INFO voc_eval.py: 171: [351 578 577  91  90 118 358 295 355  82 579 198 272 167 409 125 120 203
 202 350 593 294 493  94 395 581 354 273 126 714 164 459 414 201  87 362
 296 276 770 789  83 102 408 595 580 440 488 297 583 121  66 657  25 458
  95 394 181 648 722  80 109 388 790 175 415 301 585 664 735 106 469 410
 777 760  53 119 258 284 132 304 637 206 505 192  45 417 796 279 209 275
 390 353 360 211 389 401 101 223 566 633 487 359 108 193 728 608 274 490
 634 416 411 407 727 805 199 396  24 302 721 560 399 775 636 107 656 238
 140 131 716 600 165 278 767 627 718 781 464 793 794 363 776 466 771 200
  49  69 287 189 413 658 604 609  30 730 475 812 261 778 204 460 435 534
  13 183 338 743 698  47 361 433 639 474  68 795 733 220 725 786 286 300
 205 798 150 441 785 234 357 754  78 432 499 104 723 168 322 788 170  86
 527 739 741 564  77 515 352  11 666 717  26 344  76 386 162 210 179 555
 550 768 277 229  41 612 436 283 177 163 731 814 548 607  60  99 663 481
 750 197 157 371 227 752  63 693 509 221  61 653 524 290 264 675  32 149
 695 661 525 384  29 224 791 412 606 649 235 807 792 703 766 100 715 597
  73 761 603  48 782 242  20  89 266 323 225 461 233 231 654 207 662 702
 632 171 176 184 532 563 346 424 228 285 769  71 647 447 508 319 208 497
 614 356 660 232 729 726 652 697 324  10 562 473 496 811 762 240 745  62
 523  97 787 173 397 611 309 797 434 522 190 196 230 437 622   8 478 293
  81  28  52  50 476 159 553 160 724 552 732 212 701 325 143 628 194 385
   4 236 250 681 265 646 111 602 172 191 668 635 174 815 393 492 772  79
 736 335  14 247 345 237 554 398 765 799 551 337 268 707 280 187 153 680
 547  96 813 572 444 254 310 391 470 333 289  55 613 536 764 133 178 330
 605 689 477 186 340 624 366 506 222 113  16 705 349 158  93 533 543 145
  59 341  85 103 166 364 678   5 115  22 626 246 169 491 704 610 400 599
 665 740 305 336 365 244 252  17  65 667 369 392 239 468 751  56 216 650
  54 219 445 298 780 185 784 303 625 155 383 307  64 251 620 270 180 631
 709 530  98 549 514 737 699 816 255 188 421 110 484 471  27 651 381 339
 404 518  75 507   3 783 694 182 713 679 629 195 711   6 291 299 114 774
 568 422 380   0 249 161 744 343 467 387 567 347 342 655   1 241 420 659
  74 426 540 685 809 683 257 503 123 758 281 425  44 253 269 259 288 734
 773  43 498 779 513 569 136 452  72  46 134  84 516 638 332  67 462 576
 738 368 282 449 598 521 453 559 292 267 588 691 142 379 260 248 116 367
  12   7 556 423 378 645 557 558 719 348 156 406  88 712  92 327 405 502
  39 630 720 262 483 218 129 485 501 146 148   2 112 763 428 674 465  31
 546 117 480 742 640  15 263 226 105  18 402 147 673 495  58   9 214 403
  42 314 448 542 245 619 152 243 700 517 621 690 641 526 519 708 529 374
 442 696 479 217 438 456 520 418  40 154 271 538 590 504 574 454 544 457
 256  19 331 682 749 446 308  21 570 500 584 541 747 472 455 535 127 419
  38  70  51 482 494 561 439 429  57 537 687 382 311 463 531 670 128 328
 443 710 451 334 528 746 373 615 489 692 213 676 623 684 586 512 759 755
 329 450 573 688 644 803 306 486 642 372 313 643 808 151 575 539 677 686
 376  37 802 571 672 315 326 753 215 748 617 669 316 377 806 431 139 757
 804 427 430 375 565 587 800 320 510 138 312 135 137 370 801 321 671 810
 756 706 318 594  35 511 122 317 545 124  34 616 618 141 144  23 130 596
  36 591  33 582 589 601 592]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2907
INFO voc_eval.py: 171: [408 544 348 798 197   6 605  75 567 340 543 351 714 352 430 636  51 427
 579  40 570  76 182 513 753 622 420 561 110 809  52 461 611 777 609 259
 553 467 675 349 800 291 404 437 109 192 462 574 186 233 773 575 662 606
 556 607   7 620  26 831  41  16  47 313 315 614 218 426  12 199 774 429
 674   9 290  66 726  58 612 157 799 156  45 608 163 479 644 521 176 815
 308 355 639 166 415  14 334 693 269  54 833 406 766 643 411 621 743  53
 378 296 416 801 710  57 610 593 816 304 227 514 357 369 511  11 436 318
 234 512 811 213 359 360 569 615 267 184 477 384 112 613 265 697 618 756
 314 829 598 125 428 464 111 200 230  55 481 181 414  56 309 393  32 422
 375 686 523 358 113 168 475 312 577 303 717 822  90 755 819 776 425 487
 377  22 765 391 473 322 261 778 711 405 385 403 493 788 718 353 563 762
 222 465  67 754 215 164 599 268 114 138 640  27 542  89 637 155 139 830
 699 491 424 488 791 548 619 165 253  21 254 161 616 545 134 140 694 326
 412 600 832 741 687 382 162 400 264 383 821 492 803 518 683 343 446 515
 676 528 153 275 485 810 764 817 409 418 655 707 300 763  39 559 299 739
 132 172 225 468 206 432 692 317 387 802  86  94 825 731 836 708 120 585
 617 160 262 410 231 787 677 530 684 646 229  85 173 698  50  73 386 235
 480 146 272 812 500 255 689 696 365 236  81  37 392 380 423 183 194 407
 632  80 295 661 742 580 335 274 474 595 305 820 490 207 813 196 226 293
 572 421 224 298  79  43 824 466 287 145 297 635 837  77 216 633 180 381
 413 506  70 486 188 374 144 476 550 641 142 642 388 557 783 389 122 331
 284 748 789  31  30 775  97 634 417 587 823 210  29 772 175 444 826 601
 645 390  42 402 141 396 211 721 678 447 659 258 324 558 725 185 658  74
 419 143 638 790 459 101 546  38 102 263 827 673 690 547 273 737 129 680
  88 438  20 834 828 178 516 554 668 571 808 631 470  96 768 660 372 564
 376 807 695 552 450 472  28 366 484 760 306   8 657 656 434 232 123 508
 367 189 364 623 782 679 354  72  71 814 750 301 394 469 344  19 246 751
 682 581 379 792 463 239  83 664 591 169 498 221  87 576 533 217 650 489
 341 691  82 709 722 502 223 203 443 483 478 747 187 219 592 560 328  98
 106 715 135 524 310 179 137 681 148 482 720 818 130 347 499 665  46 729
 371 133 177 116 195 127 198 471 294 401 573 108 115 503 672 320 740 136
 716 193 562 457 370 703 685  49 688 723 292 501 752 150 342 568 346 282
 250  61 241 171 158  25  62 730 454 190 251   5 228 249 191 105 350  64
 398 159 373   0 212 578 248 395 652 149 307 835 520 270 302 793 151 208
  69 174 735 170 237 260 767 252  78 551 736  95  68  15 209 103 653 719
 505 316 117 257 529 399  13 167 769  84 271  18 630  93 704 131  65 526
 121 240 770 118 724 597  10 732 247 555 666 266 582 497 345 522  17 126
 205 549 325 147 433 285 451 538 204 654 321 805 448 201 785   1 202 119
 663 781 220 283 728 286 124 727 435 214 757 504 323 356 744 596 281 242
  91   2 128 738 441  36  63 647 627 603 602 327 154 701 100 452 238 535
 565 333 734 583 649 152 669 449 746 319 495 671 330 566 667 510 339 780
 733 594 496 329  99 670 519 311 702 749 337  48 256  44 104 651 455 397
 532 439  33 453 779 761 442 338 589 517 332 458 713 244 456 531 368 745
 363 460 431 527 525 624 705 440  92 107 494  35 445 648  34 590 706  59
 588 700 712 279 362 786 243 759 534 771 507 537 509 804  60 245 629 784
 277 584 289 586 806 625 276 794  24 361 758 541 336 540 626 280   3 288
 278 536 628   4 539 797  23 796 795 604]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3987
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3245
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.270
INFO cross_voc_dataset_evaluator.py: 134: 0.549
INFO cross_voc_dataset_evaluator.py: 134: 0.192
INFO cross_voc_dataset_evaluator.py: 134: 0.247
INFO cross_voc_dataset_evaluator.py: 134: 0.360
INFO cross_voc_dataset_evaluator.py: 134: 0.423
INFO cross_voc_dataset_evaluator.py: 134: 0.317
INFO cross_voc_dataset_evaluator.py: 134: 0.059
INFO cross_voc_dataset_evaluator.py: 134: 0.370
INFO cross_voc_dataset_evaluator.py: 134: 0.114
INFO cross_voc_dataset_evaluator.py: 134: 0.349
INFO cross_voc_dataset_evaluator.py: 134: 0.188
INFO cross_voc_dataset_evaluator.py: 134: 0.300
INFO cross_voc_dataset_evaluator.py: 134: 0.572
INFO cross_voc_dataset_evaluator.py: 134: 0.534
INFO cross_voc_dataset_evaluator.py: 134: 0.440
INFO cross_voc_dataset_evaluator.py: 134: 0.273
INFO cross_voc_dataset_evaluator.py: 134: 0.246
INFO cross_voc_dataset_evaluator.py: 134: 0.291
INFO cross_voc_dataset_evaluator.py: 134: 0.399
INFO cross_voc_dataset_evaluator.py: 135: 0.325
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.425s + 0.026s (eta: 0:00:55)
person 0.9554241
person 0.9320746
person 0.95525867
person 0.98824346
person 0.96988904
person 0.9726228
person 0.9263197
person 0.9796392
bottle 0.9483334
bottle 0.92366993
bird 0.9553476
person 0.9793453
person 0.93568593
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.346s + 0.037s (eta: 0:00:43)
person 0.9213539
person 0.92722446
person 0.93083996
person 0.97915536
person 0.95646834
person 0.9690288
person 0.9138222
person 0.95194113
person 0.917258
person 0.925218
person 0.918655
person 0.90422124
person 0.9576218
person 0.9148889
bird 0.91395074
person 0.9806599
person 0.90126765
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.334s + 0.038s (eta: 0:00:38)
person 0.92519206
sheep 0.90533864
person 0.93376535
chair 0.90397733
person 0.96593827
person 0.9080059
person 0.9386196
chair 0.9663159
person 0.9806882
person 0.98694706
person 0.9679682
person 0.9914534
person 0.9844407
person 0.90468746
person 0.90703756
person 0.9068815
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.335s + 0.038s (eta: 0:00:35)
bird 0.90639275
person 0.9147415
person 0.99303305
person 0.9790491
person 0.9222447
person 0.98450506
person 0.9558752
person 0.95702505
person 0.96882814
person 0.92887974
person 0.9566973
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.336s + 0.039s (eta: 0:00:31)
boat 0.91173625
diningtable 0.9389195
chair 0.9590777
chair 0.93087107
chair 0.94296676
pottedplant 0.9326145
pottedplant 0.93701196
pottedplant 0.9117798
pottedplant 0.9243095
pottedplant 0.92920274
pottedplant 0.9155639
pottedplant 0.9473502
pottedplant 0.94529104
pottedplant 0.94368726
person 0.9630913
person 0.98038185
person 0.98661417
person 0.9854634
person 0.9063178
person 0.95833707
person 0.9392313
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.329s + 0.038s (eta: 0:00:27)
person 0.92413753
person 0.90012705
person 0.9181091
person 0.9154183
person 0.90910757
person 0.9479348
person 0.98600036
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.328s + 0.038s (eta: 0:00:23)
pottedplant 0.92961586
person 0.9502674
person 0.9455696
person 0.9565222
aeroplane 0.92127097
aeroplane 0.93162173
chair 0.9820066
diningtable 0.925834
chair 0.9787118
pottedplant 0.9170467
bird 0.99304754
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.330s + 0.038s (eta: 0:00:19)
car 0.9241247
car 0.93359846
person 0.9899192
pottedplant 0.9146624
person 0.98274463
horse 0.96355337
person 0.92942035
person 0.95621955
person 0.95715886
car 0.9425036
car 0.96042144
bicycle 0.9533211
bicycle 0.9830657
person 0.9568241
person 0.9678044
person 0.9032389
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.325s + 0.038s (eta: 0:00:15)
person 0.96719927
person 0.95044506
person 0.90066046
person 0.96203905
person 0.9025982
person 0.9866377
person 0.93375164
person 0.9757834
person 0.97322965
person 0.9705244
person 0.9885916
person 0.9572245
person 0.9857227
person 0.9499809
person 0.98276824
pottedplant 0.93233097
person 0.9909574
bus 0.9068839
bus 0.92957586
person 0.9406588
person 0.94014496
chair 0.93238705
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.325s + 0.038s (eta: 0:00:12)
motorbike 0.9786073
bird 0.91817456
person 0.9739682
person 0.9439299
person 0.9426525
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.324s + 0.037s (eta: 0:00:08)
boat 0.98076
boat 0.94857174
boat 0.91192067
person 0.9344922
person 0.9390018
person 0.99022466
person 0.94753975
person 0.9090491
person 0.9852071
person 0.9856894
person 0.94153666
person 0.9758957
person 0.923024
person 0.9322714
person 0.9539193
person 0.9470438
car 0.9465104
person 0.908261
person 0.9671331
person 0.9848817
person 0.91529137
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.324s + 0.037s (eta: 0:00:05)
diningtable 0.9181406
chair 0.97247374
chair 0.93537307
chair 0.9212463
chair 0.96815675
chair 0.9809982
chair 0.97165966
pottedplant 0.90909475
chair 0.9139478
chair 0.963396
person 0.92382455
boat 0.9206041
person 0.9502746
person 0.9106019
person 0.9833438
person 0.9024952
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.321s + 0.036s (eta: 0:00:01)
person 0.978741
person 0.9476625
person 0.9523886
person 0.93949133
person 0.9468888
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.402s + 0.034s (eta: 0:00:54)
person 0.97320896
person 0.97355926
person 0.9773795
person 0.9426492
bicycle 0.93495274
bicycle 0.95183766
bicycle 0.9824759
person 0.90335894
bird 0.91761816
pottedplant 0.9548067
person 0.95953304
person 0.9444175
person 0.94849175
person 0.91551536
person 0.9791652
person 0.95770293
person 0.9429015
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.317s + 0.034s (eta: 0:00:40)
person 0.9630188
person 0.96526694
person 0.9132862
person 0.97776765
person 0.967271
chair 0.93341964
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.336s + 0.035s (eta: 0:00:38)
car 0.927631
person 0.96530807
person 0.95031196
person 0.974757
person 0.920773
person 0.94815683
person 0.9015692
car 0.9809833
person 0.928539
person 0.9414739
person 0.9275347
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.331s + 0.035s (eta: 0:00:34)
person 0.9710468
chair 0.9168261
person 0.91533905
person 0.9447504
person 0.95210546
person 0.9132269
person 0.9264342
person 0.9787571
person 0.9750368
person 0.9217831
person 0.98482275
person 0.97386676
bird 0.9896568
bird 0.9722568
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.330s + 0.035s (eta: 0:00:30)
person 0.91277075
person 0.9608701
person 0.9805683
car 0.9465104
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.332s + 0.035s (eta: 0:00:27)
bird 0.90332747
bird 0.9770901
bird 0.9157037
bird 0.9187739
person 0.92065716
person 0.9125047
person 0.98466116
diningtable 0.9649805
person 0.96750176
person 0.9083867
chair 0.93086034
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.338s + 0.035s (eta: 0:00:23)
person 0.9806662
bus 0.97550946
dog 0.92640984
diningtable 0.9636058
chair 0.92063755
chair 0.9811897
chair 0.9586277
chair 0.90793014
pottedplant 0.917297
person 0.9293767
person 0.9623774
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.335s + 0.035s (eta: 0:00:19)
person 0.9437012
person 0.93268657
person 0.93776375
person 0.9167886
person 0.92937773
bird 0.9614499
person 0.93920195
person 0.95574945
person 0.97198784
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.333s + 0.035s (eta: 0:00:16)
person 0.9571061
person 0.9149945
train 0.940725
person 0.94720143
person 0.95853066
person 0.95247304
person 0.9197659
person 0.94415516
person 0.9224379
person 0.98886687
person 0.9444543
person 0.9757563
person 0.93924135
tvmonitor 0.92147774
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.331s + 0.035s (eta: 0:00:12)
person 0.94514054
person 0.93277895
horse 0.90123135
horse 0.938449
person 0.96514314
person 0.9045581
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.329s + 0.035s (eta: 0:00:08)
person 0.97435015
person 0.9669917
person 0.9122447
person 0.92683923
person 0.9035079
person 0.9760114
person 0.91063315
chair 0.9613341
person 0.955035
person 0.94894123
person 0.9344993
person 0.90796775
car 0.91476715
car 0.9517722
car 0.9404935
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.329s + 0.035s (eta: 0:00:05)
person 0.97425824
person 0.9332162
person 0.95210576
person 0.9461783
person 0.9637805
person 0.9522303
person 0.9878143
person 0.98027486
person 0.9255608
person 0.97650594
person 0.9206425
person 0.9038046
person 0.91542655
diningtable 0.9129915
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.325s + 0.035s (eta: 0:00:01)
person 0.9248334
diningtable 0.9380171
person 0.9563486
person 0.91848963
person 0.987834
person 0.9570619
person 0.9171778
person 0.9618102
person 0.98251295
person 0.9337754
person 0.9610721
person 0.9511449
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.474s + 0.040s (eta: 0:01:03)
person 0.9720558
person 0.9590043
person 0.9269774
diningtable 0.9556903
person 0.9731856
pottedplant 0.93160075
pottedplant 0.9005917
person 0.9609961
diningtable 0.91334647
chair 0.9629372
chair 0.9495048
person 0.9955374
person 0.9801277
person 0.96450824
person 0.90184397
person 0.95722044
person 0.9734808
person 0.9472749
person 0.90071285
person 0.93441147
person 0.9589003
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.327s + 0.033s (eta: 0:00:41)
person 0.92790776
person 0.98058164
person 0.91724676
person 0.9607662
bottle 0.94831073
person 0.9212975
person 0.9296611
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.323s + 0.035s (eta: 0:00:37)
chair 0.95685226
bottle 0.91959864
person 0.9331189
person 0.959929
person 0.9095797
person 0.91135
person 0.9127666
bottle 0.9020903
bottle 0.9476553
person 0.9815548
person 0.9111494
person 0.94768447
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.323s + 0.038s (eta: 0:00:33)
person 0.9489855
bird 0.9059594
person 0.96201354
person 0.9508373
person 0.95953554
person 0.94428575
person 0.9261488
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.328s + 0.038s (eta: 0:00:30)
person 0.9725978
person 0.95255524
person 0.9606072
person 0.97429097
person 0.9159002
person 0.90302217
person 0.94713134
person 0.94352096
person 0.9664972
person 0.91344523
person 0.9100889
bottle 0.94999945
chair 0.94660693
chair 0.9084555
chair 0.92062753
boat 0.9185174
boat 0.90183
person 0.9662444
person 0.93527734
person 0.9146144
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.323s + 0.038s (eta: 0:00:26)
car 0.9781828
car 0.9262461
person 0.9719511
person 0.9228585
person 0.9176678
person 0.9589195
person 0.90924686
pottedplant 0.9259167
pottedplant 0.9458746
car 0.98688436
person 0.92893344
person 0.98890954
person 0.92360234
person 0.9415443
person 0.9161161
person 0.97675043
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.321s + 0.037s (eta: 0:00:22)
person 0.95500886
person 0.98001647
car 0.939614
tvmonitor 0.9253425
person 0.91717255
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.326s + 0.037s (eta: 0:00:19)
chair 0.91083133
person 0.9687269
person 0.90576506
train 0.9467174
bird 0.9783981
person 0.916209
person 0.912027
bicycle 0.95342547
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.324s + 0.038s (eta: 0:00:15)
person 0.9567984
person 0.93521297
person 0.9314619
person 0.92927635
person 0.98648524
person 0.95252764
person 0.9737607
person 0.9904703
person 0.9239943
person 0.94118345
person 0.9501437
person 0.91141814
car 0.9292506
person 0.96925956
person 0.9370866
person 0.9648743
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.322s + 0.037s (eta: 0:00:12)
person 0.9114524
person 0.9162187
person 0.91247034
chair 0.9758876
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.321s + 0.037s (eta: 0:00:08)
person 0.9718492
person 0.9123123
person 0.92926085
person 0.9528231
person 0.9762082
pottedplant 0.90572065
boat 0.9086599
person 0.9563655
person 0.9095955
person 0.9042664
person 0.9453283
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.321s + 0.037s (eta: 0:00:05)
person 0.967924
person 0.95200175
person 0.9667935
person 0.93103135
person 0.94614226
person 0.9002715
person 0.9241506
person 0.9275159
person 0.95047694
person 0.9529466
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.322s + 0.037s (eta: 0:00:01)
person 0.9215269
person 0.96808976
bird 0.9351581
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.470s + 0.028s (eta: 0:01:01)
person 0.9068821
chair 0.9030442
chair 0.9477668
person 0.9521957
person 0.91477185
person 0.9490344
person 0.9242238
diningtable 0.9201724
person 0.9616318
person 0.9507588
person 0.9176921
person 0.9557943
person 0.9105117
person 0.96005815
person 0.90375036
person 0.9085213
diningtable 0.93834186
chair 0.9761071
person 0.90814215
person 0.9872843
person 0.9211318
person 0.9734538
person 0.98578656
person 0.9258314
person 0.98302263
person 0.9682778
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.318s + 0.036s (eta: 0:00:40)
person 0.9264772
pottedplant 0.9397669
person 0.9075105
bird 0.9364874
bird 0.92316914
bird 0.9223583
bird 0.97016215
person 0.964786
person 0.94131833
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.313s + 0.034s (eta: 0:00:36)
person 0.91424197
person 0.95839554
person 0.96029544
person 0.9682971
person 0.9099111
person 0.9295836
bottle 0.94387287
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.315s + 0.036s (eta: 0:00:33)
pottedplant 0.941455
chair 0.9329873
person 0.97943145
person 0.91638696
bottle 0.92703664
bottle 0.9005497
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.317s + 0.036s (eta: 0:00:29)
person 0.9530361
person 0.9511526
person 0.90695155
person 0.9459002
person 0.90444344
person 0.9945497
person 0.97670615
person 0.94123256
person 0.95870453
person 0.93823177
person 0.93848616
person 0.9926421
person 0.9519408
person 0.9580012
person 0.9492379
person 0.9502801
person 0.9811509
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.313s + 0.036s (eta: 0:00:25)
person 0.9179965
person 0.9540612
person 0.9705669
diningtable 0.9704471
chair 0.9391848
chair 0.9246885
chair 0.9712081
chair 0.9844971
aeroplane 0.915833
person 0.96787024
person 0.9759938
person 0.9318628
person 0.9401105
person 0.9721842
car 0.9255717
car 0.9030589
car 0.96681625
car 0.9518297
car 0.95831656
bird 0.95080143
chair 0.92542416
person 0.9260003
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.316s + 0.036s (eta: 0:00:22)
person 0.9697804
person 0.9260421
person 0.9624439
person 0.96075463
person 0.9896483
person 0.92438227
person 0.9094883
person 0.93613446
person 0.98799324
person 0.93122274
horse 0.9296293
person 0.95272195
person 0.9069353
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.313s + 0.036s (eta: 0:00:18)
person 0.9325067
person 0.9139116
chair 0.95246243
chair 0.97647846
chair 0.9646966
pottedplant 0.9359978
pottedplant 0.91334313
pottedplant 0.9602739
person 0.9350216
person 0.92191625
person 0.92226195
person 0.95209455
person 0.9431222
person 0.92623043
person 0.9292519
bicycle 0.9391136
person 0.9829106
person 0.90563226
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.313s + 0.036s (eta: 0:00:15)
person 0.9914677
person 0.93372846
person 0.9144414
person 0.95568204
person 0.9782032
person 0.97291845
person 0.9717054
person 0.97519726
person 0.96959126
person 0.9399683
bus 0.93588233
person 0.9175586
person 0.9508624
person 0.9290489
person 0.9264479
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.312s + 0.035s (eta: 0:00:11)
person 0.97558177
person 0.9064381
chair 0.94605625
chair 0.9058658
tvmonitor 0.915376
chair 0.96440965
chair 0.9534417
chair 0.9370071
pottedplant 0.9047106
person 0.9635898
person 0.9390188
person 0.973612
person 0.9083903
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.312s + 0.035s (eta: 0:00:08)
motorbike 0.9667917
person 0.97756577
horse 0.9745457
bicycle 0.9478693
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.313s + 0.035s (eta: 0:00:04)
person 0.9564881
person 0.9402104
person 0.9790901
person 0.961029
person 0.97628516
person 0.92090404
person 0.91097116
bird 0.9298651
chair 0.90791714
chair 0.9580527
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.313s + 0.035s (eta: 0:00:01)
person 0.9804968
person 0.98257303
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 77.037s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 351  346 1479 ... 1240 1254 1253]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2628
INFO voc_eval.py: 171: [ 86 159  89 352 160 701 630 165 524 736 411 652 755 756 361 382  91 714
 657 168 529 167 751  92 757 771 761 775 525 633 180 179 702 704 750 738
 700 766 414 162 772 164  98  90 721 421  94 356 105 634 533 114 418 774
 575 731 383 348 107 101  88 528 735 636 416 171 532 384 373 759  87 177
 752 386 728 611 310 536 412 758  59 166 558 527  99 632 606 640 118 590
 734 585 476  93 543 743 394 560 169 469 423 354 689 754 654 415 567 349
 170 708 574 609 733 120  44 569 217 471 565 472 737 608 542 564 760 705
 643  31 253 281 440 770 749  97 773 218 548 531 644 367 372 475 665 646
 431 269 265 554 566  61 397 485 641 463 570 381 380 353 553 549 279 362
 282 635  34 358 707 100 593  47  39 623 161 261 781 710  57 396 315 256
 242 639 434 110 427 534 453 364 351 620 385 275 546 197 767 538 350 730
 776 119 629 151 748 587 360 436 769 638 764  12 178 227   6 762 596  46
  55 555 557 586 149 313 626 621  54 174 720 615 254 115 461 706  35 417
 393 245 248 103 268 768 631 688 744 729 405 780 591 325 181 357 765 290
 273 722 455  15 642  29 645 420 123 741 163 616 435 763 703 778 116 723
 182 495 669  68 147 550 753 241 398 266 484 454 425  27 726 196 530 742
 539 456  81  33  32 650 363 592 208 691   3 572  83 229 259 278 595 464
 612 376 718 375 618 339  64 509 422 625 571  28 117 607 236 745 433 551
 390 104 673 683 172 779 547 173 439 725 368  13  79 255 129   2   7 746
 267 150  48 219 713 379 727 304 399  51 277 127   5 186 614 334 237 148
  95 715 233 597 619 258 624 428 312 711 270 627 238 663 699 365 740  96
 283 175 580 183  70 109 206 220 535 637 622 610 262 195 467 408 230 448
 442 276 667 443 747 108 589 602 309  73 666  74 724 176 684 257 613 155
 154 561 407 198 314 498 263 647 297 214 655 134 326 664 366 294 402 424
  24 192 499 686 709  84 672 739  76 234 200  56 579 299 400 541 225 340
 497  10  50 199 696 264  37  36 430 246 133 260  49 512  42 213 204 559
 142 335 695 581 308 584 544 205 492  58 395 516 209 226 594 201 576 272
 413 479 426 732 301 247  16 144 324 403 185 240 153 102 371 487   4  22
  69 378 409  30  25 143 649 628 130 496 111 289 698 224  40  82 459 588
 470 406 391 291 526 697 685 445 648  75 156 332 563  17 523 599  67 404
 568 446 444 651 486 252 203 438 223 617 540  85 137 717 287 135 429 668
 675 128 504 474  72  63 537 231 125 687 674 656 583 141 316 250 716  43
   0 243 333 392 556 545 106 374 124 520 603  45 488 517 341 489 573 187
 480  21  38 507  52 319 693 458  26 441 131 502  18 670 481 228 401 712
 437   8 121 605 719 292 235 598 184 521 369 494 432 300 483 671 490 482
 239 694 410 506 345 188 552 232 342 284  65 518 191 503 505 519  41   9
 249  78  80  19 202 562 468 419 251 661 323 604 221 522 132 658 451 222
 303 138  14 122 189 190 158 690 139  71 493 653 271 244 286 136 274 126
 112  11 140 679 212 510 355 662 293 338 466 157 676 280 306 211 145 659
 321 377 210  62 296  77  20  66  60 682 370 318 113 327 298 295 508 447
   1 317 215 460  23 660 146 330 452 462 465 501 336 449 450 600 288 473
 477 478 311 337 500 359 320 601 515 692 491 331 511 305 457 322 777 347
 307 514 302 152 344 582 387 194 285 578 577 216 346  53 513 680 207 328
 388 343 389 193 677 329 681 678]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.4844
INFO voc_eval.py: 171: [ 562 1495 2799 ... 2792 2797 2565]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1930
INFO voc_eval.py: 171: [ 607  606  723 ...  127 2195 2191]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2500
INFO voc_eval.py: 171: [489 445 390 ... 549 518 921]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3530
INFO voc_eval.py: 171: [185 495  89  91  75  82  90 498  74 194 496 512  95 441  73 111  96  94
 144 190  78 288 409  37 461  77 456 178  92 263 447  83 289  52 428 149
  99 215 438  81 408 341  39  84 400 475 330 347 433 387  85  76 236 222
 368 477 248 519 300 314 172 364 216  36 153 152 302 280 533 405 101 360
 112 109 219 411 492 424 272 247  79 145 296 266 271 370  80  93 287 466
 281 394 231 345 121  63 162 421 307 208 213 143 142 530 384 326 268 193
 212 402 371 510 332 465 283 275 478 455 431 304  51 184 430 343 386 128
 290 417 262 161 120 217 357  58 209   8 490 446 150 277  19 179  53 342
 114 434  48 108 401  97  49 348 119  33 100 365 126 373 436 147  47 532
 316 218 131  98 139 138 454 508   1 295 335 531 527 514 395 489 110 399
 170 234 420 423 282 485 336 416 195 457 315 398  20 359 232 122 118 361
 169 104 214 116  18 528 197 136 452 113 238 340 463 207 432  34 513 392
 501 445 206 393 444 220 482 462 529 264 369 356 133 155 115 202 168 346
 129 385 389 391  42 407  16 156 182 293 241 210 148 274 299 278 189  64
 415 117  11 270 534 479  44 127  57 390 157 301 173  17 237 469 515  12
 358  35 524 125 137 338 414 306 481 388  10 429  40 435 439   9  21 123
 331 174 103 437 160 440   6 367 246 284 276 146 471 297 453 205 476 464
 151  86 418 470 427 226  55 159 419 362 351   5  59 309 468 158 224 269
 196 473 379 259 106 250 516 230 460 124 467 443 487 105  62 459 480 458
 451  87 355 267 211 329 130 203 177 493 520 242 474 412  15 406 426 171
 366  13 175 486  14 107 491 494 102 279 334 425 132 167   0  45 310 372
 240  50 308 187 154   3 410 324 181  56 298  88 244 292 363 422  41 200
 176 273 333 225 294 337 265  43   7 134 251   2 257 403 235 239 472 517
 186 523  38 339 291 404 442 448 344 380 349 180 318 325 254 374 249 450
 312 305 354 135 350 352 313 183 221 483 253 255  32 484 243 252  27 223
 327 413 245 311  46 229 449 509 260 521 488 261 303 328 286 258 518 233
 256 198 500 285 227  71 320 381 188 353  61  70 526 322 140  66 199 377
 319 375 192 164 141 166  68  67  72  65  31 525 378 165 204 163  22 503
 376  25 317 323  29  54 321 522 228  23 191 396  26  30   4  69 505 497
 201 507 511 397 506 499 382 502  28  60 504  24 383]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4220
INFO voc_eval.py: 171: [1505  704 1442 ... 2227  105 2220]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.3098
INFO voc_eval.py: 171: [ 29  67 236 237  21 285  25  71 283 145 105  31 126 284 125 260 146 261
  80 230 110  70  99  69  28  68  24 114  22 266  23  51  27 214 118  17
 277 102  97 227 187 135 190 144 262  52 234 217  46  84 218  53 115 156
 220  95 203 165  91 127 201 128  18 154  55 175 107  66 202 157  10  74
  73 265 267 149 255 117 112 216 279 282   7 111  79 178  56   0 176 148
 200 215 245  54 137 150 171 222  30 136 280  26 124 207 208 166  82 278
 244 123 177 106 204 179 129 108 119 147 182 116 194 281  81  20  19 205
  72 288  88 263  12 189 155 132 103 219 113 228  75 120   2 257 193 142
 143 259 181 180 264 195   1 270 152  49  43 206 104 240  13  90 209  16
 212 256 235 243 130 131 254 231 272 122  32 109 192  45 169   6 100  87
 198 233 289 246  50  36 196 258 253  98 191 273 183  61  89   3   4 199
  57  86  48 242 163 133 225 101 170  96  85   8  83  47  35 224 159   9
 232  59 158 164 168 121 249   5 247 153 160  11  78 167 141 213 186 269
 210 211  92  40  94 197 138 223 229  93 174  41  34 268 221 188 250 151
 287 248  58 173  62  33  15  42  44  65 271 162  60 274  63  39 239 161
 139 140  76 252 226 241 251  38  77 184  64 185  37  14 275 172 276 134
 286 238]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0636
INFO voc_eval.py: 171: [6255  726 2683 ...  279  283 4339]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3471
INFO voc_eval.py: 171: [552 341 169 211 216 215  37 606 258 343 338 600 220  54 103 301  36 577
 493 207 205 172 223 290 505 474 342 206 210  39  21 605 265 611 344 348
  50 599 592 193 116 487 475 208 490  48 214 286  71 294 560 249 585 419
 612  53 415 492 145 260 345 307 477 509 497 167 486  42 268 166  52 323
 200  41 479 602 250 271 125 506 508 601 491 234 346 603 581 316 108  68
 259 337 609 104 221  59 251 219 607 312 217 588 107 413 281 110 140 327
 586 328 434 191  31 278 194 553 576 209 170 584  38 198 230  19 261 476
 171 273 114 527 126 183 112 146 150 149 236 583 313 598 572 101 593 432
 436 494 322  46 109  76 142 256 197 143 128 489 582 498 420 113  86 302
 340 558 132 325 480 282 306 523 229 224 387 591 495 404 225 155 538  67
  70   9 117 397 398 437 478 488 226 231 485 580  69 111 549 175  18 304
 574 308 339 566 156 349 554 148  55 262  51 590 275 407 440 416 270 233
 254  64  20 441 309  65 556 173 180  15 405 235 324 213  61  87 295 360
 119  32 513 610 305 283 604 579 151 565 439 355 118 463 326  60 608 427
 567 232 484 247 310  35 433  58 347 218 418 189 377  10 263 331 106 190
 296 300 292  75 502 212  91 571 222 517 227 575 417 153  45 539 589 521
 529 188  13 228 511 578 178 314 147  72 596 587 185 559 540  77  30 297
 105 161 522 152 129 563 550 122 374 594 174 184 315  94   3 568 526 177
  49 144 435 115 520 379 299 383 102 421 154 541 414 196 371 595 530 438
 393 181 255  56 535 396 516  97 382 442 303  63  27 471 237 168 515 135
 366 368 470 165  88  66 284 555 311 623  33 120 139 615 204  89 272 279
 501 394   2 164 500   4 195  11 121 545  62 395  90 548 412 130 392 288
 547 367 406 597 335   0  22 557 391 137  85 472 481  74 573 280 622 100
 551 186 291 187 561 512 462 298 409 199  95   1 376  78 176 192 182 369
  73 141 363 451 399 257 136 159 127 450 365 385 537 252  23 334 534 614
   8 499  96 201 524  26 546 616 613  40 466 482 428 514 179 285 243 453
  16 617 452 510 248 443 242 388 358 287 431 564 408 536  29  93 157 375
  47  84 518 445 504  14 253  98 403  99 333  17 269  12 131 410  25 507
 264 160 202 423 162 426  24 158 473  34  43 531 528 519  44 370 496 318
 411 503 244 245  28 424  57 138 350 321 133 277 533   7 386 373 525 380
 332 384  92 467 422 378 163   5 562 483 532 430 468 203 352 372 469 317
 544 400 330 381 429 465 246 464 570 134 357 389 238 276 266 329 274 241
 336 425 542 621   6 569 401 620 454 240 390 543 319 459 267 289 444 364
 293 320 402 239 351 356 353 618 359 619 354 457 362 361 456 458 455 461
 124 460  79 447  80  81  82 449 123 446  83 448]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1350
INFO voc_eval.py: 171: [2967 1286 1342 ... 2116 2299 2115]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3532
INFO voc_eval.py: 171: [384 494 457 921 465 418 389 498 157  81 517 490  66 548 684 511 592 278
  67 853 220 417 392  60 169 927  59 164 496 890 597 804 920 884 854 512
 682 386 149 279 396 166 933 240 924 513 458 855 394 288 160 225 857 653
 242 463 882  84 566 851 238 561 917 421  65  58 856 654  85 398 303 462
 397 168 802 484 880 464 838 236 257 282  70 265 893 281 467  82  68 162
  29 509 326 248 919 923 487 740 328 639 393 250 367 876 650 253  80 640
 649 158 280 557 237 841 156 578  33 206 167 621 866  27 681 227 922 399
 567 214 284 600 926 727 886 455 163 676 803 769 492 244 863 359 305 215
 572 346 395 813 516 322 482 332 315 901  46 861 718  75 347 146 739 493
 358 641 896 488 840 483 594 709 674 159 550 848 881 525 762 170 275 171
 691 929 461 902 372 868 891  71 150 683 559 460 486 383 323 659 413 595
 320 858 228 825  28 932  48 268 255 472 799 694 459 239 795 274 290  87
 596 130 903 690 620 213 723 161 925 514 510 839 441 147 260 266 195 264
 940 710  63 523 224  30 426 273 485 811 859 155 787 772 321 430  37 165
 875  73 724 480 887 521 792 812 456 241 667  83 774  76 362 526 474 737
 368 226  62 732 495 270 476 152 466 829 770 671 918  74 593 387 898 222
  25 733  69 721 221 427 943 519  78 817 899 515 869 327 137 656 533 211
 725 283 342 612 707 388 289 468 133 312 337 680 821  47 689 647 428 552
 850 208 232 420 403  86 934 373 679 254 429 661  44 210 370 570 212 708
 941 527  41 564 115 735 744 335 662 249 246 698 849 345 261 835 475 828
 895 247 256 295 471 100  11 584 277 791 223 314 229 601 867 243 935 205
 271 391 749 272 569 910 773 618 319 766 808 827 437 589 889 742  35 688
 151 768  21  77 657 771 568 101 154 311 194 860 324 790 598  19 646 310
 911 730 736 382 820 318 938 852 734 285 129 888 478 360   5 269  72 357
 900 692 928 904 491 481 937 942 400 209  79 826 140 102 425 207 738 336
 800 245 652 599 897 939 885 267 325 390 819 128 469 186 588 864 553 906
 726 571 743 153 554 551 865 276 173 711 251 317 666 894 759 798 916 665
 252 330 375   7 185 313 699 258 696 613 603   0 148 473 259 660 339 308
 713 677 489 405 340 870 789 350 673 233 695 470 651  34 822 619 837 602
 763 823 752   6 188 622 351 892 385 198 675 862 672 604 818 638 439  38
 369 931 334 907 905 781 142 767 793 833  89 377 361 297 915 477 412 745
 717  56  50 701 415 776 499 693  51  61 304 497 741 936 436 605 419 606
 114   4 307 538 331 777  16 627 143  22 805 309 103 376 834 678 624 302
 930 615 479 131 341 371 524 316 416 500 338 748 354 176 196 685 655 783
 576 642 703 182   9 378 230 636 450 329 796 333 585 363 756 172 815  12
 794 663 145 134 374 658 449 136 343 286  64 184 422 623 200 104 668 536
  88 432 580 809 637 746   2 189 782 765 135 883  31 797 944 610 845 824
 871 831  54 144 816 406 138  98 628 780  26 106 118 754 344 287 577 582
 306 193  49 139 201 121 116 132 529  57 614   8  40 583   3  13 431 364
 107 542 174 814   1 909 349 625 352 719  45 530 546 544 111 105 587 520
 648 664 872 775 586 760 401 579 700 879 634 353  24 720 801 501 402 381
 704 301 537 591 175 806 697 534 581 296 669 502  92 714  20 191 190  32
 712 573 522  10 630 643 117 503 112 715 758 348 728 108 507  95 549 355
 810 830  55 407 644 541 908 298 574  93 873 645 670 607  99 219 836 438
 539 356 414 505 545 555 300 632 874  96  53 540 535 235 127  91 617 562
 785 832 504 216 626 291 807 454 556 747 218  14 750 120 563 784 706  18
 447  43 452 531 590  97 877  23 141 109 234 558 365 508 110 453 788 217
  52 192 435 113 575 451 608 751 635 294 753 448 518 183 878 757 199 197
  94 404 532 755 231 722 609 729 125 611 299 705 633 716 631 702  36  90
 506 408 761 119 202 433 440  42 203  39 263 786 123 687  17 616 122 262
 779 731 528 846 379 380 411 204 560 409 187 629  15 445 847 124 686 177
 181 292 442 423 543 293 410 126 565 180 424 366 179 178 444 843 547 842
 844 778 914 764 443 913 912 434 446]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2023
INFO voc_eval.py: 171: [822 114 337 745 338 826 117 120 340 839 155 652 746 116 559 832 647 521
 677 767 121 231 671 115 122 220 678 163 453 118 759 489 530 749 482 653
 157 238 462 161 126 156 158 688 240 828 676 248 339 355 525 838 562 683
 124 845 519 125 804 233 843 531 841 565 682 656 681 524 486 176 258 661
 762 800 371 147 535 465 234 236 834 854   5 307 297 810 675 750 754 723
  48 651 247 455 842  50 310 520 561  36 318 536 798 309 693 811 753 582
 805 221 770 177 772 528 373 809 368 523 578 577  88 168 493 590 478 464
 457 266  12 668 539 558 553 487 211 359 439 331 802 631  85 458 686 689
 259 572 389 243 119 808 164 210 540 315 257 605 829 796 687 180 129 222
 313 461 583 752 232 135 169 760   6 563  75 132 684 295 672 123 814 378
 216 568 484 284 367 835 492 250 820 532 669 600 454 178 267 162 173 526
 460 560 265 825  49 335 376 280 375 370 160 350 761 862 300  81 547 566
 589  38 165 795 351  13 674 469  39 880 851 585 388  90 242 587 481  84
 575 311 166 283 533 212 127 816 548 227 764 806 420 690 501 852 219 131
 763 766 819 496  33 861  91 673 827 167 756 584 171 150 225 865 214 235
 314  73 296 837  86 298 316 850 643 209 607 537  20 581 416  94 374 226
  37 541 718 213 141 441 228 503 720 778 611 256 246 394 840 823 570 815
 848 782 817  97 803 818 480 784 765 333 769 860 505 183 306 452 864 479
 644 444 680  11 551 529 317 182 554 467 522 130 456 630 499 287 223 327
 286 185 392 308 557 459 369 170 777 302 320 324 747 646 830 657 312  72
 534  93 186 151 813 396 277 336 268 148 666  69 855 229 301 395  70 133
 393 279 768 149 679 433 440 877 326  47 801 134 552 542 658 771  96 137
 292 729 224 391 323  22 466 513 285 685 281 853 179 172 299 411  98 500
 708  17 527 632 261 485  53 199 208 788 291 128 564 325 346 757 498  15
 863 866 549 869 751  95 648 833  87 494 106 665 857  30 159 799 867 239
 755 836 870 260 382   7 342 776  89 821 812 502 807 624 550 196 293 197
 614  62 797 704  92 785 255 230 410 381  80 703 574 418 701 349 399 543
 698 649 544 274 847 477 271 143 386 102 597 202 244  24   8 290 443 152
 175 417 556 334  23 571  71  76 628 442 413 181 546 617 347 849 289 728
 241  31 781  41 174 184 468 474  32 783 276 868 407 264 488 188 282 372
 385 205 288 700 858 859  29 706 272 545 824 660 490 504 348 249 273 856
 694 705 252 645 237 380 618 303 153 758 251 748 670 620 471 195 538 142
 254 364 384 844 878 715 615 451 650 518 555 662 627 846 275 625 198 269
 613  74 730 831 774 377 495 253 383  82 636  46 204 576 773 278 567 722
 881 610 400   1 353 207 473 707 775 304 206 633  67 497 616  43  34  28
 876 140 879 245 725 345 580 738 409 786 491 779 263 724 200  14 365 189
 431 609 787 744 332 379 601 329 217  40 270 105 483 387  44 404 873 709
 146 438   0 586 780  21 341 717 599 872   2 427 445 203 305 619  65 421
 108 515 201 612 509 629 154  27 390 713 423 593 697 506 475 573  19 588
 470 742  83 604  51 623  61 322  42 635 432  35  52  79 517 711 569 319
 716 446 579 740 103 871 192 402 608 654 104 100 731 622 606 664 655 412
 719 516 328 406  99 107 262 414 101 714 472 692 507 732 476 397 366 710
 702 463 435 190   4 343  25 634 215 110  16  66 621 401 362 642 194 639
 626  63 735  26 144 415  68 111  45 637 139 508 743 741 721 789 594 398
  18 191 790   9 667 510  64   3 408 659  78  77 145 712  56 113 403 363
 424 691 638  57 733 429 436 512 696 405  58 330 138 434  10 596 663 136
 514 699 792 193 112 598 422 739 419 695 734  59  60 736 640 437 187 218
 641 321 357 591 727 595 109  55 511 726 592 737  54 791 793 294 602 874
 344 794 450 875 430 356 361 425 428 426 449 360 358 354 352 603 448 447]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3166
INFO voc_eval.py: 171: [ 58 358  28 208  57 224 216  67  63 361  60 154  79 232 243 155 373  86
 209  34  42  36 383  65  68 357 364 265  92  18 212 226 173 369  32 350
  66 332  62  22 359 356 371 149 321  40 245 387 151 272 279  69 267  41
 230 119 184  23  81 322 260  84  38 239 258 266 263 365  19 215 122  16
 382 336 307   6 234 390  50 218  11  71 153 349 240 287 384 277 259 229
  30  15  74  72 120 152 211 223 298  47 271 324 288   1 214 312 257 123
 194 330  39  35 107 169 256  73 231 385 101  70 115  78  76 295 389 381
 204 225 273 308 108  95 166 297 362 207  80 127 388 386 174  17 221 268
 150  37 199 188  46 347 130 341 121  31 346  94 360   3 193   5 363 131
 283 237 309 177 323 238  20 244  87 213 354 294 198 220 251 337  43 102
 219   4   7 242 270 299 351  27 196 197 143  29 112 339 103  45 111  88
 247 325 200 210 147  10 172 301 171 303 110 261 300 249 139  90 113 333
 165 109  26  12 331  85 186 205  33 255 366 285 290  77  75 335 144 134
  64 320 281 340 125 264 106 313 192 311 286 241 161  59  82 105 338 175
 195   8  25 252  21 236 334  24  55 148  56 222 141 246 104 217  61  49
 314 135 310 329 250 233 160 132 142 293 185  13   2  51  53 133 140 124
  96 274 191 129 126 206 296   9 353 374 146 262 291  89 375 292 302 278
 235 136 163  44 370 304  93 328 227 275 128 157  52 137 276  48 345 180
 344  91 170 182 203  83 253 305 282 284 280 201 269 145 138 228 179 162
 167 254 315 306 164 248 372 355 368 343 342 178 176 327  14 316 352 168
  97 116 181 289 367 183 100 378 377 376  99 117  98 202 156 326 348 158
 319  54 159 317 318 380 189 379 114 118 190   0 187]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5753
INFO voc_eval.py: 171: [ 8281 13442  1018 ...   744   753   751]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5388
INFO voc_eval.py: 171: [2958  851  314 ...  425 2158 3065]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4357
INFO voc_eval.py: 171: [ 31 527  32 304 321 512  55  37  47 505 316 508 129 324 323 509  33 503
 461 573 218 298 249 328 392 353  62 507 334  34 156 523 471 333  64  69
 570 261 582 305 248 600 510 312 528 275 337 604 251 162 299 358 117 320
 513 120 221 506 574 395  49 525 400 354 223 607 521  59 352 132  63  44
 250 391 317 526  71 504 309 230 406 136 168 394 466 319 596 137 327 262
 583 119  51 228 126 133 118 580 603 231 585 458 356 227 226 325 522 202
 532  67 301 474 441 158 159 313 157 390 590 536 152  87 135 315 308 169
 581 462 318 568 575 276 593 268 302 588 260 203 303 332 599 216 490 279
 442 423 577 116 154  41 606 624 638 127 314 239 587 134 457 475 335 586
 488 167  57 176 131 215 625 591 225  88 405 306 597 338 460 310 531 520
  90 124 584 146 130 192 594 163 341 343 267 138 605 595  28 567 364 439
 219 217 336 478 214  97 123 307 222  43 368 145 300  65 450  36 601 529
 329 393 311 330 224 497 537 322 122 628 263  92 555 155 204 491 173 326
 602 465 547 297   5 160 396 643 579 578 501 144 331  35 389 566 598 519
  52 187 464 125 340 467 121 164 277  26  45 592 229 626 367  68 589 232
 196 569 404 571 193 576 530  50 220 558 535 524 166 540 165 147  75  91
 281  54  81 456 517 479 357 143 161 453 610 511 477 255 257 194 212 241
 359 197 195 459   4 256 243  56 264  22 422 174 634 240 485 487 290 412
 170 259 254 538 631 627 609 185  96 344 438 242 238 102 339 112 572 370
  79 534  76 493   2 128 495  98 518 420 641   0 153  89 183  93 637  29
 247 342 431 296 171 278 468 178 266 287  16 514 486 463 472 541 189  15
  23  70 181 435 539 553 208 443 113 210 608 252 150 205 265 630 640 201
 455 516 381 481 639 258 288 100 482 106 636 419 289 253 542 186 273  72
 500 184 489  60 177 611 449  73 410 114 469 360  38 483 211  42 293 556
 629 615  77 280 104  24 454 403   1 355 436 498  61  53 551 384 452  74
 565 270  25   9 632 107 421 515 198  46 642 484 633  10 411  27 434 559
 617 399 473 180 379 480 272 182 545 101 476 246 365 207 402  12  86 269
  85  78  66 561 398  82 418 401 348  13 437 533 635 175  18 373 549 408
  14  19 284  58  48 499  39 496 387 295 294 415 386 560 492 470 244 557
 271  40  30 407  83 562  17 140 383 245 151 451 199 274   6 282 283 620
  94 546  21 376 552 190 397  20 614 409 361 494 346  95 115 374  11 447
 446 103 292 148 141 291 550 564  84 362 372 109 347 548 612 502 371 554
 543 445 440 111 375 206 200 105 448 544   7 213 191   3 209 345 414  80
 108 139 428 172 110 430 233 366 179   8 621 382 622 285 613 388 563 377
 237 286 369 413 142 433 351 444  99 385 623 380 363 378 188 234 235 236
 618 619 424 432 616 425 350 149 417 349 427 426 429 416]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2817
INFO voc_eval.py: 171: [119  46  45   5 191  44 125 124  47 189 276 151 187 194 193 185 233 232
 116 126 157 252 117  12  61 236 331 273 148 243   6 277 184 156 333 235
 327 270 234 152 223 158   8  43 190 322  60 127 144 115  59  95 329 154
 174 291  80  99 155 143 188 320 153 175 231 249 149 253 147 250 101 183
 247  23 325 186  83 129 334 284  97 328 312  72 271 218  11 122 269  48
  96  82 195  49 104  30  89 245 244 106   4 251 226 105   7 103  63  71
 229 102  42 237 262 263  98 264  86 315  22  10 134  50  26 313 242 120
 110 118 240 192  53 199 248 212 321 256  31  85 132  88 332 307 246 150
 241 260 335 278 295  79 324 146  28 205  90 112 239   9  77 145  65 292
 317 225 130 330 121 224 123 286  33  16 133  24 303  81 128 318 257 200
 279 275 136 227  29  62 293  55 173  52 258 176 306 131 259 285  15 217
 230 326 220  25  84 272  75 209  54  87 274 222 302 238 265 177  76 207
 159 208 114  58 196 316 135 268 297 219 221 308  78 203  57 287  39 255
 319 309 294 107  32 288   0 142 111  64  17 310 206  73   1  74 160 296
 305  56 211 267   3 311 113  27 254 266 162 204 280 180 172 214 314  40
 139 163 202 304 164 166 281 138 213 182   2 301 210 108  19  51 201 298
  35 282 290 169  41 140 300 165 178  21 197 198  18 109 228 261  20 167
 299 161 168 141 171  36  91 170 137 181  34  69 179 289  66  38  67  68
  37  70 283  92 323 100  93  94 216 215  13  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1353
INFO voc_eval.py: 171: [594 357 595  84  88 113 366 301 362 196  79 162 273 123 115 421 356  90
 300 203 201 608 274 417  95  89 506 361 119 597 422 468 823 402 598 278
 302 740 200 416 367 613 204 737 303 684 202 802 467 500 176 593 451  65
 358  22 397 824 751  80 307 674  98 304 171 423 116 399 764 478 505 114
  77 101 401 808 283 285 692 795 126 425 177 398 208 107 365 260 653  99
 360  38 624 606 277 275 544 363 412 206 221 832 310 503 197 750 654 191
 600 420 159 185 418 747 168  44 282 473 419 830 184 742 198  92 757 831
 807 646 105  21 187 484 109 236 758 659 817 166 844 738 424 122 761 768
  68 306 803 199 799 182 685 475 129 415 835 569  41 811 486  43 833 755
 762 446 290 771 469 347  28 621  67  64 501 722 760 852 163   5 160 550
 173 414 368 586 219 816 655 232 741 364 818 445 281 767 489 616 681 179
 158 288 694 821 287 814 276   9 267 581 826 353  24 394 656 452 280  75
 172 661  57 263 293  55 629  87 717 444 678 675 806 805 791 660 392 691
 573 719  73 228 680  47  39  74 752 470  62 338 359 567 528 775 627 530
 128 825 225 238 853 289 376 728 297 165 756 231 521 548 103 784 769 450
 220 153 295 509 426 700 261 483 164 800  71 753 147 170 580 688 796  83
 190 352 585 687  36 316 309 331 846 328 437 393 487 247 428 611 146 582
 207 195 671 205 620 631 167 239 507  17 157 430 217 690 106 111 235 539
 785 458 330 662 229  60 155 502 427 230 108 222 429 180 299 494 329 801
 651 266  25 670 798 488 318 102 156 249 178 248 628 226 851  91 707 827
 447  26 572 726 536 370   7 186 759 233   6 819 854 404 727  50 745 396
 480 174 520 652 246 679 346  78 766 772 720 211 640  76 564 265 504 351
 526 746 189 311 804 485 552 188 256 332 125 194 797  97  42 242 657 695
   4 511 169 647  49 734 312 294 344 566 822 815 161 314 490 181 350 715
 234 154 454 731   2 617  45 565 100 645 828 218 407 630 264 829 538 739
  58 409 776 237 542  46 400 549 241 676 730  59 341 349 340  13 477 568
 729 625 192 391  96  93 574 576 305  19 834  85 689 686 571 455  23 672
 374 405 704 355 677 637 626 619 648 810 836 837 622 193 547 348 812 345
 787 584 706 435 253 296 408  63 339 138 658 254 777 763 855 135 291 809
 540 255 227 723 369  72 319 388 257 578 705 643 531 476 718 813 471 774
  15   1 711 570 284 518 269 575 515  81 786 395 387 683 439 175   3 782
 537 623 434  40 733 545 716  56 270 406 413  37 725 693 650 639   0 557
 673 682 529 183 512 482  61 709 754 298 744 607 325 372  82 262 663 481
 432 519 496 459 334 240 127  11 438  94 773 259 386 251 577 479 510 292
  20 110 279 794 141 556 649 820 354  14 410 514 244 436 214 555 789  48
  66 664 743 145  51 223 268 669 461 342 783 112 373 104  10  86   8 508
 474 527 749 534 516 724  35 140 589 286 449 736 245 748 441 250 120 699
 453 143 587 224 602 495 546 778 642 152 848 252  27 702 666 535  29 770
 735 142 456 148 641 457 721  54 431 216 644 131 411 465 554 150 533 543
 532 448 209 553 243 403  12 561 442 497 464 562  18 563 151  53 579 513
  16 780 517 149 271 321 551 638 323 491 139  52 636 272 559 591 708 712
 385 463 335 492 499  69 588  70 336 389 384 382 343 210 466 462 258 379
 493 668 590 842 710  34 601 326 213 788 313 377 433 765 541 472 133 703
 378 697 667 558 390 665 604 460 498 592 443 714 327 838 215 696 847 701
 371 779 713 440 781  33 632 144 136 333 793 599 322 525 845 839 212 137
 634 134 583 383 381 337 635 117 375 317 522 841 843 698 790 130 732 850
 849 124 614 840 792 118 380 523  32 308 315 324 320  30 612 633 596 132
 560 121 615  31 610 605 618 603 524 609]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2829
INFO voc_eval.py: 171: [521 330 751 391 189   4 577 542 323 333  72 551 414 677 609  49 334 385
  35 411 519 524 243 492 545 597 176 710  73 442 583 405  47 447 584 762
 732 644  48 109 387 530 183 331 100 553 443 753 421 282 592 180 550 643
 632 125  36  24 223 579 300 190 784 198 101 298 729 461 410  66 209 287
 768  56 146   5  15 585 615 413 285 593 752 148  41  10 730 293 756 689
   7 590  75 397 501 337 360 113 422 393 664 786 281 734 389  13 700 493
 317 149 567  64 771 721  40 754  50 611 304 252 596 616 249 225  51 537
 490 544 342 494 396 218  53 781 459 294 203 341 581 675 353 770 299  26
 467 177   9 245 764 587 288 713 102 557 367 412 572 712 372 445 474 456
 284 455 582 357 307 446 504 407 283 220 571 157  28 224 409 464 386 339
  43 175 297 589 552 655 335  52 470 776 154 388 473 742 720 155  84 206
 610  85 680 169 248 240 594 491 179  21 420 147  63 127 368 783 257 676
 311 359 785 539 718 153 580  20 197 251 399 755 774 666 394 130 449 381
 238 408 518 246 402 279 527 626 681 122 213 365 745 573 392 733 415 656
 401 468 278 763 129 495 430  82 162 260 578 711 586 326 290 672 769 665
 772 699 780 719  70 523 645 241 690 152 239 165 366 588 462 303 370 535
 650 217  16  71 543 661 697 186 452 591 673 168 630 595  81 161 647 352
  34 143 110 659 348 606  79 136 444 178 318  89 509 560 221 123 406  54
 194  44 529 779 612  80  55 390 618 466 698 631 135 159  32 112 160 346
 395 164 766 448 548 555 743 760 219 451 403 292 480 607 657 363 364 289
  37 369 777 103 765 340 259 272 166 398 309 192 773 520  74 613 173 778
 614 267 460 486 134 286 705 128 132 247 629 608 646 215 302  69 371 131
 691 400 384 216  76  27 531 428 617 458 310 174 273 291 533  77  25 561
 200 628 496 404 739 355 648 744 457 662 638 133 598 361 642 627 418 731
  42 327 547 358 453 775 635 660  33 107 782 787 377 336 450 788 108 574
 789 427 181 158 440 416  94   6 465 171 426 761  19 641 227 195 568 696
  67 434 728 767 222 314 723 525 736 663  83 624  62 295 139  91 565 313
 431 683 658 652 463 649 704 280 653 500 556 708 205 497 526 196 345 639
 472 634 214 187 230 738 622 373 276 688 207 707 212 172 211 244 362 746
 678 277 124 208 350  95  78 185 454 566 686 534 349 329 478 182 684 479
 324 469 188 382 651 510 138 549 117 674 679 120 546 163 106 575 471 236
 482 170 347 532 235 654 328 605 506 151 356 258  45 538 237  23 522 669
 332 266  18 481 685 105 354 184  99 150 483 255  68 502 167 499 325 193
 250 709  59  86  98   8 301 156 305 737 747 104 670 724 201  61 485 623
 433 687 210 119 695  97  65 503 528 439 636 202  11 726 140 633   0 505
 270 204 722 229 682 437 487 625 714 191 692 536 118 226  12 601 242 383
 271 114 308  90 602 121  39 701 141 254  17 199  31  46 694 142 115 376
 637 423 570 476 379 735 268 563  38 126 569 253  96 231 137 306 351 432
  14 424 640 554 228 116 380 338 489 145  87 484 435 256 234 269 758 374
 296 417 144   2 111 716 703 315 312 265 725 702 322 693  60 419 604 668
 375 321 320 508 498 621 514 706 477 558 512   1 667 564  93 441 507 438
  30 425 378 511 429 619 436 316 620 540 541 488  29  88  92 740 515 562
 671 475 741 599 603 727  57 264 513 232 559 262 715 233 344 717  58 275
 757 261 759 274 343 319 517  22 600 516 263 749   3 748 750 576]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4039
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3173
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.263
INFO cross_voc_dataset_evaluator.py: 134: 0.484
INFO cross_voc_dataset_evaluator.py: 134: 0.193
INFO cross_voc_dataset_evaluator.py: 134: 0.250
INFO cross_voc_dataset_evaluator.py: 134: 0.353
INFO cross_voc_dataset_evaluator.py: 134: 0.422
INFO cross_voc_dataset_evaluator.py: 134: 0.310
INFO cross_voc_dataset_evaluator.py: 134: 0.064
INFO cross_voc_dataset_evaluator.py: 134: 0.347
INFO cross_voc_dataset_evaluator.py: 134: 0.135
INFO cross_voc_dataset_evaluator.py: 134: 0.353
INFO cross_voc_dataset_evaluator.py: 134: 0.202
INFO cross_voc_dataset_evaluator.py: 134: 0.317
INFO cross_voc_dataset_evaluator.py: 134: 0.575
INFO cross_voc_dataset_evaluator.py: 134: 0.539
INFO cross_voc_dataset_evaluator.py: 134: 0.436
INFO cross_voc_dataset_evaluator.py: 134: 0.282
INFO cross_voc_dataset_evaluator.py: 134: 0.135
INFO cross_voc_dataset_evaluator.py: 134: 0.283
INFO cross_voc_dataset_evaluator.py: 134: 0.404
INFO cross_voc_dataset_evaluator.py: 135: 0.317
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.431s + 0.026s (eta: 0:00:56)
person 0.95905966
person 0.9339379
person 0.95390826
person 0.9889113
person 0.9039381
person 0.97050124
person 0.97489107
person 0.930783
person 0.9804134
person 0.9411499
bottle 0.94237715
bottle 0.9192821
bird 0.9540726
person 0.98057956
person 0.93791336
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.329s + 0.036s (eta: 0:00:41)
person 0.9268566
person 0.9376961
person 0.93690896
person 0.9577727
person 0.9795623
person 0.9693555
person 0.91547835
person 0.9485254
person 0.9233617
person 0.92675704
person 0.9191419
person 0.9110212
person 0.96145296
person 0.9190754
bird 0.90798104
person 0.98114395
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.333s + 0.039s (eta: 0:00:38)
person 0.92508847
person 0.93994325
chair 0.90333354
person 0.92449063
person 0.9679195
person 0.9187388
person 0.9476357
chair 0.9658559
person 0.97649676
person 0.98679674
person 0.9704014
person 0.99131167
person 0.98380107
person 0.9028895
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.335s + 0.039s (eta: 0:00:35)
bird 0.91295147
person 0.9175592
person 0.9930982
person 0.97820103
person 0.9226165
person 0.98402447
person 0.9562046
person 0.9597872
person 0.9689707
person 0.9310921
person 0.9593308
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.342s + 0.038s (eta: 0:00:31)
diningtable 0.9032342
diningtable 0.9405641
chair 0.9594798
chair 0.933895
chair 0.94473094
pottedplant 0.9270893
pottedplant 0.9175824
pottedplant 0.90592206
pottedplant 0.9307302
pottedplant 0.905677
pottedplant 0.9241663
pottedplant 0.90856224
pottedplant 0.94336957
pottedplant 0.92433345
pottedplant 0.9432651
pottedplant 0.9399544
person 0.9634398
person 0.98078614
person 0.98726636
person 0.93039334
person 0.98605007
person 0.90354115
person 0.90911907
person 0.9645607
person 0.94385433
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.339s + 0.038s (eta: 0:00:27)
person 0.9323774
person 0.9067353
person 0.9195096
person 0.92206097
person 0.914415
person 0.9496683
person 0.9244935
bird 0.9172949
person 0.90002275
person 0.9869011
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.335s + 0.038s (eta: 0:00:23)
pottedplant 0.9223153
person 0.9532574
person 0.9494008
person 0.95632225
aeroplane 0.90940547
aeroplane 0.92505205
chair 0.9820738
diningtable 0.9354717
chair 0.9782906
chair 0.92198575
pottedplant 0.9093617
bird 0.9929502
bird 0.94002116
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.335s + 0.037s (eta: 0:00:20)
car 0.92409194
car 0.9366399
person 0.9897399
pottedplant 0.9062329
person 0.96422577
person 0.93029
horse 0.9026188
horse 0.96712095
person 0.92657965
person 0.9561608
person 0.94592416
car 0.9419546
car 0.95999837
bicycle 0.97474253
bicycle 0.92694354
person 0.9529986
person 0.9699084
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.332s + 0.037s (eta: 0:00:16)
person 0.9680471
person 0.9518697
person 0.96796685
person 0.91491705
person 0.9876058
person 0.9375097
person 0.97501355
person 0.97926486
person 0.97022253
person 0.9895029
person 0.96090156
person 0.91490006
person 0.9854358
person 0.98204905
pottedplant 0.92640036
person 0.9901674
bus 0.9073043
person 0.9600455
bus 0.92587185
person 0.9443423
person 0.94371045
chair 0.93435395
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.331s + 0.037s (eta: 0:00:12)
motorbike 0.97570217
bird 0.91681284
person 0.9730575
person 0.9453102
person 0.94884634
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.331s + 0.037s (eta: 0:00:08)
boat 0.9810363
boat 0.9131026
boat 0.9131568
person 0.9342073
person 0.9393482
person 0.98999715
person 0.94460267
person 0.91111577
person 0.98481953
person 0.98649836
person 0.9359829
person 0.9754497
person 0.9199598
person 0.92611325
person 0.93401927
person 0.9595464
person 0.9479103
car 0.94734037
person 0.92454445
person 0.9661582
person 0.9854348
person 0.92230415
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.329s + 0.037s (eta: 0:00:05)
diningtable 0.92671555
chair 0.9724354
chair 0.91985464
chair 0.98024553
chair 0.97356695
pottedplant 0.9035551
chair 0.96196884
chair 0.9192686
person 0.9270148
bird 0.9037156
boat 0.92055124
person 0.9539329
person 0.9131071
person 0.98458904
person 0.90380543
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.325s + 0.036s (eta: 0:00:01)
person 0.98063976
person 0.9017785
person 0.9503129
person 0.9542585
person 0.9417847
person 0.95262444
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.436s + 0.026s (eta: 0:00:57)
person 0.9764682
person 0.97447306
person 0.9771503
bicycle 0.951083
bicycle 0.9757835
person 0.9094461
bird 0.92111325
pottedplant 0.9514518
person 0.962216
person 0.92341584
person 0.9435287
person 0.9144737
person 0.9208131
person 0.93548167
person 0.97930634
person 0.9605841
person 0.9466316
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.322s + 0.036s (eta: 0:00:40)
person 0.9650379
person 0.95537794
person 0.92170846
person 0.9112158
person 0.97849053
person 0.96893185
chair 0.93110967
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.337s + 0.038s (eta: 0:00:38)
car 0.9146385
car 0.9324707
person 0.96121466
person 0.9515247
person 0.97648776
person 0.93062115
person 0.95302606
person 0.90264565
car 0.98089784
person 0.9253726
person 0.9463221
person 0.9045436
person 0.9429372
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.332s + 0.036s (eta: 0:00:34)
person 0.97197366
chair 0.91946346
person 0.91851485
person 0.9463627
person 0.9489103
person 0.9176104
person 0.9283743
person 0.9792053
person 0.97585416
person 0.92438287
person 0.98559904
person 0.9738756
bird 0.9897461
person 0.9034456
bird 0.97011507
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.334s + 0.036s (eta: 0:00:31)
person 0.9221079
person 0.96298856
person 0.9811895
car 0.94734037
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.334s + 0.036s (eta: 0:00:27)
bird 0.97411877
bird 0.9089303
bird 0.91629845
person 0.92591166
person 0.9143857
person 0.98515147
diningtable 0.96796733
person 0.969261
person 0.9117612
chair 0.9329056
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.338s + 0.036s (eta: 0:00:23)
person 0.98232186
bus 0.97263056
dog 0.93714565
diningtable 0.9665935
chair 0.91740817
chair 0.9813892
chair 0.9606753
chair 0.90753007
pottedplant 0.913291
person 0.92403805
person 0.96571136
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.337s + 0.035s (eta: 0:00:20)
diningtable 0.90344113
person 0.9438936
person 0.94056
person 0.907205
person 0.9403539
person 0.9213572
person 0.93318766
bird 0.9599105
person 0.90936476
person 0.9585207
person 0.95197946
person 0.9039048
person 0.97199774
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.332s + 0.035s (eta: 0:00:16)
person 0.9585385
person 0.93093026
train 0.9400611
person 0.9483816
person 0.9614145
person 0.9490256
person 0.904285
person 0.9208538
person 0.94844025
person 0.98954284
person 0.97434956
person 0.9032388
person 0.9691477
person 0.93680155
tvmonitor 0.9200981
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.330s + 0.035s (eta: 0:00:12)
person 0.9453832
person 0.93421334
horse 0.90288645
horse 0.9427331
person 0.96604663
person 0.91211665
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.327s + 0.035s (eta: 0:00:08)
person 0.9742259
person 0.96657026
person 0.91222465
person 0.91955096
person 0.92719114
person 0.9773571
person 0.92829967
chair 0.96331745
person 0.952923
person 0.95273006
person 0.937162
person 0.9058885
car 0.9489134
car 0.9124568
car 0.941044
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.328s + 0.035s (eta: 0:00:05)
person 0.97339106
person 0.94387746
person 0.9544702
person 0.946384
person 0.97127736
person 0.9537937
person 0.9892402
person 0.981355
person 0.9235466
person 0.9760582
person 0.92107403
person 0.90646183
diningtable 0.91767323
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.325s + 0.035s (eta: 0:00:01)
person 0.93010956
diningtable 0.93631136
person 0.9605884
person 0.9881783
person 0.93022025
person 0.92336565
person 0.9828714
person 0.9644778
person 0.95201653
person 0.9540178
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.412s + 0.028s (eta: 0:00:54)
person 0.9609407
person 0.97311306
diningtable 0.960024
person 0.97378516
pottedplant 0.9248271
person 0.96352196
diningtable 0.91800946
chair 0.96309614
chair 0.9468783
person 0.9082003
person 0.99588335
person 0.9802571
person 0.9214738
person 0.9644955
person 0.95897937
person 0.9741305
person 0.9499868
person 0.9077648
person 0.9364271
person 0.9608713
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.305s + 0.032s (eta: 0:00:38)
person 0.9314654
person 0.98034245
person 0.9208696
person 0.9640457
bottle 0.9431521
person 0.92544806
person 0.90286535
person 0.93739825
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.321s + 0.034s (eta: 0:00:36)
chair 0.9533754
bottle 0.9117449
person 0.96361524
person 0.9077489
person 0.9184907
person 0.93825364
person 0.91908383
person 0.9214007
bottle 0.9464493
person 0.98221785
person 0.9118955
person 0.95130235
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.324s + 0.034s (eta: 0:00:33)
person 0.95370793
bird 0.91441864
person 0.96365607
person 0.90110403
person 0.9547663
person 0.9628176
person 0.94861466
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.327s + 0.034s (eta: 0:00:30)
person 0.9724033
person 0.9247166
person 0.9556055
person 0.9592576
person 0.9731813
person 0.9174294
person 0.9463867
person 0.9129391
person 0.9418953
person 0.91500574
person 0.96699935
person 0.90889037
bottle 0.94792736
chair 0.9396908
boat 0.91151434
person 0.9672213
person 0.91451186
person 0.94652754
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.323s + 0.035s (eta: 0:00:26)
car 0.9787779
car 0.9234044
person 0.92193925
person 0.9731618
person 0.9083757
person 0.9179102
person 0.96063626
person 0.9116312
person 0.90478665
pottedplant 0.94557106
car 0.98716277
car 0.92926663
person 0.93047243
person 0.9889811
person 0.93434644
person 0.9511022
person 0.94109815
person 0.9764317
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.323s + 0.035s (eta: 0:00:22)
person 0.9566647
person 0.9801444
car 0.9322252
tvmonitor 0.9212798
person 0.92047286
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.325s + 0.036s (eta: 0:00:19)
chair 0.90395135
person 0.9682994
person 0.91647947
train 0.95131594
bird 0.97900426
person 0.9112659
person 0.91364115
bicycle 0.94877034
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.327s + 0.036s (eta: 0:00:15)
person 0.95993465
person 0.93620515
person 0.9310352
person 0.92520314
person 0.986047
person 0.9522943
person 0.9734346
person 0.99027985
person 0.9229865
person 0.9455363
person 0.9688307
car 0.93292105
person 0.9667087
person 0.93552226
person 0.97819954
person 0.90012944
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.326s + 0.035s (eta: 0:00:12)
person 0.91195244
person 0.9200064
person 0.91487634
chair 0.97538483
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.323s + 0.036s (eta: 0:00:08)
person 0.97456497
person 0.917616
person 0.9431685
person 0.9535431
person 0.9769895
boat 0.90383965
person 0.9582608
person 0.9095598
person 0.91769284
person 0.94823617
person 0.90485924
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.326s + 0.036s (eta: 0:00:05)
person 0.9680645
person 0.9636726
person 0.9664766
person 0.9342735
person 0.94989413
person 0.9019974
person 0.92722136
person 0.9281295
person 0.94508046
person 0.9543111
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.326s + 0.036s (eta: 0:00:01)
person 0.92417777
person 0.96839774
bird 0.94088733
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.506s + 0.047s (eta: 0:01:08)
person 0.9129463
chair 0.9057085
chair 0.9486212
person 0.95428425
person 0.916333
person 0.9512042
person 0.9002477
person 0.93098617
diningtable 0.9218863
person 0.96444285
person 0.9595363
person 0.92871386
person 0.95938736
person 0.91326547
diningtable 0.9047353
person 0.96329856
person 0.9136532
person 0.9228149
diningtable 0.9447057
chair 0.97692204
person 0.91384417
person 0.90371823
person 0.98894846
person 0.91953236
person 0.9711448
person 0.9227381
person 0.985142
person 0.9248786
person 0.940217
person 0.90181315
person 0.98284
person 0.9675249
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.330s + 0.039s (eta: 0:00:42)
person 0.93065697
pottedplant 0.9380074
person 0.9068576
bird 0.9205012
bird 0.93089813
bird 0.97024494
person 0.96479535
person 0.9402592
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.327s + 0.036s (eta: 0:00:37)
person 0.9430268
person 0.9601608
person 0.902475
person 0.9626403
person 0.9695128
person 0.9079732
person 0.9355761
bottle 0.94303346
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.324s + 0.036s (eta: 0:00:33)
pottedplant 0.93824005
chair 0.9325966
person 0.9243527
person 0.97899836
person 0.9154551
bottle 0.9171146
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.317s + 0.036s (eta: 0:00:29)
person 0.9567364
person 0.9537717
person 0.91442925
person 0.94467413
person 0.9016728
person 0.99478287
person 0.9771121
person 0.94643
person 0.90752447
person 0.963129
person 0.9384379
person 0.9241467
person 0.99240726
person 0.92481446
person 0.9614066
person 0.90203637
person 0.92869216
person 0.9523679
person 0.9803836
person 0.9530592
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.315s + 0.035s (eta: 0:00:25)
person 0.92203987
person 0.9558776
person 0.97195643
diningtable 0.974145
chair 0.94008523
chair 0.92778605
chair 0.9724702
chair 0.98485273
aeroplane 0.9167611
person 0.96663654
person 0.9772101
person 0.9058331
person 0.9323906
person 0.93963253
person 0.9726528
car 0.9185975
car 0.91819614
car 0.96733105
car 0.9526073
car 0.9554648
bird 0.9524488
chair 0.90993863
chair 0.92487085
person 0.931415
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.321s + 0.036s (eta: 0:00:22)
person 0.97765744
person 0.9780266
person 0.9890126
person 0.9361589
person 0.91508937
person 0.9311198
person 0.988519
person 0.9279951
person 0.93240726
horse 0.9340112
person 0.95055145
person 0.90205675
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.320s + 0.036s (eta: 0:00:19)
person 0.93818325
person 0.921844
chair 0.94840455
chair 0.9754739
chair 0.96231395
pottedplant 0.92466146
pottedplant 0.9063594
pottedplant 0.9561973
person 0.9363595
person 0.92375094
person 0.90323985
person 0.9533236
person 0.9428946
person 0.9269257
person 0.93636286
bicycle 0.9336682
person 0.9835223
person 0.913398
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.321s + 0.035s (eta: 0:00:15)
person 0.9911993
person 0.90987813
person 0.9008053
person 0.9559713
person 0.97880703
person 0.9731556
person 0.97229844
person 0.9734041
person 0.939216
bus 0.9431083
person 0.9301992
person 0.9543495
person 0.9338256
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.323s + 0.036s (eta: 0:00:12)
person 0.9745928
person 0.91455615
chair 0.9062203
chair 0.9451732
tvmonitor 0.9113558
chair 0.96503544
chair 0.9499361
chair 0.9322849
person 0.96659976
person 0.94473326
person 0.9744313
person 0.9028073
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.321s + 0.036s (eta: 0:00:08)
motorbike 0.9524598
person 0.97898096
horse 0.9720328
horse 0.9047622
bicycle 0.9473098
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.319s + 0.036s (eta: 0:00:04)
person 0.95789695
person 0.9085405
person 0.9791718
person 0.93543077
person 0.96390057
person 0.9787662
person 0.91164035
person 0.92788446
person 0.9213294
bird 0.9299476
chair 0.9569359
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.317s + 0.036s (eta: 0:00:01)
person 0.9827056
person 0.982755
person 0.9437708
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 78.759s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 334 1418  331 ... 1190 1194 1201]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2595
INFO voc_eval.py: 171: [144  79 145 320 634 568  78 469 376 668 592  82 154 684 685  81 330 645
 349 599 150 473 680 149  84 688 157 571 690 669 635 637 470 164 692 679
 633 699 703 147 379  93 701 650  80 325 572 485  88 474 383 573 391 100
 151  97 106 661 671 521 387 161 350 575 315 689 681 477  83 352 548  92
 700 148 578 687 657 472 581 570 377  54 347 277 398 471 110 480 359 530
 665 430  87 322 543 507 388 493 663 152 361 381  28 593 627 583 316 319
 491 390 194 580 547 586 414 384 702 417 686 427 323 489 666  91 638 517
 333 497 253 195 574 545 511 514  29 662 640 604 516 431 321 162 227  33
 678 247 642 331 378 344 527 146  55 239  40 579 512 440 103 698  36 496
 422 348  52 476  85 392 577 395 488 243 216 254 167 639 693 660 557 160
 705 503 704 495 318 534 351 667 282 101 559 156 649 696 380 317 230  10
 111 558 138 552 673 107 165   5 584  50 176 567 659 237 538 482 531  49
 429 108 501  96 220 697 625 502 408 203 582 166 651 328 566 397  44 245
 327 137 695 356 250 223  62 358 386 419 505 155 654 407 481 636 569 707
 508 109 278 291 694 675 554 682 494 228  26 218  25 535 672 608 529 537
 683 490 241  30 368 708 385  95 215 498 629 515  14 648 335 676 338  68
  27 499 643  76 401 709  32   6 444 187 153 588  73 644 232 305 549   2
 456 421 438  57  98 513 364 556 652 136 646 346 163 113 249 240 562 674
 341 206   4 175 394   1 585  72  86  90 212 656 500 610  11 332 343 619
 404 248 426 691 555 536 655 670  47 229 119 214 369 504 114 544 225 135
 506 563 117  64 159 205 533 524 196   3 550 677 565 553 406 620 326 528
 405 653 409 211 594 158 276 560 141 424 605 373 412 209 576 231  46 382
  67 280 606  53 186 362 271 279 213 622  51 244 235 478 233 641 339 448
 403 207 551 372 484 546 486 261  75 122 168 475 603 197 589 306  23  13
 179  37  38 433 587 342 123 519 251  94  34 236 479 132 242 140 173 459
 525 178 664 523 292 264 234 561 446 609 632  63 510 298 238 201 564 130
  74 267  39 170 180 183  61 393   8  69 185 410 221 360 462 439 290 366
 299  15 131 124 304 400 595 340 374 226 258 526 188  21 259  16  31  42
 262 269 169  35 199 257 202  24 357 275 520 428 621 389 171 222 532 224
 441  56  66  41 371 468 623 445 370 102 596  77  43 451 487 367 647  22
 337 658 260 363  45 184 142 442 434 453 208 466 177  20 631 601 191 541
 129  58 115 118 402 182 492 611 607 172 463 416 435 452 336 125 630  71
 312 198 591 455 217 120 190 399 204 464  99  17 345 308 465 443 518   7
 539 542 210 509 121 396 200  60 285 449 365 252 265 329 112 143 598 286
 483 375 263 104 307 467 268 628 219  59 105 437 255  18 413 447 602  89
 425 289  12  65  70 288 246 454 450   9 126 324 597 181 303 270 128 590
 334 127 457 612 423  19 301 615 432 411 618 189   0 418 192 600 293 116
 420 133 296 436 613 134 284 283 273 540 266 274 302 624 314 281 626 287
 415 461 300 272 297 458 706 294 139 256 522 310  48 174 193 353 460 616
 309 313 354 355 617 614 311 295]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.4962
INFO voc_eval.py: 171: [ 580 1537 2878 ... 2871 2870 2637]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1934
INFO voc_eval.py: 171: [ 614  727  624 ... 2204 2205 2209]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2435
INFO voc_eval.py: 171: [456 415 360 635  11  19 730 407 414 455 728 735 460 361 697 454 328 673
 383 613 327 423  20 662 540 333 734 371 370  18 680 918 171 919 753 832
 746 638 408 650 685 362 364 424  13 807 636 744 733 379 614  14 737 416
  16 681 757 931 359 740 458 385 755 921 617 527 459 418 297 818  81 365
 552 579 310  45 375 835 342 780 729 804 643  79 732 543 615 422 377 542
 652 332 201 754 764 384 256 210 821  24 159 237  26 578 647 417 886 421
 380  23 530 671 535 381 648 447 138 820 791 335 337 641 736 704 758 688
 748 549 829 374 621 347 665 206 642 674 252  46 702 668 644 291 473 353
 234 191 587 412 528 344 738 241 687  32 348 236 506  47 420 559  15 885
 622 760 657 700 240 372  52 557 620 305 382  12 658 698 232 179 751 651
 373 285 457 666  66 902 351 637 778 193 233 655 701 363 883 663 661 837
 752  80 437 653 329 181 250 133 890 646 889 640 925 756 929 346 367 664
 794 477 776 853 936 534 768 349 767 743 793 561 331 649 309 884 762 823
 302 368 618 773  33 769 311 555 178 619 449 669 826 180 204 489 409 139
 558 556 554 803 676  91 163 682 504 795 836 172 763 922 927 677 386 691
 119 251 544 470 745 779 678 670 123 880 209 419 656 761  25 772 507  65
 731 742 472 339 343 257  74 298 623 366  92 759 217 726 114 334  90 194
 113 749 471 378 306 828 790 654 541 882 273 135 843 136 448 235 212 775
 616 278 274 314 321  17 160 426 833 304 781 892 354 684 914 774 425 303
 369  50 580 176 940 207 593 838 596 151 825 312 218 741 162  48 263 355
 645 286 376 873 214 446 152 137 144 157 659 586 881 253 161 330 411 849
 689 863 258 131 639 703 933 283 805 915 879 667 308 255 766  61  22 118
 782 750 844 690 765 282 307 770  60 777  89 916 560  21 508 125 531 845
 550 591 771 117  27 510 660 229 289 871 466 553 898  34 156 583 230 827
 747 316 450 683  99 313 595 130 388 739 854 796 428 270 132 520 824 679
 808 797 831 200  41 242 134 239 787 806 917 928 190 686 692 215 897 340
 438 675 203 582 935 111 551 562 168 672 116 281 581 792 345 545 851 356
  98 822 533 904  44 410 336 920 350   4 115 532 341 468 498 913 290 279
 338 319  42 548 238 216 413 934 588 626 292 850 112 153 592  57 121 452
 299 293 839 848 930 219 786 398 903 509 317 887  73 590 439 220 183 846
 294 589 894 605 387 469 855 228 245 227 269 192 277 301 189 852 801  97
 264 129 465 611 427 594 154  49 102 878  30  67   2 295 725 246 205 300
 451 809 231  54 566 859 941 141 727 526 536 547 858 706 484 188  69 937
 202  85 213 104 453 260 529 812 396 830 474 525 814 208 155 318 177 888
 847 924 483  51 565 186 798 158 810 392 901 140 502 467 122 584 607 521
 585 120 267  31 476 905 627 813   1 106 942 856 705 105 475 834  82  28
 819 505 288  29 182 926 197 196  35 699 284 146 597 406 841 816 538 272
  63 397 187 943 103 149 799   5  58 320 101 401 567 899 714 840  88 198
 811 865 254 900 944 609 173 109 893 247 211 718 184 694 142 225 496 606
 150 352  96 128 539 280 629 296 519 195 126 800 107  84 630 174 259 147
 287 546 631 857  55 223 872 402 895 695 175 499 522 783  87 482  68 185
 868 433   0 693 167 896   3 127 493 876 628 523 501 633 610 261 537 612
  75 165 573 399 315 708 440 170 243  70 169 516 938  56 608 864 491 503
 568 891 400 199 946 923 707 323  43 634   7 945 391 324 143 877 696 124
 875 632 441 625 444 226 713 789 599 802 442 785 524 265 490 874 110 266
 500  62 722  37 435 405  39 862 262 497  40 429 932 939 494 390 517 600
 478  72 784 603 403 575 604  59  36 432  38  53 404 486 394 100 108 712
  64 602 357  86 495 244 275 570  83 788 598 721 325 431 326 564   6 322
 480 358 861  71 624 271 518 224  78 164 576 715 724   9 601 434  77   8
 389 145 563  93 515 443 908 711 720 395 577 479 492 276 866 817 815 430
 445 268  95 842  94 719 481 488 709  10 148 569 464 461 485 717 870 574
 909  76 393 511 869 906 463 572 910 571 166 514 221 436 723 912 911 867
 860 710 907 462 716 222 248 512 249 487 513]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3477
INFO voc_eval.py: 171: [181 498  85  87  71  78 501 499  86 190  70 516  91  92 448  69 110 185
  27  90  74 411 140 466  73 461 173 289  88 262  97  46 453  79 212 434
 410 442  77 291 343 401  31 148 438  82 214  80 333 523 314  72 481  99
 232 483 230 389 293 220 371 427 366 281 349 280 109 413 539 497 168 237
 147  75 149 363 107 217 305  76 435 272 407 388  55 203 283 266 372 299
 447 271 472  81 118 159 304 288  45 424 396 189 141 178 387  89 373 347
 210 345 139 436 470 328 480  58 205 306 439  95 209 334 216 458 421 277
  42 275 402 403 117 111 158 502 535 261 174 236  43 145 138 360   7 290
 512 115  11 493 344 474 426  47 191 106 505 367 518 122 215 337 377 316
 123 193 269  13 134 423 144 235 282 202 126 116 399 420 452 108  28  98
 400 492 537 462   1 397 204  41 297 342 533 114 536 315  33 231 437 165
 534 364 468 338 211 133 362 206 113 238 444 395 164 241  93 394 119 177
 456 517 112 370 270 101 131 184  51  39 124 348 121 467 386 268 416 487
 308  35 274 450 538 392 218 285  10 486 451 295  32 201 129 419  56 278
 441 359 303  14 460 151 454 207 409 443 524 459 361 391 358 169 340 469
   8 440 242 529 103 484 100 153 152 156 393   9 199  49 473 276  30 414
 102 368 445 376 132  83 301   5 433 146 167 142 390 296 465 307 143 471
 365 125  53   4 446   6 522 477  34 213 457 120 249 163 257 258 267 355
 192 479 430  94 463 496 155 105 157 243 331 154 482 264 104 166 224 491
 422  44 519 279 208 485 350  96 183 226 418 494  29 428 357 265 182 335
  40 170 150 374 476 406 412 495 369 128 341 475 313 273 252 294 408 127
   0 336   2 381  60 298 234 425 245  50 326 176 464 478 130 332 490  12
  36 225 310 240 309 161 339 196 302 179 219  26 171  38 292 415 449  84
 375 263 525 223 526 405 515 255 504 248 254 233 175 489 320 383 354 351
 239 172 531 528  37 488 286 382 378 312 251 195 455 253 311 244 256 229
 352 327 246 259 330 356 188 329 417 300  25  65 250 404 431 180 429  15
 260  54 247 520 521 353  64 227 530 432 346  67 287  59 322 222  66 324
 284  61 135  57 221 379 325 503 187  18 200 136 162 532 323  48 380 194
 228 160  52 186 321 319  24 318 317  63 510  21 197 514 511 506 513  23
 527  19 500  16 509 508  62   3 507 398 198 137  68  17 385  20  22 384]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4264
INFO voc_eval.py: 171: [1526  709 1462 ...   99 1432 1650]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2972
INFO voc_eval.py: 171: [ 31  68 247 248  22  69  26 108 299 152 246  72 298  32 272  24 132 271
 240  71 118 111 131 153  29  80 279  74 124 100 191 219 141  52  23 274
  28 243  25 234  18  53 221 292 223  84 151  46 121 206 275 104 103 109
  54 161 226 164 181 114 278  75 281  92 207 115 133  67 134  19 173  56
  94 208 205 123 135  76 182 165  10 184 192 156 265 220  21 222 196 277
  55 297 102 213  57 294 280 235  27   7  20 177 157 183 142 110  30 229
 212 130 293 282 256 143  82  81  77 116 155 113 122 125 154 296 128 195
 209  91 188 273   1 276 105 198  73 170 117 255 301 126 185 210 241 224
 237 138 149 150 266 268 120 159 225 163  14 211 186 286 287 242  49  90
  70  11 107 251 295 187  51   2 214 200 269 244 112   0 199 264  43  93
 217 232 139  50  15 106  86 254  89 270  33 227 197 236  99  88   5 263
 137  85  87 189 127 288  48 201  37 176 119  45 101 202 136 160 231  47
 267 194  62 253 257 129 260 230 245 175 162 259 216 167   8   3 166  96
  58  36   9  17  12 218  40   6  83 228  13 172   4  97 193 283 258 262
 215  42  63 249 168 158  98 203  79 285 284  35 174 238 179 144 148  41
  34 252 180  16  95 204  44  60 178  59  61  65  66 289 171 261 250 145
 233  78 147  39 169 146  64  38 190 291 140 239 290 300]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0584
INFO voc_eval.py: 171: [6180  723 2665 ... 4653  292 4656]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3401
INFO voc_eval.py: 171: [594 285 187 376 234 242  40 240 380 226 333 642 192  39 228 619 114 230
 244 533 246 128 544 516 227 322  54 287  60 653 377 649 374 195  41  61
 239 231 248  56 641  24  62 515 381 656 385 193 292  53 634 526 378 354
 379 184 318 168 603 133 297 325 338 531 532  58 186 459 273 657 455  81
 548 382 286 518 651 537 646 189 165 274 295  44 525 630 546 476 188 348
 229 121 232 288 139 644 545 156 115 215 221 236 247 647 241 358 384 212
 530 480 363 203 604 260 597 628  73 645  66 453 625 629 310 457 361 276
 517 219  34 235 346 654 626 474 233 138 617 141 253 624 145 281 650 275
 250 570  45 307  57 308 534  22 345 301 167 218 334 623 123 124 120 127
   9 520 245  50 112 158 249 339 460 529 373 481 161  16 191 164 159 446
 482 372 538 359 279 289 207  87 633 336 375 332  64 564 353  95 199 643
 423 238 126 340 252 205 535 259 116 632 312 290 254 613 171  80 196 591
 524 451 194 483 444 163 636 608 434 592  74 615  21 355 436 386 130 298
 528 580 655 160 562 595 172 522  10 197 553 357 598 607 166 443 104 394
  71  76  77  38 635 122 398 243 326 609 360 659 452 237  14 304 118 125
  35 255  23 366 331 621  68 468  96 627 658 616 479 648 622 618 337 485
 652 549 256 458 341  36 557 211 251 162 456 201 328 475 383 220  12  59
 581 213 356 477 571 523  65 330 620 414 257 478 134 300 504  67  48 563
 170 117 450 347 258 541 605 323 206 185 271  78 190 106 179 280 631 600
 590 461 610 583  84 335 639 313  79 638  82 296 169 487 669 640 561 433
 582  99 132 637  11  55  51  83 415 572 454 100 393 511 411 588 150 143
 369   3 102 343 349 417 101 448 327 432  30 556 404 216 136 183 344 662
 314 407 418 577 342 113 155 222 157 142  98 521 527 486 602 129  97 484
 403 558  46 198  72   2  70 614 131 140 405 424 261 599 430 109 510 445
 103 362 586   0 202 223 492 513 119 182 431   4 135 283 589 491 309 593
 200 579 402 512 368 111 552 152 540 539 284 320  94  69 214 151  86 208
 670 429 519 661 437 554 425 329 601 277 217 204 269  85 317 576 311 210
 494  43 428 175 176 596 224  26 413 462 447 503 663 315 406 302   1 660
 324 421  25  29 282 209 493  75 664 509  33 153 319 267  13  49 316 469
 536 505 508  52  15 396 435 578  19 568 410  42 173 550   8 473 559 108
 606  31 409 412  93 555 449 299 560  63  47 575 146 587 442 144 547 278
 110 464 514 465 566 154 174 350  27  32 397 551 574 148 177 565 147 543
 107 268  37  28 180 567 501 542 149 467 365 266 178   7 488  17 569 422
 225 502 463 105 573  18  20 387 416 291 367 507 419 306 364 390   5 438
 420 181 408 270 506 472 262 470 471 263 303 351 388 426 611 612 294 584
 305 370 371 265 466 668 264 400 439 352 389 495   6 500 585 667 427 321
 293 391 395 272 392 665 440 401 441 666 498 499 399 137 497 496  88 489
  90  91  89  92 490]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1307
INFO voc_eval.py: 171: [2964 1292 1344 ... 2232 3189 2289]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3385
INFO voc_eval.py: 171: [392 502 938 463 472 429 397 506 559 528  64 505  78 157 498 518 613 400
 608  65 876 219 169 700  59 275  57 940 163 820 158 888 935 287 945 519
 897 168 912 391 147 166 875 898 404 276 427  80 874 224 576 521 939 566
 464 469 879  56  63 932 572  82 430 468 872 476 857 239 321 947 662 489
 405 172 698 401 921 575 817 237 425 406 241  68 255 516  66 470 650 301
 493 877 426 156 934 248 281 937 668 278 726 252 265 663  81 167 235 162
 701 757 589 375 314 654 213  79 691 747 165 288 536 860 226 160 749 561
 407 883 936  29 903  58 331 526 332 246 173 507 214 161 205 652 568 170
  72 257 906 913 783 818 612 462 236 696 336 495 830 634 466 329 307 570
 488 487 893 672 144  45 905 859 583 532 520 477 688 699 227 304 904 607
  25 438 733 881  32 368 917  84 159 569 869 499 609 756 923 311 277 911
 243 676 942 948 501 268 223 313 740 250 154 212 467 465 367 148 710  62
 382 953 858 328 811 259  97 129 800 706 164 827 941 522 708 786 286 434
 484 946 878 533 324 355 471 843  69 356 606 272  47 525 633 815 806 271
 727  67 395 402 188 725 861 145 221 150 571 902 789 504 480 578 492 828
 315 755  60 322 605  83 266 933 264 183 494 282 435 531 919  26 326 267
 285 365 474 225 220 563  74 436 753 238 738 750 491 478 447 611 907 784
 624 325  73 743 210 497 348 171 135 565  77 868 396  98 752 542 686 880
 871 211 130 416 956 232 679 669 209 677  70 660 842 490 683 838 901  30
 479 352 954 870 377 380 846 222  43 715 760 309 847 228 383 724 763 694
 615 371 918 673 194 781 779 486 240 631 823 914  27 152 389 340 804 852
 274 283 308 437 834 412 704 208 342 256 746 955 149 439  75 785 928 630
  17 263 949 292 674 567 204 697  99 778 303 247 398 951 603 351 581 537
 667 754 916 782 909 805 915 887 695 900  19 260 920 841 207 403 206 931
 128 705 482 768 943   6 244 560 302 596 270 886 446 751 741 659 707 341
 317 334   2 873 952 845 330  40  38 399 762 500 273 610 269 908 924   8
 808 409 670 689 312 728 102 323 910 761 535 580 664 632 146 151  76 653
 617 836 393 925 127 279 318 840 245 523 258 635 410 692 131 729 882 712
 803 503 690 844 655 242 678 922   0  71 253 682  46 665 231 249 582 136
 251 693 926 616 142 517 394 716 319 339 774 360 254 496 821 687 856 776
 711 115 112 193 791 347 807 513 320 636 718 481 379 885 300 483  22 884
  34 475 950   3 327 602 770 361 196 626  51 835 792 346 137 851 428 386
 384  85 110 185 780 713 899 415 510 614 824 944  54 316 343 732 671 839
   5 587 357 509 758 306 837 810 390 132 195 333 814 293 310 812 424  13
  20  49 103 832 745 184 797 153 335 957 767 378  33 182 385 765   1 423
 702 649  90 456 337 588 408 229 381 637 813 364 388 651 289 675 734 680
 640 656 618 369 280 772 666 809 759 420 622  61 796  39 106 422 681 720
 455 139 100   9 105 473 376 833 764 174 592 825 107 684  35 449 134 353
 175 133 140 543 865 927 177 191 349 116 638 849 445 529 338 431 598  95
 794 104  92   7 544 735 350 118 775 816 305 143 187 889 284 597 138 363
 553 795 101 298 366 685  23 661 892 508 721 579  55 485 709 441 198  44
  24 848 717 345 538 141 370 819 777 359 530  86 742 411 739 714 556 200
 362 896 627  48  18 595   4 354 539 790 591  10 186 113 121 850 511 831
  52 731 294 512 545 601 557 641 108 552 344 299 584 771 358 822 548 657
 117  89 295 647 109 114 826 461 736 594  88 600 297 189 562 555  50 773
 599 218 891 451  41 514 894 723  91 547 417 854 604 527 373 585  53  93
 853  31 551 639 619 628 890 550 766 155 524  28 215 658 593 730  96 290
 234  87 124  37  94 643 645 458 453 648 176 549 126 895 798 460 546 748
 457  11 111 620 515 421 387  15  42 540 586 801 230 573 448 459 233 201
 787 722 737 199  21 829 197 216 623 719 414 744 534 769 454 296 192 855
 646 621 644  36 190 261 120 541 788  16 703 450 125 262 799 444 372 413
 202 122 802 629 793  14 866 564 418 203  12 119 179 867 574 291 442 554
 419 180 123 577 862 642 432 217 374 452 433 590 558 625 181 178 863 864
 440 929 930 443]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2194
INFO voc_eval.py: 171: [894 117 368 813 897 369 119 121 371 814 162 912 609 381 116 708 567 229
 137 906 742 122 835 244 118 124 714 734 712 120 490 170 828 529 522 743
 816 901 913 744 905 164 577 499 249 569 260 163 165 370 169 611 568 128
 888 753 127 252 914 915 719 246 614 830 578 182 247 565 909 245 263  17
 736 822 403 872 492 818 527 747 153 230 932 273  51 940 322 582 401 334
   7 882 610 493 871  41 171 633 876  53 877 751 630 501 821 875 739 709
 918 715 879 532 337 345 659 838 840 583 740 628 336 518 731 496 503 175
 588 908 402  14 214 717 758 231 608  42 123 259 130 185 640 285 601 741
 392 868 359 903 756 690 172 494 491 621 423 749 477  88 272 722 926 187
 573 217 884 523 631 819 176 895 133 521 587 264 537 780 143 343  87 226
 579 754 126 376 533 752 240 500 319 619 145 341  80 495 167 718 275 652
 917 326 181 923 184 737 613 616 892  43 399 228 594 941  82 410 825 309
 173 623   8  52 748 282 638 168 829 928 935 539 570 135 636 235   4 885
 308 257 186 899 597 531 236 502 572   6 595 365 865 735 958 824 898 404
 891 479 755 131 833 907 174 304 591 831  96 911 584 147 340 232 574 581
 129 689 323 261 421 878 320 617 738 870 427 338 342 544 639 624 663 400
 221 242  93 418 637 324 271 881 520 213 604 215 541 867 283 254 943 179
 834  37 832 936 575 483 785 407 846 353 847 852  12 658  90 290 548 844
  16  84  95 713 220 190 519 498 513  79 134 546 362 525 667 887 526 447
 815 250 140 177 856 218 682 703 333 937 720 335 711 430 889 620 505 346
 155  50  75 606 874 151 705 387 745 344 507 448 312  94 890 192 158 534
 327 955 902 352 504 826 339 942 782 310 750 542 580 178 422 298 478 820
 349 746 139 154 233 351  85 589 538 132 600 144 216 927 933 707 255 444
 836 566  97 883 839 428 607  76 615 239 929 910 445 497 725  68 931  44
 540 837 325 125 612 321 328 869 350  20 286 571 793 429 311 183 180 922
 367 508 939 277 258 219  74 590  13 873 599 107 592 691 842 603 543 269
  40 643  56 857 792 315 267 302 558 306 860 426  78 596 265 771 845 238
 278 241 944 893 152 729 166  32 823 411 425 276  66 237  92 159 886 253
  23 517 274 947  25 234 268 768 853 453 201 205 471 545 149  81 385 316
 880 415 510 188  89 593  91 948 576 841 784 670 189 938 514 866 406 446
 266 480 649 827 924 728 454 417 208 481 102 904 817 921 930 314 256 732
 100  86 766 723 530 767 535 945  77 248 366 605 284 210 760 586 794 307
 634 433 920 684 551 855 919 528 791 673  33 363 287 313 207 243  83 305
 161 416 414 295 680 710 896 854 506 142 204 511 383 676  15 193  18 843
 618  35 626  31 625 956 413 916 262 598 396 419 332 674 764 203 202  47
  46 211 777 212 200 441 452 687 706 759 671 300 851 675  70 296 934 925
 900 355 564 412 635 294 585 297  67 769 602 251 330 148 536 360 105 489
 655 524 408 409 666 957 804 270 949  19 104 209 157 696 686 299  38 704
 420 160 672 669  48 692 434 225 547   1 770 394 397  49 443 951 109  30
  29 424 762 227 644 293 289 950 377 515 679 281 358 439  21 206 509  73
 331 622   2 627 716 389 380 288 405 103 681 549 677 661 849 632 786 660
 468   0 654 668 375 194 482 361 354 688 291 850  65 683 197 629 348  10
 946 512 662  60 858 460  45  54 772 812  27 553 694 775 456 516 779 664
 733 665 552 292 464 378 329  72  55 773 778  22 563  99 106 222   9 695
 449 440 384 776 398 138 550 101 774  69  98 757 727 763 761 724  39  36
 301 781 795 141 431 783 458 108 469 199 678 146 562 195 685 721 808 702
 136 432 455 556 373 693 788  28 110 730 726  24 796  34 470 386 372 797
 474  57 113 698 699 642 438 356 484 198 156 805  71 280 191 436 848 697
 809  62 555 435 765 442 395 150 196 459 475  63  26 647  59 476   5  11
 798 811 802  64 115 461 645 807 466 653 223 437 863 111 357 810 457 347
 560 450 485 803 561 559 451 650 390 859  58 701 799 954 473 641  61 554
 700 472 789 374   3 364 801 864 806 800 862 114 646 112 651 787 557 648
 317 953 391 790 279 318 861 224 952 656 462 467 382 379 393 465 303 463
 388 488 657 486 487]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3106
INFO voc_eval.py: 171: [ 58 348  29 196  57 213 204  71 333  65  59 340  81 145 146 197  90  35
  64 226 219  47  38 339 248  68 200 361  21 316  69  33 344 341  30 307
  42 364 163  70  24 255  97 332 140  72 264 212  43 250 356 175 362 320
 220 222 308  89  25 338 142 249  62  63 321 115 257 203 225 245 292 354
  22   9 206 259 335  94  14 118 272  75 366 233 246  18 211 337 144 359
 199  76  49  17 296 143 224 202 349 310  60  40 283 244 243 365  73 141
 331 279 185  36 314  79 214 106 221 242 119 192 116 158  87 215 260  39
 293   3 363 232  67  80  20 107 360  93  23  46 277 322   6 227  83 309
 195   7 311  92  31 328 208 134 117 190 165 325 329 101 112 294 194 123
 268 207 122 124 253 281 209 138  10 324  32 237  44  13   2 288 229 201
 256  82 187 188 102  91  85 184 346 167 169 284 198 228   1  86  19  84
 317 297 210  27  74 236 179 247 234 345 285 306 131 135 108  15 105   5
 176  26  45 286 110 357 186 109 315 269 174 189 343 178 342  88 156 223
  77 291 319  12 230 351 266 205 273 241 104 280  41 323 161  78 191 318
  56 183   8 130  34  61  28 299 151 162 126 152 133 121 298 355 231  66
  37 103  55  48 157  50 139  11 336 278 271  95 137 295 127   4  16  54
 182 238 125  52 350 132 267 154 263 120  99  51 254 150 251 289 274  96
 270  98 129 235 275 326 261 128 136 282 240 313 216 217 276 290 164 172
 352  53 258 218 153 327 170 265 287 262 160 193 300 239 159 148 155 353
 304 252 302 334 171 347 113 100 166 147 168 358 173 301 149 305 312 114
 330 303 180 111 181 177   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5711
INFO voc_eval.py: 171: [ 8334 13583  1024 ...  9626 10563  9630]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5473
INFO voc_eval.py: 171: [2780  798 1852 ... 2154 2879 2024]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4305
INFO voc_eval.py: 171: [ 34  35 518 312 330 528  62  42  52 521  36 131 325 334 525 329 524  37
 519 471 591 307 224 257 338 157 400 360  67 343 342 484 271 540 587 313
  75 534 526 617 599 164 283 320 125 306 543 256 362 347 259 134 138 523
 122 328 603 529 405 227 344 622 413  55 169 229  66 361 237 538 366 624
 542 326 419 316 600  69 258 337 404 535 135 332 520  76 129 602 477 270
 123  39 322 601 469 121 236 620 161 153 336 595 158  58 171  72 233 232
 455 546 136 606 604 208 403 321 221 503 363 269 486 551 327 585 159 317
  94 210  51 324 268 436 597 473 139 285 310 276 613 623 311 286 619 594
 341 616 592 220 612 641 155 225 133 656 137 346 531 350 456 246  47 323
 502 478 353 468  95 132 166 406 231 491  44 140 124 642  63  98 177 609
 146 318 314 614 195 605 168 128 345  70 222 530 275 226  41 454 584 611
 127 402 407 315 544 309 463  31 162 539 373 230 335 533 339 645 319 333
 103 331 401 308 505 607 472 144 408 545 399 209 479 165 552 145 618 340
 234 536 201 608 476 284 583 571  56 175 643 223 423 512  29 496 190 514
 188 305 586   6 235 593 376 475 621 163 596 417 563 126 196 610  57 489
 589  73 415 156 179 494 598 228 167 615 541 574 659 265 556 500 266 290
 170  53 250 627 202 247  68 371 264 467   4 263 200 527 248 198 197 550
 368  40 352 160  85 644 172 272 501 470 493 364 466  59 651 435 349 299
 249 348 511  17 267 480 354 626 130 474 118 655 490 379 216  82  61  74
 102  96 492  81 554 433 107 548 426 199 537 648   2 485 532 173 590 207
   0 295 291  84 119  43 254 274 351 452  26 557 445  32  24 186 304 180
 555  16 217 457 262 154  97 660 625 522  99 365 100 182  65 465 483 499
 588  87 298 211 106 260 657 650 273 261 296 150 553 633 562 301 255 297
 647 149 187 498 658  78 390 462  45  25 111 481  49 628 218 432 510 497
 506 181  77 110   1 397 572  93  23 575 582 185  83 416 367  27   9  71
 287 549  48 189 434 451 646 396 374 411 513 560  38  50  92  28 281 152
 453 654  60   3  19 449 495 278  79 112 431 244 437  80 422 634 253 277
 384 578 504 509 414 428  18 447 564  30 448 355  20  54 398 488 487  64
 649 653 652 450 184 410 292  10 214 302 507 576  46 279 573 212 381 482
 547 418  33 303 192 176 183 425 204 282 205 508 420 577  21 251 151 142
 369   5 252 579 289  22 421 288 409 567 385 147 280 636 631 464  88 460
 203 568 120  91 424 109 394 101 370 561 383  15 581 300  13 116 566 356
 565 206 558 459 194 357 108 213 117 629 215   7 516 380  14  90 461 114
 559 458 239  11  86 444 113 570 569 115  12   8 427 382  89 219 637 443
 174 580 375 238 141 391 389 638 359 515 245 178 393 630 104 193 395 294
 378 293 446 143 243 386 372 640 387 191 392 241 377 635 639 388 517 240
 632 242 442 440 148 441 439 438 358 430 412 105 429]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2887
INFO voc_eval.py: 171: [ 39 117   3  40 184  41 122 180 177 269 181 146 186 121  44 223 316 151
 222  11 241 113  57 123 175 319 143   4 270 320 264 232 114 147 261 182
 224   6 214 267 141 268 161 309 153 178  56 152  38  73  55 318 167 142
 185 149 144 183 176 124 150 312 125 138  20 238 179 148 168  79  90 237
 112 221 119 242  68 280 209 297 262  99 101  10 236   5 225   9 204 275
 321  43 315  91  92   2 239 233  12  98  42 259 100 187 240  96  37 216
 226 307 234  67  83 231  97  25 252   8  22  76 300 253  80 128  82 220
 235 298 245  45 295 106  19 228 219 257 254 145 311 227  94  48 284  74
  28 191  93 249 139 108  71 196 115 273 116 218 118  27   7  60 229 277
 107 322 120 140 129  85  59  24 215 308 317 303 266  58 246 281 132 306
 199 294 126  81 313 291 192  26 127 247 208 282 213 304 323 166 130  70
 110 276 198 263  75 255  54 170 248  50 302 200  77 205 271  47 305  72
 211 314  16 310 210 111 188 217 260 154 265 104 193  21 169 202  52  53
 301  78 278 131 212 289 296 251  34 244 109   0 158 283 156  51  49  69
 243 293  31 258 256 194  84 197 172 165 102 285  66  18 155 292  23 272
  32 134 157 103 159 133 136 160 195 299 190 189 279 174  46 288 137  30
 163  36 201   1 171 287 230 203  15 135  35  17 162 250 105 290  65  86
 286 164  64 173  33  62  29  63 274  87  95  88  89  61 206 207  14  13]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1718
INFO voc_eval.py: 171: [596 351 597  80 113  79 360 293 356 194 267 159 123 350 115 292  90 414
 201 268 198 603  89 850  77  88 119 507 355 469 600 599 272 294 760 105
 827 206 199 101 756 615 415 467 361 174 200 202 402 418 352 500 595 851
 694 416 298 447  94 169  64 860 116 270  22 181 396 295 781 114 843 685
 422 276 175 506 420 423 279 126 820 397 358 662 769 104 269 610  75 271
 354 855 630 257 398 107  96 606 187 302 401 192 503  38 156 205 357 663
 858 857 109 219 475 832 166 195  87  43 421 417 183 419 763 703 786  62
 757  21  99 485 301 196 297 275 163 409 830 413 792 791 580 776 236 654
 283 780 613 696 881  68 486 129 668 761 197 121 617 784 833 755 782 468
 839 161 502 449 501 157 775  66 779 491   6  42 444 569 777 177 171 586
  25  40 771 737 363  81 794 854 872 217 552 838 848 543 664 277 481 155
 282 412 828 627 232 128 686 846  37 665 274  56 100 359 106 490 691  24
 733 172  53 393 637 673 286   8 861 237 689 602 245 692 160 702 670 443
 411 347 426 734  61 847 451 281 162 790 391 796 247 327 224 814  73 290
  45 184 509 572 424 774 632 231 178 260 353 186 821 624 699 743 567 168
 308 428  72 634 322 442 488 223 578 529 671 372 489 508 346 712 880 845
 164 522 165 849 549 111 762 143 365 204 238 852  82 246 218 150 188 425
 203 435 288 291 228 700 142 392 427 770 863 807 873 697 864  59 639 683
 531 829 806 477 633  65 581 235 705 170 720 244 259 153  60 825 458  57
  92 766 303 660 216 853 324  18 865 672  98 505 364 604 540 785 682 233
 325 102 125 701  35 305 323 512 239 504 636 882 229 772 341 574 765 537
 230 758 653 878 304 193 742 690 108 450  11 778 635 879 553  74 263 261
 225 252 494 395 626 220 345 856  67 167 527 403 695 564 840 824 306 152
   7 800 487 342  95 158 344 176 492 741  71 618  93 180 704 234 191 844
 822 798 648 631 590  76 521 326   4 285 542  47 836 728 751  46 788  27
  41 296 349 657 405 478  86 666 661 473 483 189 566 179 240 707 826 484
 399 479 339 190 390 831 764 687 688 746 787 568 862 453 454 539 551 556
 185 432 565 646  23 809 638 456 287 783 875 541  14  55 744 474 459 154
 343 582 209   5 745 570 407  97 408 548 262  83 841 226 253 472 684 859
 834 520 278 254 716   2 538   1 883 250 773 724 334 516 284 394 410 377
 110 584 532 400 575 273 573   9 593 823 387 667 718 340 625 738 735 706
 137  39 289 367  54 370  63 513 804 251 386 182 717 476 433 348 333 808
  85 480  12 579 740  28 337 607 264  36 482 651 329 629 628 842 719 519
 227 265 571 431 151 242 810 793 645  58 438  70  78 722 698 366 730   0
 173 258 515 462 759 362 655 837 530 385  17 316 118 368 406 749 659 310
 307 658 127 835 747 103 576 675   3 280 754 511 430 656 693 112 517 300
 436 133  20 464 739 130 559 797  48 221 434 534  84 812 381 819 496 588
  15  91 768  52 795 446 452 249 598  34 148 243 555  44 729 677  26 557
 550 141 547 711 138 535 805 680 561 371 647 256 439 649 674 528 314 495
 336 222 248 404 767 457 460 134 753 440 650 752 380 731 605  50 311  10
 652 546 714 465 335  16 445 144 140 736 591 554 241 585 146 799 429  13
 533 514 577 545 536 145 382 589 319  19 384  51 587 679 332 562 681 213
 318 266 497 499 876 789 373 215 139  69 725 563 463 721 147  49 669 388
 802 723 870 328 448 389 317 374 338 560 732 866 611 643 437 678 207 715
 441 455 676 312 510 383 592 750 471 466 461 255 470 709 713 321 331 803
 601 558 493 211 874 375 208 498 544 379  33 726 727  32 518 816 811 369
 708 801 608 149 320 640 135 817 212 136 309 710 378 210 313 124 526 523
 117 330 214 315 867 616 818 376 594 813 524 621 131 583 877 299 748  31
 868 641 871 869 815 132 642 644 619  29  30 120 614 622 122 623 525 609
 620 612]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2947
INFO voc_eval.py: 171: [506 318 727 374 176 523   2 561  42  70 321 531 310 232 591  30 392 394
 657 322 477 568 395 326 526 579 508 689 164 504   9  43 512 426 430  68
 272 566 370 703 378 172 625 372   3 427 536 740  93  94 403 319 572 169
 611 530 623  19 289 441 369 745 571 387 762 287 178 562 209 134 115  49
 596 195 567 577 396 136 728  36  62 324 475 393 570 274 184 284 707 376
 478  44 266 732 343 517 453 270 380 402 668  95 447 236 645 765 328  34
 305 269 551 137 680 240 747 748   8 292 592 710  59 730 149 525 563 282
 211 379 761 457 243 573 597 459 177 106 275 443  20 450 190  46 330 440
 431 565 204 520 576 247 690 569 165  12 297  32 555 574  96 357 691 743
 338 705 371 429 696 557  51 210 487 340 381 655 391 145 375 479  38 271
 315 286 327 389 454 534 234 323 163 718  23 206 141 349  45 300 135 751
 449 311 238 434 228 489 373 697 564 635 301  47 267 575 590 299 235 476
 731  78  10 194 660 167 398 606 764 698  15 117  79 763 350 157  16  58
  65 242 118  66 647 365 656 401 342 229 451 756 277  22 390 249 227  99
 148 436 503 636 348 388 609 481 558 708 652 414 265 331 173 154 746 511
 119  35 288 729 152 382 750 524 278 721 669 484 383 355 661 458 199 183
 129 662 624 448  50 653 166 351 626 757 428 630 509 587 142 638 642 677
 546  74 679 150 664 766 132 493  76 594 125 239 739 377 752 709 144 102
  48  27 405 578 337 335 480  75 203 678 298 346 122 528 352 671 432 770
 207 353 749 742 116 276 385  39 588  28 437 598 515 670 442 607 133 637
 513 155 280 108  84 260 455  21 505 347 755 182 334  97 706 537 683 273
 595 593 248 384 255 113 279 608  61  31 444 368 737 759 344 161 720  67
 124 413 127  69 580 760 404 205  72 465 290 471 741 120 386 345  13 618
 354 123 753 719 614 589 162 358 261 640 619 285 356 121 143 201 542 233
 312 225 445 409 186 100 341  71 518 438 187 644 754 758 202 268 622 360
 627 446 610 189 460 112 516 146 302 714  29 712 411 168  11 767 641 104
 768 639 216   4  77 130 264  64 339 552 744 191 738  63 613 333 456 483
  14 482 109 174 361 103 151 435 159 538 676 549 556 433 629  87 316 418
 200 643 633 170 192 604 559 208 769 687 439 241  85 667 510 533 529 602
 425  26 160 452 485 397 281  57 214 527 366 539 198 658 632 704 686 522
 464  73 494 550 175 722 666 153 646 659 665 634 317 631 621 140 110 320
 713 628 715 463 415 158 313 682  18 196 171  41 156 139 654 466 180 193
 507 711 651 650 258 521 237 212 138  88 291 226 111 612 467  40 535 223
 147  33  54 295 617 314 231 469  80 616 185  60 128 101 675 188 283 514
 259 519 688 254 179 681 296 181 701 367 586   6  92 417 672 699 294  90
 495 532 293 472 603 197 303 407 605 663 359 131 700 114  91 496   7 230
 246  56 584   5 245 256  37 213 488 486 244 336 474 492 224 105 553 473
 421 329 544 468 674 222 490 554 126 257 615 325 461 673 304 582 416 419
 491 692  98 363   1 307 684  86 734 309   0 543 107 215  89 217 400 462
 470 308 620 364 601 399 408 423 412 685  81  25 253  55 540 599 406 694
 410 649 218 600 648 498 362 420  83  82 424 422  24 219 716 548 702 583
 332 497 717  52 581 251 541 545 252 220 221 693 547 250  53 695 502 263
 735 736 733 585 262 306 725 499 501  17 500 723 726 724 560]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3905
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3178
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.259
INFO cross_voc_dataset_evaluator.py: 134: 0.496
INFO cross_voc_dataset_evaluator.py: 134: 0.193
INFO cross_voc_dataset_evaluator.py: 134: 0.244
INFO cross_voc_dataset_evaluator.py: 134: 0.348
INFO cross_voc_dataset_evaluator.py: 134: 0.426
INFO cross_voc_dataset_evaluator.py: 134: 0.297
INFO cross_voc_dataset_evaluator.py: 134: 0.058
INFO cross_voc_dataset_evaluator.py: 134: 0.340
INFO cross_voc_dataset_evaluator.py: 134: 0.131
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.219
INFO cross_voc_dataset_evaluator.py: 134: 0.311
INFO cross_voc_dataset_evaluator.py: 134: 0.571
INFO cross_voc_dataset_evaluator.py: 134: 0.547
INFO cross_voc_dataset_evaluator.py: 134: 0.430
INFO cross_voc_dataset_evaluator.py: 134: 0.289
INFO cross_voc_dataset_evaluator.py: 134: 0.172
INFO cross_voc_dataset_evaluator.py: 134: 0.295
INFO cross_voc_dataset_evaluator.py: 134: 0.390
INFO cross_voc_dataset_evaluator.py: 135: 0.318
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.499s + 0.034s (eta: 0:01:06)
person 0.9619801
person 0.9579851
person 0.95564556
person 0.989242
person 0.91809386
person 0.91310084
person 0.970073
person 0.9764425
person 0.9336309
person 0.9024853
person 0.98027694
person 0.93831855
bottle 0.9386404
bottle 0.9181743
bird 0.95295215
person 0.9809684
person 0.9111997
person 0.93884295
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.327s + 0.035s (eta: 0:00:41)
person 0.93058854
person 0.94409883
person 0.94012254
person 0.95823383
person 0.9692639
person 0.9798998
person 0.9158219
person 0.92806596
person 0.9456514
person 0.9276785
person 0.9187236
person 0.91479313
person 0.964651
person 0.9209922
bird 0.9017472
person 0.98083204
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.319s + 0.034s (eta: 0:00:36)
person 0.9246324
person 0.9434893
chair 0.90393126
person 0.9241653
person 0.9502512
person 0.92423123
person 0.95261896
chair 0.96592826
chair 0.9026225
person 0.97687066
person 0.9869234
person 0.9382369
person 0.9716428
person 0.9912074
person 0.9829585
person 0.9017384
person 0.90395427
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.327s + 0.034s (eta: 0:00:33)
bird 0.9182042
person 0.91944945
person 0.91879594
person 0.9931514
person 0.97707736
person 0.92253447
person 0.9834146
person 0.95630914
person 0.96114683
person 0.9393597
person 0.9688256
person 0.93229795
person 0.9295307
person 0.9548423
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.336s + 0.036s (eta: 0:00:31)
boat 0.90393686
diningtable 0.9413739
chair 0.96068704
chair 0.93411183
chair 0.94720453
pottedplant 0.90076643
pottedplant 0.9220228
pottedplant 0.90032923
pottedplant 0.92578536
pottedplant 0.9204977
pottedplant 0.90168595
pottedplant 0.9400339
pottedplant 0.9197284
pottedplant 0.9427903
pottedplant 0.9374117
person 0.96378607
person 0.9809653
person 0.9878256
person 0.9276333
person 0.986505
person 0.9069988
person 0.91044194
person 0.97387344
person 0.90010655
person 0.90279686
person 0.9459307
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.334s + 0.038s (eta: 0:00:27)
person 0.9382182
person 0.9102724
person 0.9197432
person 0.927945
person 0.918946
person 0.9498558
person 0.92821544
person 0.91659355
bird 0.9173581
person 0.90705687
person 0.98696345
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.333s + 0.037s (eta: 0:00:23)
pottedplant 0.91542953
person 0.9550948
person 0.9536015
person 0.9562106
aeroplane 0.9180096
chair 0.9819402
diningtable 0.9422018
chair 0.94132096
chair 0.978466
pottedplant 0.9048326
bird 0.99293643
bird 0.94352436
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.332s + 0.039s (eta: 0:00:20)
car 0.9237001
car 0.93907386
person 0.98918355
person 0.9639162
person 0.9503606
person 0.9303987
horse 0.90915364
horse 0.9680185
person 0.93578357
person 0.9537563
person 0.94219583
car 0.943129
car 0.9616656
bicycle 0.9712379
bicycle 0.92650545
person 0.9479189
person 0.97142035
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.328s + 0.039s (eta: 0:00:16)
person 0.9672597
person 0.9058281
person 0.96881986
person 0.9878764
person 0.9861578
person 0.93838507
person 0.96824664
person 0.9621463
person 0.98419166
person 0.9368261
person 0.9385895
person 0.94168836
person 0.9864645
person 0.98115486
pottedplant 0.91966486
person 0.98965925
bus 0.90869796
person 0.9583687
bus 0.923418
person 0.94774514
person 0.94560355
person 0.9464775
chair 0.9360306
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.332s + 0.039s (eta: 0:00:12)
motorbike 0.972681
bird 0.9155469
person 0.9724201
person 0.94678664
person 0.95323056
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.330s + 0.039s (eta: 0:00:08)
boat 0.9812289
boat 0.92343783
boat 0.9141313
person 0.93282324
person 0.93915546
person 0.98974997
person 0.9853709
person 0.9126482
person 0.93954897
person 0.9871321
person 0.974813
person 0.90181285
person 0.9259411
person 0.9281511
person 0.97887254
person 0.9480342
car 0.9480053
person 0.9267064
person 0.940075
person 0.9645902
person 0.985658
person 0.9260339
person 0.9035719
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.331s + 0.038s (eta: 0:00:05)
diningtable 0.9330546
chair 0.9727436
chair 0.91945577
chair 0.98007834
chair 0.97557926
chair 0.9608234
chair 0.92337775
person 0.9299444
bird 0.9098975
boat 0.9201012
person 0.95548576
person 0.91482514
person 0.98551786
person 0.9047813
person 0.9006518
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.326s + 0.037s (eta: 0:00:01)
person 0.9812587
person 0.90873486
person 0.95165676
person 0.9539715
person 0.9425526
person 0.95597935
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.536s + 0.042s (eta: 0:01:11)
person 0.9778145
person 0.976922
person 0.9764822
bicycle 0.95084417
bicycle 0.9746386
person 0.9152984
bird 0.9255207
pottedplant 0.94861627
person 0.9730076
person 0.9529898
person 0.9104204
person 0.9427649
person 0.9586154
person 0.9324161
person 0.979071
person 0.9616624
person 0.9233616
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.335s + 0.034s (eta: 0:00:42)
person 0.96635365
person 0.9348551
person 0.962996
person 0.92873585
person 0.97837925
person 0.96909887
person 0.9275433
chair 0.92904013
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.341s + 0.036s (eta: 0:00:39)
car 0.91364664
car 0.93660486
person 0.95638186
person 0.95357436
person 0.97738624
person 0.901801
person 0.9355983
person 0.9558706
person 0.9026357
person 0.91047364
car 0.9807403
person 0.9213143
person 0.9488205
person 0.90792346
person 0.96242666
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.337s + 0.034s (eta: 0:00:34)
person 0.97275454
chair 0.9214573
person 0.9196458
person 0.9465741
person 0.9430073
person 0.9202894
person 0.93009347
person 0.97923297
person 0.97565985
person 0.925595
person 0.9858629
person 0.97358805
bird 0.98994875
person 0.9058463
bird 0.96764004
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.331s + 0.034s (eta: 0:00:30)
person 0.9283385
person 0.96382415
person 0.9810543
person 0.9004701
car 0.9480053
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.333s + 0.035s (eta: 0:00:27)
bird 0.9144131
bird 0.97181904
bird 0.9149156
person 0.92829114
person 0.9141243
person 0.9853888
person 0.969343
diningtable 0.97012895
person 0.91222465
chair 0.935151
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.337s + 0.035s (eta: 0:00:23)
person 0.98312193
bus 0.96928024
dog 0.9427219
diningtable 0.9682898
chair 0.98589814
chair 0.9657908
chair 0.9146643
chair 0.96285284
chair 0.9074865
pottedplant 0.9101042
person 0.9157738
person 0.96766424
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.335s + 0.035s (eta: 0:00:19)
diningtable 0.9058919
person 0.94284725
person 0.94588965
person 0.91183347
person 0.9418737
person 0.92562836
person 0.9359593
bird 0.9583525
person 0.91433257
person 0.9591261
person 0.95330465
person 0.90579545
person 0.97100115
person 0.90828454
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.331s + 0.035s (eta: 0:00:16)
person 0.9582707
person 0.9405295
person 0.9026949
train 0.93906313
person 0.9482638
person 0.9625422
person 0.9482487
person 0.91106486
person 0.9217988
person 0.94978637
person 0.94103867
person 0.9901052
person 0.9743259
person 0.90145785
person 0.9674501
person 0.9349717
person 0.9232552
tvmonitor 0.9184453
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.330s + 0.035s (eta: 0:00:12)
person 0.9450615
person 0.9353453
horse 0.9011931
horse 0.9557059
person 0.9653103
person 0.91542643
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.327s + 0.034s (eta: 0:00:08)
person 0.97397983
person 0.9657855
person 0.92220896
person 0.9137705
person 0.92799944
person 0.9267157
person 0.97769016
person 0.9310101
person 0.9583644
chair 0.9656482
person 0.95449686
person 0.9210026
person 0.9541161
person 0.9347012
car 0.94671994
car 0.91114277
car 0.94291466
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.329s + 0.035s (eta: 0:00:05)
person 0.9713841
person 0.937258
person 0.9559774
person 0.9458948
person 0.9757146
person 0.95437056
person 0.90252715
person 0.98955894
person 0.9812804
person 0.97604305
person 0.92064357
person 0.90702355
diningtable 0.92041844
person 0.9102843
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.327s + 0.035s (eta: 0:00:01)
person 0.93303543
diningtable 0.9297174
person 0.96295244
person 0.98769206
person 0.9710359
person 0.9390821
person 0.92563856
person 0.98307043
person 0.96412826
person 0.95208675
person 0.9533001
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.459s + 0.046s (eta: 0:01:02)
diningtable 0.9014806
person 0.96063834
person 0.97339284
diningtable 0.9627819
person 0.92109144
bottle 0.90650934
person 0.97411364
pottedplant 0.92201227
person 0.9614931
diningtable 0.91902566
chair 0.9635039
chair 0.9452631
person 0.9195574
person 0.99592763
person 0.98010343
person 0.9228425
person 0.9646306
person 0.9599104
person 0.90485454
person 0.9736361
person 0.951087
person 0.9136674
person 0.93786585
person 0.9615556
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.326s + 0.040s (eta: 0:00:41)
person 0.93265027
person 0.9794039
person 0.92293894
person 0.9656891
bottle 0.94016427
person 0.92655903
person 0.91594857
person 0.94102114
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.326s + 0.040s (eta: 0:00:37)
chair 0.9497453
bottle 0.9091182
person 0.9658622
person 0.9244775
person 0.91322416
person 0.9412978
person 0.90766275
person 0.93035704
person 0.9265931
bottle 0.9473307
person 0.98190373
person 0.9093625
person 0.949476
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.335s + 0.038s (eta: 0:00:35)
person 0.9564991
person 0.90365726
bird 0.9188018
person 0.96385175
person 0.9039174
person 0.95681536
person 0.9640167
person 0.95101535
person 0.94135964
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.334s + 0.038s (eta: 0:00:31)
person 0.97217304
person 0.92520416
person 0.9576424
person 0.9168117
person 0.9567096
person 0.9717818
person 0.9166501
person 0.94171625
person 0.9269152
person 0.93989986
person 0.91374505
person 0.96740925
person 0.9356246
bottle 0.94767195
chair 0.9351886
boat 0.9066958
person 0.90557206
person 0.9666275
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.331s + 0.037s (eta: 0:00:27)
car 0.97899777
car 0.9217748
person 0.92004913
person 0.97379273
person 0.9022974
person 0.9066802
person 0.9177139
person 0.9615234
person 0.90729094
person 0.9078638
pottedplant 0.94094336
car 0.98745036
car 0.9245617
person 0.9202165
person 0.9884155
person 0.9402237
person 0.9850995
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.326s + 0.037s (eta: 0:00:23)
person 0.95738024
person 0.9797289
car 0.95947003
tvmonitor 0.9164973
person 0.92227304
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.325s + 0.037s (eta: 0:00:19)
chair 0.90068847
person 0.9677999
person 0.92446905
train 0.9127665
train 0.9558212
bird 0.97986686
person 0.9106601
person 0.9140881
bicycle 0.9441047
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.323s + 0.036s (eta: 0:00:15)
person 0.96238625
person 0.93558776
person 0.9294191
person 0.9238954
person 0.98545235
person 0.95236135
person 0.97295886
person 0.9899899
person 0.97179747
person 0.9772323
person 0.94724995
person 0.96773136
car 0.9362207
person 0.96624404
person 0.9339049
person 0.9690092
person 0.97561556
person 0.9100877
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.321s + 0.036s (eta: 0:00:12)
person 0.91027486
person 0.9213433
person 0.91543293
chair 0.9751082
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.323s + 0.036s (eta: 0:00:08)
person 0.97616154
person 0.92118895
person 0.9005615
person 0.9499736
person 0.95344996
person 0.9055212
person 0.9012686
person 0.97707486
boat 0.9005122
person 0.95957464
person 0.9086499
person 0.9270972
person 0.9558939
person 0.90953404
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.325s + 0.036s (eta: 0:00:05)
person 0.9665191
person 0.9664677
person 0.96501344
person 0.9652775
person 0.929397
person 0.9007234
person 0.92624885
person 0.9270976
person 0.9188976
person 0.94596463
person 0.9552797
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.323s + 0.036s (eta: 0:00:01)
person 0.92326593
person 0.9694289
bird 0.93250394
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.481s + 0.028s (eta: 0:01:03)
person 0.91637045
chair 0.90716106
chair 0.9505603
person 0.9550922
person 0.9022889
person 0.9511246
person 0.9335302
diningtable 0.9221235
person 0.9665208
person 0.962172
person 0.9356871
person 0.961264
person 0.9139566
diningtable 0.9058594
person 0.964258
person 0.9194331
diningtable 0.9509131
diningtable 0.9015576
person 0.93099606
diningtable 0.9184905
chair 0.97753066
person 0.9282272
person 0.98887223
person 0.9692337
person 0.91900843
person 0.984429
person 0.9221675
person 0.9375292
person 0.93008274
person 0.9712857
person 0.9826012
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.336s + 0.035s (eta: 0:00:42)
person 0.9298817
person 0.9357912
pottedplant 0.9364237
person 0.9028365
bird 0.9287836
bird 0.9215336
bird 0.9703776
person 0.9645127
person 0.9373262
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.334s + 0.035s (eta: 0:00:38)
person 0.94331616
person 0.96013045
person 0.9050888
person 0.96395034
person 0.970721
person 0.9040596
person 0.946697
bottle 0.94413805
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.327s + 0.035s (eta: 0:00:34)
pottedplant 0.93637353
chair 0.93282044
person 0.916837
person 0.9779795
person 0.92293787
bottle 0.91304266
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.332s + 0.036s (eta: 0:00:30)
person 0.9581646
person 0.95486766
person 0.91840243
person 0.9422845
person 0.9947984
person 0.92209154
person 0.9773467
person 0.948721
person 0.90118134
person 0.9655716
person 0.9370208
person 0.9262148
person 0.9921212
person 0.94470304
person 0.96321064
person 0.9143255
person 0.9283922
person 0.93153244
person 0.9792037
person 0.93873626
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.326s + 0.037s (eta: 0:00:26)
person 0.94533014
person 0.95602024
person 0.9718005
diningtable 0.97678065
chair 0.9414437
chair 0.9739007
chair 0.9853904
aeroplane 0.9157316
person 0.9656068
person 0.977485
person 0.9111221
person 0.9238605
person 0.9408087
person 0.972529
car 0.91051435
car 0.9213668
car 0.9682967
car 0.95351386
car 0.95405906
bird 0.95377254
chair 0.9156037
chair 0.9256066
person 0.9338683
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.325s + 0.037s (eta: 0:00:23)
person 0.97924083
person 0.9227121
person 0.98347235
person 0.94325453
person 0.98631674
person 0.9337247
person 0.93207157
person 0.90731794
person 0.98848677
person 0.92537576
person 0.93377066
horse 0.9362939
person 0.94785684
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.323s + 0.036s (eta: 0:00:19)
person 0.94154006
person 0.9283039
chair 0.94608057
chair 0.9769315
chair 0.9605556
pottedplant 0.9215628
pottedplant 0.9011782
pottedplant 0.95312786
person 0.93670505
person 0.92488223
person 0.904383
person 0.95405143
person 0.9414415
person 0.9273689
person 0.9414297
bicycle 0.927749
person 0.98355144
person 0.92018044
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.323s + 0.037s (eta: 0:00:15)
person 0.99103254
person 0.9042306
person 0.90306073
person 0.9556741
person 0.979075
person 0.97438294
person 0.97159684
person 0.974262
person 0.9381145
bus 0.94803554
person 0.93378
person 0.9566744
person 0.93718636
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.326s + 0.037s (eta: 0:00:12)
person 0.97295433
person 0.9038708
person 0.91924894
chair 0.9083501
chair 0.9455637
tvmonitor 0.9069602
chair 0.9660975
chair 0.94627696
chair 0.95232934
person 0.9677519
person 0.94769996
person 0.9749176
person 0.9321385
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.322s + 0.037s (eta: 0:00:08)
motorbike 0.95038795
person 0.979114
horse 0.9690823
horse 0.91026175
bicycle 0.9462745
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.322s + 0.036s (eta: 0:00:05)
person 0.9572505
person 0.97898144
person 0.96534234
person 0.9803002
person 0.9233866
person 0.9359784
person 0.92556036
bird 0.9276029
chair 0.95663685
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.321s + 0.036s (eta: 0:00:01)
person 0.9837173
person 0.9840625
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 81.247s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 315 1343  312 ... 1118 1123 1119]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2650
INFO voc_eval.py: 171: [135  75 137 591 298 527  74 346 438 630 551 147  79 644  76 645 599 312
 558 142 327 442 136 641 651 531 149 652 647 632  80  77 592 595 590 661
 640 439 154 139 547  90 665 304 349 532 608  78 444  85 456 361 353 145
  94 635 622 102 357 535 602 642 509 295 140 668 153 540 650 629 538 441
 530 449 624 663 300 347  51 358 368 628  84 335 627 106 400 621 493 141
 329 325 144  23 459 351 258 541 654 477 453 297 542 504 467 485 552 301
 336 539 582 534 296 664 648 328 354 181 299 594 440 658 303 464 360  88
 472 508 463  54 596 396 182 383 461 237 480 387  29 626 321 151  26 498
 229 506 659 655 562 401 537 348 138  40 486 487 522 445 457  93  35 468
 488 490 213 625 157 362 410 634 666 148 392  81 543 221 156 597 202 367
 326 330 660 470 649 238  57  10 512 233 107  47 103 223 379 523 307 529
 262   6 517 617 609 350 600 130   7 603 159 374 309  37 605 215 601 462
 365  59 494 399 378 205 452 104 227  83 333 593 469 458 317  42 657 516
 623 656 653 501 105 667  46 169 219 499 473 618  22 356 334 152 589 188
 355 528 662  50 305  99 208 235 584 481 451  27 389  92  24 372 565 146
 492 271 338 201 466 316 397 129  97 128 606  71 611 314 560 324  87 203
 670 633 671 259 643 598 567  63 637 179 364  96  28  82 231 212 646 510
 426 615  13 311 319 217  68 525 631 545 343 607 614 639   2   1 415 526
 285 390 230 323 479 371 306 495 533 225  67 143 638 471   5 408 155 548
 150 476 193 443 616 577  89 566 515  11 497 513 447 519 199 222  32 167
 555 168  49 158 214 110 619 115 578 132 521 455 544 376  62 450 195  34
 454 496 210 514 579 563  52 337 511  43 505 112 507 394 474 344 524 536
 375 191   4 183 118 243 491 315 232 194 419  91 196 216 257 636 404 585
 318  70 251 484 482 190 286 244  20 553 564  58 125 339 218 119  31 546
 489  36  60 111  69  33  12 561  30 448 198 417 556 253   3  66 211 352
 220 518 520 430  25   9 320 246 587  48 173  38 124 165 123 160 342 414
 184  64 283  14 409  73 247 241 172 500 345 432 206 412 171 581 209 240
 185 162  18 272 284 187 373 610 270 398 411 192 369 446 249  41  19 322
 161  61 612 436 170 437 613  39 465 363  72  21 550 604 133 186 416 557
 359 403 163 236 313 422 226 424 207 234 245 366 293 100 405 177 377 423
 427 340 121 292 503 197 483 435 370 475 114 308  16 117 418 341 460  86
 386 413 109 569 434 263 248 134 242  65 108 175 113 189 620 549 588 288
 583 120 204 224 116  56  15  55  53 228 478 256 559 420   8 425 164 200
 433  95 554 586 250 287 395 268 266 176 174 382 269  17 260 407 302 310
 239 421 101 280 282 380 388 428 274 252 264 393 568 391 276 402 502  98
 180 406 122 576 294 580   0 574 281 255 381 261 126 127 265 384 267 431
 254 277 429 279 131 669 570 575  44 385 289 278 331 166 290 571 332  45
 178 572 273 291 573 275]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.4920
INFO voc_eval.py: 171: [ 610 1600 2994 ... 2736 2989 2986]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1951
INFO voc_eval.py: 171: [ 612  675  726 ... 1981 2220 2219]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2343
INFO voc_eval.py: 171: [450 408 617 353  11  19 712 400 324 407 710 449 717 454 356 680 659 448
 373 593 416  21 323 647 526 365 664 902 718 364 168 901 736  20 800 634
 596 418 730 620 355 359 668 618 401 357 594  13 716 371 720 727 741  17
  15 369 409 731 665 917 802 711 354 376 452 738 790 903 328 514 672 411
 453 744  14  79 325 561 330 541 293 799 308 368 327  46 625 415 714 595
 764 370  77 751 817 638 252 529 528 329 756 199 740 605 414 410  25 233
 632 372 338 631  24 420 331 656 719 868 156 649 208 136 601 652 517 774
 520 342 743 536 539 686 624 442 204 658 628 288 812 465 343  47 570 247
 746 227 721 745  45 670 543 642  48  32 404 545 516 600 237 869 190 335
 374 604 683 366  12 303 431 681 347 236 883  16 643 295 619 281 564 653
 646  53 494 174 451 684 245 687 735 641 132 762 737 180 621 192 866 739
 635 819 804 358 251 753  78 912 919  65 228 341 367 755 469 759 725 361
 838 644 909 650 921 748 629 445 622 915 547 867 597 776 873 326 878 808
 777 173 299 362 633 722 307 757 309 734 599 654  90 197 480 671 544  34
 655 176 786 763 662 402 749 138 666 728 637 660 598 412 246 462 870 169
 907 493 708 805 117 444 723 463 337 640 778 729 230 158 863 306 661  73
 159 360 818 858  89  91 602 713 760 733 806 627 639  26 715 215 530 747
 135 119 275 865 193 918 542 345 112  51 111 538 527 816 270 134 773 209
 269 157  64 301 563 332 419 149 578 673 827 758 283 626  18 820 317 521
 495 443 417 648 413 569  49 768 234 405 904 897 575 279 310 248 305 926
 674 348 137 685 258 154 603 645 336 623 754 211 568  23 142 750  22 864
 761 349 129 300 862 809 363 497 787 277 441  88 116 849 675 375 667 657
 377 752 537 898 566 232  61 231 577 546 742  59 304 250 496 663 724 333
 229 726 507  27 828 899 765 286 226 133 690 573 302 807 732 122 831 458
  36 839 422 630 130 432 131 115 312 378 165 540 238 769  56 110 779 669
 198 913 278 153 780  99 464 217 636 775 880 900 266 905 789 339 225 788
 651 565 562 433 885 276 213   4 531 791 287 834 205 114 189 340 803 879
 930  43 460 202 403  44 533 350 178 840 235 182 488 113 334 289 857 344
 832 389  98 214 380 206 290 884  60 571 574 608 316 150 297 461 871 519
 810 896 120 914 406 518 223 191 821 829 466 446 216 572 294 128  71 274
  35  97   1 876 282 830 212 259 140 585 267 498 200 576 688 793 421 188
 836 298 260 241  50 151 911 590 833 292 551 139 826 920 224 691 908 532
 172 927 203  30 515 835 794 171 513 508 459 242 837 843 314 102  83 784
 567 201 842 255 476 147 285 296 383 782 512 447 550 207 792  66   5 398
 187 313 475 103  52 795 509 874   2 709 185 280 814 923 105  28  31 882
 467 121  68 815 841 587 928 118 860 155 609 886 682 468 152 144 291  29
 801 813  81 811 315 104 101  57 195 179 910 929 781 822  38 524 170 387
 184 253 872 186 704 697 881 181 141 175 797 510 823 391 284  62 249 534
 109 107 346 273  96 589 194 126 579 783 552 586 535 220 221 243 506 677
 210 177 611 434 525  82 427 106  86 612 696 851 164 875 678 254 388 489
 148 124 127   3 922 613 676  85 392 592 423 474 615 906 491 679 183 877
 523 487  54   0 522 196 503 482 390 925 311 689 145   7 264 588  67 616
 166 610 591 256 320 379 397  55 558  74 167  69 553 861 850 484 123 319
 771 481 429 607  87 614 239 435 859 384 580 511 854 222 261 490 108 916
 824 706 581 438 393 426  39 394 767 470 606  33  40  42 436 924 504  41
 766  63 785 492 162 125 477  37 695 257 485 583 582 271  84 100 584 395
 351 555 486  58 505 425 263 798 703 772 322  70  80 321 318 549 262 382
 548 240  95 846 847 472 428 699 396 845  76 352 848   8 268 694 483 161
   9 707 889  72 560  92 386   6 399 796 143 502 272 424 439 770 471 825
  10 440  94 437 265 702 473 457 163 478 852 559 705 554 455 856 385  93
 890 146 456 844 692 893  75 556 557 887 501 855 892 430 698 891 895 894
 479 853 700 693 888 244 381 701 218 499 160 219 500]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3516
INFO voc_eval.py: 171: [165 468  81  83  67 471 469  65 177  82 486  87  86 422  26  97 171  66
 105 384  85  78  70 426 439  77 433  91 129 159 199  42 245 272  69  76
 409 383 417  71 376 201  29  92 413 325 265 402  75 274 494 297 127 401
 455 315  68 207 135 190 225 233 467 387 347 264  72 509 268 377 366 104
  73 344  74  54 453 276 232 341 365 134 410 154 474  88 256 330  41 136
 250 204 101 161 350 283 194 288 364 255 421 146 110 176  50 381 327 249
 445  55 414  40 351 443 411 399 473 287 286 261 222 506 372 197 203 452
  43 483 431 466 396 189  10 178 234 160 447 108 144  84 259 338 378 128
  12 246 423 326 487 374 130 191 266 202 107   5 223 395   6 398 317 463
 273 102 252  44 349 113 324 355 427 185 117 435 192 124  89 103 505  27
 339 100 488 462 375 173 368 489 269  31 106  93 179 132 373 412 504 188
 216 441 298 503 281   1 217 390  30 299 163 416 342 371 227 168 198 253
  39 114 151 346 507 318 370  36 150 109 258 328 145 115 316 212 457  33
 112  21 432 254 420 363 419 508 394 405 193 428 429 458 186 247 434 280
  53 442 123 195 111 285 122 220 388 329 340 196 157 289 119 218 277 321
 440 393 446 230  95 137 263 354  96 226  94 425 415  46 493 164 359 142
 499  28 336 260 408 337 345  79 438  98   7 367 456 200 131 133   4 343
 369 465  99 149 116 279 444 418   8  51 392 140   3 152 221 229 404  90
 139 436 322 153 240 251 138 449 464 262 166 403 313 292 430 248 180 490
 143 170 141 450 331  32 290 389 209 385 238 352 278 460 461 424  37 497
 454 213 257 296 485  25 295 348 448 118 156  38 451 323 380 120 397 496
 206 121 282 320   0 400 162  11   2 270 332 148 382 437  34  47 309 210
 242 475 211 275 219 291 353 314 361 459 302 235 231  48  35 293 319 175
 236 228 406 158 284 502 386  52 391 294 407 155 379 182  80 498  13 267
 237 241 356 215  62 244 358 312 491 239 169  24 492 224 311 271 304  63
 495 243 472 333 310 334 208  56 306 480  57 335 500 205 303  23 187  49
 147 125 307 305  61 174 477  45  17 484 481 501 167 479 470  60 126 183
  18 214 300 478 301 308 357 360 181   9 482  22  58 184  14 172  15  59
 476  64  20  16  19 362]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4299
INFO voc_eval.py: 171: [1543  708 1478 ... 1666 1669 1672]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.3123
INFO voc_eval.py: 171: [ 29  62 258  63 259  20 104 151 311  24 109 283 257 310 117 281 251 106
 152  66 130 122  27 292 193 226 134 254  96 285 141  48 244  81  49 286
  73  16  65 208 231 228  47  72 173  21 105 111 304  41 150 100  78  26
 163  87 113 288 207  68  89 166 293 182 234 167 212  61 197 121  75 183
 220  17  98 168 132  69 289 294  51 185 213 245 155 229 195 291  18 162
  53 214 135  50  22  77 131 227  19 246 277   9  86  25 309 116 107  23
 110 184 112 306  70  52 156 178  28 114 154 239 198 129 142  79 252   6
 305 219 287  80 308  95 284 250 133   4 157 139 120 153 143 123 290 124
 101 268  67 126 233 232 215 165 190 118  84  88 200 216 312 149 148 248
 282 280 159  46 253 256 278 230  12 108 186 188 298 299 263  85  44 267
 175  64  83 187 307  43  97 276  10  40  38  82 218 103   0 210 242 211
 176  45 119 224 273 201 217 247 235   1  13 138 102  30 161 199 279   3
 189 191 300 194 260 271 137 128  90 223  42  35 202 241  99 177 196  94
  33 136 203 272 127 240  93 266  57 164 115 204 170 125 237 225  32  36
 238 169  91  15 275   8  39   7 236   5 269  11 265 206  54   2 255 221
 295  92 296 158 297 205 270  56  31  37 180 209 181 261  76 171 144 147
  14  55  71  58 274  60 301 243 174 160  74 262  59 264 172 192 222 145
 146 303 179  34 249 140 302]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0628
INFO voc_eval.py: 171: [2596 6039  721 ... 4198 4536 4533]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3281
INFO voc_eval.py: 171: [232 289 388 182 611 252 242  37 394 186 344 229 126  36 661 636 247 548
 393 228 559  57 531  56 240 331 369 669 188 234 666  59  47 395 660 120
  60 180 608 248 213  52 370  49 530 674 400 290 350 165  50  21 251  44
 181 390 236 386 616 541  55 184 652 399 334 637 138 304 547 475 318 293
 545 277 183 533 492 471 164 560 564 397 664 553  46 212 222 233 360 122
  40 668 231 154 114  78 279 239 493 662 241 632 308 208 540 237 133 306
 665 617 282 562 398 243 643 368 532 221 376 255 673 546 142 648 204 647
 490 663 375 468 474   4 262 345 136 249  31 139 278  63  71 257 385  13
 659 498 635 220 187 497 549 642 466 341 205 351 323 671 387 389 300 166
 458 359 535 343 298 189 476  45 544  18 156 238 311 346 101 246 112 646
 158 125  67 167 677 316 561 191 651 202 501 554 579 195 650 230 381 455
 169 121 199 129 115 258 438 391 118 256 163 550 124 465 160 157 412 162
 574 253 605   5 640 291 628 539 472 372 622 672 104 294 244 128 371 168
  10  77 297  35 631 179 447 406 496 263  19 130 495  92 254 621 567 537
 392 235 196 219 185 645 117 494 170 649 329 448  17 373 491 618 623 131
 259 619 310 610 374 340   7 342 641 543  73 245  74 171 684 161 633 283
 207 335  69 462 203 638 596  98 159  32  54 380 609 675 565 484  84 127
 593 473 578  75 260 227 116 585 317 321  53 353 347 538  93 396 639 576
 339   0 261 123 250 667 657 571   6 670 477 100 595 358 209 634 685 349
 502  42 141  97 625  62  96 644 312 658 427 459 281 557 292  33 313 607
 590 296 295 155 147 654 613 299 327 653 446 332 419 356 223 526 325 470
 575 382 137 439  51 688 430 215 352 132 500 193 363 152 197 354 418  76
 604 357 319 284 140 601 102 570  95 629 445  99 478 178 594 469 287 536
 401  82 307 503  41 288 542 425 592 194 218 508 656 507 148  80  29 687
 689 361 417 580 206  20 457 198 422 355 431 362 615 655 416  79 499 326
 149 528 348 599 467 201 572 113 214 320 432 211 606 510 520 111  70  39
 337  64 150 224 177 265  94 602 612 200 217 134 678 280 525 119 210   1
  65   8 555 216 686 577 109  23 443 420 190 527  91 442 509 524  24 225
  83 338  22 268 679 407 174 192 328 534  12 676 105 226 556  27 523 614
 175 569 428 568 324 461  48 421 591 521 286 589 551 516 449 436  16 143
 274 581 270 285 322 485 424 333  61 522  66 573 450  81 271 172 489 444
 456  38 146 624  72  43 153 482 600 460 518  68 144 426  28 423  14 563
  58   9 620 145 463  15  90 454 309 108  25  11 379  30 529 517 566 110
 587 464 365 173 588 377 480 586 275 415 479 433 437 552 558 584  34   3
 408 583 106 582 151  26 272 519 402 481 378 429 483 434 451 405 107 440
 273 364   2 176 504 487 486 103 435 366 626 409 314 411 303 336 367 267
 383 384 414 488 597 264 683 403 266 627 452 603 302 269 330 315 404 682
 515 441 301 305 598 276 630 410 680 453 514 513 413 681 135  86 505 511
  87 512  88  85  89 506]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1896
INFO voc_eval.py: 171: [2932 1290 1340 ...  399 2261 2262]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3444
INFO voc_eval.py: 171: [397 468 501 937 399 428 558 503 527  61  77 465 158 517 497 611 402 171
 606 871 699 225  62  57 165 160 429  54 499 283 940 466 170 498 467 818
 396 167 889 933 882 148 294  56 890 944 577 407 904  76 426 870 570 230
 869 169 329 405 519 875 276 176 564  53  80  60 927 472 698 572 939 290
 424 852 490 157 915 866 515 321 278 477 664  63 536 168 815 495 947 305
  65 931 259 408 253 473 243 285 162 651 161 935 574 258 282 246 700  79
 721 561 589 505 872 690 177 220 669 166 316 752 267 381 534 409 750 657
  78 232 518 878 934 504 666 912 855  55 469 250 164 566 895 293 655  95
 233 406  82 899 173 945 211 778 489 159 145 343 816 610 464 319 531 697
 854 687  42 829 607 260 604 578 896  27 328  68 488 897 520 568 375 877
 909 277 242 214 229 338 694 729 226 339 437 798 919 745 303 562 336 678
 306 863 493 583 241 911 885 742 146 902 605 476 672 254 753 154 425 218
 782 853 942 953 485 247 739 163 322 261  25 707  64  81 827 810 270 150
 941 470 398 432 271 803 471  29 898 532 523 227 494 149 704 491 403 385
 480 806 370 720  96 928 602  59 273 496 249 722 894 433 910 826 946 291
 323 192 874 251 737 608  44 521 665 126 785 633 705 856 735 333  67 492
 955 813 862 331 289 356 244 529 779 362 313 335 865 565 217 711 231 743
 893 268 677 132 266 680 695 746 128  72 317 435 216 839 360 542 575  70
 487 430 363 215 481  75 188 174 475 175 613 234 213 255 287 219 864 394
 747 663 819 312 400 775 228 710 835 172 332 383 404 153 686 284  30 719
 245 434 907 525  40 257 401  73 906 269 601 926 759 198  17 387  97  98
 151 415 905 308 774 938 923 674 900 275 892 948 693 436 833 688 382 841
 348 951 248 840 634 443 914 675 265 913  71 838 210 324 630 297 516 684
 777 751 876 147 349 689 537 802 239  19 749 272 805 127 310 483 784 252
 703 567 636 952 479 848 274 920 609 908 603  66 758 879  28 901 411 358
 133 631 350 801 315 725 868 376 788 757 873 656 764 442 671 500 598   8
 280 327 326  24 141 723  26 956 140 918 129 916 834 632 581 748   5 262
 538 921 559 256 325 668 238 614 658 917 237 679 817 502  74 661 125  43
 691 354  34 347 617 209 692 212 903 337 560 412   2 670 683   0 134  69
 484 807 804 618 616 612 837  21 580 512 949 309 954 115 728 850 152 112
 766 482 867 366 304 708 891 318 958 330 712 314 427 667 573 771 950 144
 624 635 200 292 776 881 393 334 767 744 582 320 811 600 100 103 756 142
 202  48 135 880   3 367 706 341 510  31 130 390 588 599 943 395 845 423
 351 673 587 110 715  83 809 761   6 681 831 957  20 652 812 353 138 101
 105 181 410 696 625  51 414 615 458 654 298 281 637 106 474 822   9 389
 755 794 197 820  13 344 388 107 762 340 345 622 922 342  37 836  46 235
 781 676 526 457   1 307 190 545 638 754 104 522 195 701 832 591 102 369
 823 717  38 421  58  88  99 486 352 730 808 659 116 199 579 682 641 384
 187 859 629 386 556 528 814 131 189 178  92 792 368 843 346 420 372 685
 359 301 936 724 772  22 552 930 884 355 765 113 844 155 770 377 736 357
 932 449  90  32 288 286 311 478 179 709 136 506 595  18 929 205 431 143
 114 204  41 279 789 544 727 714 539 371 763 302 535 639 791 793 787   7
 842 886 888 592 441 524 555 193  36 139 716  52  23 373 821 549 507  85
 508 653 513 364 713 590 299 540 108 379   4 300 546 365 361 117 422 440
 137  84 584 554  45  10 731 593 413 660 849 180 419 740 121 224 648 830
 543 191  35  87 824  89 391 883 416 596 156 547 594 530 509 514 662  93
 647 550 463 732 640 194 585 122  91 551 847 887 455 597 548 760 203 734
  50 459  86 619 718 650  49 780 221 460 825 851 109  47 295 828 627 124
  94 417 795 769 646 768 374 462 626 111 741  33  11 726 644 643 620 236
 439 511 119 586 118 461  39  15 196 418 799 571 206  16 445 201 783 733
 533 207 846 623 576 296 240 738 786 263 621 448 702 378 773 800 628 456
 120 797 569 208 222  14 860 444 642 541 790 123 563 264 861  12 223 185
 183 553 645 380 557 452 796 446 186 649 450 184 857 858 453 454 182 392
 451 447 924 438 925]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2255
INFO voc_eval.py: 171: [945 122 396 ... 690 514 691]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3231
INFO voc_eval.py: 171: [ 53 330 183  27  52 190  71 316 194 322  54  77 184 141  76 140  59 201
 208  85  57  33 320  63  46  60 187 237  37 216  17 341 302 319 323  40
  35  31 291 243  26 252 344  41  68 163  82 306  66 338 239 199 137 238
  90  15 293 142 155 191 314 321 204 307 218 325  21 212  55 342  61 335
 198  56   8  93 193 233 277 139 111 245  91  67  22  30 185  11 259 317
  48 114 220 281 234 189  78 329 295 266  88 346 340 207 328 180  95 200
  73  38 231 203 271 103  14 131 116 244 299  65 232 248 138  18  34 174
 230 152  29 308 115 132  69 345  64   2 102  86 196 278  89 135 294 343
  25  19 210  44   5 312 315  74 242 296 195 100 268 264 112 215 213 177
   6 113 227  81 182  24 255 279  83 310 120  79 119 217 282  84 273  45
  42  20   1 303 118 176 181 197 290 101  23  12 309  16 339 175 151 214
 211 157 186  28 130  75 128 165 270 235 188 301 164 127 327 105 272 300
 104 324 258 107 304 221   4 205 159 192   9 236 173  99 305 228  39 276
 284 253 129 332  80 158 167  43 267  47 106  87 179 150  70 209 145  51
 229 178 261 121 283   7 126  92  36  58 219 331  62 146  10 226 172 246
  98 265  72 124 117 134 136 153  32 336 257   3 171 240 250 280 148 133
  50 251 125 254 123 206  13  49 225 297 122 269 292 154  94 311 274 162
 275 222  96 223 249 260 247 298 256 326 147 262 202  97 285 263 241 224
 333 161 160 143 149 318 334 337 288 156 287 144 109 289 286 110 313 169
 168 108 170 166   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5653
INFO voc_eval.py: 171: [ 8415 13704  1038 ...  9735   766   763]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5439
INFO voc_eval.py: 171: [2715  787  282 ... 1965 2805 1993]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4333
INFO voc_eval.py: 171: [ 37 315  38 532 332 542  64  46  57 139 535  40 327 336 538 329 533  41
 480 344 606 309 558 229 168 259 550 404 539 365  72 343 347 635 494 314
  49 549 287  79 170 275  78 132 540 628 308 322 610 614 409 537 346 147
 331 350 416 234 543 258 130 366  71 236 241  67 150 262 553 557 175 143
 611 318 534 328 338 613 137 408 323 478 364 169  74 423 597 165 260 129
 334 363 612 273 339 630  43 514 145 173 486 609 460 148  76 144 225 367
 599 615 330 608 240  60 151 496 324 213 411 523 272 636 616 443 407 320
 566 349  54 621 224 326  99 637 312 559 142 291 313 353 283 631 178  68
 625 289 232 171 348 481 342 146 356 177 140 215 141 488 658  50 249 263
 410 231 166 605 477 672 325 500 226 230 100 103 345 237 136 461 545 660
  66  73 186 562 370 544 131  47 157  45 623 203 319 316 619 227 172 134
 274 617 238 459 261 418 337 598 239 406 498 501 547 176 311 632 620 233
 149 403 317 405  87 554 341 310 156 333 600 321 209 335 284 214 548 634
 340 629 369 519 618 376 174 109 373  35 567 661 633 607 506 485 511 525
 271 551 205 268 133 659 627  33 198 228 601 159 235 184 135 430  77 378
 202 556 622 674 196 307 483 267  59 588 624 626 179   6 560 663 505 160
 603 421 294  53 571  56 250 207 354 425 522  70 269 675 253 180  39 526
 579 561 640 476 158 541 206 503 251  20 181 352 489 167 482  12  44 138
 351 504 502 479  48 495 670 572 208 355 303 546 277 212 270 204 126 182
 513 252 475 381  90  30 639   2 101 127 299 257 118 552 441 564 191 276
 516 279 669 368 662 604 569 492 220  62 107  36 667 432 419   0 221 266
  69 102 396 104 462  89 510 295 536 587 306 555 638  19 194 520 390 450
 188  93 570 512 457 453 602 473 112 265  98  29 666  75 302 290 264   3
 673 436 671 185 217 499 463  28  51 301 300 568 676 414 508 223 595 420
 456 278 163 195 197 470  52  55 517 577 586 509  31  86   1 641  10  96
  82 106 281 490  22 575  21 193 114 357 515 288 434 565 458 292  81 474
 664 592 437 439 507 417 438 524 286 123 665  84  42 440 518   9  32  61
 246  63 115  85 285 254 668  65 256 578 426 429 116  58 124  23 117 442
 402 385  14 465 113   4 431  91 590 589   7 491 455 497 581  80  97  34
 413 219  24  26  27 296 472 304 454 282 646 487 521  94 427 582 280 108
 484 585 305 120  13 371 164 189 563 591  88 216 255 493 384 471 293 248
  25 424 576 153 412 468 161 372 128 218 121 210 187 397 647 119 122 580
 594  18 201 358 388 655 422 211 360 573 200 644 125 105 433 393 643  15
 449 389  95 469 398 528 648 466 467 383 359 382  11 192  92 574  16 583
 428   5 593 222 190  17 361 447 375   8 584 391 183 642 400 401 387 527
 242 110 399 394 645 395 656 298 243 379 374 152 464 244 386 452 392 155
 652 199 154 297 380 245 657 596 650 529 377 247 530 531 446 448 653 444
 445 651 649 362 654 162 451 415 111 435  83]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2925
INFO voc_eval.py: 171: [ 39   4 119  40  43 191 326 183 124 276 188 152  47 193 228 156 123 185
 181  11 150 227 325  58 180 278 327 272 153 189 115 241 231  44   5 229
 126 320 267 249 274 103 146 220 116 184 142 144 275 190 168 232  74  57
 277 316 172 324 148 186 125 157 182  55 319 192   3 121  19 154 104  76
 230 101 248 158 159  81 155 216 140 302 173 211   8  46  68 303 246  10
 270 127 187  45  94  93 245 151 281 329 287 321  95 242 114  41 105 100
 240 250 233 314 259 223 102  21 247 265  42 129  56  67 244 162 243  88
 234 306 300  26 317 253  84 260  82 149  85 225 109  99  96  92  97 289
 304 120 206 111 141  77 236  18 122 257  61 194 130  50 280  29   7 261
 199 110 226 299 128   9 311  59  69 282  72 269 143 239  87 273 218 322
 331 310 254 312 318 330 323  28 117 315  25  60  24 222 313 118 112  71
 135 237 214 207 255  27 328 131 221 217 201   6  80 295 288  52 202 170
  79 266 279  73 308 298  83 208 175 256 262 212 301 145  48  53  22 147
 108 197 235 215 258 268 165 113  78 307  20  75 203  14  54 296 283 160
 224 290 292 133 167   0 219 251  51 252 210 291   1 204  70 174 263 264
 294  17 200 107 297  32 177 196 166 164  23 195 205 106 171 285  86 138
 271  33 161 136 284 163 305  15 198 238 139  31 179  49 137  37  16  13
 176 169  38 309   2 209  36 134  66 132 293 286  65 178  34  89  63  35
  30  64  90  98  62  91 213  12]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1625
INFO voc_eval.py: 171: [603 358 602  77 112 196  76 374 299 363 622 271 157 121 357 114 298 272
 854 610 199  86 200  75 426 118 371  85 172 425 480 423 276 761 195 759
 504 100 198 619 608  96 422 831 359 855 300 601 286 179 306 866 115 168
  90 274 787 113 698 368 411  61 173 188 456 605 305  89 301 430 510  21
 405 686 847 666 125 281 365 273 202 360 774 203 859 407 823 364 111 275
  99 492 429 862 362 861 161 509 309 634 154 408  74 261 187 667 302 793
 166 108  44 796  36 482 303 217 427 765  81 424 776 838 194 618 292 163
 289 193 797  60 834 511 197  93 412  87 476 280 585 758 706 127 788 491
 700 781 232 421 184 175 791 569 162 858 784  67 659 496  19 417 155 506
 835 120 205 800 428 590 126 548 505 170 672  65   6 486  58 230 457 283
  80 740 760 845 687 245 473 842 153  23 313 287 192 867 215 453 864 104
 668  40 775 780 249 885 799 692 158  54 515 278  84 736 236 103 669  39
 433  45 786 174 876 832 143 176 671 171  22  95 182  35 689 642 841 639
 782 186 849 475 705 160 403 314 431 557 513 285   8 633 373 165 737  59
 420 704 279 494 785 512 352 366 493 223 401 824 315 697  71 460 304 801
 779 574  52 432 227 820 189 329 452 380 333 746 167 164 816 851 870 221
 699  56 244 833 247 869 102 695 583 361 226 580 436 169 297 204 224 483
 149 798 636 767  97 181 438 529  79 852 723 445 883 349  63 611 264 607
 402 141  43 644 216 372 415 517 139 536 124 863 860  70 451 685 587 881
 711 151 664 675 231 661 246 465 239 829 710 813 228 435 214 350 637 201
 331  38 886 837 658 856 768  92 857 790 763 708 294 332 508  72 238 684
 674  66 641 490 107  17 577 265 640 693 233 434 884 804 794 437 538 156
 546 868 256 544 635 263 312 330 556 354 237 825 865 745 185 296 853  88
 479  82 625 159 770 814 458 497 178 353  91 307 848 534 783  73  46 248
 882 258  33 789 488 396 638 485 225 229  10 836 495 694 709 413 731 691
 222 240 293 462   7  42 498 191 177 632 334   4 183 277 267 545 571 409
 356 651 288 754 830 572  69 463 828 295 749  47 875 550 441 778 652  57
 528  20 690 670 477 370 282 744 101 218 520 554 589 551  41 718 404  13
 839 209 570 729 588 575  53 461   1 378 553 643 747 599 887 516 466 375
  11 810 351 846 665 152 290 748 581 343  83 843 384 771 109 712 598  24
 252 180 400 657  64 266 243   9 518 117 614 721 579  37 539 738   5 617
 377  55 662 418 521 136 316 257 190 484 741 336  34 573 284 547  18 342
 392 129 345 442 826 406 701 395 663 725 130 650 150 578   2 444 253 576
 815 291 743 369 827 655 630 106 140 802 440 594 720 394 584 468 376 268
 525 262 844 777 355 341 132 660 416 722  25 481 840 410 367 850 762 656
 414  68 688 676 707  98   0 110 122 766 728 471 792 526 752 803 742 537
 308 105 219 560 439 235 234 323  62 795  16 703 593 524 470 564 459 681
 344 144 541 555 522 393 443   3  78 600 612 818 319 702 455 772  94 469
 388 242 732 757 419 542 764 379  32 696 631 137  14 822 251  48 733 220
 503 317 318 447 678 464 348 756 326  49 552 389 812  15 499 653 500 260
 535 566 147 582 680 562 682 654 677 321 146 446  12 879 250 558 755 142
 454  51 609 559 734 561 591 514 683 540 871 716 739 135 679 467 448 543
 338 769 563 383 138 450 269 726 241 398 335 805 873 751 391 730 347 320
 673 735 519 507 727 478 724 567  26 270 449  50 596 501 568 145 808 397
 489 626 719  30 346 487 773 811 325 649 474 753 148 807 565 381 877 339
 399 387 527 714 213 472 502 604  31 523 210 586 821 549 382 206 806 819
 259 211 131 717 324 713 809 116 592 123 628 207 133 715 616 817 530 337
 606 212 340 386 533 385 645 531 208 878 390 595 597 327 880 627 647 328
 322  29 310 872 750 648 311 621 128 874  27 629  28 254 646 134 119 255
 532 615 620 613 623 624]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2969
INFO voc_eval.py: 171: [310 499 712 362 174 514 550   2 225  42  72 522  29 313 577 386 303 388
 557 318 389 471 647 518 561 314 163 676  43 501 422 419   8 497 555 505
 420  70 165 364 526 616 172 690   4  96  36 398 723 431 602 311  98 521
 729 282  18 363 615 100 176 280 393 135 745  49 371 192 117 556 713  63
 205 137 264 560 387 567 381 728 551 229  22 369 317 182 470 396 258 277
 374 441 468 730 265 337  44 726 322 270 105 382 566 275  34 637 298 138
 542 285 747 732 148 509 552 516 450 527 694 262 444 658 582 451 207 443
 423 433   7 267 544 236 554 559 366 434 696 290 380  53 175 438 187 562
 365  31 586 669 324  60 227  46 677   3 565 351 206 447 140 558 735 678
  99 385 201 421  52 546 685 512 233  38  11 164 372 481 146 714 335 427
 263 166 279 320 703 528 332 316 383 142 473 307 553 136 259 368 390 228
 222  45 646 737 716 162 579 293 304 292 692  67 240  68 596 312  81 683
 202 564  32 469 373  14 563 346 379 600 345 119  35  20 741 223 194 624
 651 367 395 343 746 425 272 638   9 243 120 281 359  59 375 445  65 150
  15 453  83 625 642 515 753 152 111 426 384 221 156 496 342 378 474 547
 350 409 442 684 650 715 659 480 731  50 734 232 695 736 261 121 576  19
 181  51 400 645 171 476 143 504 619 234 130 643 661 706 584 274 127 575
 597 437 144 622 490 578 432 114 626 344 486 628 472 660 634 196 520 336
 581 340 133 291 667 748 537 134 725 668  66 269 502  48 237 226 123 666
 430  26 448 104 112 377 743  78 742 671 733 153 739 744 580 653 376 348
  79 498 652 253 627 399 329 339  27 273 266 248 585 347 268 271  47 568
  69 338 331 599 583 752 738 612 529  41 513 588 603 341 242 203 101 167
 606 439  75 610 404 219 328 199 160 315 260  21 722 129 278 180 360 126
  71 283 464 122 440  64 693 103 397 115 613 124 436  30 630 125  12 145
  87 704 720 598 601 633 161 618 254 370 452 294 724 705 401 257 428 429
 533 531 629 349 149 614 235  76 107 740 190 327 698 507 352 169 605 211
 197 458 635 449  10 354  80 700 424   5 184 147 750 530 479 621 749 155
  73 543 435  28  62 446 721 727 632 198 517 158 540 191 210 525 296 617
 675  13 355  95 524 508  25 656 592 636 334  17 109 701 545 751 188 412
 487  91 391 204 457 189 141 168 503 648 674 649 541 173 594  88 654  77
 655 151 691 231  58 241 689 251 200 620 102 631 672 418 623 511 308 178
 110  33  61 154 157 640 707 664 305 106 118 608  74 159 113 309 697 333
 284 139 604 459 289 483 609 456 220 132 477 463 195 500 288 699 644 410
 185 286  56 239  37 252 510 170 467 523 295 641 177 402 361 460 230 247
 306 128 276 670 475 506 186 183 179 209 193 662 358 319 688 287 323 217
 353  39 214  84 321 686 465 589 218 657   6 595 489 394 325  40 249 407
 519  93 411 208 484 488 116 238  90  97 330 607 482 131 611 548 297 250
 687 535 224 478 108 665 574 663 461 300 466 212  94  57 572 213 462 413
 403   1   0 415 392 673 406 717 534 454  24 405 301 302 485 587 455 593
  89 639 357 590 570 532 591  86 679 356  92  82  85 414 417 416 492  23
 681 571 215 326 702 539 536 216  54 538 491 246 569 719 718  55 245 680
 255 256 495 573 299 682 710 494 408 493 244 708 711  16 709 549]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3609
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3205
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.265
INFO cross_voc_dataset_evaluator.py: 134: 0.492
INFO cross_voc_dataset_evaluator.py: 134: 0.195
INFO cross_voc_dataset_evaluator.py: 134: 0.234
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 134: 0.430
INFO cross_voc_dataset_evaluator.py: 134: 0.312
INFO cross_voc_dataset_evaluator.py: 134: 0.063
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.190
INFO cross_voc_dataset_evaluator.py: 134: 0.344
INFO cross_voc_dataset_evaluator.py: 134: 0.226
INFO cross_voc_dataset_evaluator.py: 134: 0.323
INFO cross_voc_dataset_evaluator.py: 134: 0.565
INFO cross_voc_dataset_evaluator.py: 134: 0.544
INFO cross_voc_dataset_evaluator.py: 134: 0.433
INFO cross_voc_dataset_evaluator.py: 134: 0.292
INFO cross_voc_dataset_evaluator.py: 134: 0.163
INFO cross_voc_dataset_evaluator.py: 134: 0.297
INFO cross_voc_dataset_evaluator.py: 134: 0.361
INFO cross_voc_dataset_evaluator.py: 135: 0.320
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.513s + 0.041s (eta: 0:01:08)
person 0.964946
person 0.95984864
person 0.95726645
person 0.98972964
person 0.9218353
person 0.9701708
person 0.9783927
person 0.91876256
person 0.9371937
person 0.9053454
person 0.9806818
person 0.93801624
bottle 0.92486686
bottle 0.91835326
bird 0.9518987
person 0.98154885
person 0.91292286
person 0.94024247
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.322s + 0.039s (eta: 0:00:41)
person 0.93268925
person 0.90222245
person 0.9496616
person 0.9431584
person 0.9592824
person 0.96918416
person 0.917712
person 0.9323995
person 0.94331557
person 0.9673169
person 0.9285381
person 0.98353934
person 0.92245597
person 0.9196969
person 0.9180115
person 0.91920865
person 0.96852523
person 0.90437156
person 0.9805588
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.326s + 0.039s (eta: 0:00:37)
person 0.92566925
person 0.9140477
person 0.94749737
chair 0.9059526
person 0.9248554
person 0.952308
person 0.93060505
person 0.95734584
chair 0.964668
chair 0.9351684
person 0.9778121
person 0.98674494
person 0.94263047
person 0.9732039
person 0.99135005
person 0.9821843
person 0.92732745
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.328s + 0.040s (eta: 0:00:34)
bird 0.92472434
person 0.9214902
person 0.919416
person 0.9933508
person 0.9763892
person 0.9444445
person 0.98386544
person 0.9721985
person 0.9571561
person 0.9629606
person 0.9324604
person 0.9691835
person 0.9341013
person 0.9247254
person 0.9537941
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.331s + 0.039s (eta: 0:00:31)
boat 0.9016468
diningtable 0.94467497
chair 0.9590159
chair 0.9625892
chair 0.9349948
pottedplant 0.9164621
pottedplant 0.92101437
pottedplant 0.9175633
pottedplant 0.91696316
pottedplant 0.9153713
pottedplant 0.9367738
pottedplant 0.9424065
pottedplant 0.93534625
person 0.9643405
person 0.98162305
person 0.9884985
person 0.92545855
person 0.9872061
person 0.9105716
person 0.92032605
person 0.90032595
person 0.91264296
person 0.9071913
person 0.9641222
person 0.94896394
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.326s + 0.038s (eta: 0:00:26)
person 0.9446778
person 0.9140565
person 0.9200903
person 0.93059796
person 0.9246954
person 0.95080185
person 0.92903036
person 0.9195616
bird 0.91824853
person 0.91709036
person 0.9871475
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.320s + 0.038s (eta: 0:00:22)
pottedplant 0.9100663
person 0.95779794
person 0.9532156
person 0.95685595
aeroplane 0.91113615
chair 0.9819411
diningtable 0.9494733
chair 0.9430851
chair 0.9788094
chair 0.91436565
pottedplant 0.9015717
bird 0.99299014
bird 0.9468868
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.321s + 0.038s (eta: 0:00:19)
car 0.9244852
car 0.9416885
person 0.98891735
person 0.9637707
person 0.9474444
person 0.93184286
horse 0.9141743
horse 0.9678861
person 0.9356199
person 0.95311767
person 0.96865237
person 0.91115254
car 0.94421107
car 0.96338695
bicycle 0.9680071
bicycle 0.9270763
person 0.9441731
person 0.90416884
person 0.9727624
chair 0.9073214
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.319s + 0.037s (eta: 0:00:15)
person 0.97377986
person 0.91555244
person 0.9033203
person 0.9702303
person 0.9013701
person 0.98829424
person 0.986226
person 0.93964434
person 0.96796453
person 0.963295
person 0.94112253
person 0.9896348
person 0.9213665
person 0.98630875
person 0.9803253
pottedplant 0.9123762
person 0.9899448
bus 0.9101774
person 0.9574818
bus 0.9195336
person 0.93766344
person 0.94954723
chair 0.93847346
person 0.9006909
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.320s + 0.037s (eta: 0:00:12)
motorbike 0.96963555
person 0.9731459
person 0.9020665
person 0.94926494
person 0.95771366
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.323s + 0.037s (eta: 0:00:08)
boat 0.98140633
boat 0.92029613
boat 0.92433995
person 0.9317785
person 0.9405556
person 0.9896522
person 0.985472
person 0.91421837
person 0.9346426
person 0.98779863
person 0.97475284
person 0.9039392
person 0.9318969
person 0.9304234
person 0.9799116
person 0.948239
car 0.9483287
person 0.930774
person 0.94264054
person 0.9860139
person 0.96276766
person 0.9299026
person 0.90899694
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.321s + 0.037s (eta: 0:00:05)
diningtable 0.9398904
chair 0.97313064
chair 0.91995424
chair 0.98021096
chair 0.9774754
chair 0.9349445
chair 0.9605225
person 0.93406457
bird 0.91399914
boat 0.9196606
person 0.95752656
person 0.9186232
person 0.9866345
person 0.90637046
person 0.9117184
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.319s + 0.036s (eta: 0:00:01)
person 0.98209953
person 0.93764895
person 0.9534548
person 0.95448893
person 0.944351
person 0.9597211
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.512s + 0.026s (eta: 0:01:06)
person 0.9793016
person 0.90334177
person 0.97708386
person 0.9686651
person 0.91448
person 0.9520256
bicycle 0.9509051
bicycle 0.97410935
person 0.922323
bird 0.9285944
pottedplant 0.9465208
person 0.97269416
person 0.9549766
person 0.95315635
person 0.90945715
person 0.9608112
person 0.98536575
person 0.9305617
person 0.9066512
person 0.9633417
person 0.9267945
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.321s + 0.032s (eta: 0:00:40)
person 0.9683338
person 0.92305046
person 0.9629298
person 0.9358441
person 0.90063345
person 0.9786572
person 0.9643956
chair 0.9273156
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.335s + 0.033s (eta: 0:00:38)
car 0.91342646
car 0.94076586
person 0.952224
person 0.9557603
person 0.9544848
person 0.9788654
person 0.9366949
person 0.93999904
person 0.9590251
person 0.9016491
person 0.91324496
car 0.92627496
car 0.9807602
person 0.92003864
person 0.9517025
person 0.9111193
person 0.9681197
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.327s + 0.035s (eta: 0:00:34)
person 0.9745014
chair 0.9247814
person 0.92294466
person 0.9481215
person 0.92196923
person 0.9372717
person 0.9237108
person 0.9335039
person 0.9797132
person 0.97608715
person 0.92753685
person 0.98615336
person 0.9738314
bird 0.9902092
person 0.91738343
bird 0.9656047
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.325s + 0.036s (eta: 0:00:30)
person 0.935049
person 0.96585345
person 0.9813937
person 0.91325325
car 0.9483287
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.328s + 0.036s (eta: 0:00:26)
bird 0.9129087
bird 0.9699673
bird 0.91422325
person 0.9302547
person 0.91458505
person 0.9862222
person 0.9700855
diningtable 0.97331184
person 0.9260546
chair 0.93720615
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.332s + 0.036s (eta: 0:00:23)
person 0.914529
person 0.98403317
bus 0.95365804
dog 0.9456446
diningtable 0.97034836
diningtable 0.9007403
chair 0.9013841
chair 0.98366773
chair 0.91323036
chair 0.9653445
chair 0.9306922
chair 0.9072563
pottedplant 0.9074492
person 0.911388
person 0.97073436
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.329s + 0.036s (eta: 0:00:19)
diningtable 0.9107505
person 0.94396603
person 0.95111585
person 0.91638523
person 0.94600326
person 0.9289214
person 0.93956363
bird 0.95710665
person 0.91831285
person 0.9559735
person 0.9602462
person 0.9235226
person 0.9711636
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.322s + 0.035s (eta: 0:00:15)
person 0.9589975
person 0.94939494
person 0.90161014
train 0.937783
person 0.9482973
person 0.96311796
person 0.94802845
person 0.918502
person 0.9348487
person 0.9249304
person 0.9029971
person 0.9519277
person 0.93452793
person 0.99090856
person 0.974441
person 0.90065104
person 0.9658572
person 0.9352813
person 0.9249476
person 0.9082357
tvmonitor 0.91665584
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.322s + 0.036s (eta: 0:00:12)
person 0.94614613
person 0.93797827
horse 0.95739514
person 0.96508783
person 0.9197076
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.322s + 0.036s (eta: 0:00:08)
person 0.9743155
person 0.9658044
person 0.92032653
person 0.9156802
person 0.9365571
person 0.9077439
person 0.92732304
person 0.9779438
person 0.93541247
person 0.9607734
chair 0.9676862
person 0.95670485
person 0.916365
person 0.9553518
person 0.9320215
person 0.9016725
car 0.9452465
car 0.91172725
car 0.9453317
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.325s + 0.036s (eta: 0:00:05)
person 0.9700565
person 0.9391482
person 0.95788795
person 0.9466287
person 0.9791804
person 0.9560342
person 0.9104771
person 0.9897194
person 0.98167944
person 0.9767542
person 0.9225711
person 0.91006225
diningtable 0.9253223
person 0.91178906
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.318s + 0.036s (eta: 0:00:01)
person 0.9355833
diningtable 0.92448586
person 0.9657386
person 0.98752373
person 0.96947765
person 0.9581469
person 0.94605
person 0.93997514
person 0.9579255
person 0.98189926
person 0.94314045
person 0.9569373
person 0.95308703
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.372s + 0.028s (eta: 0:00:49)
diningtable 0.91074896
person 0.9607735
person 0.97420496
diningtable 0.96644455
person 0.91630626
person 0.9200553
bottle 0.9056045
person 0.9751956
pottedplant 0.91966563
diningtable 0.9029251
person 0.96304196
diningtable 0.92148054
chair 0.9640949
chair 0.944573
chair 0.9088253
person 0.9230395
person 0.9960413
person 0.98024637
person 0.92575127
person 0.9653976
person 0.96188194
person 0.9023896
person 0.96809554
person 0.9533191
person 0.9207384
person 0.9402067
person 0.9625602
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.305s + 0.033s (eta: 0:00:38)
person 0.9365187
person 0.9855861
person 0.92537695
person 0.9674754
bottle 0.93835
person 0.92936957
person 0.9270227
person 0.945223
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.312s + 0.034s (eta: 0:00:35)
chair 0.9364465
bottle 0.90662354
person 0.96815497
person 0.92988366
person 0.9180442
person 0.9450268
person 0.91550094
person 0.93632084
person 0.93237865
bottle 0.9486269
person 0.9819342
person 0.93747985
person 0.92104965
person 0.9486551
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.315s + 0.035s (eta: 0:00:32)
person 0.95979285
person 0.91015327
bird 0.919845
person 0.96164066
person 0.9200316
person 0.960169
person 0.9660788
person 0.9537495
person 0.9429207
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.313s + 0.036s (eta: 0:00:29)
person 0.972639
person 0.92612827
person 0.9593751
person 0.955986
person 0.9691056
person 0.91751796
person 0.9409654
person 0.90115327
person 0.9061223
person 0.93933696
person 0.9129734
person 0.96827996
person 0.9369146
person 0.9069436
bottle 0.94814044
chair 0.93231493
boat 0.9052229
person 0.90693194
person 0.92450714
person 0.9682626
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.314s + 0.036s (eta: 0:00:25)
car 0.9793743
car 0.92041427
person 0.9204257
person 0.9747321
person 0.9050559
person 0.9103468
person 0.91859776
person 0.9626302
person 0.90355545
person 0.91315985
person 0.9107617
pottedplant 0.93659043
car 0.9876928
car 0.92036605
person 0.9232249
person 0.98800105
person 0.94297993
person 0.9853263
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.315s + 0.036s (eta: 0:00:22)
person 0.9587244
person 0.9796649
car 0.9603325
person 0.9179001
tvmonitor 0.9141567
person 0.9252067
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.314s + 0.035s (eta: 0:00:18)
chair 0.90040624
person 0.96847916
person 0.9316272
train 0.95628333
bird 0.90354824
bird 0.9807953
person 0.9117714
person 0.91646445
bicycle 0.94049144
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.315s + 0.035s (eta: 0:00:15)
person 0.96504074
person 0.93611187
person 0.9294414
person 0.9251537
person 0.98508716
person 0.95435137
person 0.97291124
person 0.98982936
person 0.9712016
person 0.9756678
person 0.9257154
person 0.95010734
person 0.96690357
car 0.94085133
person 0.9668929
person 0.9322375
person 0.96927917
person 0.97611815
person 0.9185979
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.314s + 0.035s (eta: 0:00:11)
person 0.9118391
person 0.9200677
person 0.91787857
chair 0.97507995
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.313s + 0.035s (eta: 0:00:08)
person 0.9782139
person 0.92822933
person 0.91109824
person 0.95551866
person 0.9613288
person 0.92159516
person 0.9021993
person 0.9776127
person 0.90066546
person 0.9617322
person 0.9123324
person 0.93474203
person 0.9569682
person 0.9148348
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.315s + 0.035s (eta: 0:00:04)
person 0.96619606
person 0.96898466
person 0.94190735
person 0.96498626
person 0.96634924
person 0.93034446
person 0.92483187
person 0.9278632
person 0.9201934
person 0.9469939
person 0.95694965
person 0.9047749
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.315s + 0.035s (eta: 0:00:01)
person 0.9267823
person 0.97060937
bird 0.92479783
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.444s + 0.047s (eta: 0:01:00)
person 0.922411
chair 0.9116248
chair 0.9528581
person 0.956433
person 0.9517219
person 0.9366851
diningtable 0.92469484
person 0.9692648
person 0.96550995
diningtable 0.908667
person 0.9419943
person 0.9636395
diningtable 0.9088277
person 0.965029
person 0.92571324
diningtable 0.95648897
diningtable 0.9116853
person 0.93739736
diningtable 0.9272169
chair 0.9786982
person 0.92792183
person 0.90413284
person 0.9888798
person 0.9845858
person 0.9655514
person 0.91683275
person 0.9221178
person 0.92941296
person 0.9697066
person 0.9827361
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.324s + 0.037s (eta: 0:00:41)
person 0.93305993
person 0.9425383
pottedplant 0.9348086
person 0.9007401
bird 0.91977113
bird 0.950564
bird 0.9708147
person 0.9649384
person 0.93634486
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.317s + 0.033s (eta: 0:00:36)
person 0.90090454
person 0.9170186
person 0.960816
person 0.9658107
person 0.9723154
person 0.90335697
person 0.9479782
bottle 0.94533926
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.321s + 0.035s (eta: 0:00:33)
pottedplant 0.93457043
chair 0.9351396
person 0.9125751
person 0.97754556
person 0.9292952
bottle 0.9103943
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.326s + 0.035s (eta: 0:00:30)
person 0.9600164
person 0.95664746
person 0.94904256
person 0.99480975
person 0.9205521
person 0.97803056
person 0.951403
person 0.9082672
person 0.96819234
person 0.9063714
person 0.93658626
person 0.92952704
person 0.99202174
person 0.94238806
person 0.90675247
person 0.96686244
person 0.9632144
person 0.9828928
person 0.95204073
person 0.92183864
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.322s + 0.035s (eta: 0:00:26)
person 0.9463628
person 0.9564718
person 0.9718621
diningtable 0.9797204
chair 0.943884
chair 0.9754229
chair 0.9860902
aeroplane 0.91473365
person 0.96577895
person 0.9779357
person 0.91715765
person 0.9229724
person 0.94310915
person 0.9730218
car 0.90188545
car 0.9778755
car 0.96954614
car 0.95468247
bird 0.9550264
chair 0.9213361
chair 0.92718583
person 0.93764865
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.327s + 0.036s (eta: 0:00:23)
person 0.98092276
person 0.9210125
person 0.9826513
person 0.9495219
person 0.9858845
person 0.92735374
person 0.9246608
person 0.9332576
person 0.91478115
person 0.9886541
person 0.9253311
person 0.9350788
horse 0.95878166
person 0.94543093
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.325s + 0.035s (eta: 0:00:19)
person 0.9462096
person 0.93463904
chair 0.945038
chair 0.9770677
chair 0.9594023
pottedplant 0.92108816
pottedplant 0.950246
person 0.9377459
person 0.92696965
person 0.9277185
person 0.95526904
person 0.9404697
person 0.92885154
person 0.9479149
bicycle 0.92266643
person 0.9838488
person 0.92707455
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.325s + 0.035s (eta: 0:00:15)
person 0.99106383
person 0.9191533
person 0.90682185
person 0.9562031
person 0.9798853
person 0.97550756
person 0.97112626
person 0.9753703
person 0.93791807
person 0.90280735
bus 0.9506705
person 0.9380676
person 0.95934254
person 0.9409884
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.324s + 0.035s (eta: 0:00:12)
person 0.97127515
person 0.90058136
person 0.92467755
chair 0.9108337
chair 0.9465676
tvmonitor 0.90335655
chair 0.9676568
chair 0.9426504
chair 0.9494432
person 0.9694046
person 0.9510853
person 0.97541714
person 0.9281917
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.322s + 0.035s (eta: 0:00:08)
motorbike 0.9483809
person 0.9790982
horse 0.964642
bicycle 0.95661867
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.322s + 0.035s (eta: 0:00:05)
person 0.91777927
person 0.957801
person 0.97985774
person 0.9669944
person 0.9815238
person 0.93340427
person 0.943384
person 0.93142205
bird 0.9256594
chair 0.9559677
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.321s + 0.035s (eta: 0:00:01)
person 0.98475224
person 0.9847506
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 80.262s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1301  318  313 ... 1101 1105 1100]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2611
INFO voc_eval.py: 171: [130  70 561 132 278  69 503 327 529 422  71 615 144 617 148 588  72 131
 535 140 292 433 506 420 610 307 619 622 145 623  79 600 562  73 628 523
 136 560 611  87 417 579 633 507 578 293  99 331  78 342  77 142 337 632
 424  90 603 566 516  74 340 429  86 102 572 612 512 485 592 149 135 635
 519 453 280 620 285 276 339 596 514 591 418  43 594 328 518 349 598 520
 141 137 333 317 380 624 434 308 105 469 509 426 522 589 281 517  20 460
 306 451 563 630 279 382 553 291 335 318 481 244 309  83 277 107 464 170
 423 286 441  25 341  41 462 626 171 597 595 146 474 511 375 461 483 152
 226 218 445 343 515 508  23 364  35 133 513 368 330 416 381 425 537 446
 482 593 290  88 150  31 151 634 604 498 316 567 344 586 466 631 613 222
 419 621  75  29 373 154 387 564 568 627 191 606 294 570 431 310  53 204
   5  48 421 358 585 359 209 571  42 153   7 450 569 488 580 283 505 134
 497 106  56 357 227 540 590 454 625  32 436 216 477 348 616 336   4  96
  76 536 629 193 224 470 332 475 104  18 378 103 282 248  97 143 559 491
 449 493 555 321 353 315 458  82 305  22  91 601 297 428 582 338  52 126
 504  21 288  37  94 605 576 214 177 468  95 299 575 455  66 447 207 284
 190 370 476 161 139 609 577 376 365 430 583 221 443 346 618 196 614 314
 301 487  24 168  93 219  63 599 303 404 637 484 250 607   1 203 350 527
 147 435 138 206 500   9  64 211 471  80 501 205 521  60 125 371 124 325
 533 267 457 587 473 100 444 393 448 302 329 432   3 495  85 202 492  59
 602  47 490 550 386  28 584 128 255 356 220 188 355 180  19 322   8 556
 551 319  38 296 115 502 538  54 184 489 351 463 118 383 119 574 472 510
 459 232  30 298 452 401 438 320 531 374 486 268 160 396  17 313  65 112
 182 465 198 210   2 200 110 233 395 172 467 201  27 185 439 239  15 397
 212 208  62  26 499 287 236 496 304 581 179   6  11 532 243  89  33 539
 215 552 407 183 181 354 324 266 246 388 123 225 289 389  61 323 235  34
 229  50  16 230 440 241  55  81 411 158  36 413 101 528 163 391 565 427
 187 326 176  14  58 494 526  44 377 194 347 223 557 334 525 217 174 415
 111 155 173 558 129 352 524 295 405 573 157  67 456 116 384 256 156 412
 162 360  57 273 402 197 554 394 189 400 437 390 274  45 238 108 480 167
 192 534  13 541 530 175 258 345 414 178  51 186  10 234 442 403 367 113
 109 121 399 195 166 164 252 379 270 117 114 385 120 543 213  49 231 392
 199  92 254 165 398 300  46  98 363 249  12 369 269 478  68 362 408 237
  84 263 372 251 265 245 479 253 240 361 228 242 275 264 548 247 544 542
 169 410 261 366 122 409   0 406 636 257 549 127 545  39 271 311 546 262
 272 312 159  40 260 608 547 259]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.4427
INFO voc_eval.py: 171: [ 613 1629 3065 ... 2802 3057 3058]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1943
INFO voc_eval.py: 171: [ 613  674  725 ... 2237 2231 2228]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2273
INFO voc_eval.py: 171: [398 436 601 343  12  21 689 389 317 397 687 435 441 694 661 347 364 626
 437 577 654 406  23 632 316 512 355 710 696 166 874 873 346  20 618 581
 774 351 345 652 348 604 704 623 578 318 390 714 697 361 693  18 703  16
  24 399 705 688 650 887 776 711 366  15 602 439 344 763 362 611 875 401
 586 739 322 500 546 717  81 440 527 775 330 359 407 301 319 287 405 773
 690  27 579 614 664 242 698  79 404 621 741 195  13  48 400 514 363 515
 225 636 729 715 323 320 365 695 408 789  22 640 635 334 615 138 839 720
 718 156 525 506 647 336 522 749 668 610 607 631 335 201 198 649 503 282
 719 451  26 625 529 430 219  50 585 655 556 748 238 393 354 531  35 840
 228 358  14 855 587 663 327 421 241 620 296 727 624 502 669 630 549 187
 603 641 227 236 276 709 730 177 665 170 734 134 289 753 712 438  17 737
 713  76 701 883 837 339  55 792 189 333 455 722 353  80 349 605 881 627
 809 582 478 220 533 633 779 169 838 892 628 357 877 433 637 783 360 606
 612 352 302 738 843 193 292  91 759 732 530 465 699 849 648 651 638 616
 173 780 723 584 751 402 300 889 728 167  37 583 140 700 237 331 449 448
 646  49 120 619 708 736 841 477  65 222 685 642 350 707 622  90 782 835
  75 391 580  92 151 691 645 692 781 432 123 207 137 243 268 725 836  28
 524 528 562 788 324 403 721 516 278 548 148 190 513 157 299 295 409 754
 202 394 790 115 262 609 356 555 733 136 116  51 274 261 329 791 507 893
 884 508 554 321 656  19 102 176 328 667  25 559 158 431 303 297 735 784
 639 310 724 801 205 870 340 481 139 552 154 561 325 726 250 608 629 272
  89 204 898 888 670 239 523 760 479 144 135 224 800 493 634 532 644 803
 223 341 716  66 218  61 820 643 429 411 743  30 298 273 229 480 871 702
 337 450 810 280  39 885 114 133 423 257 872 557 901 221 830 422 750 744
 444 132 706 653 617 125 332 367 305  60 526 194 812 878 209 270 294 551
 742 119 613  58 845 740 731 856 153   5 517 175 547 761 762 518 804 281
 851 764 199 752 217 519 179  46 206 890 446 186  47 778 903 378 474 118
 392 283 850 326 117 271 256 395 447 829 200 197 284 277 210 558 293 188
 101   2  54 122 164 291 854 590 226 369  70 799 131 142 396 505 802 793
 767  38 100 807 869 141 902 208 671 309 452 288 171 251 215  73 560 569
 412 258 504 846 882   6  72 152  53 537 574 269 196 387 880 553 185 494
 501 445 808 482  99 805 768 798  85 286 499 876 246 307 248 806 813 766
 814  34 460 290 232 275 374 498 900 769 233   3 168 787 495 434 459  29
 306 216 662  32 184 285  33 785 108 203 121 106 453 756 904 571 191 182
 811 263 897 591 124 107 150 786 853 145 794 308 857 454 475 834 172  86
 777 244  31 178  59 149  68 181 105 155 520 371 684 686 496 757 770 279
  41  52 110 510 267 338 143 550 852 174 521 259 677 573 113 418 183 896
  83 755 765 570 109 894 380 492 660 772 594 795 511 563 213 509 538  63
 180 129   4 879 375 234 595 659 842 386 657 467 895 413 844 192 599 461
 163 572 165   8 377 381  87 489 458 127 891 304 596 600 245 666 676 313
 822 379 847 254 575 589 368 672   1 130 576 473 658 592 848   0  56 539
  57 419 593 112 832  77 544 833  88 162  64 247 382 598  69  43 126 469
 497 466 415 146 886 899 831 240 565  67 424 214 383  44 564 821  42 312
 456 160 476 252 746 490 597 796  45 230  36 472  93 825 427 462 758  40
 128 425 264 491  97 675 384 681 567 253 588  82  84 416 568 535 315 265
 342 311 104 534 249 470 471 682 566 314 541 373 747 111 385 417 457 674
 536 468   9 680 818 816 819 817 488 103 260 388 745 863  71  10  74 771
  94 231 414 159 266 426 797  11 410 487 428   7 372  62 255 376  95 463
 161 683 828 545 442 486 540 862  96 864 868 823 815 542  98 867  78 679
 543 865 858 212 824 370 420 147 464 827 826 866 483 678 235 443 673 859
 211 484 860 861 485]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3511
INFO voc_eval.py: 171: [170 450  91  92 171  77 451 452  74  99 471  96  93  95 116  28 413 378
 176  76  79 415 105 101  80  89 206 427 420 404  46 165  88 400 248 142
 377 115 408 274  82 104  85 267 368 393  32 480 392 296 269 379  47 370
 213 439  78 276 229  83 449  62 454 185 239 495 266 313  87 112 340 114
  84 140 359 196 166 457 401  55 337 361 111 237 335 258 203  44 146  60
 159 360 323 342 154 120 284 210 147  86  81 277 257 194 372  75 343 365
 467 263  49 403 326 288 290 438 411 209 407 253 391 389  10 432 348 201
 426 437 384 197 448  11 418 289 227 268 208 152 118 425 493 164  33 322
 473 331 198 261 271 430 193 214 238 321 251 183 341 390 316  94 141  50
   5 255 129 386 113 117 110 445 444  29 369 139 124 402 332 287 226 362
 174 135 221 275 428 256   7 168 406 191 366 363 297 232 339 153 388 119
 222 367  22 298 419  98  34 334   1 260 474 492 200 435 204 102 195  40
 161 144 396  36 126 412 151 441 364 157 162 482 314 199 225 494 103 123
  43 315 422 358  58 429 333 433 410 249 207 387 286 324 318 282  97 409
 478 442 279  90 223 107 202  53 417 130 122 291 149 125 121 108 169 133
 325 231 109 173 265 217 405  31 338 414 351 319 262 205 399 106 395 100
 416 447   6 134 158  45 163 336 382 486 270   4 440 156 456  30 127 374
 469 250  35 143 423  21 264 175  57 446 394 484 145   3 234   8 252   9
 228 431 148 311 244 254 281 475 483 242  69 160  59 150 436 272  48 344
 294 434 330 381 212  42 280 259  41 184 216 424 352 128 320 295 131 317
 132 186 167 292 421 155  38 224   0 180 283   2 380 356 385 397 345  70
  25 308 236 240 218  39 398  54 278  23 349 371 328 246 443 300 488 293
 383  12  37  71 481 327 347 476 373 312 477 302 285 485 230 329  26 241
 220 376 375 235 215  56 188 310 233 273  61 309 455 461 479 306  16  64
 247 245 192  13 304 468 243 182 464 465 178 458 211 172 350 462 472  51
 303 487 137  68 190 346  17  18 179 491  67 354 299 466 181 305 490 353
 355 489 138 219  73 459  24 301 177 460  15 187 307 189  63 453  65  20
 136  52  14  66 463  72  19 470 357  27]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4293
INFO voc_eval.py: 171: [1559  713 1492 ... 1690 1684 1688]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.3029
INFO voc_eval.py: 171: [ 27  62 263 264  63 107  18 316 151 314 108  22  64 286 117 315 262 284
 256 152  66 125 197 295  25 233 132 259 287 136  82 289  73 250  46  99
  47 109 142 239  14 120  45 237 172 213  91  74  19 116  93 162  24 235
 307  39 150 201  68 291 166 101 296  79 185 243 167 186 297 218  61 124
  15 292  75  70 168 225  16 118  90 234 155 294 161 199  51 238  20 251
 134 114 188  48 119  17  71 102  49 110  21 153 219 236 187  23  50 203
 257  78 154 156 281  26 184 246  80 313 130 290 214   9 309 220  81 288
 177 308 143 255 293 133 224  83 311 285 241  92 103 137   6 123 240 127
 190  85 135 121 140  67 126 112 113 144 128 145 261  88  44  86 149 189
 148 165 227 221 158 222 317 258 111 283 272 195 242 267  84 205 192 122
  11   0  42 193  65 301 310  41 279 312 131  38 217 223 276 226 191 175
  36  43 265 244 253  87 100  10 160 252 245 231   1 106  12 302 104   5
 105 139 248 194 206  94 215 274  28   4 280  98   3 176 207 230  33  40
 200 115 198 282 204 129 275 138  13  89 212 278 247 170  56  32 163  95
  34 174   2 260 169 210 208  35 209 232 271 164  96  97   8  54   7 303
 266 157  55 299  37 228 298 273  29  52 300 182  30 305 202 216 269 211
 270  72 146 159 183 171 277  53  31 147  77 268 196 249 304  59  58 173
  76  57 179 229  60  69 178 141 254 181 180 306]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0762
INFO voc_eval.py: 171: [5969 2564  719 ... 4134 4473 4469]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3186
INFO voc_eval.py: 171: [308 243 190 415 643  52 253 263 194  43 363 421 240  56 266 697 247 239
 597 132  62 258 420 668 251 213 585  61 389 566 269 246  44 248 390 708
 351 189 309 704 701 696 422 359 173 197 192 315 565 188 191 426 419 522
  55  25 169  58 316 644 598 335 669 652 354 221 687 414 577 425 568 418
 232 293 712 127 582 388 599 700 502 244 699 126 501 427 162 380 361  60
 523 249 323 592 242  63 120 217  59 250 524 301 681 254 144  51 651 666
 260 227   5 674 584 702 252  14 411 609 576 364 424 327 527 601 149 139
 519 218 526 196 412 292 677 399 583 325 219 261 398 319 416 204 141 145
 108 271 231 370   4 504 673 497 493 485  35  85 187 587 317 276 198 326
 714 175  69 533 358 703  78 256 616  49 365  50  57 570 255 195 706 212
 267 567 313 676 711 621 166  46 341 506 334 680 682 529 441 379 393 581
 241 705 600 164 483 167 193 698 417 199 177 272  22 391 119 710 265 525
 593 208 259 121 270 492 612 168 679 678  42 466 171  75 344 709  10 203
 672 392 124 395 675 396 264 435 369 138 640 575 360 245 655 210 520 661
 178   7 686 605 312 123 586 172 134 664 654 413 128  41 474 273 362 302
 572 614 604  83 135 170 428 653 176 257 656  98 230 216 346 277 665 107
 476  23 671 262 226 394  21 103 310 130 387 489 530 136   6 588 692 580
 237 603 670 329 630 366 404 722 268 633 498 646 397 622  64 297 658  79
 503 148 505  82 617 122 106 163  37 514 179 355  76 486 667 274 296 707
 104 507 320 578 694  40 695 574 374 339 131 372 532 102 233 632 318 685
 521 174 467 275 405 154 381 688 645 473 423 528 142 202 225 377  99 693
 165 683 684 642  81 726 159 627 448 304 336 689 595 303 376 500 531 456
 222 224 368 153 330 429 306 378 468 206 133 475 728 307  12  54 215 534
 629 367 562  68 129 450 494 143 200 101 345 311 460 662 314 571 375  15
 300 125 690 539 725   0 637 499 541 649 105 408 373 508 406 639 112 156
 620 543 186 229 352 371 207 211 446 343 496  48 594 234 691 715   8 727
 615 479 608 579 447 455 635 220  84 452 338 641 478 111 294 550 236 305
 137 650 100 299  90 610  36 449 647 298 461 238 337  26 118 560  24  80
 542 487 657 638  38 185 436 607 209 716 228 491  27 606 462 723  33  77
 350 557 155 158 631 235 150 563 279  34 223 713 561 559 628  30 214  88
   1  19   9  97 146  53 477 182 205  89 484 115 590 201  86 623 342 183
 618  71 488  72  17 648  11  67 724  18  87 458 357 434 454 626 161 611
 340 569  28 451 347 465 659 515  47  65 556 512  73 518 113 180 544 490
  16 553 151 283 109  74 353 402  45 116 624  66 558 602 400 636  70 152
 551 495 591 554 540 407 552 453  32 160 445 482 549  31 564  13 147  96
 328 181  20 383 114 117 289 287 509 589 110 511 401 625 295 457 613 430
  39 596 619 157 510 286 470 555 403   3  29 513 459 331 480 660 291 438
 463 433 516 464 384 288 184 471 382 469   2 332 517 386 282 410 409 431
 535 634 439 720 385 573 278 280 281 444 481 348 324 322 547 437 548 333
 440 663 356 290 349 284 472 321 432 719 285 717 442 721 546 443 140 718
  92 536 545  93 538  94  95  91 537]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1841
INFO voc_eval.py: 171: [2981 1303 1361 ... 2235 2381 2297]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3430
INFO voc_eval.py: 171: [364 436 469 905 367 394 526  54 498 473 486  71 431 370 148 165 467 842
 666 150 576  55  51 211 395 154  47 162 577 434 908 262 365 474 433 159
 787 901 858 528 852 138  50 539 374 859 911  70 160 839 873 215 397 298
 841 490  73 488 439  53 846 546  46 895 501 147 295 260 484 157 391 459
 470 254 533 373 822 883  56 507 151 837 267 465 899 907 235 784  58 444
 153 257 239 840 635 290 581 914 440 375 903 641 530 238  72 690 261 621
 167 660 226 559 716 525 206 158 487  88 505 282 725 363 437 229 843 902
 495 352 723 218 217 256 246  69 149 880 485 878  49 503 155 135 458 575
 824 637 864 535 291 198 462 315 668 868 658 785 770 542 669 573 536 214
 432 140 263  35 255 345 865 232 799 874 848 821 879 755 574 714 628 240
 457 866 651 212 277 709 201  90 697 887 404  75 710 493 296 916 442  57
 754 236  61 657 833 664 163 454 624 225 460 312 543 392 781 241 205 143
 152 435 921 249 278  20 371 136 912 823 311 881 648 796 400 307 366 677
 463 832 855 284 553 726 472 532 213 224 248 909 689 448 340 896 571 468
 715 401 534 867 492  52 438 862 910 795 756 777 139 675 461 674 166  18
 310 692 182 456 860 161 119 711 871 250  76 706 356 288 748  21 801 227
 869 327 200 306  67 923 204 913 649 585 845 331 681 665 164 464 449 499
 121 266 125 372 362 642  38 544 745 512 216 405 875 788 629 287  62 207
 355 835 285 718 466 894 142 604 156 202 231 579 203 445 906 247 305 369
  64  74 834  60 570  93 396 688 233 443 782 680 636 877 861  13  36  92
 334 335 634 309 890 730 178 719 188 808 228 645  31 882  66 537 659 691
 253 245 607 402 137 919 917 297 357 123 720 802 403 776 849  34 321 237
 320 199 673 244 774 368 656 580 663 844  48 809 230 252 381  15 572 223
 301 749 268 694 791 132 299 602 120 836 915 724 251 271 126 452 130 377
 353 920 728 646 632 258 804 141 722 300 775 293 786 807 302  63 242 885
 884 818 758 122   6 471 608 286 838 410 735 234 584 622 870 567   1 924
 693 221 654 773 886 578 803 627  59 292 453 647  68 643 222 721 319 753
 605 583 412  37 325 847 269 851 603  89 743 500 606 888 922 294 329 744
 265  65  19 361 587 347 589 582 482 289  95 127   4 322 850  17 346 918
 717 876 491 662 820 379 393 747 118  26 190 698 778 872 639 736 678 661
 451  22 667 104 108 508  25 129 779 588 682 558 502 741 586 652 737 541
 527 926 337 308 313  91 193 551 806 192 732 513 128 390 596 479 134 676
 814 496 925 441 338 863 281  42   2 780 638  98 280 889 640 568 145 323
 358 446  77 406 800 550 259 303  94 644 423 376 455 549 751 729 497 100
 557   7 733 529 609  99 789 699 552 610 695 727   5 103  97 687 169 304
  96  30 594 185 422 316 272 317 380 399 793 904 283  45 765 318 898 354
  44  10 792 686 220 324 597 314 900 783 172 110 447 742 342 339 897  86
 561 734 653 275 264  40 670  82 828  23 805 569 279 522 811 378 187 341
 707 519 177 386 679 131 276 124 326 854 655 180 813  84 506 168 194  81
 475 330 856  33 601 336 383 684  14 547 102 416 106 344 650 683 388 189
 633 618 328 790 350 509  79 450 752 179  27 763 332 476 757 359 759 564
 857 274   0 133 701 671 523 273 514 481 562  24 477 810 184 762 520 101
   3 333 146 510 210 764 696 611 712 623 613  78 554 409 631 853 560 761
 515 565 343 171  83 144 111 705 563  39 478 421 566 798  87 517 750 713
 483 794 105 427 425 170 114 116 424 626 387 181  85 407 620 494 518  29
 556 516 614  80 612 740 183 630 731 398 426 812  28 702 625 504 797  43
 746 191 208 195 430 591 109 700 819 196 703 766 685 429 600 219 117  32
  16 704 738 107 817 598 538   8 408 599 739 593  41 615 186 590 708 489
 816 617 616 540  12 480 197 428 548 385 348 771 382 672 772 415 769 595
 829 413 815 243 209  11 115 555 384 113 270   9 511 760 825 545 619 531
 351 174 592 521 524 112 768 389 173 767 419 176 175 360 349 827 420 417
 826 418 831 830 414 892 411 893 891]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2269
INFO voc_eval.py: 171: [123 968 873 ... 707 708 709]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3028
INFO voc_eval.py: 171: [ 54 299 165  28  53 172  55 284 175 291  56 166 130  74 182  88  59  85
 288  33 169  63 129 190  96 212  47  90  80  37 195 184 273  18 287 308
  64 290  40 220 264 228  35  27  68 213  32  41 151 275 310 185 306 293
  92 265  79 180 276 131 173 127 289  16  57 219 302  22  62 176 186   7
 283 215 143 309 249 206  91  69 111 168 218 128  23 285 110 300  12 216
  31 234 252 171  49 296  78 239  89 164 267 295 207 125  60 181 311  38
  65 189 242  26 208 126 124  19 307 277 103 187 223 243  66 178  75 270
  86 102 141 266   2  25  87  34 281 250   5 113 112  70 177 191 160  11
  44 101  20 268  82  77 238 162 253  93   6  30 154  45  21  73 263   1
 282 179 251 193 222 121  94 229 274 167 203 279 217 305  83 278 100 245
 117 161  42   0 116 123 174 170 152 120  24  14 115 209 292 233 104 244
  99  17 194  48 271 257 256  61 210  76 272   4  58 188   9 140  29 122
 231 155 196 204 144 301  39 134 163 240 248 197 298  52 145  10 136 254
 119  36 139 205   8 106 236  71 118 105  43  46 192 214 255 146  98  95
 224  81 303 241 202 159 158   3  13 227 114 137 133 232  51 221 280  67
 200  50 149 142  72  15 247 246 201 198 230 225 226 294 211 183 235 258
  84 269 237 286 199 150 304 138 147 297 148  97 135 260 132 259 108 262
 261 157 109 156 107 153]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5642
INFO voc_eval.py: 171: [ 8466 13830  1048 ...   774   769 10741]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5409
INFO voc_eval.py: 171: [2612  755  274 ... 1889   45 1914]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4394
INFO voc_eval.py: 171: [ 37 327 539  38 345 548  54  46 140 357 542  39 340 322 544  76 339 540
 334  40 486  53 319 613 167 234  79 264 553 545 414 355  66 324 500 352
 373 554  74 644 169  49 353 294 133 279 632 546 318 620 420 335 148 543
 241 337 342 426 238 549  67 132 152 263 173 145 541 643 374 242 618 168
 562 138 330 278 266 346 435 519 149 153 144 484 419 616 131 164 146 604
 171 239 619 635  71 232 468  42 617 326 378 265 344 277 621 329 494 615
  75 669 372 528 631 606 623 276 503 336 423 245 229 371  70 451 343 555
 143 176 332 646  61 137 360 141 323 417 338 175 298  98 573 633 325 640
 147 236 418 142  48 293 628 296 356 246 612 639 495 567 354 422  69 505
 255 489  45 483 102  99 622 165 375 267 230 351 244 170 160 233 235 174
 328 670 679 185 624 134 159 349 231 429 469 331 150 550 204 243 217  52
 625 215 416 321 413 216 467 605 415 629 151 350 291 641 320 135 563  85
 333 347 551 172 237 626 668 158 348 218 341 512 247 525 634 642 607 552
 275 614  60 637 501 515 566 206 272 178 630 667 136 568 511 440 385 645
 565 108 492 271 569 377  44 574  73 383 557 421 627 491 240  47 638 177
 198 362  32 596 609 203 210 386  65 208 636 302 196 317 256  58 673 280
 183 570 274  55   8 610  19 577 490 547 608 359 578  34 207 508 209 482
 499 509 282 532 650 683  43 258 179 139 358 672 180 214 446 681 268  13
 524 363 481 527 181 498  28 281 128 361  78 586 556  64 671 262 116 521
   2 273 127 166 283 678 529 285 487 205 309 510 100 390 558 648 564  41
 103 257 572 270 101 611 677 647 430  72  88  97 189 449  35 470   0 576
 313 507 518  63 187  18 675  90 253 594  27 226  87 312 303 297 376 107
 424 400 193 269 466  20 575 111  26 480 461  50  57 485 464 680 431 220
 462 514  93 311 310 427   3 676 471 445 674 387 530  84  51   1 228 560
 194 443 365 288 520 582  29 192 593 488 682 432 522 315 657 478   4 129
 523 595 162 516 225 114 497 651 284 599 295 184  81 448 465 473 585 513
 649 506 559 259 112  80 531 441 299  62  59 453 444 447  10 224 292 195
 561 261 290 252  82 412 598 584 579 113  83  24  22 118  25 479  77 452
 124 197 439 105 438 115  31 597 316  89  68  56  21 393 450  15   5 289
  23   6 286  94 314 526   7 304  17  33  36 517 222 504 603 121 502 123
 219 163 379 493 122 591 428 260 300 380 434 106 120 588 659 496  91 589
 117 436 287 592 463 459 130 476  30 301 437 161 119 391 221 571 536  14
  86 202 399 211 409 583 104 405  16 366 587 201 656 125 442 381 212 126
 580 223 655 155 661 601 456 368 364 665  96   9 654 477 186  95 367  12
  92 188 475 403 474 433 392 600 581 191 369 535 590 602 227  11 410 182
 401 408 411 109 534 382 248 249 653 402 200 394 308 389 396 384 658 250
 404 190 533 666 472 251 406 199 398 663 395 407 388 254 154 157 537 370
 305 156 397 306 307 457 213 664 652 455 454 538 458 660 662 460 425 110]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2911
INFO voc_eval.py: 171: [ 36   3 108  41  37 173 260 314 177 113  44 174   2 143 215 179 170 140
 146 167   9 114 313 175  53 166 142 261 316 144 214  39 256 217 228 219
 258 106 233 169 308 237 176 116 131 218 132 206 171 259  70 135  93 115
 156  42  89  52   4  71 160 312 254 168 307  92 303 111 107 147  17 138
   8 196 216  51 178 145  64 129  43 288 161 120 172  94 283  87 253  90
 221 227  38 141 201  85 117  67 234 232 229 304  91 266 150 309 236  40
  95 119 301 273  84 220  19 245 231  86 235 105 210 244  81  63 306 285
 239 230 315 139 291 110 250   5 192  77 275 298 212  99 246 112  56 130
 101  76  72 299 289 243 310  54 223  16 180 263 226 203 100   7  79 319
 282 189  66  27  23 103  22 257 247  46 240  75 297 185 186 284  68 118
 296 133  80 300 318 209 181 207 311 194 109  55 213 242  25 317 302 241
  26  24 224 251  48  20  69 274   6  74 187 294 136 262 137 151 102 162
 158  47 183 202  45 292 286 287 248 252  98  18 198 104 155 305 204 270
 222  78  73 134 276 277 293 200 191 281 205  49  50 238 269   0 208 211
   1 122 249  11  65 149 123 278 193 195 154 190 268 182 280  97  14  21
 272 152 164  30  96 126 267 153 127 159 255 184 264  31 225 124 188 125
 148  12 128 271  13  10  15 165  29 163  34 197  35 121 157 295  62 279
  60  28  61 265 290  58  33  32  82  59  57  83  88 199]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1546
INFO voc_eval.py: 171: [629 376 118 375  87 641 211 316  83 289 125 171 396  81 876 633 315 634
 288 121 390  92 215 505 185 381 126 216  90 295 786 783 447 453 208 537
 647 377 628 214 103 877 450 657 194 393 317 855 321 134 890 122 631 119
 181 192 814 291 379 299  93 452 462 296 318 727 509 294  66 320 386 545
 290  95 881 695 485 136 218 219 428 292  21 523 378 714 884 848 869 436
 169 525 787 395 106 117 325 455 541 380 826 174 819 145 203 800 663 630
 638 432 696  45 886 794 646 790 277 382 111 179 302 196 738 651 449 454
 451 175 234 343 442 507 209  37 543  79 862 460  65 188 815 139 880 101
 446 858 199 825 212 538 817 210 730 614 173 168 524  23 529 764 307 389
 508 298 248 888 808 170 261 828 539 186 715 619  72 859 221 130 701 182
 300 891 810 907  88 688 167 247 599 172  46 329 265  70 822   5 865 767
  63 123 495 486 720 577  19 448 109 200 324 232 187 189 207 550 332 868
 252 301 108  58 197 483 697 802 813  42 177 698 723 297 115 458 107 506
 184 672 704 457 159 866 806 548 669 319  22 716 894 586 546 849 736 426
  36 856 899 851 801  40 308 526 811 330  97 383 260 263 371 728 827   7
 742 602 239 818 857 424 400 803 805 871 183 703  56  59 345 882 178 878
 463 243 237 893 661  78 314 311 632 691 180 844 514 812 439 348  98 240
 807 220 242 367 349 392 622 839 607 612 754 135 879 333  68  76 551 889
 795 482 560  86 885 873 434 384 100 431 725 368 163 741 461  99 262 674
 473  44 425 310 156 874  61 820 906 465 830 176 254 665 233 892  47 394
 322 165 713 900  71 522 244 480 391 271 792 909 202 836 496 853 519 250
 281 510 204 721 662 464 373 527 337 245 280 670 671 274 739 797 255 652
 568 305 231 347 346 606 253 435 309 860 722 217 191 264 666  39  80  89
 530 481 824 459 837 668 256 213 699 791  48 528 829  16 712 542 518 737
 875 416 438 102 718  94 760 487 313 575 870 585 456 565 104 774 809 867
 388  77 687 246 574 372 190 198 610 740  84 850 569 493  41 816 681 306
 883 279 854 603 303 554 905  20  11 559 335 823 600 124 777 627  60 427
  33 398   6 693 468 516 238 887 833  10 241 617 304 433  43 771 142 645
 531   3 692 780   1 758 861 618 598 549  69 604 798 660 747  91 793 259
 583 609 910 521  49 195 642 206 729 230 552 579 162  57 639 582 621 268
 407 423 429 852  38 491 673 694 272 497 370 374 750 752 835   9 772 105
 414 283 731 578 441 624 570 717 116 908 387 166 282 511 360  96 351 576
 765 902 515 601 773 385  35 768 667 193 608 547 682 755 430 469 872 201
  62 637 353 735  34 605 753 863 164 415   4 334 706 278 616 359 150 664
  24 785 804 285 751 273 269 799 700 685 658  18 864 770 838 557 154 341
  82 133 689 205 719 613  55  64 710 769 589 444 312 358 776 440 726 225
 249 686  75  26 788 258 732 821   0 362 410 573 513 144 762 512 413 110
 437 556 500 417 707 779 235 757 445 158 567 626 323 113 112 488 775 733
 623 467 584 474 566  85 484 284 501 251 114 470 840 555 680 336  67 339
 594 293 659 571 708  50 411 503 342 572 781 471 517 796 536 443 328 399
  32  17 361 475 761 895 690  73 724 266   2 236 152 476 845 734 472 492
 789 369 257  52 466 366 904 588 611 580  54 406 847  15 553   8 784  14
  12 782 711 684 532 498 338 267 357 540 412 587 593 709 756 596  13 157
 590 350 766 683 620 533 151 153 477 149 405 276 502 479  74  25 420 286
 494 155 898 365 746 749 778 654 759 763 591 834 489  30 520 419 499 287
 490 592 748 705 702 137 364 421 418 363 655 534 397 544 160 597 901 422
 402  53 648 161  51 558 354 831 832 615 678 355 744 535 581  31 478 595
 408 129 141 842 356 653 562 403 326 561 745 846 352 229 635 625 743 344
 143 275 226 409 504 340 404 643 227 327 222 636 903 228 140 675 401 223
 147  29 331 564 224 656 650  28 677 676 132 897 843  27 896 644 679 649
 120 841 146 138 148 128 127 131 270 563 640]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.3239
INFO voc_eval.py: 171: [283 470 660 284 332 482 156 200 519   2  40  26 492  66 544 525 356 358
 278 359 528 486 392  41 479 146 633 604 447 523 289 390 472 391 148 345
   7 494 334 477 644 285 582  84  65  82 150 368  32 125 446 675 567 489
 256 286 333  17 671 157   4 362 119 583 442 254 693  47 463 205 527 204
 357 474 661 237 172 524 365 291 342 340 104 471 534 183  21 677  59 422
 344 412 234  95  30 294 129 250 163 249 520 580 253  86 308 295  83 439
 242  42 418 414 259 484 395  31 121 272 533 511 679 161 496 552 696 441
 244 185 513  88 212 428 262 202 668 336 352 139 404 614   6 241 335 408
  50 649 413 553 184  28 400  10 683 529 297 634 625 355 323  48 650 341
  55 149 240 532 393 635 126 516 369  43 526 522 235 290 655 239 306 521
   3  11 179 203 147 662 664 120 338 353  63  62 640 444 197 548 562 267
 281 639 266 685 164 304  44 288 351 217 566 279 343 490 455 145 417 531
  13 198 689 366 530 440 603  19 133 106 607 255 398 181 411 315 245 317
 427 608 107 136 140 483  85 596 173 423 318 328 399 684 694 589 587 207
 645 410 337 169 210 600  33  54 350 586 468 322 127 445 663 354  14 273
 517 407  18 581 616 618 314  49  29 113 563 154   8 374 678 682 381 201
 543 403 554 584  74 617 130 464 112 443 108 647 601 676 547 688 542 118
 488 451 238 695 691 630 265 460 692 602 116 370 346 316 421 347 476 673
  94 624 588 590 686 309 246 690  96 101 545 549 579 576 311 213 555  61
 224  20 697  93  46 622 196 243 310 595 571 535 312 564 236  72 612 680
 377 623 559 174  25 592 469  64  24 247 214  70 248 320  89 401 502 551
 380 473  45 500 498 565 313 415 349 574 568 251 577 151 610 409  71 330
  68 319 402 160 300 687 110 258 550 700  87 111 134 348  58 681 109 426
 609 546 180  69 367 135 339 301 303 307 233 615  37 480 435 656 405 591
 416 162 293 299 144 131 230 453 570  91  39 229  38  35  90  76 503 153
 424 652 189 396 165 648 578 137 499 419 672  27 493 321   5  73 448 699
 478 670 175 406 653 485 287 420 425   9 698 270 171 491 512 142 326 324
  16 394 594 188  60 585 669 654  67 166 632 209 509 397  80  23 593 514
 674 228 128 327 124  12 606 182 208  81 100 613 361 155 510 261 631 475
 611 497 598 515 372 177 457 158  99 218 646 431 557 627 223 384 620 206
 138 268 176  57 605 305 152  92 178 257  34 280 132 481 432 168 167  97
 364 269 536 371 575  56 141 123  53 122 114 651 438 434 296 569 143  36
 331 263 282 159 459 452 264 211  52 115 495 329 572 292 643 626 216 252
 619 325 192 187 458 433 170 462 260 227 363 195 102 487 436 454 225 450
 599 382  75 105 560 186 641 360 190 573 376  98 271 215 449 226 302 379
 375 103  78 505 621 383 117 461 378 275 199 373 191 642 456  22   0 561
 556 629 385   1 437 506 628 430 277 429 276  79 597 387 540 388  77 558
 501 504 538 665 386 389 220 636 298 193  51 666 539 508 194 507 667 465
 638 221 466 231 537 274 658 232 467 637 222 219 541 659 657  15 518]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3516
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3163
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.261
INFO cross_voc_dataset_evaluator.py: 134: 0.443
INFO cross_voc_dataset_evaluator.py: 134: 0.194
INFO cross_voc_dataset_evaluator.py: 134: 0.227
INFO cross_voc_dataset_evaluator.py: 134: 0.351
INFO cross_voc_dataset_evaluator.py: 134: 0.429
INFO cross_voc_dataset_evaluator.py: 134: 0.303
INFO cross_voc_dataset_evaluator.py: 134: 0.076
INFO cross_voc_dataset_evaluator.py: 134: 0.319
INFO cross_voc_dataset_evaluator.py: 134: 0.184
INFO cross_voc_dataset_evaluator.py: 134: 0.343
INFO cross_voc_dataset_evaluator.py: 134: 0.227
INFO cross_voc_dataset_evaluator.py: 134: 0.303
INFO cross_voc_dataset_evaluator.py: 134: 0.564
INFO cross_voc_dataset_evaluator.py: 134: 0.541
INFO cross_voc_dataset_evaluator.py: 134: 0.439
INFO cross_voc_dataset_evaluator.py: 134: 0.291
INFO cross_voc_dataset_evaluator.py: 134: 0.155
INFO cross_voc_dataset_evaluator.py: 134: 0.324
INFO cross_voc_dataset_evaluator.py: 134: 0.352
INFO cross_voc_dataset_evaluator.py: 135: 0.316
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.344s + 0.033s (eta: 0:00:46)
person 0.9670469
person 0.92871386
person 0.9914961
person 0.95848304
person 0.97143507
person 0.9519486
person 0.9801031
person 0.9402925
person 0.907871
person 0.9809827
person 0.9380731
bottle 0.9238864
bottle 0.91924876
bird 0.951754
person 0.9819489
person 0.9155627
person 0.94115216
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.319s + 0.033s (eta: 0:00:40)
person 0.9418997
person 0.9096178
person 0.95461994
person 0.9438045
person 0.9802199
person 0.943911
person 0.96880966
person 0.9190009
person 0.93621635
person 0.9293828
person 0.90449053
person 0.9824938
person 0.92061055
person 0.93249243
person 0.91851205
person 0.9221688
person 0.97141844
person 0.9077022
cat 0.9006554
person 0.9802263
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.322s + 0.036s (eta: 0:00:37)
person 0.92696166
person 0.91305697
person 0.9502759
chair 0.909815
person 0.97079325
person 0.9007424
person 0.95046586
person 0.9613787
chair 0.96515155
chair 0.93649685
chair 0.9109476
person 0.9785209
person 0.9877296
person 0.93323994
person 0.9748068
person 0.9914726
person 0.98159677
person 0.9203129
person 0.9048128
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.325s + 0.039s (eta: 0:00:34)
bird 0.93153626
person 0.92386454
person 0.9202817
person 0.9935523
person 0.9756237
person 0.9535761
person 0.9843634
person 0.9717999
person 0.9007337
person 0.95811814
person 0.9641377
person 0.9106658
person 0.91356224
person 0.9696201
person 0.93621135
person 0.95937806
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.329s + 0.038s (eta: 0:00:30)
boat 0.9000153
diningtable 0.9476761
chair 0.9613343
chair 0.9643596
chair 0.936612
chair 0.9538974
pottedplant 0.903294
pottedplant 0.9185535
pottedplant 0.9155611
pottedplant 0.9049173
pottedplant 0.912598
pottedplant 0.9429677
pottedplant 0.93639433
pottedplant 0.9335593
person 0.96490973
person 0.9820889
person 0.92339617
person 0.978564
person 0.98792493
person 0.9096667
person 0.9127752
person 0.9111688
person 0.91338897
person 0.96838355
person 0.9510905
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.332s + 0.037s (eta: 0:00:27)
person 0.9495127
person 0.9174917
person 0.9197265
person 0.9377741
person 0.9296184
person 0.9516332
person 0.929863
person 0.92287236
bird 0.92036927
person 0.9259071
person 0.9872851
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.326s + 0.036s (eta: 0:00:23)
pottedplant 0.90692693
person 0.9596542
person 0.9095503
person 0.9012563
person 0.9530638
person 0.9574303
aeroplane 0.9041526
chair 0.98226327
diningtable 0.9551969
chair 0.9178721
chair 0.9454874
chair 0.9796587
chair 0.930643
pottedplant 0.9000514
bird 0.9930647
bird 0.95042974
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.325s + 0.036s (eta: 0:00:19)
car 0.9266565
car 0.93836063
car 0.90633345
person 0.98862934
person 0.97350967
person 0.92780584
horse 0.9692851
horse 0.9182682
person 0.9362523
person 0.95257264
person 0.9680656
person 0.91409403
car 0.94492185
car 0.96491456
bicycle 0.96489954
bicycle 0.92822677
person 0.94034344
person 0.9095922
person 0.97395444
chair 0.91234887
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.323s + 0.037s (eta: 0:00:15)
person 0.97381514
person 0.95685464
person 0.9715137
person 0.9063915
person 0.9062358
person 0.9883994
person 0.9859489
person 0.9411204
person 0.9671678
person 0.9640044
person 0.9907466
person 0.9664257
person 0.91024613
person 0.98664176
person 0.9798431
pottedplant 0.9063877
person 0.9899727
person 0.9625339
bus 0.9118248
bus 0.91526264
person 0.93870986
person 0.9511164
chair 0.94150996
person 0.9036564
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.324s + 0.036s (eta: 0:00:12)
motorbike 0.96709967
person 0.9737289
person 0.9117113
person 0.9082357
person 0.952
person 0.9619061
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.326s + 0.036s (eta: 0:00:08)
boat 0.9816327
boat 0.9466416
boat 0.91672724
person 0.9898461
person 0.9362714
person 0.985502
person 0.91606224
person 0.9286975
person 0.98831683
person 0.97481436
person 0.90619236
person 0.9376643
person 0.93236405
person 0.97526675
person 0.9055002
person 0.94812286
car 0.94962656
person 0.93312645
person 0.9446526
person 0.9862818
person 0.96093667
person 0.9138197
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.325s + 0.036s (eta: 0:00:05)
diningtable 0.9557596
chair 0.9734581
chair 0.92008764
chair 0.9805665
chair 0.905861
chair 0.97918093
chair 0.9314506
chair 0.9605675
person 0.93712646
bird 0.9188214
bird 0.9034376
boat 0.91944367
person 0.96084994
person 0.92276615
person 0.9876632
person 0.9086012
person 0.9048089
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.320s + 0.036s (eta: 0:00:01)
person 0.98271227
person 0.9639461
person 0.9545962
person 0.95472497
person 0.9460723
person 0.96257716
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.510s + 0.037s (eta: 0:01:07)
person 0.98034763
person 0.9107185
person 0.9777314
person 0.97650355
person 0.9091905
bicycle 0.973077
person 0.9041927
bicycle 0.9740762
person 0.92812383
bird 0.9455847
pottedplant 0.94538945
person 0.9696316
person 0.9265433
person 0.95695204
person 0.91040784
person 0.9628598
person 0.98590964
person 0.9285293
person 0.9645791
person 0.9297469
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.315s + 0.037s (eta: 0:00:40)
person 0.90979004
person 0.96293277
person 0.91482013
person 0.9702923
person 0.9332622
person 0.93595207
person 0.9795721
person 0.9643195
chair 0.9270229
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.329s + 0.041s (eta: 0:00:38)
car 0.9146723
car 0.94548506
person 0.9491504
person 0.95795566
person 0.9562981
person 0.9799416
person 0.94437516
person 0.9432889
person 0.9614651
car 0.9267355
car 0.9809311
person 0.91719836
person 0.95410246
person 0.9182081
person 0.9048015
person 0.9723442
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.326s + 0.039s (eta: 0:00:34)
person 0.9761153
chair 0.9284501
person 0.92550683
person 0.9491973
person 0.9233453
person 0.9293454
person 0.9268701
person 0.9364075
person 0.9801145
person 0.97649425
person 0.9298573
person 0.9863521
person 0.97456634
person 0.92211175
bird 0.99049836
person 0.91897976
bird 0.95048046
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.325s + 0.039s (eta: 0:00:30)
person 0.9398469
person 0.9684247
person 0.9817749
person 0.9232476
person 0.92199403
car 0.94962656
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.324s + 0.038s (eta: 0:00:26)
bird 0.9122265
bird 0.96858436
bird 0.9120128
person 0.9310811
person 0.90039325
person 0.9057712
person 0.9134593
person 0.9871013
diningtable 0.9461981
person 0.9707408
diningtable 0.9761031
person 0.92982453
chair 0.93965477
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.331s + 0.037s (eta: 0:00:23)
person 0.90076023
person 0.9265908
person 0.9847134
bus 0.95020086
dog 0.94995373
diningtable 0.9719383
diningtable 0.9109272
chair 0.9062081
chair 0.98414546
chair 0.91280156
chair 0.96775323
chair 0.9328217
chair 0.9062768
pottedplant 0.9050108
person 0.90820813
person 0.97299284
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.326s + 0.037s (eta: 0:00:19)
diningtable 0.9156127
chair 0.9026281
person 0.94443953
person 0.9645881
person 0.9493435
person 0.96014374
person 0.94223344
bird 0.9564027
person 0.92164993
person 0.95755064
person 0.96159065
person 0.92639726
person 0.970851
person 0.9055854
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.323s + 0.036s (eta: 0:00:15)
person 0.96019596
person 0.955914
person 0.90091896
train 0.93887913
person 0.9476124
person 0.962785
person 0.94768244
person 0.9252882
person 0.9342774
person 0.9266148
person 0.90339035
person 0.9537445
person 0.9272297
person 0.99156696
person 0.9741786
person 0.9636708
person 0.9346015
person 0.9258939
person 0.9175353
tvmonitor 0.915819
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.325s + 0.036s (eta: 0:00:12)
person 0.9611433
person 0.9006299
person 0.9404344
horse 0.90028256
horse 0.9592958
person 0.9652609
person 0.92380637
person 0.92450947
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.327s + 0.036s (eta: 0:00:08)
person 0.974288
person 0.96572745
person 0.9204365
person 0.9174512
person 0.9436474
person 0.91647476
person 0.92931616
person 0.97813725
person 0.9054863
person 0.963048
person 0.9375124
chair 0.9698598
person 0.9585708
person 0.94752795
person 0.95639
person 0.9026775
person 0.9089088
car 0.9454012
car 0.9135691
car 0.94827044
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.327s + 0.036s (eta: 0:00:05)
person 0.96886617
person 0.9407545
person 0.9598332
person 0.9474005
person 0.9814432
person 0.95726436
person 0.91773176
person 0.9897267
person 0.9816727
person 0.92012817
person 0.9771926
person 0.9244403
person 0.9335704
diningtable 0.9287839
person 0.91322905
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.321s + 0.035s (eta: 0:00:01)
person 0.93653274
diningtable 0.91912705
person 0.96776307
person 0.98735374
person 0.9675945
person 0.9583529
person 0.95166594
person 0.9026599
person 0.94032097
person 0.96402776
person 0.9816242
person 0.94558424
person 0.95648843
person 0.9540165
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.407s + 0.046s (eta: 0:00:56)
diningtable 0.9183947
person 0.9603725
person 0.9749039
diningtable 0.96956843
person 0.9193259
person 0.9195341
bottle 0.90559137
person 0.97609806
pottedplant 0.91847825
diningtable 0.9146298
person 0.9643119
diningtable 0.922743
chair 0.94956726
chair 0.96479654
chair 0.9164191
person 0.90098035
person 0.92625344
person 0.9959176
person 0.98040354
person 0.92790705
person 0.9658215
person 0.9632858
person 0.9016496
person 0.9691665
person 0.9557295
person 0.9318825
person 0.9632846
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.299s + 0.031s (eta: 0:00:37)
person 0.900104
person 0.9407217
person 0.985685
person 0.9287615
person 0.96886474
bottle 0.9375778
person 0.9315972
person 0.9344145
person 0.9489297
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.310s + 0.033s (eta: 0:00:35)
chair 0.9350119
bottle 0.9071362
person 0.9701114
person 0.92098916
person 0.93378466
person 0.92392755
person 0.948105
person 0.9221938
person 0.90337294
person 0.9418538
person 0.9371717
bottle 0.9030806
bottle 0.9507656
person 0.9818747
person 0.9368537
person 0.9212921
person 0.9481791
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.316s + 0.033s (eta: 0:00:32)
person 0.9627827
person 0.9151587
bird 0.9202967
person 0.9628092
person 0.924241
person 0.963277
person 0.9679165
person 0.9565965
person 0.94520366
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.315s + 0.034s (eta: 0:00:29)
person 0.97312546
person 0.92734283
person 0.9603626
person 0.95503235
person 0.9696723
person 0.9180383
person 0.94070154
person 0.9112656
person 0.9384427
person 0.91160023
person 0.9688222
person 0.9529474
person 0.93038344
bottle 0.9490588
chair 0.9315737
boat 0.9051761
person 0.90610045
person 0.97281355
person 0.95209897
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.314s + 0.033s (eta: 0:00:25)
car 0.98007613
car 0.9215507
person 0.9216841
person 0.9756305
person 0.9073949
person 0.9143092
person 0.93073064
person 0.9009417
person 0.96337533
person 0.9086847
person 0.91936034
pottedplant 0.9210618
pottedplant 0.93385684
car 0.98815596
car 0.9177431
person 0.9272718
person 0.9877699
person 0.9480475
person 0.93674976
person 0.9853935
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.314s + 0.034s (eta: 0:00:22)
person 0.95959973
person 0.9794755
car 0.9617689
person 0.9204234
tvmonitor 0.92512
person 0.9274388
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.318s + 0.033s (eta: 0:00:18)
chair 0.9020047
person 0.9693312
person 0.9380478
train 0.91034883
train 0.9588458
bird 0.90710825
bird 0.9818233
person 0.91832995
person 0.9216222
bicycle 0.93818337
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.315s + 0.034s (eta: 0:00:15)
person 0.9668814
person 0.936292
person 0.9292226
person 0.92704606
person 0.98569643
person 0.95587575
person 0.9727342
person 0.98951006
person 0.97070754
person 0.9000966
person 0.95245385
person 0.965896
car 0.946788
person 0.9678179
person 0.9269764
person 0.9805226
person 0.9243967
person 0.90514433
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.315s + 0.034s (eta: 0:00:11)
person 0.9117387
person 0.9440489
person 0.9199544
chair 0.9751179
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.317s + 0.033s (eta: 0:00:08)
person 0.9798666
person 0.93251777
person 0.91797197
person 0.9589498
person 0.96133035
person 0.9235641
person 0.9045836
person 0.90046483
person 0.9780187
person 0.9013955
boat 0.90116644
person 0.9638936
person 0.9152161
person 0.94027066
person 0.9019506
person 0.9574949
person 0.91929334
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.318s + 0.033s (eta: 0:00:04)
person 0.9663123
person 0.97115
person 0.96581316
person 0.96732056
person 0.9223008
person 0.92878336
person 0.921199
person 0.93683785
person 0.9582115
person 0.91156703
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.318s + 0.034s (eta: 0:00:01)
person 0.9312864
person 0.97086334
person 0.9035596
bird 0.9160769
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.557s + 0.047s (eta: 0:01:14)
person 0.92741317
chair 0.9154251
chair 0.955215
person 0.95724416
person 0.9249248
person 0.9521085
person 0.9402946
diningtable 0.9279648
person 0.97155154
person 0.9681571
diningtable 0.9267296
person 0.946889
person 0.96578974
person 0.9003891
diningtable 0.91051686
person 0.9648464
person 0.9273036
diningtable 0.9610549
diningtable 0.9196067
diningtable 0.93375415
person 0.946244
chair 0.9015739
chair 0.9798987
person 0.92766076
person 0.98893577
person 0.9624528
person 0.98483604
person 0.91632086
person 0.92238986
person 0.96774745
person 0.98319167
person 0.92181915
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.348s + 0.040s (eta: 0:00:44)
person 0.935605
person 0.9476916
pottedplant 0.93396693
bird 0.916187
bird 0.94973034
bird 0.97143304
person 0.9656718
person 0.93554026
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.336s + 0.039s (eta: 0:00:39)
person 0.94922113
person 0.90348697
person 0.96081996
person 0.9672291
person 0.9739327
person 0.90165454
person 0.9487077
bottle 0.9011056
bottle 0.94910663
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.332s + 0.039s (eta: 0:00:34)
person 0.9013942
pottedplant 0.93305475
chair 0.936934
person 0.9078497
person 0.97667503
person 0.93400925
bottle 0.91033435
bottle 0.90320766
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.335s + 0.038s (eta: 0:00:31)
person 0.96093863
person 0.9584311
person 0.9483601
person 0.9947843
person 0.9193659
person 0.97878575
person 0.9531065
person 0.91483223
person 0.9702096
person 0.90503705
person 0.9100955
person 0.9358321
person 0.9324064
person 0.9919785
person 0.94098836
person 0.9701263
person 0.90691763
person 0.96226746
person 0.9831715
person 0.95226955
person 0.9240216
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.331s + 0.038s (eta: 0:00:27)
person 0.96136075
person 0.9713879
diningtable 0.9817733
chair 0.9465944
chair 0.9768703
chair 0.9867229
aeroplane 0.915009
person 0.96575403
person 0.97854304
person 0.9251889
person 0.9078738
person 0.94449615
person 0.97324705
car 0.9789397
car 0.97081316
car 0.9564564
bird 0.95634896
chair 0.9261761
chair 0.9287878
person 0.9405119
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.332s + 0.038s (eta: 0:00:23)
person 0.9822525
person 0.918379
person 0.98195595
person 0.9543954
person 0.98554313
person 0.92226493
person 0.9299379
person 0.9340217
person 0.9192276
person 0.9888228
person 0.926023
person 0.90549713
person 0.93615645
person 0.90120053
horse 0.9613145
person 0.9436266
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.327s + 0.037s (eta: 0:00:19)
person 0.94948256
person 0.9394649
chair 0.94455874
chair 0.97741324
chair 0.9587855
pottedplant 0.92130244
pottedplant 0.9476701
person 0.93926275
person 0.92901564
person 0.9312792
person 0.95658755
person 0.9396158
person 0.93022394
person 0.95121133
bicycle 0.9197294
person 0.9842821
person 0.9335958
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.327s + 0.038s (eta: 0:00:16)
person 0.9910343
person 0.91023546
person 0.95692366
person 0.98075074
person 0.9127676
person 0.9761016
person 0.9765287
person 0.9711227
person 0.93761814
person 0.9106142
person 0.90331185
bus 0.9526547
person 0.9021714
person 0.94224095
person 0.96156263
person 0.94426507
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.326s + 0.037s (eta: 0:00:12)
person 0.96963567
person 0.90250987
person 0.92897975
chair 0.9127852
chair 0.9478591
tvmonitor 0.90179557
chair 0.96922016
chair 0.93854153
chair 0.9418406
person 0.9707159
person 0.95348233
person 0.9762927
person 0.9243573
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.322s + 0.037s (eta: 0:00:08)
motorbike 0.9157306
motorbike 0.9528283
person 0.9787948
horse 0.96143645
bicycle 0.9573216
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.323s + 0.037s (eta: 0:00:05)
person 0.9204014
person 0.9580579
person 0.9802712
person 0.9682911
person 0.9826345
person 0.9409665
person 0.9491596
person 0.9360944
bird 0.92287034
chair 0.95535237
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.322s + 0.037s (eta: 0:00:01)
person 0.98543996
person 0.9852158
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 82.249s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1239  308  304 ... 1053 1055 1058]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2540
INFO voc_eval.py: 171: [126 127  67 540 261  66 481 137 313 130 506 403 128 593 596  69 569 512
 415  70 274 598 484 590 401 602 601 290 582  68 134 541 501 136 607 539
  83 591  71 558 485 135 613 399 557 276 327 516  77  74 322 405 132 611
  89 545 494 584 514 139 410 129 314  85 571  72 324 589 552 497 498 145
 490 464 263  98 268 323 492 599 614 575 496 570 400 572 259 146 433  39
 141 315 603 489 152 363 302  18 409 495 579 264 542 608 448 262 101 291
 578 320 434 365 404 533 442  79 422 289 269 303  21 460 328 605 103 292
 487  20 577 167 493 453 150 574 329  37 331 232 491 168 486 131 140 441
 358 206 317 148 215 463 424 427 499 407 573 609 273 260 348 351 610 364
  84 546 517 568 151 461 209  30 301 547  33 612 435  24 600 592 398  27
 332 402 564 549 408 149 586 446 543 275   3 567  17 413 550 477  75 138
 428 443 330 339 147 356 606  38  50  28  45 548 293 344 266 371 580 196
 187 561 604 595 343 321  53 456   5 476 430 102 520 205 335 411 436 279
 143 200  15 421  80 583 454 212 142 306 133  73 265  95 271  93  99 298
   2 267 216 534 202 507  49  88 340 563 585 594 288 538 100 555 318 467
  86 412 188 361  19 455  22 587 429 470 440 483 565 447 236 282  32  92
 554  64 597 300 471 210 185 359 333 287 207 174 284 505 500 416 144 352
 197 199 299 165 280  31 389  78  91 449  61 513   0 198  44 192 159 462
 425 414 450 354 189 465  96 452 326 319 479 478 466 445 119  58   7  82
 237  57 122  62  16 437 251 316 406 355 310 515 420 426 201 535 482 325
 472 377 208 278 116 307 468 417  25  52 304   1 342 432 177 553  23 305
 369  29 336 566 511 531 366 444 384 115 195 281 418 475 439 241 111 480
 488 176 509 203  14 518 183 286 551 194 357 431 451   6 376 252 473  34
 178  63 337 204 469 180 160 562 347 272  12 385 576 270  60 379 221  87
 108 181 285  97 502 220 503 474 341 532 380 169 179   9 227 334  47 213
 158 372 536 214 125 193 224 106  26 250 175 309   4 391 397 338 504 544
  40 217 581 223  13 373  56 277 211 218 419  59 519 559  11 360  43 537
 311 173 186 438 233 395 156  54 560 231 113 367 396 191 556  90 172 345
 257 229 374 243 510  65  76 163 508 308 184 362 370 170 616 171 387 154
 104 388 423 164 123 153  48 155  41 190 378 383 234  10 457   8 182 107
 161  55 368 350 386 109 105  42 226 222 240 381 375  46  81 114 117 118
 124 283 522 110 459 162 458 254  51 238 353 346 312 112 219 382  94 392
 253 225 239 249 235 230 525 248 258 247 228 524 246 394 528 393 523 349
 521 166 242 529 121 615 390 296 526  35 256 294 527 295 120 530 245  36
 297 588 157 255 244]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.4180
INFO voc_eval.py: 171: [ 623 1651 3127 ... 3118 3119 3126]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1905
INFO voc_eval.py: 171: [ 627  660  737 ...  126 2004 2247]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2221
INFO voc_eval.py: 171: [392 602 430 333  11  22 679 383 307 391 676 601 433 429 595 683 652 337
 354 618 571 431 400  23 627 306 697 508 159 346 685 857 858 574 777 615
 336 342 596 335 338 572 644 702 693 599 308 686 351 682 384  19  16 763
 692 653  24 677 699 695 750 393  15 641  21 868 597 434  14 350 729 394
 334 312 580 355 542 860 705 404  74 643 498 320 349 521 435 752 399 289
 678 309 573 275  27 657 762 233 687 398 613 655 616  72  12 314 353 861
 395 684 217 190 709 771 626 511 510 718 704 402  47 357 325 310 327 629
 707 783 326 132 639 608 522 827 612 501 706 640 518 148 604 738 193 619
 645 578 271  87 526 340 445 600 212 387 194  26 499 840 713 348 716  13
 553 528 828 221 737 425 728 582 859 229 317 545 723  46 284 720 598 227
 698 264 232 700 128 690 621 220 163 182 432 711 575 277 701 726 322  75
 453  18 825 785 162 184 356 798 826  45 524 339 630 725  52 647 603 214
 877 188 496 873 174 622 428 343 290 748 642 579 347 712 396 280 722 637
 635 527 656 461 741 352 831 167 345 607 415 688 719 474 767 628  73 254
 576 218 323 689 577 160 285 829 727 450 444 134 228 611 143 288 443  34
  31 330 636 114 344 617 341 739 772 696 631 614  85 715 770 823 703 473
 742  64 680 841 116  88 397 388 871 558 681 674 265 252 131 401 200 315
 520 781 544 824 319 234 525 634  61 551 552 260 708 878 509 283 403 769
 149 385 195  48 427 784 141 512 694 235 247 774 318 185 109 110 311 130
 502 581 606  25 358 503 724 150 557  17 782 549 714 620 171 198 286 659
 717  68 792  20 291 609 475 129  96 257 487 605  84 426 625 146 519 133
 632 749 854 624 169 240 419 222 791 801 883 870 211 300 197 867  57 529
  28 256 331 216 406 766 795 287 732 270 230 108 610 244 638 809  36 691
 799 328 721 886 733 321 127 633 855 269 818 872 856 555 646 476 554  62
  56 213 863 294 424 523 623 440 359 513 730 543 189 119 215   4 166 833
 710 751 202 753 740 794 514 869 875 175  43 515 768 754 113 145  54 837
 731  44 442 370 469 199 765 386 112 449 210 181 258 389 416 316 281   1
 111 183 836 282 324 888 817 203 154 126 272 164 136 313 135 876  86 790
 757 115 253  95 660 192 279 137 839 773 381 259   5 585 786  35 778 201
 390 500 550 361 219 556 793 853 488 268 797 241 295 864 866 532 245 446
 276 168  69  80 563 569 834 298 497 209  50 441 144 248 418 278 180 263
 237 191 887 796 100 780 296 755 654 779 452 161 759 495 493 274   2 494
 273 489 803 842 775  29 457  32 880 367 456 239 675 776 889 546 186 117
 224 101 516 477 103 447 884 787 172 179 173 744 236 565 882 266 586 533
 196 448  30 548  83 470 297 491 177 764 178 105 251 118  49 800 142 547
  81 243 138 865 743 451 472  55  51 822 261 255 170 746 650  99 490  39
 661 838 147  94  65 506 267  77 517 412 879 246 564 176 165 568 504 802
 745 862  78 262 102 107 486 329 299 463 122 756 881   3 566 534 662 507
 206 761 559 187 658 668 648 372   7 788 157 758 593 380   0 483 567 407
 832 369 594 225 584 589  59 649 303 293 830 158 590 124 373 570 505  60
 413 371 375 360 835 468 885 535 587  82 156  41 409 874 592 669 819 292
 125 588 561 462 104 811 376  89 492  71 465 821 541 155 238  40 374  53
 420  66 820 152 591 368 560 231 810 484 120 208 454 485  42 139 467 302
 422 249 242 562  63  38 471  33  76 121 747 735 665 223 421 410 305 666
  79 815 123 530 531 301 411 651 332 377 304 106 664  98  91 378 672 455
  37 466 366   8 464 583 482 417 537 807 734 805 736 671 808 806 379 207
 847 382 760  97  90 789 362   9  10  70 408 250 405 365 423  58  67 151
 480   6 846 536  92 481 673 437 364 670 848 538  93 851 540 850 849 812
 363 205 460 813 539 814 843 414 852 804 816 478 226 844 439 140 663 458
 204 438 153 667 845 459 436 479]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3250
INFO voc_eval.py: 171: [436 163  88  89 164 437  74  86 438  96 458  72  93  92  90  73 111 365
  25 170 399  71 100  76  98 401 197  77 405 411  40 390 244  83 158 136
 386 270 364  79 101 258 394 379 357 289  41 391 440 378 262  29 366 442
 187 431  57  85 359 135  49 178  80 204  75 425 257 213 482 218 332 159
 107 229 109 284  82 387 306 269  54  38 327 247 106 328 349 194 315 351
  11 133 453 110  78 115 147  84 152  43 240  81 462 254 191 279 415 200
 246 333 389 188 324 360 140 337 242 185 201 375 371 434 397 377 355 348
 410 259 186 239 199 265 281 317  30 422 435 404 157 219 409 167 145 113
 176 193  12 313 331 280 323 314 460  24 283 250 423 406   9 376 308 274
 237  44 480 112 168 245 358 253 108 183 278 124 374  26 430  31 353 388
 212 205 222 350   5 352 138  91 146 325 114 393 419 161 330 192 235 119
 216 128  17 134 189 382 143 356 334 214 421 326 132 292 290 461  32  35
 121 267 190  94  53 154 195 291 243 418 217 150 469 481 479   1 155   6
 427 118 103 144 413 310  95 373 120  48  99 354 271 307 198 305  87 428
 215 104 369 396 311 105 276 125 398 395 441 139 162  28  97 316 465 225
 165 117 381 264 403 221 151 433 341 392 385 169 414  39 196 252 329 256
  55 255 400 454 318 238 472   4 319 116 380 149 224 122 432 407 208 228
 102 466   3  52 251 156 263  42 273 137 230  37  27  33 261  65 232 220
 303 402   8 141 362   7 368 249 426 202 416 342 272 153 412 301 420 322
 417 241 275  36 424 142 248 287 367 123 309 223 172 408 127 312 126 372
 160  66 384 148 383 346 207  70  34 227 370 468 177 339 288 475 268 277
   0 335 467   2  51  10 300 439 463 429 456 210 282  56  50 266 286  15
 233 206 320 338 464 363 448  23 285 443 231 361 184 302 203 446 473 450
 175 211 236 166 321  58 304 470 298 226 459 209 260 234 297 180 173  45
 174 182 296  20  63  46 340 455  64 293 478  14 451 336 344 129 474 343
 299 295 444 345 477 445  69 171  21  19  61 476 294 131  59 179  18 181
 447  22 457  13  47  16  62 449 471 130 452  67  68  60 347]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4272
INFO voc_eval.py: 171: [1593  729 1523 ...  108  114  111]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2983
INFO voc_eval.py: 171: [ 28  64 267 111  65 322 160  19 320 112  23 291  66 121 268 161 321 259
  74 289 130 207  68 269  26 301 237 262 139 295  76  50 292 113  48 253
 206  85 217 126  46 143 244 266  15 104 150 242  95 119  96  25 171 181
 239  71 177 312 297  77 175  20  40 159 302 194 195 107 249  82 129 231
 223  93 298  72 178  17  16 124 125 300 117 238 164  21 170 205 254  53
 243 258 162  63  73 114 197  22 260  18 210 136 141  52 163  24 240  97
 201  83 248 209 193 116 296 123 109 176  49 158  54 224  79 246 299 319
  84 221 293 284 208 290 313  86  92 314 316 132   8 199 151 186 128  27
 127  89 228 227 245 265 144 152   6  51  45  69 198 142 225 157 156 131
 196 140  87 147 230 174 122 241 247 166 115 153 133  90 261 229 108 220
 137 273 288 323  47 270 294  42  88  14 282 138 317 315  67 222  43 169
 200 120  44 306   9 226 250 211  39  10 256 277 318   0 255 106  37 184
 103  94  98 235 279  91 105 215 134 118 185  34 307 202 146   1  41 234
  29 204 287 110   4   3 219 283 280 135   2 145  33 264 251 285  59  99
 101 172 263 102  35   5 179 286 173 271  36 100 236 213  58 216 183 165
 212  12   7  30 304 278 276 308  57 303 232 310 190 214 305 167  31 275
  55 274  75 203 218 281  32 311  13  56 154 180 168 252  62  61 192  70
  60 309  78 272 148 233 182  81  80 187  11 155 188 257  38 191 189 149]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0961
INFO voc_eval.py: 171: [5956 2562  712 ...  291 4120 4445]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3174
INFO voc_eval.py: 171: [330 255 199 438 682  54 268 205 278 386  46 741 444 254 251  64 736 267
 253 443 635 270 411 715 226 277  65 258 624 605 136 412 259 331  59 198
 124  47 280 382 735 409 201 334 200 183 338  52 375 445 743 207 554 204
 604 180 442 450 441 232  61 437 368 243 275  62  28  58 745 230 636 564
 739 256 607 738 131 384 170 378 726 308 637 262 557 533 690 751 449 620
 322 403 615 307 209 281 123 451  66 530 265   6 228 555  17 711 435 273
 434 631  63  68 606 684 387 722 558 748 359 419 208 446 526 244 393 559
 112 197 149 345 740 339 274 648 266 439 154 707 713  53 448   5 264 211
 549 625 622 148 639 150 417 718 333 261 422 245 513 284 231 621 203 423
 218 521 614  45 349 341 367 534 626 440 285 206 225  37 389 321 346 202
 466 290 185 686 654 138 413 381 556 721 716 177 562 176 742 609 720 510
 737 272 717 252  49 348  86  75 279 656 188 239 213 623 536 383 223 153
 219 638 395 174 619 425 418 179  57  13 260  26 749 125 366 651 332 283
 714 156 561 182 129 719 398 552 415 491 361 460   8 551 436 747  25 263
 242 210 257 692  44 189 531 512 323 420 613 128 286 643 276 563 710 410
 696 337 725 678 385 127   7 318 695 502 704 371 311 414 237 408 229 282
 181 642 416 701 344 652  80 216 396 271 693 705 729 611 105 247 155 753
 452 171  60  85 147 518 527 744 627 390 746  70  89 514  99 137 178 503
 732  27 315 666 641 421 246 708 187  42 683 238 659 234 109 733 126 553
 269 750 709 139  43 723 106 316 532 706 535 184 164 212 670 103 492 340
 161 241 618 537 762 671 158 145 326 730 394 186 543 351 550 287 313 764
 566 325 568 364  40 616  81 289 687 175 612 501 522 685  18 767 336 698
 379 724  88 169 399 474 329 493 146 159 214  14 190 454 288 712 335 140
 133 664 235 727 400 130 312 314 447 401 529 507 765 236 343 477 681 134
 574 360 494 342 222 100 633 576 483 240 702 224 392 584 453 569 115 766
 634 388 107 578 500 667 610 602 327 528  87 560 486  11 162 397 657 221
   0 328 754 565 506 317 476 319 372  51 653 755  82 676 630  23 734 647
 160 391 472 320 567 402 135 431 525 249  74 728 515  30 196 731  32 152
  12 617 673 141 233 102 363 132 220 157 601 677 475 104 473 646 309 599
 461 680 505 110  19 482 577 645 108  22 376 217 355 632 250 479 370 650
 519  29 142 101 227  31  55 143 756  20 215  84 665 116 122 511 598  90
  38 195 516 248 459 694 629  21 358 596  56 374 588 429 488 504 763 362
 689 324  36  72  78 655 669 593  94 523 487 193 674 168   1 668  73  69
 660 365 587  16 194  67  33 424 517 165  50  15 541 679  34 661 663 649
 590 600 752 524 427 471 699 119 478 544 490 575 520  93 548  39 369 430
  24   9 111 640 485 191 120 697 688 608 595  91 294  83  79  48 172 675
 151  76 117 586  10 498 167  77 597 481 377 585 589  71 579  92 480 380
 592 603 628  35 539 591 192  98 166 296 662 455 540 350 163 538 310 404
 496 428 121 463 426 173 353  41 484 304 295 118 542 302 658 700 644 508
 497 113 594   3 293 546 458 405 691 306 489 464 354 407 495 114 545   2
 300 547 456 433 432 406 470 298   4 761 582 570 672 509 703 465 457 462
 291 347 303 352 373 297 301 583 356 357 467 499 468 305 760 299 757 292
 144 469  95 759 581 758 571  96 580  97 573 572]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1852
INFO voc_eval.py: 171: [2942 1301 1356 ... 1208 2352 2272]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3408
INFO voc_eval.py: 171: [369 434 893 470 371 398 526  52 504 472 488  68 431 528 374 467 144 159
 656 142 825 571  49  53 207 156 149 395  45 370 572 253 897 153 539 473
 298 545  48 779 890 846 433 839 129 379 438 847 474 294 211 822 820 900
  70  67 140 549 436 862 884 486  51  44 490 151 502 543 836 145 458 400
  54 288 378 872 397 511 888 810 532 465 231 262 252 657  56 147 775 263
 823 631 567 237  85 896 892 558 905 236 679  69 161 445 611 255 441 258
 650 279 380 541 702 489 870 903 510 575 368 152 289 201 833 527 222 507
 711 215 143 254 440 628 461 361 891 475 487  47 213 759 537 498 439 709
  66 266 127 148 867 457 824 827 468 210 866 678 223 315 225 243 648 629
 843 570 776 809 195 349 495 852  34  87 132 700 856 853 697 640  55 295
 868 808 460 745 432 658 619 647 627 208 744 232 685 202 277 615 198 834
 375 157 407 453 771 228 238 456 854 701 282 819 463 244 435 536 637 443
 245 471 404 239 912 146 789 372 209 811 654 667 128 818 885 160  58 221
 342 312 447 696 158 710 494 568 360 131  72 405 898 130 746 162 455 280
 831 869 459  50 214 848 863 787 850 376 197 307 464 246 311 469 855 552
  71 235 150 859 899 446 638 737 579 329 366 333 680 858  22 265 114 655
 155 448 154 860 178 300 356 437 694 767 664 462 670 399 883 306 286 163
 620  64 915 717 119 116 895 285 735 618 503 574 373 540  62 849 134 506
 548 778 828 515  17 677 196 141 632 135  90 310 260  23  20 212 199 136
 408 466 538 224 763 894 259 292 705  61  35 200 600  89 634 830 358 290
 601  46 879 444 649 871  63 297 264 826 715 598  33 301 322 707 305 299
 185 573 569 906 250 764 291 791 535 837 766  57 683 625 909 706 126 773
 409 118 337 602 821 248 406 321 227 124 220 249 783 302  31 878  16 864
 336 777 120 542  86 832 663 377 387 910 904 713 229 115 646 247 241 908
 230 738 226 666 382 782 797 653 716 234 636 645 133 309 451 596 714  15
 874 261 242 612 493 838 644 793 267 296 873 452 117 861 635 682 695 577
 271 218 662 835 626 749 708 652 913 800 578 414  36 916 365 501 792 320
 762 350 359 765 327 401 500  92 681 268 734 722 743 703 233 564 484 138
 357 796 278 293  38   1 284 219 688 857 580 736 733  65 123 576 581 623
 396  60 582 287 907  59 505 876 712 807 121 865 875 597  88 187 557 769
 914 641 450 102 723 454 599 105 325 551 251 384 668 113 442   5 772   6
  21 729 724 719 313 877 189 351 188 367 741 917 331 531 394 415 665 317
 499 323 829  29 122  95 686 257 803 770 481 673 851 887 425  93 651 918
 339 381 768 529 603 901  91  73 886 589 362  94 740 512 281 780   2  97
 790  11  96 902 175 565 889 614 304 617 795 516 108 424  30 182   7 283
 720 125 730 633 319 556 587 784 630 341 303  25 344 316 272 774 388  83
 308 674 343 642 785 386 275  42 383 107 340 165  43 911 669 217 314 276
 672 624 814 560 523  39 328 256 841 318 799 659 844 553 509 671 754 639
  79 721 191 520  81 590 476 643 348 167 392 354 330 385  99 566   3 781
 324 338  37 101  14 326 794 103 174 335 164 418  26 391 742 428 478 347
 332  75  28 449 184 660 274 137  19 748 676 334 513 804 805 845 522 561
 177 595 699 690 363 480 684 273 524 530 563 139  27   4 544 482 750 798
 206  32   0 691  98 840  76 755 186 842 181 517 402 176 675 508 554 518
 514 427 788  80 622 390 616 422 604 739  24 698 727 687 613 559 166 426
 752 412 479 190 112 753 109 100 562 585  77 485  84 605 610 193  82 786
 555 179  78 491 583 519 621 110  74 180 689 546 718  41 607 692 106 216
 584  18 806 346 693 204 497 104 423 410 430 389 345 429 492 550 192 547
 477 533 194 802  40 725 183   8 496 747 726 521 593 203 732 608 591 411
 205 352 534 728 483 704 111 592  13 586 609 606 661 761 760 757 403 588
 815 801 594  10  12 817 270 240 751   9 525 393 355 416 269 756 171 758
 364 413 353 173 172 170 168 421 169 813 816 812 419 420 417 881 731 882
 880]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2267
INFO voc_eval.py: 171: [ 133 1025  926 ...  752  753  751]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3172
INFO voc_eval.py: 171: [ 50 292 275 153  27  49 159  51 162  52 154 298 121  76  36 170  72  33
  54  81 183 156  61 203  58 179 120 262  34 280  16 283 276  57 213 301
 219  39 254 206  63  26 285  32 140 265 286  30 172 255  78 277 295 266
 302 160 299 122 167 211 161 118  56   7 176 166  21 231  55  82  65 155
 240  86 198 181 257 303 274 103 210 282  66 133 289 119 158 204 279  77
  22  44  28 244  12 177  23 102 152 296  87 225  70 116 169 117  64  38
  91  24 281 233 199 178  17 267  18 278 112 290  69 164  37 216  71  60
  62 200 174  98  84 304 207  11 132 234 300 256  83 260 163   5 272   1
 215 241  88  74  96 104 143  80  41 243 253  68 258 105  19  89 209 173
 208 113 148   6 165 288 150  20 264 168 212 235  79  92  29 220 242  73
 141 157 269  97 149  40  13   0 115 109 180  31  43 194 291  95 284 224
 201 248 191 107 188 202 261   4 229  85  93  99 263 268 106   9  25 144
 114   3 187 175  53 108 230 247 184  15 151 110 293 222  94 196  75  10
 131 271 126 111  48 127 182 239 130 297   8  35 245 197 205  14  42  59
 217 135  90 100 228 232  45 185 134 246 221 186 227 294 226 214 236  67
 147 218 270 189   2 193 223  46 237 128 138 238 190 195 287 171 136 249
 259 192 124 139 129 125 137 123 251 250 101 252  47 146 145 273 142]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5338
INFO voc_eval.py: 171: [ 8478 13863  1051 ... 10756   770 10758]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5409
INFO voc_eval.py: 171: [2541  747  268 ... 2765 2752 1868]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4251
INFO voc_eval.py: 171: [ 38 328 546  39 346 556  55  48 143  69 359 549  41 350 342  78 551 341
 547 321  43 618 167 491 233 560 266 146 552 357 326 353 169 422 561  52
 650 135 356 296 378 637 319 426 625 553 550 335 420 432 344 499 152  71
 360 555 238 280 174 239 504 548 154 148 168 133 153 147 264 320 330 613
 623 649 570 241 621 348 172 324  65 492 164 132 425 267 624 377 639 237
 474 232 345 438 622  75  45 626 629 620 536  73 176 229 279 500 421 144
 204 429 456 347 564 638 382 508 376 265 338 281 365 652 332 131 140 245
 177  66 501 509 358 325 149 300 145 340  97 617 327 580  47 644  44 235
  63 428 375 628  72 503 640 297 295 634 102 648 435 244  98 175 490 355
 678 141 150 576 675 256 231 230 339 136 160 242 151 134 434 243 630 268
 329 633 323 158 424 475 423 331 183 557 352 615 234 379 646 293 571 322
 686 611 635 632 473 215 214 173 627 514 216 171 519 349 515 419  62 333
 278  85 574 354 673 619 351 532 522 336 159 676 674 446 246 334 179 277
 337 343 139 206 577 537 559 573 236 275 641 558 636 612  54 383 137 647
 427 643 631 645 565 138 497  77 459 495 364 437 575 178 390 108 240 642
  49 603 651 207 363 381 583 304 586 209  32 202 272 578 513  19 614 388
 191  59 496 391 507 196 318  76 518 517  42 257 554  40 208  60  57 616
 452 362 534 679 585 166 361 283 489 502 213  70  29 203 205 282 367 182
 258 531 181 540  46 128 127 562 572  10 271 263 180 366 688 115 286 656
 269   2 274  99 101 488   0 259 309 677 100  96 126 566 284 276 395 593
 684  28 654  33 170 516 685 526 314  50 142  12 430 476  79 584  58 253
 185 188 165  35  20  18 653 313 601 103 298 455  27  88  68 273  53 472
 539  93 682 451  86 224 568 370 493  84 450 394 305 440 470 312  51 530
 111 192 226 193 527 433 466   1 310 129 106 406 194 380 270 219 525 687
 448 600 477 447  30 311  91 602 113 290  15 479 469 454 524 223 458   3
 681 567 485 439 512 607 689 683  67 521 186 260 117 569 505 690 316 471
 498 655  61 606 445 195 162 582 581  25 292 610  81 587 222 487 657 453
  83  21  80 417  64 285 112 294 252  82  23  74 299 289 592 412 457 287
   8 523 659 114 604 563 124 291 190  87 529 533 605 680  56 494  36   5
  22  24   4 317 218  14  26 444 538 436 595  94 510 480 486 399 262 121
 528 116 511 384 217 303 315 197 122 506  34 464 261   7 442 590 301 302
 163 520  37 599  31 220 483 385 118 120 288 119 663 130 105 221 201 535
 104 371  89 405 597 449   9 443 161 200 125 414 544 468 397  16 210 369
 594 410 386 408 591  92 596 579 588 372  90 107  17 441 609  11 368 211
 228   6 666  95 123 373 662 187 484 227 481 416 225 482 462 589 598 608
 543  13 398 184 542 415 411 248 109 157 418 199 409 247 249 407 387 189
 393 389 402 661 668 308 664 250 198 478 672 374 541 212 255 403 396 400
 401 669 413 392 254 155 467 307 156 404 660 545 670 306 251 460 461 431
 463 667 465 110 665 658 671]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.2961
INFO voc_eval.py: 171: [ 37   4 111  43 171  38 176 267 325 116 180  46   3 148 220 173 152 183
 145  12 324 177 107  55 149 269 170 327 143 147 211 219 265 225 263  40
 233 223 178  56 224 133  94 238 174  72 266 110 319 117 243  74 268 182
   5  93  54  44 153  91 140 114 164 318 154 139  10  18 323 259 234 222
  53  95 201 221 296 123 109 300  45 132 175 181 165  66 150 227  39 314
 232 258 315 205 136 151 179  89  11  87  69  98  92 146  41 172 120   8
 240 237 121 118 241 277 157 320 236  21  88  96  97 250 226 312 242 317
  67  82  68 309 298 326 246 113 239 283 311 144 196  77 235 115 321 303
  59  42  86 200 251 256 108 134 284  20  57 217 104 209 231 249 252  78
  73 192  76 330   7 301 308 189 184  22 103 272 106 208 228   9 213 329
 185 247 119 294 310 297  70  27 112 199 229 253 141  23 264 245 322 257
  71  81 135 190  75 158  48 122  50 313 215 105  24 248 328 218 270 288
 137  58 306   6  25 299 191 304 316 289 261 142  19 138 287 212  34 187
 305 207 166 163  47 131 286 206 202 293 254 101 195 260 102 216  17 162
 214  52  51 279 244  80 285 210 197  26   1  79   0 125  49 194 161 280
 255   2 186 291  15 278 129 156 100 290 160 295 198 282 159 292 130  99
 188 127 168  30  16 273 230 271 193 262 126 274  14 167 128  31  29 124
 281  13 203 155 169  35  65 307  36  63 275  28 302  64 276  61  33  60
  32  84  62  90  85 204  83]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1542
INFO voc_eval.py: 171: [621 370 628 119 369 633  86 309 205  82 283  81 124 392 168 874 625 626
 282 308 122  89 385 181 209 781 127 498 376 289  88 778 529 211 442 204
  90 445 371 619 875 191 654 101 202 441 623 389 310 123 890 314 120 883
 190 176 293 285 852 809 322 373 288 375  93 311 503 456 284 637 290 537
 690 784 446  95 286 133 886 164 212  67 729 213 391 200 516 819 480 422
 518 135 845  20 815 888 884 319 532 622 103 713 106 117 448 374 865 449
 170  45 377 660 430 791 297 691 894 823 645 796 271 174 111 783 171 136
 447 443 779 501 425 183 880 811 738 337 426 226 197 818 522  75 436 889
 169 814 216 255  99 860  37 855 300 530 732  22 502 292 384 604 714 185
 517 166  79 824 165 531 761  46 891 609 178 699 804 545 856 241 636 453
 294 444 129  71 163 259 799 215 317 295 184 719 862 239  70 806 194 764
 327 173 479  62   3 695 721 224 245 107 683 440 372 291 589 821 203 361
 452 692 182  57  19 864 567 808 312 381 702 876 694 659 214 536 490 893
 476 539 180 670 451 871 879 499 431  42 907 206 157 195 665 848  21 108
  97 301 254 257 519 577 693 686 715 179 420 797 324 846 737 853 807 177
 365 624  36 395 592 854  98 457 701 115 899 340 231  58 307 505 419 387
  55 743   6 478  40 304 813 794 229 132 175 906 102 829 887  77 235  78
 742 816 232 792 753 363 544 611 658 379 386 455 869 841 256 315 343 551
 802 234 486 597 620 837 390 892  47 720 885 602 800 208 433 459 509 247
 302 199 172  85 423 672   7 812 159  69 330 500 126 153 268 642 866  44
 264  60 167 515 803 905 872 556 467 388 521 475 717 275 540 458 857 786
 367 646 225 161 877 527 258 711 788 826 870 243 834 454 898  87 100 666
 488 248 186 850 249  80 246 668 667 863 523 740 207  92 450 299 342 144
 427 112 474 758 306 142 728 341 835 595  48 881 210 105 412 534 563 223
 663 237 552 739 510  94 867 125 139 698  39 535 192 710 344 559 669 196
 273 303 805 313 565 677 682 428 749 576 296 238 617 482 873 600  15 421
 878   8 507 851 366 393 847 828 730  41 741 320 770 832 590 362  59 432
 593  84 253 607 298 801 910  43 858 543 462 520   0 825 756 716 318 688
 882 116 588 137 608 265  91 613 594 689 424 222 513 768 904 631 790 599
   9 776  49 687 639 188   1  38   4 230 697 187 336 909 418 402 380 150
 410 908 104 489 382 574  34 236 233 731  61 364 671 109 868 274 751 485
 262 538 696  56 198 573 346 546 769 703 118 560 722 569 368 494 765 591
 468 707 657 656 664 506 437 566 162 762 568  25 859 754 276 201 383  35
 354 661 378 662 752 411 328 272 251 901 615 849 549 347 266 596 606 131
 598 676 772 773 333 160 787  65 267 189 836 580 113 405  33 193  63 439
 726  23 861  64 655 240 733 718 817 680 766 263 353 305 277  66 685 332
 684 244 151 767 771 278 287 795 409 438  83 491 260 603 706 564   2 725
  76 279 822 504 704 470 110 154 413  26 352 793 524 618 394 335 114 681
  96 481 541 477 760 798 356  50  72 612 227  74 435 558 727 774 548 316
  18 511 434 736 575 838 723 562 557 461 789 323 465 407 242 464 429 528
 561 895 572 508 547 777  68 250 228 712 785 469 334 495 579  11 630  73
 138 842  17 533 748 759 734 724 148 329 483 903 158 261 735 463 460 601
 584 355 648  54  14 350  10 345 709 408 705 360 678 746 525 610 398 400
  13 155   5 679 782 496 218 651 780 399 484 473 820  24 581 578 492 763
 896 149 487 512 844 359  51 755  12 280 466 643 810 147 493 416 583 526
 708  16 775 571 700 471 587 417 252 757 827 750 134 152 270 582 281 747
 514 542 357 414 415 831 605 358 833 900 550 397 570 585 348 649 472  53
 156  32 349  52 830 128 745 351 339 401 627 629 403 675 554 140 331 586
 653 553 321 616  31 840 406 146 902 744 143 640 396 614 497 404 641 325
 843 220 638 269 121  27 673 217 326  30 644 338 221 634 219 839 130 145
 650 674 141 897 555 635 632  29  28 652 647]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2937
INFO voc_eval.py: 171: [445 263 625 310 188 458 150 495 466  26   2  39 343 266 502 521 335 264
 371 338 505 420 260  40 501 462 453 138 603 369 496 368 578 140 323  64
 447 311 372 331   8 265 558  78 639  82  29 120 454 472 541 612 239  19
 463 344 192 113 191 636 361  46 336 316 656   4 345 142 238 626 559 168
 163 648 392 271 388 642 376 151 398 321 220 100 511 465 268 446  65 217
 123 232  21 174 396  57 274 468 156 234 583  84  94 236 195 556  80 497
 460 419 286 241 379 226 115  42 198 527 417 189 337 488 645 490 314 401
 255 225 399 500 660 245 377 176 510  27 451 132 383 389  18 506  49 175
 385 190 313   7 633 650 339  30 319  47 528 143 326 219 301 587 499 429
  11 566 507 604 223 622 498 269 597 629 222  43  81 121 493 155  53 503
  61 317  60 114 661 224 370 471 627 524 318  12 139 536 328 325   3 459
 333 172 422 540 395 186  41 605 249 250  59 609 608 320 470 202 348 652
 145  45 312 262 187 227 284 128 387 508 148 237 275  31 157 649 404 133
 581  14 137 130 103 418 504 509 374 332  85 101 384 293 119 576 582 315
 428 306 391 173 295 537 519  25 197 657 330  48 628 575 443 354 590 340
 380 641 423 166 563 297  52  33 562 334 643 300  15 572 529 421 439 640
 655 589 352 588 124 409 557 647 560 292 193 464 106 112 107 523 322 230
 613 644 324  20 651 158 550 248 104 520 437 658   9 425 357  98 218 538
 185 600 110 159  92 545 167 209 525 289 351  72 615 653  91 228 596 637
 288 375 522 513 294 290 221 564 554  95 229 565 287 555 548 329 654  62
  88 102 378 594 360 231 662 476 577  71 449 646 526 568 291 444 390 127
 298  69 477 479 539  44 571 533 276 233 586 141 327  63 595 125 386 105
 273 216 403 405 309  38  34 169 448  24  86 129 567 347  67 199 394 285
 240 302 279 161  89 349 543  87 373 623 296 659  70 171 281 617 146  56
 131 136  37 214 475 455 280 213  36 402 180 413 267 154  68 480 283 433
 381 619 397 400 461 664 194   5 299 253 393 620 452  74 382 487 638 621
 469  10   6  17 635 170 456 473 546 179 663  28  79 147 135  97 602 162
 244 270 118 483 474 561 212  58 303  32  23 634 122 491 467 580 616 489
 342  13 346  90 117  35 165  66 164 252 570 160 573 614 152 305 242 585
 569 203 126 512 592 208 450 149 584 552 599 601  76 341  77 531 251 549
 435 416  96 116  93 261 457 408 431 492 153  54 108 247 411 544 246 350
 363 436 579 144 308 134  55 196 618 426 427 109 235 307 211  51 430 272
 183 542 591 611 438 181 184 243 414 210 598 178 412 201 359 177 547 410
 534 424 356 355 358  83 254  73 200 362 353 282 553  99 574  75 434  22
 111 482 593 432 182 530   0 551 257   1 364 407 304 415 259 535 406 258
 610 532 516 365 478 481 367 366 630 205 631 632 278 486 515  50 484 607
 517 440 485 206 215 256 277 624 441 514  16 606 204 518 442 207 494]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4037
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3133
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.254
INFO cross_voc_dataset_evaluator.py: 134: 0.418
INFO cross_voc_dataset_evaluator.py: 134: 0.190
INFO cross_voc_dataset_evaluator.py: 134: 0.222
INFO cross_voc_dataset_evaluator.py: 134: 0.325
INFO cross_voc_dataset_evaluator.py: 134: 0.427
INFO cross_voc_dataset_evaluator.py: 134: 0.298
INFO cross_voc_dataset_evaluator.py: 134: 0.096
INFO cross_voc_dataset_evaluator.py: 134: 0.317
INFO cross_voc_dataset_evaluator.py: 134: 0.185
INFO cross_voc_dataset_evaluator.py: 134: 0.341
INFO cross_voc_dataset_evaluator.py: 134: 0.227
INFO cross_voc_dataset_evaluator.py: 134: 0.317
INFO cross_voc_dataset_evaluator.py: 134: 0.534
INFO cross_voc_dataset_evaluator.py: 134: 0.541
INFO cross_voc_dataset_evaluator.py: 134: 0.425
INFO cross_voc_dataset_evaluator.py: 134: 0.296
INFO cross_voc_dataset_evaluator.py: 134: 0.154
INFO cross_voc_dataset_evaluator.py: 134: 0.294
INFO cross_voc_dataset_evaluator.py: 134: 0.404
INFO cross_voc_dataset_evaluator.py: 135: 0.313
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.356s + 0.026s (eta: 0:00:47)
person 0.9693857
person 0.9631711
person 0.9921365
person 0.96014804
person 0.972867
person 0.92539513
person 0.93598264
person 0.9817992
person 0.94395965
person 0.9103147
person 0.9814548
person 0.93854004
bottle 0.92293143
bottle 0.9199756
bird 0.95181054
person 0.9823904
person 0.91751456
person 0.94354326
person 0.90490824
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.321s + 0.034s (eta: 0:00:40)
person 0.94391644
person 0.91708416
person 0.9589876
person 0.94550806
person 0.9805611
person 0.94683075
person 0.9681267
person 0.9815715
person 0.9391384
person 0.9208801
person 0.92929155
person 0.93124497
person 0.90540606
person 0.922107
person 0.9199661
person 0.9260446
person 0.9743518
person 0.9105426
cat 0.9005404
person 0.9802578
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.321s + 0.035s (eta: 0:00:37)
person 0.92860466
person 0.9129257
person 0.954586
chair 0.9133978
person 0.9719814
person 0.91037387
person 0.95388544
person 0.9650688
chair 0.96566117
chair 0.9373757
chair 0.9141209
person 0.9008225
person 0.978945
person 0.98791236
person 0.92982227
person 0.90139925
person 0.9764217
person 0.9915799
person 0.9814667
person 0.9140836
person 0.9066037
person 0.90747046
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.317s + 0.036s (eta: 0:00:33)
bird 0.93737024
person 0.92695373
person 0.92105764
person 0.9937657
person 0.9748043
person 0.95587826
person 0.98024637
person 0.91026884
person 0.95917904
person 0.9655165
person 0.90668523
person 0.9170101
person 0.96977556
person 0.93956107
person 0.95891076
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.323s + 0.036s (eta: 0:00:30)
diningtable 0.9496068
chair 0.96319246
chair 0.965698
chair 0.9378455
chair 0.95660925
pottedplant 0.90171057
pottedplant 0.9183443
pottedplant 0.9150277
pottedplant 0.9047274
pottedplant 0.9105442
pottedplant 0.9436208
pottedplant 0.93778974
pottedplant 0.93057907
person 0.9655582
person 0.9826605
person 0.92179245
person 0.9794519
person 0.9887201
person 0.9138802
person 0.902511
person 0.9127469
person 0.9187423
person 0.91969013
person 0.9721686
person 0.9537768
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.322s + 0.037s (eta: 0:00:26)
person 0.9553049
person 0.92174655
person 0.90263474
person 0.92041725
person 0.9445909
person 0.9112249
person 0.95285225
person 0.9357144
person 0.92738324
bird 0.9224849
person 0.9337911
person 0.9875729
person 0.91685337
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.320s + 0.036s (eta: 0:00:22)
pottedplant 0.90422094
person 0.9617477
person 0.9177593
person 0.9084753
person 0.95373213
person 0.9030592
person 0.9584137
diningtable 0.90648663
chair 0.9826681
diningtable 0.9597216
chair 0.91708845
chair 0.9476963
chair 0.9804513
chair 0.9284539
bird 0.99309
person 0.9010378
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.318s + 0.036s (eta: 0:00:19)
car 0.9282714
car 0.94121504
car 0.912122
person 0.98852193
person 0.97397137
person 0.92924625
horse 0.97454983
horse 0.9138605
horse 0.9202679
person 0.93767184
person 0.95318604
person 0.968222
person 0.9174164
person 0.92457765
car 0.9456768
car 0.9647849
bicycle 0.9619317
bicycle 0.9299609
person 0.9379447
person 0.915357
person 0.9727495
chair 0.9171741
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.318s + 0.036s (eta: 0:00:15)
person 0.9742066
person 0.9584617
person 0.97305286
person 0.91376716
person 0.9112487
person 0.98850805
person 0.9862942
person 0.94275475
person 0.91701835
person 0.9642514
person 0.9636447
person 0.99107856
person 0.9050775
person 0.966765
person 0.9105462
person 0.9875359
person 0.97991586
pottedplant 0.9132263
bus 0.91311187
person 0.98995227
person 0.96191704
person 0.94063497
person 0.9533402
chair 0.9451457
person 0.90545857
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.318s + 0.037s (eta: 0:00:12)
motorbike 0.9647436
person 0.9745302
person 0.9113699
person 0.9137889
person 0.95548034
person 0.9658144
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.317s + 0.036s (eta: 0:00:08)
person 0.9015046
boat 0.98182964
boat 0.9477155
boat 0.9177344
person 0.9902154
person 0.9369618
person 0.9855672
person 0.9178582
person 0.92312485
person 0.98881733
person 0.97496086
person 0.9093143
person 0.9739576
person 0.9763104
person 0.91148007
person 0.9484582
car 0.9503312
person 0.9369906
person 0.9464787
person 0.9866223
person 0.959995
person 0.90338975
person 0.918723
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.321s + 0.036s (eta: 0:00:04)
diningtable 0.9584662
chair 0.9230526
chair 0.9738458
chair 0.98077756
chair 0.9126787
chair 0.98068273
chair 0.9353699
chair 0.96061635
person 0.94080025
bird 0.9234038
bird 0.91164786
boat 0.9199489
person 0.9579149
person 0.92489165
person 0.98853534
person 0.9124753
person 0.9101785
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.318s + 0.036s (eta: 0:00:01)
person 0.98329365
person 0.9643956
person 0.95581436
person 0.95533895
person 0.9481485
person 0.96503896
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.385s + 0.026s (eta: 0:00:50)
person 0.98159295
person 0.916729
person 0.9785805
person 0.97699
person 0.91269034
bicycle 0.9725893
person 0.9310687
person 0.911631
bicycle 0.974524
person 0.93339974
bird 0.9432031
pottedplant 0.9441696
person 0.97019356
person 0.92799693
person 0.90208334
person 0.95992666
person 0.965093
person 0.98658466
person 0.92756236
person 0.9657954
person 0.952213
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.310s + 0.033s (eta: 0:00:39)
person 0.91218644
person 0.90675586
person 0.9637554
person 0.9169682
person 0.97270465
person 0.9397933
person 0.9348901
person 0.9805366
person 0.96443176
chair 0.9263585
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.313s + 0.034s (eta: 0:00:36)
car 0.91560787
car 0.94914734
person 0.9476548
person 0.96062624
person 0.981187
person 0.9204871
person 0.94754744
person 0.963878
car 0.98206776
cow 0.9163387
person 0.9159192
person 0.956622
person 0.9218378
person 0.9106653
person 0.9801586
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.320s + 0.036s (eta: 0:00:33)
person 0.97768515
chair 0.9326958
person 0.9286902
person 0.9266455
person 0.9507987
person 0.9252659
person 0.9309228
person 0.9180033
person 0.9391136
person 0.9805604
person 0.9867627
person 0.9769529
person 0.968696
person 0.97657293
person 0.9001411
bird 0.990756
person 0.9212133
bird 0.9515526
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.321s + 0.036s (eta: 0:00:29)
person 0.9441792
person 0.9702949
person 0.9823949
person 0.922754
person 0.9459983
car 0.9503312
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.319s + 0.036s (eta: 0:00:26)
bird 0.9114539
bird 0.96043307
bird 0.91361624
person 0.9104198
person 0.94275206
person 0.9137699
person 0.9145447
person 0.9880592
diningtable 0.9477981
person 0.97185105
diningtable 0.97809315
person 0.920404
person 0.9339788
chair 0.94213164
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.329s + 0.036s (eta: 0:00:23)
person 0.91102695
person 0.9019846
person 0.9242538
person 0.94543046
person 0.98551524
bus 0.94758916
dog 0.9517692
diningtable 0.97328556
diningtable 0.9195132
chair 0.9107265
chair 0.9848401
chair 0.91261715
chair 0.9701624
chair 0.9351663
pottedplant 0.903108
person 0.90572894
person 0.97536
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.327s + 0.035s (eta: 0:00:19)
diningtable 0.91971135
chair 0.9087888
person 0.9467195
person 0.9659851
person 0.95364344
person 0.9627268
person 0.9442576
bird 0.95544463
person 0.92517656
person 0.9602198
person 0.9633394
person 0.9302152
person 0.98030305
person 0.9064768
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.326s + 0.035s (eta: 0:00:15)
person 0.96234727
person 0.96146345
person 0.90272963
train 0.9394751
person 0.9568645
person 0.96247756
person 0.9470866
person 0.9372077
person 0.9098438
person 0.9330846
person 0.928468
person 0.9043344
person 0.9555132
person 0.9303908
person 0.99228597
person 0.9741554
person 0.96146435
person 0.91776764
person 0.9273253
person 0.9265169
tvmonitor 0.91693324
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.328s + 0.035s (eta: 0:00:12)
person 0.9630964
person 0.90419346
person 0.94353795
horse 0.95984936
person 0.9662574
person 0.92876273
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.328s + 0.035s (eta: 0:00:08)
person 0.9749586
person 0.9656285
person 0.92190224
person 0.91944593
person 0.94958466
person 0.9246591
person 0.9351389
person 0.90144145
person 0.91257375
person 0.9864948
person 0.9435812
chair 0.97177255
person 0.95602685
person 0.9020708
person 0.9455883
person 0.9580029
person 0.9048351
person 0.9158867
car 0.945514
car 0.9165836
car 0.950864
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.324s + 0.035s (eta: 0:00:05)
person 0.96811587
person 0.94295734
person 0.9617579
person 0.9484586
person 0.9729996
person 0.95972633
person 0.9191523
person 0.9021251
person 0.9249023
person 0.98807704
person 0.98161054
person 0.9186736
person 0.9779418
person 0.92703664
person 0.9367623
diningtable 0.93183076
person 0.9160969
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.318s + 0.034s (eta: 0:00:01)
person 0.9067951
diningtable 0.91455525
person 0.9698009
person 0.93878007
person 0.987375
person 0.9663302
person 0.9583284
person 0.9571261
person 0.90440726
person 0.9053995
person 0.94097227
person 0.96367383
person 0.98130566
person 0.9073887
person 0.94839907
person 0.95668596
person 0.9554494
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.456s + 0.028s (eta: 0:01:00)
diningtable 0.90004796
diningtable 0.92489755
person 0.96033347
person 0.97571725
diningtable 0.9721881
person 0.9216156
person 0.91950375
person 0.90362924
bottle 0.90654266
person 0.97746426
pottedplant 0.91795814
diningtable 0.9241577
person 0.9655559
diningtable 0.90472054
chair 0.94982636
chair 0.9656504
chair 0.92131203
person 0.914267
person 0.9046191
person 0.93041515
person 0.9957659
person 0.9807443
person 0.9324464
person 0.9663608
person 0.96456105
person 0.90394473
person 0.97035235
person 0.95876753
person 0.93654644
person 0.93615264
person 0.9646747
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.328s + 0.032s (eta: 0:00:41)
person 0.94531965
person 0.98589665
person 0.93258816
person 0.9702325
bottle 0.9375143
person 0.93519235
person 0.9410961
person 0.95223993
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.320s + 0.032s (eta: 0:00:36)
chair 0.9337001
bottle 0.9093441
person 0.971837
person 0.92487234
person 0.937836
person 0.9298695
person 0.95113033
person 0.92899793
person 0.96691304
person 0.9418706
bottle 0.90680754
bottle 0.95272267
person 0.98209596
person 0.9536006
person 0.9488665
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.321s + 0.033s (eta: 0:00:33)
person 0.96552426
person 0.91986483
bird 0.91931343
person 0.953628
person 0.90420735
person 0.9664351
person 0.9698253
person 0.95884657
person 0.90156674
person 0.93445134
person 0.9046388
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.320s + 0.034s (eta: 0:00:29)
person 0.9692441
person 0.96930236
person 0.95401305
person 0.9038638
person 0.9703625
person 0.91986686
person 0.94099617
person 0.9131551
person 0.9171056
person 0.93777734
person 0.9696156
person 0.95325667
person 0.927407
bottle 0.95366174
chair 0.9316604
boat 0.9049825
person 0.90610075
person 0.9732627
person 0.954295
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.316s + 0.034s (eta: 0:00:25)
car 0.9805658
car 0.9239249
person 0.92488366
person 0.9768411
person 0.9095444
person 0.90698665
person 0.91885525
person 0.9337307
person 0.9645394
person 0.92240983
person 0.91467005
person 0.9260509
pottedplant 0.91709936
pottedplant 0.9324963
car 0.98470485
car 0.9384878
person 0.93099135
person 0.9877958
person 0.9531749
person 0.91701007
person 0.9369496
person 0.98581564
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.318s + 0.035s (eta: 0:00:22)
person 0.9796794
car 0.95881665
person 0.9497614
person 0.9238179
tvmonitor 0.9259352
person 0.9033359
person 0.93016785
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.318s + 0.035s (eta: 0:00:19)
chair 0.9029096
person 0.970448
person 0.9431539
train 0.91679364
train 0.9604032
bird 0.91030955
bird 0.9826749
person 0.92094225
person 0.9223869
bicycle 0.92329043
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.314s + 0.035s (eta: 0:00:15)
person 0.9688987
person 0.90143216
person 0.93663615
person 0.9293159
person 0.92909753
person 0.98650324
person 0.95791346
person 0.97278106
person 0.98938423
person 0.97071403
person 0.90813804
person 0.9545464
person 0.9655042
car 0.95052713
person 0.92353714
person 0.96876925
person 0.98047286
person 0.9309811
person 0.9144073
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.315s + 0.035s (eta: 0:00:11)
person 0.9139266
person 0.9448533
person 0.9226051
chair 0.9752821
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.314s + 0.035s (eta: 0:00:08)
person 0.98109335
person 0.93838096
person 0.92559445
person 0.9624259
person 0.96194094
person 0.90163547
person 0.92559946
person 0.9067593
person 0.93752146
person 0.97868
person 0.9036039
person 0.90643126
boat 0.90289044
person 0.96619153
person 0.9216535
person 0.94527715
person 0.9096178
person 0.95802844
person 0.9015514
person 0.92395586
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.316s + 0.035s (eta: 0:00:04)
person 0.9671112
person 0.9730225
person 0.9778657
person 0.96116346
person 0.9205005
person 0.930146
person 0.92397416
person 0.949978
person 0.9600075
person 0.91653883
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.316s + 0.035s (eta: 0:00:01)
person 0.93523884
person 0.97127604
person 0.9192549
bird 0.9057005
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.432s + 0.028s (eta: 0:00:56)
person 0.9403683
chair 0.9183412
chair 0.95742023
person 0.9581421
person 0.926363
person 0.9534077
person 0.9390276
diningtable 0.93070245
person 0.97376007
person 0.97064364
diningtable 0.93418
person 0.9516653
person 0.96774274
person 0.90423995
diningtable 0.9115254
person 0.9269226
person 0.96502376
diningtable 0.96456015
diningtable 0.9262148
diningtable 0.93869853
person 0.9514455
chair 0.9135494
chair 0.9810426
person 0.92904085
person 0.9890666
person 0.9219438
person 0.9619518
person 0.9851998
person 0.9168034
person 0.92329055
person 0.98354566
person 0.9679524
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.330s + 0.035s (eta: 0:00:41)
person 0.9383664
person 0.95259136
pottedplant 0.93368834
person 0.9021593
bird 0.9493691
bird 0.9719263
bird 0.9188199
person 0.96728146
person 0.9205767
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.330s + 0.034s (eta: 0:00:37)
person 0.9499947
person 0.90797406
person 0.9618485
person 0.96917814
person 0.9757533
person 0.90025777
person 0.94793653
bottle 0.9043378
bottle 0.9515065
bottle 0.90047723
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.328s + 0.036s (eta: 0:00:34)
person 0.9062654
person 0.91274375
diningtable 0.9089331
pottedplant 0.932133
chair 0.9391487
chair 0.9160214
person 0.9033565
person 0.9760947
person 0.9391131
bottle 0.91086763
bottle 0.9075547
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.323s + 0.036s (eta: 0:00:30)
person 0.96211714
person 0.9600498
person 0.9480768
person 0.99480754
person 0.9419047
person 0.94073486
person 0.9197217
person 0.9792788
person 0.90974265
person 0.95730245
person 0.97237253
person 0.9141062
person 0.93539935
person 0.9366407
person 0.99204236
person 0.9403125
person 0.9730228
person 0.90563685
person 0.9616097
person 0.9836691
person 0.9651019
person 0.92654717
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.320s + 0.035s (eta: 0:00:26)
person 0.96375704
person 0.9711493
person 0.9013384
diningtable 0.9833614
chair 0.9154687
chair 0.9475021
chair 0.97853297
chair 0.9872953
aeroplane 0.91334254
person 0.96638304
person 0.98112226
person 0.9300625
person 0.9137244
person 0.9106262
person 0.94658643
person 0.9735512
car 0.93005276
car 0.98007053
car 0.971711
car 0.95818347
bird 0.95792866
chair 0.9301138
chair 0.92956144
person 0.9437602
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.323s + 0.035s (eta: 0:00:22)
person 0.9835532
person 0.9349228
person 0.904335
person 0.9815793
person 0.9577693
person 0.98961496
person 0.9358775
person 0.9244506
person 0.9901877
person 0.9481939
person 0.9379123
person 0.9058434
horse 0.9622462
person 0.94362617
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.319s + 0.034s (eta: 0:00:19)
person 0.9533562
person 0.94413847
chair 0.9752917
chair 0.9204916
chair 0.9580015
pottedplant 0.92184347
pottedplant 0.94517046
pottedplant 0.9000328
person 0.94148016
person 0.92073333
person 0.93536806
person 0.9583971
person 0.9389799
person 0.9318169
person 0.9003844
person 0.90060717
person 0.9543331
bicycle 0.9161519
person 0.9315759
person 0.9795791
person 0.9003903
person 0.93981415
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.316s + 0.034s (eta: 0:00:15)
person 0.9881534
person 0.918484
person 0.90141976
person 0.9148443
person 0.95857096
person 0.98490983
person 0.9404576
person 0.9767603
person 0.9775925
person 0.97162277
person 0.9375942
person 0.9190419
person 0.91261566
bus 0.9544744
person 0.90495396
person 0.9464087
person 0.96404994
person 0.94779164
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.316s + 0.035s (eta: 0:00:11)
person 0.9127728
person 0.96833503
person 0.91081065
person 0.93349665
chair 0.91307795
chair 0.9486477
tvmonitor 0.9020484
chair 0.97077054
chair 0.9338299
chair 0.94091374
person 0.94656837
person 0.96900314
person 0.9559081
person 0.97709936
person 0.92077994
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.314s + 0.035s (eta: 0:00:08)
motorbike 0.9156647
motorbike 0.9520532
person 0.97858083
horse 0.9579521
horse 0.9029148
person 0.90116704
bicycle 0.9582342
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.313s + 0.035s (eta: 0:00:04)
person 0.9234386
person 0.9587116
person 0.9808414
person 0.9694784
person 0.98376113
person 0.9475452
person 0.95430434
person 0.9411282
chair 0.960387
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.312s + 0.035s (eta: 0:00:01)
person 0.9035127
person 0.98617923
person 0.98563355
person 0.9039352
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 85.722s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1204  288  284 ... 1026 1029 1028]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2516
INFO voc_eval.py: 171: [122 123  68 517  67 243 458 271 293 126 134 124 487 570 387  70 546 397
 573 494 572 462  71 567 515 385 577 576  69 557  83 483 130 274 132 533
 584  87 463 568 534 131  72 590 258  94  77 382 128 389 302 522  75 383
 467  89 589 525 135 559 125 548  90 294 474 304  73 566 244 475 249 140
 468 473 552 466 442 518 100 574 547 591 549 478 137 141 381  18 148 578
  41 303 523 488 545 472 295 241 393 248 247 413 585 285 346 245 524 388
 554 428 300  78 251 102 581 464 348  21 538 416 509 469 275 386  20 286
 146 550 433 310 307 104 439 143 161 521 276 273 136 269  39 308 465 353
 405 127 582 520 391 481 562 162 587 543 256 551 341 197 422 218 206 199
 443  86 529 586 147  35 412 575 477 242 471  24  27 284 569 588  32 528
 331 561 579 347 407 334   2 440 495 145 133 259 311 418 257 142 423  76
 319 426  17 298 580 309  40 555 571 277  42 395  31 380 325 583 301 323
 263 436 339 537 541  53  81 449 278 558 250 394 144 253 539 493 289 255
 193   5 201  15 138 246  91 411 313 434 103 409  95 178 129 454 560 563
 396 420 196 101  88 498  85 281 187 530 510  97  74 270 403 322  28 514
 435   1 542  44  23 343  19 479 207 455 265 297 446 179 266 139 516 408
 486  93 531 427  65 272 460 384  47 198 200  79  36 283 342 404 176 402
 372   0 160 222 306 519 282 424 321 165 335 190 452 287 447  98  63 429
 191 314 194 431 430 188 441  33 511 564  58  84 337 183 425 540 120  59
 491 417 457  30  25 445 115 189 327 180  26 154  52   7  29  16 202 288
 492   3 338 459  45 299 532 195 296 415  11  80 223 236 399 317 349 254
 484 315 185 324 448 476 192 291 114 444 360 507 421 164  14 482 352 166
 305 318 544 398 414  99 268 235  64 453 186 312 556 110 496 410 432 169
 526 359 456 252  61 512  22 227 470 173 340 450  12 553 155 361 401 535
 366 209 400 211 451 508 379 171 170  48   6 168  34  10 108 320 210 406
 354 121  54  92 362 205 485 368 392 204 378 163 215 167 234 290  50   4
  57 419 373  13 262 214 208 513 536 260 213 497 527  55 112 355 184 107
 489 344  60 158 229 152 153 238 356 390 350 203 377 326 371 151 461 177
 182 316  66  49   8 370  43  82 159 219 105 239 216 217 175 181  56 369
 480 351 438 490 367   9 174 149 150 156 358  51 172 106 437 333 109 357
 157 345 226 111 220 363 267 328 292  46 336 329 264 212 116 113 261  62
 117 500 330 374  96 365 221 225 224 240 506 232 233 501 375 231 376 228
 502 503 332 119 592 499 237  37 279 504 118 230 505 280  38 364 565]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3992
INFO voc_eval.py: 171: [ 644 1697 3211 ... 3460 3209 3203]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1975
INFO voc_eval.py: 171: [ 639  673  749 ... 2368  125 2024]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2344
INFO voc_eval.py: 171: [415 379 588 320  12  20 665 371 662 292 378 587 581 416 668 640 322 606
 343 558  21 387 422 615 298 683 151 494 671 841 561 334 602 842 768 701
 324 331 582 325 559 629 419 670 297 293 585 679 340  25  17 667 752 372
 641 663 339 738  22 685 678 681 857  19 380 299 716 626 338 381 583 527
 565 390 418 689 321  15 345 844 336 308  75 625 506 386 484 664 740 275
 420 560 295 326 645 846 513 850 226 692 264 302 599 751 672 385 603 342
  13 669 643 382 323 761  74 613 183 211 314 312 705 688 389 497 347  47
 853 616 694 313 745 624 421 594 690 507 564 486 329 125 589 503 632 374
 185 774 814 727 142 600 586 261 704 205 699 828 435 214 337  14 709  27
 715 186 530 707 515 569 538 714 485 843 220 726 270 584 684 677 628 411
 222  18 254 598 686  46 607 121 698 849 855 225 156 417 753 307 213 713
 441 157 344 300 266 175 778 711 813 811  76  16 630  55 696 592 787 177
 181  86 167 617  45 509 330 383 736 697 627 276 566 644 328 706 712 414
 622 590 562 449 341 482  24 268 311 207 160 870 674 271 757 401 675 335
 137 245 614 601 152 647 812 610 730 820 333 568 221 433  31 815 439 127
 431 327 461 760 109 604 535 563 702 511 682 621 274  48 596  34 375 827
 631 384  85 332 666 210 597 809 111 529 388 294 255 687  72 543 691 251
 178 859 460 303 242 765 728 316 191  87 537 306 810 124 731  67 512 228
 693 595 373 187  49 567 495 605 703 854 676  26 346 775 660 143 304  23
 534 135  64 758 141 239 648 710 105 487 680 104 542 700 505 296 189 593
 123 488 122 272 784 413 404 227 164 474 162 591 277 247 504 608  84 246
 737 856 773 858 623 611 783 140 204 708  60  28  93 126 619 412 839 215
 233 103 756  37 265 273 208 721 609 260 789 871 845 309 120 238 206 695
 722 717 612 159 528 633 797   5 286 499 540  59 620 223 840 496 508 805
 317 259 759 279 618 539 182 788 114 514 501 821 739 785 428 168 348  43
 510 434 224 741  65 500 729 193 402 673 456  44 410 742 359 755 782 873
 107 139  57 244 269   1 432 147 190 248 376 463 310 462 209 108 301 305
 369 119 128 176 129 174 649 202 718 766   6 744 106 763 158  54 194  99
 824 804 536 781 249 110  50  92 769 377 154  81 776 826 475  36 184 130
 573 192 258 851 161 430 541 243 531 848 234 518 403 280 253 429 642 786
 250 483  51 872 770 267 153 793 771 762 155 212 350 556 772 829 436 230
 550 440 262  70 822 764 746 502 201 282 138 864  97 863  29   2 479 533
 816 743 480 284 767 179 173 476 481 263 792 165 136 865 166  96 112 444
  53 256 445  32 747 777 867 357 806 170  83 754 100  30 232 457 636 437
 438 229 257 252 552 283 572 477 532 661  52 464 172 459 113 732 188 790
 866 733 861 169 236  58 171  82 862  78  39 650 489 132 451  91 825 847
 396 808 551 491  95 646 315 553  79  68 368 498 555 734 791 285   0   4
 102 473 180 281 117 519  98 554   8 470 493 163 150 571 199 869 634 580
 544 289 392 735 364 101 360 779 278   3 557 657  63 218  62 577 576 720
 443  41 492 149 490 398 635 455 819 393 520 817 458 469 349 361 574 823
 115 749 818 579 363 548 217 362 795 450 148 547  40  88 478 860 719 575
 578 146 526 406 807 231 118 549 237 453 442 472 799  35  69 545 454 798
  42  56 240 546  33 868 427 852  77 291 407 653 654 405 516 639  80 395
 288 287 116 394 133 365 370 517 235 367 652 290 366 318 452  38   9 638
 637 408 750  66 796 723 319 216  94 356 200 724 397 802 522 837 836  11
  90 725 780 748 198 351 570 658 399 471 354  10 241  71 355 391 409  89
 358 131  61 521 467 144   7 655 468 425 834 659 656 523 835 352 838 525
 203 353 794 833 448 465 801 800 830  73 803 524 400 219 197 424 831 651
 195 446 423 832 145 426 134 447 196 466]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3300
INFO voc_eval.py: 171: [442 176  94 177 100  95 443  92  79 104 102 101 463  99  77  96 446 373
  78 122  28 184 408 213 109 106  76  81 410  82 415  44 399 420  88 257
 169 394 270  98  84 372 146 278  34 387 403 450 447  45 366 300 374 400
  52  91  60 202 110 368 192 171 163 269  86 221  89 487 433 341 119  80
 145 233 228 121  43 439  87 245 296 398 326 337 112  12 210 260 316 357
  90  47 206 267 118 279  83 120 338 203 216 289 342  85 467 157 201 126
 253 200 360 346 343 397 379 142 275  10  32 402 386 215 334 259 181 271
 369 406  56 255 422 383 356 425 150 385 191 252 170 340 323 217  27 419
 291 107 413 441 161 182 124 416 155 234 430 395 438 333 198  48 328 325
 123 465 384 364 264 288 362 266 367 319 250 236 273 290 382 143 295 231
 258 204 396 133 358 174  29  33 283 125 156 437 421 227 207 390 361  18
 427 131 335   5 229 431 303 262 129 152 336 137 365  37 426 302 223  97
 377  40 144 205 359  51 466 448 381 301 115 211 130 320 256 141 474 486
 168 414 321 128 116 178 209 423 105 280 485 434 363 214 230 172 117 315
 134  35 277 318 389 476   0  57 154  93 435  31 111 285 183 175 405 460
  38 274   6 162 167 103 239 404 232 272 208 237 348 393 401 339 251  46
 327 265 475 407 212 412 428 282 240  26 424 242   4 132 417 478 218 268
 440 160 254 113 147   3  55 409 159 127 246  70 349  42 329 151 244 148
 263 292 314 376  14 219  30 375 324 153 429 458 330 186 235 469 432 281
 284 224 149 392  58   8  71 297 248 164 380  75 391 312  53 370 261 332
 249 286   7 473  11 378 418 445 135 173 188 331 411 481 238 354 322 347
 158 310 114  59 276  39 317   9 241 456 449 444 344 299 108 293 453 470
 287  19 298   1 225 136  41 199 388   2 222 455 471 165 294 179 436 345
 193  36 464 313 166 371 220  54  61 468 247 243  49 187 479  24 472 195
 311  65 226 461 307 451  21 457 484 304 189  68 351 350  64  15 306 308
 185 305 309 353 452 138  23  73 483 480  25 454  62 482  20 197 140 194
 190 180  16  13 196 462  67  50 459  17  74  63 139  69  72 477  22  66
 352 355]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4241
INFO voc_eval.py: 171: [1593  726 1525 ...   87 1502 1720]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2891
INFO voc_eval.py: 171: [ 30 266  67 112 160 318  68  20 316 113 289  25 122 267  69 159 258 317
 205 288 129 158  77  71 268  27 296 234 262 292 114  78 214 204  87 290
  53 137 126 252  49  98  51 243  97  16 121 265 142 238 106 149 170 119
  73 177 175 236 181 195 308 194 297 157  43  21  79 108  28  66 128 235
  95 229  83  74 221 294  18 178 208 120 298 125  17 295 169 203  99 257
 246  29 253 163 117  56  22  23 287 115 197  76 260 240 219  84 207 176
  55 134 171 193 293 244  24 237 259  19  52 156  94 200 206  89  86 161
 291 162  90 218 131 312 284 309 111 315  80  58  57 248 263 310 127 222
 151 123  54 241  26 242 150   5 187 198 226 155 154  48 228 196 269 239
  72 143  92 245 130 165 141 225  88 139 135 132 116 174  91 247 261  50
 145 220 168 272 110 227 286  85 136  45 311 280 319 152 313 138 224  46
  15  70  11 118 199  96 250  47 105 255 212 314  42 299 254  93 249 217
   9 278 109 100 209   3   0 107  40 186  36  44 202 232 282 302   7 124
 148  31 223 104 140 283 103   1 270 133 144 279 185  10  35 264 216 173
 101 213  62   6 285 172 179   8  37   4 102  39 233 273   2  61 211 164
  13  32 301 201 166 184 277 210  60 304 182  75 276 167  33 281 190 300
 274 306 230 307  34 215  59  14 303 153 271  65  81 251  82  63 188 146
 305 180 231  64 183 192  12 256  41  38 275 147 191 189]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0966
INFO voc_eval.py: 171: [5887 2539  712 ... 4409  289 4410]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3127
INFO voc_eval.py: 171: [336 264 201 449  59 207 284 703 391 283  51 260 763 263 272 457 285 259
 418 262 756 455  72 651 227 732 337 415  74 618 200 639 161 452 202 203
 340  66 387 185 755  56 565 141  55 182 206 458 261 209 380 233 131 453
 758 617 454 279 130 450 230 274 448 211  71 463 371 388 266 566 268 760
 173  61 759  70 327 446  34  68 652  65   7 743  22 545 277 447 761 270
 228 567 408 117  73 425 635 464 462 542 199 570 287 569 728 708 621 526
 630  69 210 248 399 451 251 392 647 312 620  76 741 619 214   6 363 729
 155 459 707 267 768 762 461 150 271 348 538 723 560 149  50 524 205  58
 654 637 292 345 432 640 221 326 225 365 204 350 398 636 293 480 208 180
 232 736 642 417 727 735 278 522 424 739  12 276 546 704 249 629 674  42
 354 673 573 733 243 282 386 186 757 738 222 764 115 389 401 426 339 427
 189 269 655 280 351 281 456  52 181 223 623 730 675 568 241 216 416 152
 445 669 765 296  16 290  64 143 653 247 434 428 548 562 213 338 638 190
 178 656  92 471 129 291 563 419 184 575 188 315 128 156 737 352  81 710
 134 422 571 343 226  31  49 726 328 288 362  11 322 501 191  30 265 525
 242 135 370 659 770 641 543 393 133 774 663 132 414 238 628 183 390 430
 147 421 767 724 157 320 747 235 722 286 429 514 423 766 176 108 433 364
 714 220 254  47 240 713 420 751 527 698 740 718 396 250 187 319 564  67
 137 402 385 631 643 752 169 530  78 711 162 658 625 179 465 215 721 686
 431 273  48 374 165 329 725 579 344  23 244 317 395 342 689 515 275 289
 502 561 110  87 746 577 539 665 535 341 113 318  96 785 544 687 104 148
 787 245  32 771 172 218 549 702 400 484 547 163 670   8 731  91 140 706
 512 334 742 368 347 487 297 594 439  17 294 142 504 555 528 634 405 786
 576 690 346  46 109 715 769 212 772 632 256 744 121 627 321 316  88  44
 404 489 357  28 246 586 164 384 513 572 574 298 332 734 505 409 684 671
 541 676 466 783 295  14 660 394 166  15 588 754 111 719 120 460  25  93
 324 224 145 672 488 105 550 193 578 518 649 646 540 231  54  24 776 467
 497 748 701 624 775 750 745 664 495   0 160 333  35 397 749 753 650 615
 376 537 788 217 598 403 174 503 159 695 407  94 470 589 633 661 532 406
 144 106 692  27 325 377 198  37   9 486 606 773 323  33 517 367 612 313
 229 107 696 219 523 112 372  89  80 258 138  60  26 234 531 700 533 114
 712 614  21 667 253  36 136 330 645 139 443 611 777 597 494 697 607 239
 516 335 123 491 127 255  84 648  20 381 116  63   2 688 171 331 175 122
 529 511 237 678 682 119  95 369  79 379  29 600  75 613 236  99 666 603
 595 536  53 677 437  82 683 596 170 694 534  41 553 784 375 705 195  39
 151 252 498  38 499 167 257 587 366  13 716 438 657 442 196 153 699 373
  19 599 360 559 197 192 490 355 307 558 644 556 500 158 478 125 580  83
  86 693 475 124 605  18  10 154  98 496 602 168 608  43 680 468 622  90
 493 510 177 508 604  62 668 118 601 382 609  85 492  97 485 616 679  77
 358  40 435 194 314 103 509 552 662 681  57 685  45 551 383 302 436 554
 410 474 477 717 519 126 303 440 506 311 472 557 412 610 300 309 507 441
   3 709   1 411 359   5 469 483 444 413 626 592   4 476 305 782 520 308
 479 482 378 720 310 691 353 299 481 361 356 593 304 301 473 781 778 521
 306 146 100 581 590 349 101 780 591 779 102 584 583 582 585]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2101
INFO voc_eval.py: 171: [2897 1277 1330 ... 2811 2798 2314]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3283
INFO voc_eval.py: 171: [343 409 346 853 445 502 375 353  48 481 465 449 629 550 141 349  62 503
 406 443 155 782 139 542  45 628 507  49 147 154 278 199  40 514 376 523
 855 150 451 235 849 742 355 805  47 274 123 796 411 407 450 136 806 201
  64  44 462 527 778 779 148 843 477  39 858 519  61 467 344 268 793 822
 434  50 374 487 846 371 811 441 834 217 354  81 626 768 143 145 627  52
 245 738 780 544 234 851 647 223 222 157 534 269 586  67 516  63 464 246
 414 863 551 241 669 484 486 506 548 419 605 415 356 854 829 861 237 259
 149 790 437 281 345 221 140 724 463  53 194 833 676 452 412  43 852 646
 413 121 210 203 471 236 521 474 675 433 336  60 547 436 146 617 783  51
 295 250 667  83 827 828 325 275 613 739 781 662  30 602 189 830 225 666
 709 812 815 127 595 350 801 708 767 766 408 590 218 213 512 734 604 792
 211 430 152 651 257 195 153 439 611 447 156 263 549 192  84 384 777  68
  66 410 227 379 226 751 869 432 470 347 280 431 546 813 417 844 440 125
 202 224 372 142 380 710 639 423 126 515 789 769 677 204 319 808 144 131
  46 776 138 435 191 215 856 122 623 159 600 744 610 158 809 819 636 293
 335 342 554 438 228 746 424 248  55 444 832 818 308 842 260 625 312 112
 648 752 107 817 172 348 272 265 687 707  57 645 420 857 807 814 702 109
 151  59 513 266 480 287 382 270  14 288 873 593 351 704 292 490 872 741
 200 190 543 333 137 124 575 129  86  41 193 784 525 787 634  65 243 700
 442 279 678 619 242 728  90 785 130 576  58 606  24 282 220 831 302  29
 684 230  56 839 650  88 794 418 120 468 271 520  21 283 729 179 731  92
 291 232 673 864 239  18 118 352 478 231 609 277 740 838 276 244 795 791
 867 624 618 572  85 113 745 212 621 599 668 133 428 383 672 574 680 110
 286 868 273 862 301 359  54  13 685 786 229 327 837 247 683 823 587 108
 249 553 835 529 475 788 616 111 614 214  87 620 208 671 569 615 821  99
  12 446 633 426 622 364 556 341 706 300 545  28 655 116 314 448 216  31
 460 219 258 251 313 482 753 714 727 373 736 389 607 674 252 539 429 757
 755 307 705 732   4 479 825 870 558 552 290 528 866 735 686 305 267 555
 573 691 209 264  17 820 100 865 114 733 181 824 416 533 816 845 826 730
 653 381 871  33 334 297 262 427 649 638 847 836 476 326  89 692 233 328
 405 688 571 557 859 261 597 294 693  91 681  82 637 696 664 848 399 182
 850 361 322 518 641 370 183 115 860 577 635   3 509 761 119 358 570 285
 128 810 459 698  19 102 508  25  69 338 682 398 592 874 316 670   9 299
 303 601 310 320 747 589   5   1 177 661 318 169  27 392 504 608  80 598
 540   0 564 679   2 737 642 403 337 360 256 240 532 488 689  42 612  77
 517 255 184 238 526 603 754 562 296 289 253 306 799 772 317 284 803 207
 103 298 498 362 748 331  78 491 697 363 323 494 324 453  37  38 756 535
 368 743 640 315  26 421  34 309 630  75 161  93 174  11 485 456 718 665
  22 541 565 394 168  97  96 644 690 117 762 134 304 800 365 712 186  16
 163 759 340 505 536 455 496 132 425  32 652 160 537 311 591 205 658 198
 454 367 643 466  23 178 522 499 483 422 254 538 559 660 458  72 560 750
 798 176 135  76 715 530 171 400 397 694  94 401 473 366 596  20 797 492
 402 654 719 377 804 493  95 489 101 187 578 582 588  71  73 357 457 583
 170 180 656 469 717 162 585  79 173 472 584 461 531 561 749 404 321 657
  74  70 105 663 511 387 659 175  98 594  15 197 713 206 764 495  36 510
 581 566 701 765 703 695 196 185 188 760  35 524 106 568   6 711 329 388
 802 497 385 631 726 104  10 378 580 773 579 369 763 758 775 632 563 339
 725 720 716   8 332 723 167   7 386 567 330 722 390 721 166 165 164 771
 396 774 770 500 395 501 393 840 391 841 699]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2138
INFO voc_eval.py: 171: [133 950 437 ... 568 765 766]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3090
INFO voc_eval.py: 171: [ 47 280 266 145  26 150  46  48 154 146  49 288 119  72  34 162  68  50
  31  78 148 275  58 191 176  57 171 254 117 274  74  33 267  54  19 200
 209 194 133  36 276 277 245 257 290  25  32 164 248 258 268  30  77 284
 270 151 118 291 152 158   6 199  53 157  16  82 168  61 116 220  52 147
  22 283  84 198 273  62 186 229 101 173 149 192 206  75 292 115  85 144
 271  60 285 161 196  18  43 247 114  10 265 236  67  99  35 216 272 169
 246 128 110  23 223 269  27 259 155  17  66 170  55  65  59  80 249 187
 166  96  87 203 127  79 137 250 153  70 188 224   4  12 252  29 235  38
 244   0 263  88  94 240 289  20 100 159 278  14  39 195 112 156  64 102
 256   5 201 134  76   8 142 160  42  91 165 225  11 279  71  95 211 141
 140 107  21  28 233  37  93 260 286 190  24 242 105 172 222  81 214  69
 253 138 205 106  15 189 255 179  97 183 167 104  86 136   3   2  51 143
 219 177 103   9 121 231 108 193 111 282 213 287  89 123 184 109 125 174
  40  45   7 207 228 175  83 239 126  73 238  56  92  13  41 178 185  90
 221 113 130 218 215 241 217 202 180 208 212 262 182 210   1 129 120  44
 226  63 227 234 124 163 204 197 181 281 230 251 232 237 132 122 131 261
 243 139  98 264 135]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5270
INFO voc_eval.py: 171: [ 8529 13946  1048 ...   764 10817 10819]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5395
INFO voc_eval.py: 171: [2507  263  721 ... 2726 2725 1843]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4252
INFO voc_eval.py: 171: [ 66 319 541  33 338 550  67  56 136  40 544  49 342 335 545  44 334 323
 618 542 159 351  34 484  46 223 139 554 347 546 257 317 345 161 555 549
 413 130  36 648 288 142 349 369 547 635 419 311 625  71 429 150 327 149
  68 336 165 425 543 141 353 160 492 229 230 322  60 271 497 228 128 567
 414 324 624 622 162 339 614 564  51 127 485 626 157 255 231 315 222 418
 627 258 629 640 467 638 569 337 355 321 620 167 621  39 531 221 352 623
 144 502 236 558 269 422 493 340 651 503 332  45 168 198 370 134 639 348
 368 421 326 143 495 138 374 617 520 270 320  43 316 237 256 293 630 333
  92  62  65 166 318 147 644 652 498 225  97 415 137 579 483  93 135 416
 233 289 636 647 679 235 350 676 428 331 129 346 155 632 314 573 247 341
 259 312 426 234 508 512 645 565 146 153 367 131 325 313 164 148 140 513
 175 615 570 283 417 268 439 329 611 619 343 515 633 287 224 574 145 528
 628 170 568 373 412 687 266 328 677 344 209 133 201 674 267 330 634 154
  79 532 553 675 641 420 631 612 643 489 559  42 226 132 642 273 308 163
 490 169  50 551 202 358 584 469 646 453 466 601 572 232 496 103  15 104
 650 507 298 486 263 382 575 510 204   5  27 501 649 637 195 357 511 203
 613 371  55  63 272 446  35 581 468  70 111 383 566 158  24 680 354 548
 385  38 274 616 359 189 208 248 494 125 525 174 557 184 197  32 653 261
 199 360 260 124 254 227 482  96 356 304 279 171   0  95 264 689  91 534
 521  94 200 196  23 516 262 523 249  16 509 685 123  72 191 678 381 423
 265 278 654 275 306 655 389 397  98 244 480  37  22 560 658 276 470  47
 656 177 290 445 180 363  14 432 375  12 522 444 524 686  88  30 562 487
 305 250 526  48  61  78 172 450 433  82 592 582 441 126 472 108 449 110
 186 452 113 114 107 185 514 561  53  80 682 463 506 218 427 517 563  25
 438  85   1 303 600 188 173 215 440 552 297 607 465 442 460 211 464 101
 688 251   2 491  21 282 477 519 216 430 605 690 583 372  17  57 213 286
 657 602 518 585 399 683  18   7 610   6 684 481 178 580 309 691 448 214
 527 401 409 252 500  19  58 556 243 479  77  74 451 281 577 606 578 529
 659  75 109 603 376  31  41  69  20 533 478 488 284 285 292 571 112  81
  76 681 404  52 447 296 505  29 187 310 277 662  73 473 119 458   3 116
 115 424 210 604 183 377  89  59 504 530 435 253 599 295 437 431  54 589
 499 291 390  64 307  26  28 475 156 398 294   4  99 280 194 443 362 379
 598 193 212 364 121 596   8 665 205 434 407 405 436  13 462 365 122 380
  87 220  83 593 120 361 387 588 378 538 591 590 100 403 609  84 219 586
 576  90 595 179 206 410 217 476 102 587  86 663 474   9  10  11 117 118
 667 239 411 393 597 366 536 181 106 594 608 384 455 240 391 537 192 238
 402 176 388 207 241 395 669 299 245 666 246 302 190 471 664 673 242 396
 386 400 535 182 671 392 406 151 461 300 152 670 301 408 661 456 394 668
 454 539 457 540 672 660 105 459]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3048
INFO voc_eval.py: 171: [ 39   4  43 115 181 175 271  44 120 331 185   7  46 179 228  48 149 153
  11 182 187 327 146 150  38  57 273 334 173 142 121 225 178 148 215 269
 223 177 134 266 229 238 141 267  97 270 122  76 242  74 330  45  96 272
   6  55 247 118 101 114 224 154  56 186 167 143 323  21 155 176 123 138
 319  54 263 206 239 299 128  40 180 168 320 303 227 174 237 231  68 183
 102  71 226 209   5 151  42 113 184 158 152  95 262  92 126   9  47 244
 241  99  12 147 329 245 100 314 124 316 254 333  85  24 322 240  13  93
 259 136  70 281 246 116  69 243 230 317 251 324 201  41 325 332  98 301
 119 205 144  86  79  60 213  80  94 236 162 313  58 255 196 335 286  23
 109 253 306 260 222 300 287 193 221 189  25  75 111   8 112  81 217 139
 108 188 117  10  91 276 127 233 212 315 204 250 305  78 159 297 125  72
 261 249 110  59  30 140 326 145  52  73 234 268 256 218 307 257 291 318
 216 328 321 264  26  27  84 302 310 309  36 282 220 194 137 252  22 135
 290 336 295 210  61  51 207 169 211 289 107  28 163 166 199  49 202 232
 214 288 106  53 198 219 308 191  20  77  29   2 275 298 283 129   0 258
 133   3 248 190   1 195  82 203 274  83 292  16 161 132 104 296 200 293
 165 192 103 160 285 197 294 235 156 171  17  33 277 164 131  19 278 130
 265 170  32  15 105  18  50  14  34 284  67 172 157  37 280 279  65 311
  31 304 312  66  63  35  62  64  88 208  90  89  87]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1480
INFO voc_eval.py: 171: [634 374 642 395 121 647 373 311  84 205 283 127  80  82 170 887 639 640
 284 313 310  87 124 182 389 210 130 796  97 208 287 501 792  85 380 533
 206 632  86 447 190 445 636 655 375 670 290 104 204 448 125 392 312 902
 122 316 189 896 889 296 327 177 286 824 506 799 285  92 379 459 870 519
 843 704 897 834 542 167 394  94 139 214 141 216 201 829 387 291 901 522
 449 895 635  65 321 484 538 381 427 857 740  45 378 315  20 106 118 215
 377 172 300 839 806 876 675 142 725 196 314 705 193 662 654 798 450 892
 112 175 826 452 794 504 271 435 197 526 255 900 812 171 430 168 431 340
 828 183 749 219  98 303 227 726  73 186 102 618 295  21 390  46 505 872
 169 534 868 746 819 521 903 198 537 207  35 319 890 297 179 715 908 298
 884 166 185 259 549 440 174  68 735 731 133 241  77 456 833 708 873 212
 195 289 823 238  69 446 451 888 483 507 108 830 778 294 775 244  44 891
 211 224 862  59 455   2 444 184  90 711 540 376 536 905 365 385 875 697
  54 842 714  18 810 181 495 707 598  96 700 575 324 257 180 254 706 684
 894 304  99 680 503 882 523 753  19 160 921 479 637 328 869 633 864 676
 178 586 203  60 425 391 827 461 859 748 821 398 109 776 728 368 309 844
 116 865 138 317 105 822 906 453 344 510 481 432 766  53 436 306  33 231
 809  43 673  74 230 393 815 732 383 813 867 603 914 460  79 458 176 623
 366  40 920 173 256   5 233 881 754 904 490 545 514  37 348 199 148 334
 235 535 428 246 899 270  75 347 854  67 424 605 502 850 556 801 687 858
 841 871 874 101 818 737 612 658 266 525 739 249 520 276 258 663 457 292
 129 923 816   7 463 907 681 145 164 159 209 371 543  58 119 804 107 226
 165 188 883 472 218 243  78 128 652 308 437 772 527 247 245 570 751 615
 724 492 848 531 912 820 682 683 515 103 478 539 550 346 880 602  47 192
 345 417 885 898  39 213 878 569   9 596 685 738 234 200 733  76  93 630
 832 486 462 750 299 426 223 143 678 237 692 162 729 723 217 560 251 260
 849 117 863 434 893 760 840 132 696 866  38 301 396 524 847 239 572 512
 322 886  57 610 557 619 563 454 433  42 305 585 156 805  56 599  41 712
 469 548 429 601 267 225 769 727 624   0 273 320 191 703  14 187 814 646
 384 783  91 369 597 922 617 838 702 600  83 817 784 341 803 517  49  36
 564 551 415 541 606 335 789 386 718 742  63 493 423 924  61 113 919 627
 382 155 120 388 497 194 677 409 879 734 232 350 489 574 752 763 336 367
  11  23 764 686 573 326 236 565 302 278 263 679  89   3 674 268 136 621
 782 511 583 272 786 410 779 709 765 555 831 582 777 616 710 416 741 338
 293 860 110 767 609 846 240 785 604  32 473  34 576 269 288 372 252 351
  62  55 277 589 438 144 242 577 730  64 307 780 699 571 442 274 370 915
 339 414 691 736 358 837 202 607 717 701 608 474 672 695 698 496 835 762
 644 631 721 264  95 157  48 115 280 418 747 111  22 114  31 471 480 793
 613  81 877 554 485 509 357 781 861  70 397 743 622 774  17  72 546 811
 909 568   8  88 528 802 100 561 360 318  24 412 800 745   1 562 248 439
 228 513  15 790 787 468 580 553 229 532 719 441 588 584 467 332 275 578
 261 465 566 567  71 836 917 552 279  16  66 343 855 770 807 744 443 773
 413 498 349 720 611 667 771 331 250 355 487 488 163 651  52 620 499 253
 464 797 795 664 591 529  10 693 262 405 808 791 500 466 694 404 359 594
 470  12 544 516 477 364 491 722 825 482  50  13 910 579   6 154 421 153
 363   4 590 713 587 281 494 788 768 152 140 761 547 518 420 716 614 666
 530 593 475 856 508 422 845 690 282 643 913 361 419 362 592 399 158 759
 476 581 342 401 352 323 161  30 354 353 641 625 333 220 408 406 656  51
 657 758 356 123 325 147 151 851 558 137 628 400 411 559 629 595 653  29
 916 660 626 329 661 689 149 757 756 222 407 918 853 755 659  25 403 852
 402 337 330  28 150 688 146 221 668 134 265 649 135 126 645 131 638 911
  26 650  27 669 671 648 665]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2665
INFO voc_eval.py: 171: [429 256 601 177 304 443 141 451  25 477   1  38 485 504 259 327 358 257
 483 487 329  40 405 254 447 427 356 129 357 131 478 583 316 557 305  59
 359 614 430  72 258 540 110  75 134 330  31 236 323   3 603 457 524 448
 439  17 335 182 347 591 355 310 185  44 326 602 632 159 623 379 232 612
 106 364 264 374 154 617 541 382 314 450 377 112 226 114 453 142 211 480
 214  89 493  19 328 268  78  28 229  74 149 445 165   9  53 563 308 538
 511 479 312 212 472 279 491  60 387 404 383 107 482 620 471 240 331 402
 368  41 370 191 365  30 249 373 637 124 167  26 220 338  16 166 492  48
 271 627  58 436 488 180  46 133 606 306   6 414 320 599 307 215 481 293
 509 262 584 352 610 549  42 484   8 311  76 313 489  55 230 147 321 567
 566 123 576 508 105  50 219 216 378 234 344 181 441 318 315 523 519 636
 444 336 456 130 218 407 324 455 175   2 384 221 628 435 222 163 486  39
 434 217 194 136 269   5 624  32 121 585 588 587 176 178 197 389 231  45
 233 125  95 120 363 362  79 560 616  83 372 403 278 109 520 490  12 413
 554 186  93  29  47 128 555 309 341 502 367 605 287 561 631 299 190  24
 570 406 512 408 423 615 618  18  13 183 290 633 116 164 552 626 449 158
 135 104 545 325 547 349  57 317 213 462 506  92  98 542 174 568 535 622
 604 521 150 203 291 539 562 528 420 243 148 286 283 569 503 102 411 394
  99 284 282 634 496 579 619 629 366 245 369  96 613 531 630 507 153 505
 270 575 461 223 118 225 385 340 115 592 113  34  87 160  90  67 288  56
 339 285  82 594 548 210 510  94 625 261 375 573 224 196 227 319 537  43
  65 464 281 621 143 391 376 551 152 428 526 267 522  64 639 322 550 132
 119 292 280  97 390  80 303 361 516  81 381 184 556 438 546  85  23 192
 122 274 260 235 574 460 431  63  36 578 337 417 171 386 388 360 446 127
 380 208 635 207 247 275 458  52 162 289 111 465 294 596 640 371 398 440
  91 146  54   4 473  37  35 597  33 263 239  62 437  15 277 298 454 459
 598 332 529  11  68 346 582 156 343 170 108  73 452 151 638 126 248  86
 206 161 474 611 237 432 559 334 419   7 138 593  22 144 117 553 543 468
 494 495 572  27 565 536 155 157 534 195 295 544 202 244  61 433 564 533
 139 145 297 401 140 246 189 581 532 515 333 416 301  88 412 418 241 205
 100 187 442 302 255  71 228  70 527 415 172 173 410 396  51 571 265 137
 475 188 300 101  10 350 204 169 266 525 238 421 558 179  77 348 590 399
 397 345 353 530 168 351 342 577  84 517 595 409 193 395 580  66 276 422
 513 103  21  69 242 467   0 393 514 296 354 253 518 589 400 392 498 463
 252 466  20 609 608 251 607 273  49 469 497 470 424 499 199 586 272 201
 600 425 209 250 500  14 426 198 200 501 476]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3505
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3079
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.252
INFO cross_voc_dataset_evaluator.py: 134: 0.399
INFO cross_voc_dataset_evaluator.py: 134: 0.198
INFO cross_voc_dataset_evaluator.py: 134: 0.234
INFO cross_voc_dataset_evaluator.py: 134: 0.330
INFO cross_voc_dataset_evaluator.py: 134: 0.424
INFO cross_voc_dataset_evaluator.py: 134: 0.289
INFO cross_voc_dataset_evaluator.py: 134: 0.097
INFO cross_voc_dataset_evaluator.py: 134: 0.313
INFO cross_voc_dataset_evaluator.py: 134: 0.210
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.214
INFO cross_voc_dataset_evaluator.py: 134: 0.309
INFO cross_voc_dataset_evaluator.py: 134: 0.527
INFO cross_voc_dataset_evaluator.py: 134: 0.540
INFO cross_voc_dataset_evaluator.py: 134: 0.425
INFO cross_voc_dataset_evaluator.py: 134: 0.305
INFO cross_voc_dataset_evaluator.py: 134: 0.148
INFO cross_voc_dataset_evaluator.py: 134: 0.267
INFO cross_voc_dataset_evaluator.py: 134: 0.351
INFO cross_voc_dataset_evaluator.py: 135: 0.308
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 5499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.414s + 0.043s (eta: 0:00:56)
person 0.96961975
person 0.9633189
person 0.9921944
person 0.9603482
person 0.9730085
person 0.92566675
person 0.9366292
person 0.98196214
person 0.94431853
person 0.9106134
person 0.98151636
person 0.9386395
bottle 0.9227406
bottle 0.91984236
bird 0.95167553
person 0.9824456
person 0.9177366
person 0.94382364
person 0.9049592
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.319s + 0.036s (eta: 0:00:40)
person 0.94416714
person 0.91777176
person 0.9593537
person 0.94565153
person 0.9805921
person 0.9470732
person 0.96804684
person 0.9816549
person 0.93928915
person 0.92111295
person 0.9288098
person 0.93140006
person 0.9056207
person 0.9222875
person 0.92027587
person 0.9263362
person 0.9746153
person 0.9108174
cat 0.90013885
person 0.9802532
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.319s + 0.037s (eta: 0:00:37)
person 0.92871356
person 0.9128559
person 0.9551008
chair 0.9136816
person 0.9720997
person 0.9114425
person 0.95430976
person 0.96539104
chair 0.9656722
chair 0.9373442
chair 0.91429937
person 0.90159494
person 0.9790061
person 0.98793447
person 0.9294939
person 0.90233123
person 0.9765801
person 0.9915925
person 0.98146915
person 0.9135536
person 0.90709966
person 0.90775365
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.324s + 0.039s (eta: 0:00:34)
bird 0.9377541
person 0.9271323
person 0.921189
person 0.9937875
person 0.9747468
person 0.95614815
person 0.980161
person 0.90982556
person 0.95930564
person 0.96566784
person 0.9064089
person 0.91745913
person 0.9697555
person 0.93990123
person 0.95886767
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.329s + 0.039s (eta: 0:00:30)
diningtable 0.9000455
diningtable 0.939442
chair 0.9633275
chair 0.96575534
chair 0.93792886
chair 0.956808
pottedplant 0.91833466
pottedplant 0.9149345
pottedplant 0.91028744
pottedplant 0.90469617
pottedplant 0.9436479
pottedplant 0.93790686
pottedplant 0.930252
person 0.9656184
person 0.98272127
person 0.9215902
person 0.97953105
person 0.98879516
person 0.91426677
person 0.9028243
person 0.9126921
person 0.91940135
person 0.92026395
person 0.9725221
person 0.95407
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.325s + 0.037s (eta: 0:00:26)
person 0.9559129
person 0.922199
person 0.9035267
person 0.9204357
person 0.94519496
person 0.91197085
person 0.9529958
person 0.93597776
person 0.92789346
bird 0.92248255
person 0.9345501
person 0.98760855
person 0.9169911
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.325s + 0.037s (eta: 0:00:23)
pottedplant 0.9040168
person 0.9619359
person 0.9184608
person 0.9093341
person 0.95381117
person 0.9172617
person 0.9585311
diningtable 0.90737015
chair 0.9826764
diningtable 0.96016
chair 0.91697544
chair 0.9478684
chair 0.9805147
chair 0.9281671
bird 0.993082
person 0.9014495
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.323s + 0.037s (eta: 0:00:19)
car 0.928435
car 0.9414766
car 0.9127017
person 0.98852074
person 0.9001759
person 0.97403145
person 0.92941636
horse 0.974446
horse 0.91387916
horse 0.92041427
person 0.9377973
person 0.953282
person 0.9682693
person 0.91777015
person 0.9250146
car 0.94575495
car 0.9647203
bicycle 0.9616688
bicycle 0.93018085
person 0.93791735
person 0.9158687
person 0.9729149
chair 0.9174699
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.326s + 0.037s (eta: 0:00:15)
person 0.97425836
person 0.95861274
person 0.9732135
person 0.9144719
person 0.91165924
person 0.9885461
person 0.9863473
person 0.9429387
person 0.91696095
person 0.96430504
person 0.9635899
person 0.99110776
person 0.90575045
person 0.9667897
person 0.9107184
person 0.9875979
person 0.9799478
pottedplant 0.91296804
bus 0.91306853
person 0.9899519
person 0.96185756
person 0.9409072
person 0.9536342
chair 0.94549185
person 0.9055813
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.328s + 0.037s (eta: 0:00:12)
motorbike 0.9644907
person 0.9746177
person 0.9112983
person 0.91418517
person 0.9558079
person 0.9661696
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.324s + 0.037s (eta: 0:00:08)
person 0.90263677
boat 0.981888
boat 0.94783306
boat 0.9178878
person 0.99025834
person 0.9371423
person 0.9855716
person 0.91798466
person 0.92250884
person 0.9888699
person 0.9749692
person 0.90957624
person 0.9742097
person 0.9764147
person 0.911971
person 0.94854754
car 0.95042783
person 0.9377006
person 0.94664556
person 0.959999
person 0.98667055
person 0.9042958
person 0.9192884
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.330s + 0.037s (eta: 0:00:05)
diningtable 0.95875084
chair 0.9234007
chair 0.9738617
chair 0.9807787
chair 0.9131867
chair 0.98078007
chair 0.93565184
chair 0.96053946
person 0.9411926
bird 0.92344576
bird 0.9123817
boat 0.91998255
person 0.96185195
person 0.92754877
person 0.9886035
person 0.91288817
person 0.900177
person 0.910815
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.325s + 0.036s (eta: 0:00:01)
person 0.9833666
person 0.96439207
person 0.9559504
person 0.9554197
person 0.9483362
person 0.9652857
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.466s + 0.026s (eta: 0:01:01)
person 0.9817191
person 0.91733134
person 0.9786856
person 0.9770546
person 0.9130514
bicycle 0.97258526
person 0.9313437
person 0.91226184
bicycle 0.9745812
person 0.9339218
bird 0.9432431
pottedplant 0.94401354
person 0.970273
person 0.92821723
person 0.90236676
person 0.96016675
person 0.96530306
person 0.98665464
person 0.9275216
person 0.9659005
person 0.9523903
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.319s + 0.033s (eta: 0:00:40)
person 0.91239965
person 0.90680575
person 0.9638511
person 0.91716784
person 0.97293943
person 0.94039255
person 0.9347132
person 0.98062724
person 0.9644662
chair 0.9262783
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.331s + 0.036s (eta: 0:00:38)
car 0.9158401
car 0.9495379
person 0.9476045
person 0.96083736
person 0.98131806
person 0.9213471
person 0.94790196
person 0.9641239
car 0.98214066
cow 0.9173231
person 0.9159322
person 0.9568672
person 0.92218095
person 0.9111786
person 0.9803668
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.323s + 0.036s (eta: 0:00:33)
person 0.97785634
chair 0.9330434
person 0.9290339
person 0.92648363
person 0.9509815
person 0.92550254
person 0.93136305
person 0.9180994
person 0.9393991
person 0.9806096
person 0.98682445
person 0.97701865
person 0.9688205
person 0.97675157
person 0.9012125
bird 0.9907679
person 0.9214231
bird 0.9515444
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.320s + 0.038s (eta: 0:00:30)
person 0.9445997
person 0.97047573
person 0.98247474
person 0.9227002
person 0.94645715
car 0.95042783
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.324s + 0.038s (eta: 0:00:26)
bird 0.9112263
bird 0.96026564
bird 0.9135677
person 0.91125214
person 0.9431048
person 0.9148142
person 0.9147222
person 0.9881379
diningtable 0.94803154
person 0.97197354
diningtable 0.97828066
person 0.92119205
person 0.93440056
chair 0.94232124
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.328s + 0.038s (eta: 0:00:23)
person 0.9120735
person 0.9030085
person 0.92535573
person 0.94615626
person 0.9855999
bus 0.94742715
dog 0.95182836
diningtable 0.97341657
diningtable 0.9203515
chair 0.9111009
chair 0.9848935
chair 0.9124847
chair 0.9703535
chair 0.9353471
pottedplant 0.9029512
person 0.9055263
person 0.9756143
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.324s + 0.037s (eta: 0:00:19)
diningtable 0.92002535
chair 0.9092909
person 0.94708496
person 0.9661222
person 0.9541183
person 0.96294224
person 0.94448096
bird 0.95523727
person 0.92540216
person 0.9605015
person 0.9634999
person 0.930632
person 0.98034465
person 0.90709436
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.323s + 0.037s (eta: 0:00:15)
person 0.96249294
person 0.9619167
person 0.9029279
train 0.9396597
person 0.9569309
person 0.962476
person 0.94704634
person 0.9378495
person 0.9108237
person 0.93280864
person 0.9285718
person 0.90457416
person 0.9557209
person 0.9299508
person 0.99234664
person 0.9741468
person 0.9612253
person 0.91751474
person 0.92738694
person 0.90045613
person 0.9273981
tvmonitor 0.9170639
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.326s + 0.037s (eta: 0:00:12)
person 0.96332866
person 0.9045929
person 0.94388056
horse 0.9599379
person 0.9663804
person 0.9292086
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.324s + 0.037s (eta: 0:00:08)
person 0.9750397
person 0.9656181
person 0.9220218
person 0.91946465
person 0.9501366
person 0.9253984
person 0.9354629
person 0.90187925
person 0.9132439
person 0.98656136
person 0.9441492
chair 0.9719126
person 0.9560767
person 0.9030453
person 0.94541836
person 0.9581522
person 0.9050992
person 0.9164082
car 0.9454814
car 0.9169104
car 0.9510993
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.323s + 0.037s (eta: 0:00:05)
person 0.96805453
person 0.94319105
person 0.9619393
person 0.948592
person 0.9732708
person 0.9600077
person 0.919859
person 0.90307415
person 0.9256169
person 0.9881144
person 0.98161316
person 0.9184657
person 0.97803646
person 0.92729396
person 0.93710834
diningtable 0.9322138
person 0.9164406
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.317s + 0.036s (eta: 0:00:01)
person 0.90719134
diningtable 0.9140308
person 0.9700586
person 0.9391911
person 0.98738503
person 0.96621597
person 0.9583639
person 0.95759493
person 0.90083987
person 0.9054906
person 0.906399
person 0.9410185
person 0.96363163
person 0.9812555
person 0.90817803
person 0.94862807
person 0.9566889
person 0.95561516
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.527s + 0.028s (eta: 0:01:08)
diningtable 0.90049696
diningtable 0.9255669
person 0.96034175
person 0.97582203
diningtable 0.97246516
person 0.9218617
person 0.91941124
person 0.90447855
bottle 0.90657073
person 0.9776098
pottedplant 0.91789705
diningtable 0.9250113
person 0.9656601
diningtable 0.9051847
chair 0.94979525
chair 0.965729
chair 0.92169136
person 0.9154084
person 0.90515745
person 0.93090785
person 0.9957545
person 0.9807844
person 0.9329346
person 0.9663861
person 0.9647422
person 0.9041821
person 0.9704707
person 0.95901436
person 0.93701017
person 0.9360609
person 0.96486914
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.336s + 0.031s (eta: 0:00:41)
person 0.9457452
person 0.98591924
person 0.9328242
person 0.9703728
bottle 0.93739563
person 0.9355691
person 0.94169426
person 0.9525667
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.325s + 0.032s (eta: 0:00:37)
chair 0.9334321
bottle 0.90932554
person 0.9720497
person 0.9253097
person 0.9382588
person 0.9304317
person 0.9514123
person 0.92968994
person 0.96725255
person 0.94233185
bottle 0.90705675
bottle 0.95284885
person 0.982144
person 0.95348233
person 0.9489596
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.327s + 0.033s (eta: 0:00:33)
person 0.96579874
person 0.92017645
bird 0.9188836
person 0.9537065
person 0.90473735
person 0.96676666
person 0.9700075
person 0.95906043
person 0.9020268
person 0.9349008
person 0.9052746
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.324s + 0.034s (eta: 0:00:30)
person 0.9693328
person 0.9693132
person 0.9539665
person 0.9044767
person 0.92942005
person 0.9704549
person 0.9201205
person 0.9410581
person 0.9134018
person 0.91767526
person 0.9377343
person 0.9697063
person 0.95331514
person 0.9295706
person 0.903602
bottle 0.9536486
chair 0.9315765
boat 0.90494114
person 0.90592617
person 0.9733132
person 0.95451444
person 0.9010398
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.320s + 0.035s (eta: 0:00:26)
car 0.98063654
car 0.9352587
person 0.925221
person 0.97696495
person 0.90965843
person 0.90775603
person 0.9192218
person 0.93405265
person 0.96465486
person 0.92260504
person 0.9153057
person 0.9565372
pottedplant 0.91667354
pottedplant 0.93234956
car 0.98477453
car 0.9384086
person 0.9313127
person 0.98780906
person 0.95367366
person 0.91933537
person 0.93704265
person 0.98587483
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.320s + 0.036s (eta: 0:00:22)
person 0.97972256
car 0.9590969
person 0.94998574
person 0.92426574
tvmonitor 0.92602164
person 0.9038123
person 0.93048686
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.319s + 0.036s (eta: 0:00:19)
chair 0.90284204
person 0.9705693
person 0.9428079
train 0.9174997
train 0.96062434
bird 0.91050094
bird 0.9827351
person 0.9212454
person 0.92245597
bicycle 0.9228755
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.315s + 0.035s (eta: 0:00:15)
person 0.9690972
person 0.9014963
person 0.93663514
person 0.92935634
person 0.9292621
person 0.97842145
person 0.9865767
person 0.9581383
person 0.9728002
person 0.9893652
person 0.90889597
person 0.9547317
person 0.96548635
car 0.9509136
person 0.9235848
person 0.9688468
person 0.98047435
person 0.931621
person 0.915231
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.316s + 0.036s (eta: 0:00:11)
person 0.9143839
person 0.94500744
person 0.922988
chair 0.9752805
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.317s + 0.036s (eta: 0:00:08)
person 0.9811739
person 0.9390306
person 0.92640007
person 0.96272975
person 0.9620488
person 0.9018682
person 0.9258367
person 0.90695304
person 0.93809575
person 0.97875595
person 0.904241
person 0.9068324
boat 0.90303636
person 0.966464
person 0.92247015
person 0.9457163
person 0.9101881
person 0.95810324
person 0.9020772
person 0.9245377
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.317s + 0.035s (eta: 0:00:04)
person 0.9672143
person 0.9731759
person 0.97793007
person 0.9614123
person 0.9202588
person 0.93032897
person 0.92421705
person 0.9501531
person 0.96021885
person 0.9171143
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.317s + 0.035s (eta: 0:00:01)
person 0.9356
person 0.97132194
person 0.9209611
bird 0.90470135
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.429s + 0.028s (eta: 0:00:56)
person 0.9407201
chair 0.9185637
chair 0.957617
person 0.95822376
person 0.9262874
person 0.95348495
person 0.9394812
diningtable 0.930998
person 0.9739448
person 0.9709055
diningtable 0.9348625
person 0.95211333
person 0.96790457
person 0.9048277
diningtable 0.911654
person 0.9278906
person 0.9650294
diningtable 0.96488804
diningtable 0.926865
diningtable 0.939138
person 0.95193386
chair 0.9139335
chair 0.98114324
person 0.9294834
person 0.98908126
person 0.9216198
person 0.9619018
person 0.9852315
person 0.9167811
person 0.92348003
person 0.98357856
person 0.96806604
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.330s + 0.034s (eta: 0:00:41)
person 0.93860936
person 0.9530137
pottedplant 0.9336836
person 0.90261346
bird 0.9718946
bird 0.94918567
bird 0.91877675
person 0.9673405
person 0.9207126
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.326s + 0.033s (eta: 0:00:37)
person 0.9501746
person 0.9085793
person 0.9620356
person 0.96940875
person 0.97594255
person 0.9001255
person 0.94776154
bottle 0.9045684
bottle 0.95168066
bottle 0.9010336
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.323s + 0.034s (eta: 0:00:33)
person 0.9069111
person 0.9137186
diningtable 0.9088247
pottedplant 0.93200326
chair 0.9392943
chair 0.9160423
person 0.9029709
person 0.9760524
person 0.939573
bottle 0.9106912
bottle 0.9078036
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.318s + 0.034s (eta: 0:00:29)
person 0.96223
person 0.9602161
person 0.9481578
person 0.9948125
person 0.94208664
person 0.94075024
person 0.9198953
person 0.9793443
person 0.91032535
person 0.95749456
person 0.972593
person 0.9144991
person 0.9353419
person 0.9370801
person 0.9920459
person 0.94028395
person 0.97331697
person 0.92554533
person 0.961562
person 0.983721
person 0.96536446
person 0.9267946
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.314s + 0.034s (eta: 0:00:25)
person 0.9639915
person 0.9711151
person 0.9023993
diningtable 0.9835157
chair 0.91565627
chair 0.94762266
chair 0.97869027
chair 0.987329
aeroplane 0.91315
person 0.9664887
person 0.9812245
person 0.9306481
person 0.91444457
person 0.9110104
person 0.94684345
person 0.9735908
car 0.9289903
car 0.98014176
car 0.97176445
car 0.9583282
bird 0.9579927
chair 0.9304341
chair 0.929642
person 0.9441762
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.317s + 0.034s (eta: 0:00:22)
person 0.98368007
person 0.9347903
person 0.9052775
person 0.9815694
person 0.9579933
person 0.9896315
person 0.93612295
person 0.92497283
person 0.9902182
person 0.9484124
person 0.9381483
person 0.9063815
horse 0.9622807
person 0.9115169
person 0.9054876
person 0.9196336
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.313s + 0.033s (eta: 0:00:18)
person 0.9537772
person 0.94455725
chair 0.9752805
chair 0.9207618
chair 0.9578255
pottedplant 0.9219879
pottedplant 0.9449259
pottedplant 0.90056086
person 0.94163203
person 0.9006907
person 0.921017
person 0.9357804
person 0.95858765
person 0.938916
person 0.93198335
person 0.9007591
person 0.9013489
person 0.9546526
bicycle 0.9158416
person 0.9317932
person 0.9796556
person 0.900825
person 0.94040096
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.314s + 0.034s (eta: 0:00:15)
person 0.9881593
person 0.9185183
person 0.90177435
person 0.9152919
person 0.9587478
person 0.98502755
person 0.9404517
person 0.97682196
person 0.97768813
person 0.97169656
person 0.93758464
person 0.9196882
person 0.91338396
bus 0.95471746
person 0.90523773
person 0.9467751
person 0.9387746
person 0.9642718
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.314s + 0.035s (eta: 0:00:11)
person 0.9123389
person 0.9682368
person 0.9115979
person 0.93396956
chair 0.913055
chair 0.9486549
tvmonitor 0.90206486
chair 0.97087806
chair 0.93327886
chair 0.9408168
person 0.94715244
person 0.96921355
person 0.9561421
person 0.9772096
person 0.9204935
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.311s + 0.034s (eta: 0:00:08)
motorbike 0.9156368
motorbike 0.95193595
person 0.97852135
horse 0.95764214
horse 0.90269256
person 0.9018859
bicycle 0.95838404
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.311s + 0.034s (eta: 0:00:04)
person 0.9237494
person 0.95881957
person 0.9808851
person 0.9489525
person 0.96387315
person 0.98388577
person 0.9481431
person 0.9547375
person 0.94168013
chair 0.9602695
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.312s + 0.034s (eta: 0:00:01)
person 0.90416044
person 0.9862509
person 0.9856779
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 85.466s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1197  285  281 ... 1023 1022 1021]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2503
INFO voc_eval.py: 171: [123 124  69 513  68 243 454 292 270 127 135 125 483 566 385  71 542 395
 569 490 458 568  72 563 511 383 573 572  70 553  84 479 131 273 133 580
  88 459 530 564 132  73 586 258  95  79 380 129 387 519 301 463 381  76
  90 136 585 523 555 126 544  91 470 293 303  74 244 562 471 249 141 464
 469 548 515 439 462 101 570 543 587 474 545 138  18 142 379 149 574 484
  42 302 520 468 541 294 390 241 248 247 581 412 284 245 345 522 386 426
 550 299  78 251 577 460  21 103 534 347 504 415 465 384  20 274 147 285
 546 431 309 105 437 306 144 518 514 162 137 275 272 268  40 517 128 461
 307 352 578 404 477 389 558 583 163 547 256 340 198 421 200 207 219 540
  87 440 582 148  36 473 571 411 467 242  24  27 283 565 584  33 525 557
   2 330 333 346 575 406 134 438 491 146 259 310 257 417 143  77 422 318
 424 297 576 308  17 567  41 551 521 393 276  32  43 324 579 300 262 322
 434 378 338  82 537  54 533 250 554 445 253 277 145 535 391 489 288 194
 255 202   5 529 139  15 246  92 432  96 312 408 104 556 394 130 410 179
 450 559  89 197 419 102  86 494 526 280 506  75  98 188 269 321 402  28
 538 510 433   1  45 475  19  23 342 264 208 451 296 442 180 140 265 512
 482 407  94 527 425  66 382 271 456  47 199  80 201  37 282 341 403 401
 177 370   0 305 161 516 223 281 320 286 166 191 334  99 448  64 443 313
 192 195 427 428 392 429 189  34 507 560 536  59  85 423 184 336 487  60
 116 121  31 416  25 326  26 441  53 453 181 488   3 190 287   7 203  46
  29  16 196 337 155 528 455 298  81 237 295  11 224 414 254 348 316 397
 315 480 186 323 115 472 193 358 502 444 290 165 420  14 317 478 167 396
 304 100 539 351 267 413 311 236  65 552 449 357 430 187 409 508 111 170
 492  62 252 452 228  22 466 359 446 364  30 549 156 210 377 174 531 339
 398  12 212 503 447 171 172  35  49 399  10 169 319  93 109   6 211 206
 360 353 122  56 481 405 366 205 376 235  51 164 289 216 168   4  58 532
 418 209 261 509 372  13 215 524 260  55 493 214 113 354 485 343 230 400
 159 185 108  61 204 388 239 153 325 154 355 349 375 369 457 152 178 183
 314  67   8 368  83  50 160  44 106 176 220 486 217 436 218 350 476 182
  57 367   9 365 173 175 151 150 157  52 356 435 107 344 110 227 332 158
 266 327 112 361 291 328 221  48 335 117 263 213 114  63 118 495 371 329
  97 505 226 363 225 222 233 501 240 234 496 232 374 373 229 498 497 331
 588 120 238  38 278 499 500 231 119 279 362 561  39]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3995
INFO voc_eval.py: 171: [ 643 1704 3223 ... 3217 3215 3221]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1966
INFO voc_eval.py: 171: [ 639  673  749 ... 2369 3244  122]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2348
INFO voc_eval.py: 171: [412 376 588 319  12  21 665 368 662 291 375 587 581 413 640 668 321 342
 606 558 384 419 615 297 682 148 491 843 671 561 333 602 769 701 842  18
 323 330 582 324 559 629 416 670 296 292 585 339 678  25 667 752 855 369
 641 663 337 738  22 684 677 680 844 377 298 716  20 338 626 378 583 387
 527 565 415 688 320 344  16 335 307 845  76 625 503 383 482 664 740 274
 417 294 560 645 325 849 510 853 691 224 263 301 599 751 672 603 382 526
 341  13 669 643 379 322 761  75 612 181 209 313 311 705 386 687 494 346
 847  47 616 312 693 745 624 418 594 689 504 564 328 484 122 632 500 589
 371 183 775 586 727 815 139 600 704 260 203 699 829 432 709 212  15 336
  27 715 707 184 512 569 538 714 218 726 483 676 584 269 683 628 253 598
  19 408 220 685  46 607 118 698 852 857 306 414 153 223 753 211 154 713
 439 343 300 265 711 814 779 812 630  77  17  54 175 695 788 592 179 174
 164 617  86  45 506 329 380 627 697 736 275 566 644 327 712 706 622 447
 411 562 310 340 590 480  24 267 205 157 270 673 872 398 674 134 757 334
 243 647 614 149 601 610 813 730 821 568 332 430 529 219  31 173 436 816
 124 428 326 459 604 108 760 534 563 703 681 508 621  48 596 273 828 372
  34 631 381  85 331 666 208 597 385 810 254 110 293 686 543  73 690 863
 861 249 176 302 766 458 240 305 728 537 315 189 811  87 121 509 226  67
 185 731 692 370 595  49 567 492 605 702 856 345 776 675  26 140 303  23
 660 533 132 138 648  63 758 237 710 700 105 542 104 502 679 295 593 485
 120 119 271 785  14 410 401 225 472 159 161 591 245 276 501 696 608  84
 244 859 858 737 623 784 774 611 202 708 137  28  59 123 367  93 619 409
 213 840 231 103  37 264 756 272 609 206 848 721 308 873 790 259 694 204
 117 717 236 722 156 613 633   5  58 798 496 285 540 493 841 620 505 806
 221 759 316 258 618 180 539 278 511 789 113 498 165 739  43 347 425 822
 786 431 222 507 741  64 729 497 191 399 875 454  44 407 357 742 755 528
 876 783   1 242 268 144  56 136 246 188 373 309 429 461 207 460 299 107
 304 125 116 126 649 172 767 744   6 200 718  80 764  53 106 155 192 805
 825  99 535 782 247  50 109 867 770  92 374 151 777 474 827  36 127 182
 573 190 854 536 257 158 427 541 241 530 851 400 515 232 846 252 642 279
 426 874 187 787 248 771 481  51 152 150 349 556 266 763 794 830 772 210
 773 438 228 261 433 550 499  66 765 199 746 135  29 823 281 532   2  97
 477 866 817 743 478 283 177 768 475 170 479 162 262 793 133 868 163 111
 442 255  96 747  32 869 778 443 167  83 356 100 807 754 636  30 516 256
 434 435 230 251 455 552 282 227 473 572  52 531 457 661 169 112 732 462
 171 186 865 864 791  57 166 733  82  39 168  79 762 650 234 486 250  91
 129 449 437 850 393 826 551 488 809 646 314 553 366  95 495  68  70 555
 734 792 284   0 178 280   4 102 471 517 554 115  98   8 468 490 160 571
 147 197 871 634 580 288 389 101 544 735 362   3 780 557 358 657 277 216
  62  61 577 720 576  41 146 453 635 395 441 390 489 487 467 820 818 456
 518 348 359 579 749 824 819 361 548 574 114 215 547 448 796 145 360  88
  40 862 719 578 476 575 524 808 143 229 549 403 235 451 860 470 440  35
 800  69 545  42 799 452 870 546  55 424  33 238 404 525  78 653 290 392
 287 513 286  81 402 654 639 391 365 130 514 364 363 638 750   9 317 289
 652 233 723 450  71 637  38  65 797 405 355 318  94 394 198 214 724 803
 838 837 520  11  90 781 748 396 350 196 725 570 469 658  10 239 354  72
 353 388 406  89 128  60 519 465 422   7 141 655 466 835 659 656 521 351
 836 523 834 201 839 352 795  74 802 446 463 801 831 522 804 217 397 421
 195 651 832 193 444 420 142 131 833 423 445 194 464]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3301
INFO voc_eval.py: 171: [438 173  90 174  91 439  87  76  95  99  97 460  96  74  92 442 370  75
 117 181  26 405 209 104 101  73  88  78 407  43 411 396 416  84 254 165
 391 266  94  81  79 369 141  32 275 384 446 443 400  44 363 371 164 297
 397  53  86  52 199 105 365 189 166 237  82 265 158 217  85 484 338 429
 114  77 140  42 224 116 435 202  83 395 294 242 323 107 334  12 206 257
 313 354  46 113 264  80  14 276 200 115 335 339 198 286 212 464 152 121
 197 250 357 270 343 376 340 394  30 137 399 211 178 267 383 256 366 331
 403  56 418 252 380 353 421 382 145 188 249   9 337 320  25 415 213 102
 410 288 179 156 229 392 437 412 119 426 150 195 434 330 118  47 322 462
 381 359 261 285 325 263 361 231 316 364 247 268 227 287 379 293 201 255
 138 393 355 128 170  27  31 120 151 417 280 433 388 203 223 358  17 423
 126 228 332   5 225 300 259 124  35 147 299 333 422 374 427 362 132 219
 139  93 444  39 356  51 463 110 378 317 298 136 253 125 207 483 471 318
 163 271 111 175 123 419 100 277 205 360 112 210  57 430 482 167 226 129
 387 312  33 315   0  89 473 274 431 149  29 457 106 282 180 402 171 162
  36  98 157 234 385   6 401 204 390 345 232  45 398 324 472 336 248 262
 279 208 409 404 424 235 127   4 238 413 420 475 436 251 108 155 214 142
  55 154   3 122 243 346  41 406  66 326 146 143 260 241 215 373 311 289
  28 372 321 455 148 230 281 466 425 183 327 278 428 144 220  67 295   8
  71 377 389 245 159 470 169 309 441 258 283 272 329 367 246  11   7 185
 375 269 172 414 408 328 130 478 351 233 109 153 453 307 344 319 445 314
  58 236  37  10 440 341 103 296 290 450 467 168   1 284  18 196 221 452
 131  40 273 468   2 176 218 386  38 292  34 160 432 190 342 461 310 216
 291 161  54  59 368 465 184 240 244  48 476 469  23 192 308  72 239  62
 222 458 304  20 447 454  21 481 301 186  65  61 348 347  15  50 303 302
 306 448 305 182 350 133  69 477  63 480  24 451  60 479  19 191 135 194
 177 187  13 193  16 459 456  49 449  70 134 474  68  22  64 349 352]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4243
INFO voc_eval.py: 171: [1591  728 1524 ...  286 1716  118]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2898
INFO voc_eval.py: 171: [ 29 264  65 110 157 317  66  19 315 111 288  24 120 265  67 156 256 316
 202 287 127 155  69 266  26 295 232 260 291 112  75 201 211  85 289  52
 124 135 250  96  48  50 241  95  16 119 263 140 236 104 146 117  71 174
 172 234 192 178 307 191 154 296  42  20  76 106  27  64  93 233 126 227
  72  81 219 293  17 205 175 118 297 123 167 294  18 200  97 255 115 244
  28 160 251  21  55  22 113 286 166  74 194 258 217  77 238 204  82 173
  54 132 168 190 242 292  23 235 257  51  92 153 203 197  87  84 158 216
 159 215  88 290 311 129  57  78 308 314 109 283 246 261  56 121 125 309
 148 220  53 239  25 240 184 147 195   5 224 152 151 267 226  47 193 237
  70 141  90 243 162 128 139 137 223  86 130 133 114  89 245 171  49 259
 218 165 225 108 270 285  83 134  44 310 149 280 312 318 222 136  45  14
  68 116  10  94 196 103 248 209  46 253 313 252 247  91  41 298 214   8
 276  98 107 206   3 105   0 183  39  35  15  43 199 142 230 122 281  30
 145 102   7 301 221 101 282 138 268 277   2 213 131  34 182 262 170 210
  99   9 278 284  61   6 169   1 271 176 231   4 100  36  38 208  60 161
  12  31 163 198 300 181 275  59 207  73 164 179 303 274  32 299 279 187
 305 228 272 306  33 212  13  58 302 150  63 269  79  80 249  62 185 143
 177 304 229 180 189  11 254  40  37 273 144 188 186]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0971
INFO voc_eval.py: 171: [5878 2538  713 ... 4403  293 4404]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3125
INFO voc_eval.py: 171: [335 263 200 449  59 206 283 390 282 706  50 259 766 262 271 457 284 258
 417 261 759 455 653  71 226  78 735 336 414 620 199 641 159 452 339 201
 202 386  66 184 758 567  74  55 139  54 181 205 260 458 232 453 208 379
 130 761 619 454 278 128 229 450 273 448 210  70 370 463 387 265 568 267
 172 763 762  61 326 446  69 654  33  68  65   7 447 276  21 746 269 764
 546 569 115 227 407  72 425 637 464 198 572 543 462 571 287 731 623 711
 527 209 633 247 391 451 250 398 649 311 622 621 744   6 213 732 362 153
 459 266 710 461 765 771 347 270 148 562 726 539  49 147 525 204 291 344
 656  58 431 325 639 220 642 203 364 224 397 292 480 638 207 179 349 739
 416 644 231 730 738 423 277 742 275 524  11 547 677 575 248  41 676 736
 631 353 242 707 281 385 741 760 185 221 767 113 670 388 268 400 424 426
 338 456 188 279 280  51 180 350 240 733 222 570 625 415 673 150 445 768
 295 669 215  15 289  64 246 337 427 655 433 212 564 657 177 141 549 471
 290 314 640 565 577 418 127 187  91 126 183 154 713 740 573  80 342 351
 132 421 225 729  48 286 327  30  10 321  29 361 264 189 501 526 241 660
 133 369 643 544 392 777 773 413 238 129 131 145 182 389 630 420 429 155
 727 319 234 428 770 750 537 285 725 769 422 514 239 175 106 432  46 219
 252 363 717 419 754 716 743 528 186 701 721 249 395 168 318 566 167  67
 645  76 402  81 755 135 632 659 384 714 627 178 214 724 465 689  47 272
 430 163 328  22 288 581 373 394 343 728 341 243 274 316 692 502 563 749
 340 108 535 516 579 540 111  86 317 666 790 788 690 545  95 146 102 774
 217 244 484  31 550 399 161 548 734 705 671   8 512 745 333 709 346  90
 596 138  16 438 487 296 367 578 504 404 557 293 789 693 345 211 530 140
 107 636 772 775  45 254 718 634 315 320 119 629  27 747 245 162 403 588
 576  43 489 331 574 356 383 513 737 297 505 674 687 408  13 661 678 294
 786 164 542 466 675 757  14 590 393  24 722 109 118 323 460 223  92 580
 648 103 191 551  56 519 488 651  23  53 230 467 672 626 143 541 751 497
 753 748 779 778  34 704 332 756 752 664   0 495 158 160 396 375 173 216
 652 600 401 791 503 617 157 698 406 662 532  93 405 470 104 324  26 635
 591   9 376 695 486 608 142 322 197 523 529 776 366  32  36 614 518 312
 218 228 699 105 233 137 110 257  60  25 371  79 112  88 533  20 616 538
 703 715 556  35 251 667 647 136 329 334 613 237 121 443 599 134 780 494
 700 491 517 125 255 609 253  19 650 114  84 170 380 174 330   2  63 691
 681 236 120 531 511 117 685 605  28  94 615 235 441 597  77 378 368 602
 598 436 665  73 534 536 554 169  52 686 679 697 787 256  40  82 149 165
  38 193 374  37 708 719 499 498  12 589 437 658 365 151 442  18 194 702
 601 372 196 478 354 490 561 646 359 500 190 156 696 123 582  17 475 307
  85  83 607 171 604 122 152 166  97 683 496 493  42 468 510 610 603 508
 116  89  62 668 606 624 176  87 492  96 381 682 618  75 611 485 559 515
  39 357 313 509 434 192  44 663 684 688 553  57 101 680 435 552 558 382
 555 720 477 474 301 409 124 520 439 302 506 310 299 560 472 612 411 507
 440 308   3 712   1 410 358   5 469 483 195 412 444 628 594   4 304 476
 521 479 785 482 309 723 377 694 298 352 481 306 360 355 595 300 473 784
 303 781 522 305 144  98 583 348 783 592  99 593 782 100 586 585 587 584]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2095
INFO voc_eval.py: 171: [2896 1283 1336 ... 2810 2797 2316]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3273
INFO voc_eval.py: 171: [344 409 347 851 445 502 376 354  48 482 465 449 628 550 142 350 503 407
  62 443 157 779 140 541  45 627 507  49 148 278 156 200 514  40 377 523
 853 151 451 235 741 847 356 802  47 274 124 793 412 408 450 138 201 462
 803  64  44 527 775 776 149 143 478 841  39 518 856 268 467 345  61 790
 820 433  50 375 487 844 372 217 808 441 832 355  81 470 625 145 765  52
 626 245 737 777 543 849 234 647 222 223 159 269  67 534 584 516  63 464
 551 415 246 860 241 668 484 547 506 486 420 603 416 357 827 858 852 259
 237 787 150 281 436 346 221 141 721 463  53 195 831 675 452 413  43 646
 850 414 122 203 472 210 236 475 674 432 521 546 435 336 616 147  60 780
  51 295 666  83 250 825 277 325 611 826 738 661 778 828  30 190 665 225
 600 351 707 812 128 809 593 588 798 706 218 764 763 512 213 789 733 602
 429 154 651 257 155 211 196 438 609 447 158 548 263  66  68  84 193 774
 385 227 411 380 226 749 867 280 431 471 348 430 410 842 439 545 418 817
 126 810 202 224 373 381 515 144 638 422 708 153 786 676 204 146 127 805
 766 319  46 773 132 139 192 434 161 215 854 743 123 622 608 806 865 160
 818 554 635 342 598 293 335 437 745 423 228 248 840 444  55 312 308 830
 260 624 113 272 648 107 750 349 265 816 705 173  57 645 685 700 421 804
 855 270 109 513  59 811 266 481 152 383 287  14 352 288 591 440 702 292
 137 740 574 490 781 191  41 870 542 333 784 125 194  65  86 129 525 633
 243 279 618 698 677 242 725 442  90 131 575  58 782 813 282 829 682 302
 230  24 604  29 220 791 520 121 468 837  88 650  56 283 271 419  92 726
 180 353 232 728 479  21 231 861 239 291 672 119 607  18 739 836 788 792
 276 244 275 617 864 623 114  85 427 667 744 134 570 573 273 620 671 597
 384 212 678 327 859 301 110 783 360  13 683 681 286 585 835 247  54 229
 108 553 833 476 111 249  87 614 619 670 208 785 613 529 612 569 214  99
  11 556 425 446 300 632 544 365 621 117 341 655 704  28 216 460 314 448
 251  31 219 258 428 313 712 751 374 724 390 673 549 538 605 754 252 307
 703 735 558 823 866 480 730 734 528   4 552 305 290 684 555 572 267 689
 863 209 819 100 731 264 822 115 182 417 821 843 653 862 343  17 533 814
 815 824 382 297 727 845 334 262  33 233 326 649  89 426 834 637 690 477
 686 406 557 261 857 571 729  82 679 328 294 691 663  91 846 848 519 595
 183 732 694 636 400 641 362 322 371 184 116 285 758 576   3 509 634 359
 120 807 130 696 868 459 102 869  19  69 338  25 590 508 680 399 871 299
 316 746   9 669 320   1 587   5 303 310 660 170  80 178 318 606  27 599
 596 393 539 504 736 642   2 564   0 361 256 240 337 532  77 610 526  42
 687 488 517 255 238 185 601 289 253 112 769 306 296 752 562 796 298 800
 317 363 284 615 207 103 331  78 695 498 323 494 364 753 324 453 491  38
 535  37 315 309 369 742  26 629 640 485  34  93  75 175 664 163 456  12
  22 715 565 540 118 395 169  96  97 639 797 688 135 644 710 759 366 304
 455 536 187 133 340 496  16 756 164 589 537 424 652 311 162 658 505  32
 466 454 205 199 368  23 643 254 659 483 179 499 559  72 522 560  76 748
 795 458 136 177 398 692 713 530 405 401 474 172 402  94 367 594 493 378
 492 403 794 801  20 654 188 101  95 716 580 577 358 489 469  73 581 457
 656 171 586  71  79 473 181 461 174 583 511 747 561 531 657 582 321  74
 176 662 388  98 105  70 592  15 711 198 206 761 495  36 510 579 699 186
 189 197 701 566 693 762  35 757 524 106   6 568 404 389 329 799 709 497
 386 630  10 723 104 379 578 770 760 755 370 631 772 717 339 332 722   8
 168 714 563 720 387 719 330   7 567 391 718 167 166 165 768 397 771 500
 767 501 396 394 838 392 839 697]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2136
INFO voc_eval.py: 171: [133 953 436 ... 567 767 768]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3040
INFO voc_eval.py: 171: [ 48 280 267 146 151  27  47  49 155  52 147 287 120  73  35 163  69  32
  79 275 149  60  58 193  57 172 255 118 274  75 268  34 165  54  19 202
 176 210 197 134  37 276 277 246 258  26 290  33 166 259 249 269  31  78
 271 284 152 119 291 154 160 288   6 201 158  53  16  83 169  62 116 220
  51 148  22 283 200  85  63 188 230 101 150 195  76 208 115  86 145 272
 292  61 162 199  18  44 248 114  10 237  89 266  99 273 117  36  68 217
 110 247 170 129  24 223 270 260  28 156  17  66 171  55  67  81  59 250
 189  96 167 205 138  80 128 153 251  71 190  12 225   4 236 253  39 245
  30   0 264 241  94 289  20 159 278  14 100  40 198 112 157  65 102 257
 135 203   5  77 161  43   8 143  91 226  72 279  11  95 142 107 212 141
  21  29 234  38  93 192 222 243  82 261 173 285 224 105  25 215  70 207
 254 139 106 256  15 181 191 185  97  23 168  87 104 137   2   3 144  50
 174 103 219   9 177 122 232 111 196 108 282 286 214 124  88  41 109   7
 126 186  46 229  84 178 239 240  56 175  92  13  74 127  42 180  90 187
 179 113 221 216 242 131 218 204 182 263 209 213 211 184   1 121  45 130
 228 227 235 164 206  64 125 183 194 281 231 238 252 233 133 123 262 132
 244 140  98 265 136]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4980
INFO voc_eval.py: 171: [ 8539 13973  1052 ... 10834   771 10836]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5399
INFO voc_eval.py: 171: [2492  717  259 ... 2709 2697 1830]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4310
INFO voc_eval.py: 171: [ 65 318 542  33 337 551  66  56 135  40 545  49 341 334 546  44 322 622
 543 159  34 350  46 485 223 332 138 555 346 547 257 316 161 344 556 550
 413 127  36 652 287 141 349 640 419 548 310 369 629  70 429 149 148 326
  67 335 165 544 425 140 353 160 493 628 230 229 321 630  60 270 498 228
 125 568 414 323 627 162 338 618 124  51 565 631 486 157 255 231 222 314
 418 468 633 258 644 336 570 355 320 624 167  39 625 532 352 220 626 143
 503 559 236 422 339 495 331 654 168  45 504 348 197 132 421 643 368 370
 325 142 137 496 521  43 621 319 374 269 315 261 237 256 292 634 333  62
  64  91 146 317 166 647 225 655 499  96 136 415 484 581  92 133 416 221
 233 288 651 641 347 351 235 680 428 677 330 126 154 345 313 636 574 247
 340 259 311 513 234 426 145 509 566 649 147 152 129 324 164 312 139 514
 367 571 175 619 439 282 417 268 328 615 623 342 639 575 144 516 637 170
 286 569 529 224 266 327 412 632 343 131 688 208 678 373 200 675 267 329
 638 153 128 645  78 533 554 676 420 635 616 560 648 490  42 226 130 646
 307 272 491 169 552  50 163 358 201 587 650 605 470 466 453 573 497 508
 232 102 487 103 653  14 297 576 511 382 263 203   5 502 642  27 357 512
 194 202  63 617  55  35 371 446  69 271 469 567 583 383 110 549 158 385
 354  38 681  24 273 359 620 248 188 526 207 122 558 174 494 196 657 183
 360 260 121 198  32 356 227 483  95 171 278 303 264 245   0  94  90  93
 690 517 522 535 195  23 199 524 262 510  15 190 686 249  71 381 120 265
 277 679 423 274 305 656 389 398 561  37 481 471  47 662  22  97 275 658
 177 134 445 523 432 289 375  13 363 180 687 525 444   8  87 578 563 156
  30 488  48  79 304 527 250 433 172  61 107 584 441 123 450 449  77 473
 109 252 185 113 595 562 515  81  53 184 683 112 452 106 427 438 507 463
 217  25 518 604 564  84 173 302 553 214 440 187 296 611 442 492 210 465
 100   2 281   1 689 460 464  20 478 661 430 609 215 691  57 520 585  16
 606 285 659  17 519 212 372 400 588 684   7   6 482 614 178 528 582 685
 308 692 213 410 253 402 448 557 501 243  18 480  58 579 451 530  73  76
 610  74 108  68 580 479  19 607  31  41 280  21 660 534 111 284 283 489
 572 291 376 295  80 447  75 682  52 405 506 186 309 276 474  29  72 118
 115 424 114 209   3 458 608 377 251 182 531  88  59 290 435 505 592 254
 294 500 603  54 431 390 437 476 399 306  26  98 293   4 155  28 362 193
 279 443 379 602 364 192 211 599 119 408 467 434 204 365  12 462 436 406
  82 219 380 361 596  86 378 387 591 539 667 594  99 613 593 404 589  83
 218 205 179  89 411 577 598 216 477 590 101 116 117   9 475  85 665 586
  10  11 394 239 600 366 537 669 181 105 384 597 455 391 538 612 240 191
 601 238 403 388 241 396 176 206 668 244 298 189 246 301 666 472 674 242
 397 536 401 386 393 392 407 150 672 671 151 461 395 300 299 409 456 664
 670 454 540 541 457 459 673 104 663]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3045
INFO voc_eval.py: 171: [ 38   4  42 113 178 172 268  43 118 328 182   7  45 176 225  47 147 151
 179  11 184 324 145 148  37  55 270 331 170 141 119 175 222 212 146 266
 174 133 220 263 226 264 235 140  95  74 267 120  72 239 327  44  94 269
   6  53 116 244  99 112 152  54 142 221 153  21 183 320 164 173 137 121
 316  52 203 260 236 296 126  39 317 300 165 177 224 171 228 234  66 100
 180  69 223  41 206 149   5 111 156 181  93 150 259  90 124   9  46  97
 241 238  12 326  98 313 311 242 251 122 330  83  24 319 237  13 256 135
  68  91 278 240  67 114 243 227 248 314 322 321  40 199  96 329 117 143
 202 298  84  77  58 210  78 160 233 310  92 252  56 193  23 332 283 303
 257 107 250 190 219 297 284 218 186  25  73 109   8 214  79 110 138 106
 115 185  10 125 273  89 201 209 230 312 247 157  76 302 123  70 294 258
 108 246 139  57  30 144  50 323 231  71 304 253 254 265 288 215 318 325
 213 307 306 315 261  36 299  82  27 136 217 249 191 279  26  22 333 287
  59 292 134 207 286 204 166 208 161  49 105  28 163 229  48 216 211 285
 195 104  51 305  75 188  29  20 272   2 295 127 280 132   3   0 255 245
   1 187 192  80 200 289 271 159  16  81 130 102 197 198 290 293 189 101
 158 282 232 194 154 168 291  17 274  33 128  19 129 196 275 167 162 103
 262  15  18  32 131  14 281  34  65 169 155 277  63 276 308  31 301 309
  64  61  35  60  86  62 205  88  87  85]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1326
INFO voc_eval.py: 171: [636 377 644 398 120 649 376 314  86 205 286 126  80  82 170 890 641 642
 287 316 313  89 122 182 392 210 129 798  98 290 208 794 503  87 383 535
 206 634  88 190 450 638 657 448 378 672 293 102 204 451 124 395 315 905
 121 319 189 898 892 299 330 177 289 826 508 801 288 382  93 462 873 521
 845 900 706 836 544 167 397 139 214 140 201 216 831 390 904 294 899 524
 637 452  64 324 540 486 384 860 428 743  45 381 318 105  20 117 215 172
 380 303 841 808 141 677 879 196 317 727 707 193 656 664 453 895 800 110
 796 828 175  84 455 506 197 274 438 528  90 257 903 111 171 168 814 830
 432 434 343 183 219 752 306 728 227 186 101  72 620  46 298  21 507 393
 875 169 536 749 871 821 906 523 198 893 300 539 207 322 179  35 717 911
 301 887 185 166 261 551 737 174 733 835  67 710 443 459 242 133  77 876
 212 195 292 825 239 891  68 449 454 108 485 509 297 832 781 778 894 246
  44 211 224 458 865  58   2  91 184 447 542 538  97 908 713 379 388 368
 812 878  53 699 844 716  18 181 702  96 497 709 327 600 180 577 259 686
 708 897 256  99 307 682 505 885 525 756 635  19 924 867 872 639 331 160
 481 178  59 588 394 431 829 203 862 427 464 751 401 779 823 730 312 846
 371 115 320 909 138 868 104 824 456 347 512 483 435  52 769 309 811  74
 675 231  43 734 396  33 386 439 230 605 817 463  79  40 870 625 678 173
 176 917 461 369 258 923 884 815 233 492   5 907 516 757 547 149 337 351
  37 199 103 429 537 902 273 248 504  75 857  66 350 853 607 426 803 558
 861 689 877 843 100 874 660 740 269 614 251 820 260 522 527 460 279 926
 665 910 295 128 466   7 683 742 146  85 818 118 374 209 159 164 545  57
 107 226 188 127 218 806 654 165 886 475 245 247 249 311 775 529  78 440
 572 262 754 617 517 685 494 533 726 851 684 916 822 552 541 349 901 883
  47 192 604 888 213 419  39 348 571 881 200 236 235   9 741 687 598 632
 234 735 142  76  94 834 465 753 488 302 223 731 562 162 694 680 238 217
 896 116 725 866 253 763 852 131 842 437  38 698 849 869 304 513 399 526
 240  56 621 574 325 436 889 457 565 559 612  42  55 308 807 156 601 430
 587 270 472 550 772  41 225 714 323 191 603 705 626 729   0 187 276  14
 387 925 648  92 599 805 619 372 704 602 785 786 519 816 840  83 819  36
 344 553 417  49 566 338 543 791 389 608  62 720 745  60 495 112 425 629
 927 194 922 391 433 119 385 499 850 882 155 736 411 679 339 767 329 491
 353 766 755 576  23 232 237 265 575 282 688 676 305  11 568 370 135 681
 515   3 784 412 557 271 585 623 768 275 782 833 711 788 418 848 770 780
 618 241 254 712 611 341 296 291 584 744 606 787 476  34 578 375 143 591
 272  61 863  54 244  32 280 354 441 732 579  63 783 445 573 342 701 918
 310 373 739 703 277 719 416 361 693 609 610 646 477 839 697 674 837 723
 202 750 700 633 266 498 114 765 420  48 283 157  95 482 795 109 113  22
  31 880 556 487 511 615 360 474 864 746  69 624 400 106  81  73  71 912
  17 548 804 777 813 748 570   8 802 267 363 321  24 530 563 414 250 228
 471 564 442 792 789   1 590 514 582  15 229 534 586 721 335 278 243 580
 555 470 444 468 263 919 838 567 569  70 554 281  16 346 773 747 809 858
  65 415 669 776 446 352 500 774 722 334 613 489 653 358 252 490 255 163
 501 799 622  51 467 738 810 666  10 531 593 797 264 695 408 502 696 793
 469 407 473 362 493 724 546 145 518  12 480 827 581 596  50 423 154   6
 484  13 367 913 366 153 715 592   4 284 589 496 152 771 790 422 520 668
 764 718 549 532 136 595 616 510 859 478 847 645 424 692 285 365 594 421
 479 364 402 158 345 762 404 355 583 326 356 643 627  30 161 357 658 336
 410 659 409 359 220 761 920 148 328 151 137 413 630 915 854 560 561 655
 403 631 921 597  29 662 628 332 663 123 150 222 759 856 760 691 758 855
  25 661 333 340 406 405  28 147 690 221 670 132 268 651 125 134 640 647
 144 130 914  26  27 652 673 650 671 667]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2665
INFO voc_eval.py: 171: [428 256 603 177 442 304 141 450  25 477   1  38 486 506 260 327 357 483
 488 329  40 404 446 258 254 426 355 356 130 128 478 585 316 559 305  59
 358 269 616 429  71 259 543 110  74 133 330  31 236   3 323 605 453 527
 447  17 438 335 182 485 346 593 354 185 310  44 326 604 633 624 159 379
 232 105 363 614 619 373 265 154 382 544 314 449 377 226 112 113 452 142
 211 481 215 328  88  19 270 494  77  28 229  73 149 444 165   8  53 565
 308 513 541 312 473 479 212 279 492 387 383  60 106 482 471 240 403 622
 331 370 367 401  41 364 191  30 372 249 123 638 338 167  26 220  16 491
 166 493  48 628  58 435  46 180 132 608 306 413 320   6 601 307 214 480
 293 511 521 263 433 586 351 484 551 612  42   9 311 313  75  55 489 230
 321 147 122 578 568 510 569 104 219 378  50 216 344 234 181 440 318 315
 526 336 218 637 324 129 406 384 222 175 629   2 221 434 487  39 163 194
 135 217 271 625   5  32 120 178 587 176 590 197 589 389  45 231 119 233
 124 618  94 562 361 362  78 443 371  82 402 278 109 522 490  12 412 556
 187  29  47  92 127 557 341 309 366 607 504 287 563 632 405 299 572 514
 190  24 422 617 407  18 183 571 460 620  13 290 115 634 554 627 164 448
 349 134 103 158 317 462 325 213  57 549 547 508  91 174  97 538 523 570
 545 623 606 150 203 531 291 564 243 419 542 368 286 283 148 101 505 284
 410 282 635  98 393 497 630 621 365 369 581 534 615  95 631 245 509 507
 153 577 117 223 196 461 340 225 114 385  34  86 594  89 160 285  56  66
 339 525 288  81 596 550 512 210 262  93 626 224 374 319 227 575 458 540
 375  43 390 322 529 143 152 281 464 524 268 427  64 553 552 131 640 118
 280  96  79 376 292 360 184  80 303 518 381 107 558 456 437 121  84 548
 192  23 274 261 235 459 576 430  36 580  63 416 171 337 386 388 445 359
 380 126 138 208 636 207  52 275 162 247 454 465 111 598 641  54  90 289
 439 294 397   4  35 146 472  33  37 599 264 239 436 332 600 532  15  62
 457 298 277  11  67 345 343 155 108 584 170 151  72 248 206 125 639  85
 474 161 561 613 334 237   7 347 431 418 144 137 595 116 495 555  22 496
 468 567 574 157 539 546 156  27 537 202 195 244 295 432 145  61 139 455
 400 536 246 140 297 566 189 583 535 517 411 415 333 301  87 186 417 241
 205  99  70 441 255 451 228 302 414 266 530  69 188 573 173 172 136 409
 395  51 100 475 204 300  10 169 528 267 238 179 420 257 560 348  76 592
 398 396 350 533 168 352 342  83 579 519 597 193 408 394 582  65 276 421
  21 515 102  68 467 242   0 392 516 296 353 520 253 591 399 391 500 252
 463 466  20 611 610 251 609 273  49 469 498 499 423 199 470 588 201 272
 424 602 209 503 250 501  14 425 198 200 502 476]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3366
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3049
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.250
INFO cross_voc_dataset_evaluator.py: 134: 0.399
INFO cross_voc_dataset_evaluator.py: 134: 0.197
INFO cross_voc_dataset_evaluator.py: 134: 0.235
INFO cross_voc_dataset_evaluator.py: 134: 0.330
INFO cross_voc_dataset_evaluator.py: 134: 0.424
INFO cross_voc_dataset_evaluator.py: 134: 0.290
INFO cross_voc_dataset_evaluator.py: 134: 0.097
INFO cross_voc_dataset_evaluator.py: 134: 0.312
INFO cross_voc_dataset_evaluator.py: 134: 0.209
INFO cross_voc_dataset_evaluator.py: 134: 0.327
INFO cross_voc_dataset_evaluator.py: 134: 0.214
INFO cross_voc_dataset_evaluator.py: 134: 0.304
INFO cross_voc_dataset_evaluator.py: 134: 0.498
INFO cross_voc_dataset_evaluator.py: 134: 0.540
INFO cross_voc_dataset_evaluator.py: 134: 0.431
INFO cross_voc_dataset_evaluator.py: 134: 0.305
INFO cross_voc_dataset_evaluator.py: 134: 0.133
INFO cross_voc_dataset_evaluator.py: 134: 0.266
INFO cross_voc_dataset_evaluator.py: 134: 0.337
INFO cross_voc_dataset_evaluator.py: 135: 0.305
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 5999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.371s + 0.035s (eta: 0:00:50)
person 0.969827
person 0.96344614
person 0.9922464
person 0.96049577
person 0.97312057
person 0.9258749
person 0.9372633
person 0.98211735
person 0.9446465
person 0.9108441
person 0.9815539
person 0.93869287
bottle 0.92259437
bottle 0.91977274
bird 0.9516031
person 0.9824812
person 0.9179493
person 0.9440747
person 0.90380436
person 0.9049836
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.320s + 0.037s (eta: 0:00:40)
person 0.94433266
person 0.9184333
person 0.9597146
person 0.9457862
person 0.98061395
person 0.947298
person 0.9679487
person 0.98172516
person 0.93949205
person 0.9213142
person 0.9282924
person 0.93153566
person 0.90582335
person 0.9224584
person 0.92048115
person 0.9265606
person 0.9748819
person 0.9114298
person 0.9802295
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.322s + 0.037s (eta: 0:00:37)
person 0.92880857
person 0.9127481
person 0.95546204
chair 0.91401774
person 0.9722056
person 0.9124408
person 0.9546296
person 0.9657147
chair 0.9656913
chair 0.9373178
chair 0.91449463
person 0.90234363
person 0.9790526
person 0.9879469
person 0.92911696
person 0.903192
person 0.9767263
person 0.9916015
person 0.98146063
person 0.91298753
person 0.9075864
person 0.9080472
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.324s + 0.039s (eta: 0:00:34)
bird 0.9383146
person 0.9273084
person 0.92127585
person 0.99380577
person 0.9563846
person 0.9746823
person 0.98005325
person 0.90933067
person 0.9594122
person 0.9657858
person 0.90600485
person 0.91778797
person 0.9697202
person 0.9401949
person 0.9588126
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.325s + 0.038s (eta: 0:00:30)
diningtable 0.9006611
diningtable 0.9393435
chair 0.9634684
chair 0.96581715
chair 0.9380172
chair 0.9570086
pottedplant 0.9182995
pottedplant 0.914824
pottedplant 0.9104535
pottedplant 0.904652
pottedplant 0.9436777
pottedplant 0.9380292
pottedplant 0.92990965
person 0.9656763
person 0.9827771
person 0.9213938
person 0.9796062
person 0.98886967
person 0.9146206
person 0.9031247
person 0.9125572
person 0.92002696
person 0.920788
person 0.9728538
person 0.9543058
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.324s + 0.038s (eta: 0:00:26)
person 0.95638305
person 0.92260337
person 0.90429926
person 0.9203624
person 0.94578147
person 0.9126583
person 0.95314705
person 0.93620473
person 0.92828655
bird 0.92262316
person 0.9351866
person 0.98763263
person 0.9170445
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.320s + 0.037s (eta: 0:00:22)
pottedplant 0.9037883
person 0.9620828
person 0.91909325
person 0.91012245
person 0.95382464
person 0.91768146
person 0.9585848
diningtable 0.9081608
chair 0.9826894
diningtable 0.9605568
chair 0.9168521
chair 0.9480392
chair 0.9805793
chair 0.92784166
bird 0.993082
person 0.9017824
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.320s + 0.037s (eta: 0:00:19)
car 0.92854875
car 0.94171226
car 0.9132721
person 0.988509
person 0.90028083
person 0.9740737
person 0.9295599
horse 0.97434705
horse 0.9139075
horse 0.9206463
person 0.9378821
person 0.95333195
person 0.96827376
person 0.9180449
person 0.9254218
car 0.9457995
car 0.9645912
bicycle 0.9613853
bicycle 0.93037325
person 0.9377262
person 0.9163656
person 0.9730758
chair 0.9177231
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.317s + 0.036s (eta: 0:00:15)
person 0.9742827
person 0.95875627
person 0.97336143
person 0.91518176
person 0.91204196
person 0.9885666
person 0.9863637
person 0.9431206
person 0.91681683
person 0.9642932
person 0.9634805
person 0.991131
person 0.906397
person 0.9667796
person 0.9110046
person 0.98766077
person 0.979963
pottedplant 0.91268826
bus 0.91295683
person 0.9899451
person 0.96177775
person 0.94108766
person 0.95388967
chair 0.9458301
person 0.90566903
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.321s + 0.036s (eta: 0:00:12)
motorbike 0.9642619
person 0.974697
person 0.9112168
person 0.9145913
person 0.9560883
person 0.96650857
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.319s + 0.036s (eta: 0:00:08)
person 0.90346915
boat 0.9819318
boat 0.9479434
boat 0.918015
person 0.9902955
person 0.9372805
person 0.9855714
person 0.918106
person 0.92175174
person 0.9889225
person 0.97497636
person 0.9098436
person 0.9744485
person 0.9764939
person 0.9124129
person 0.94860286
car 0.9504959
person 0.93839556
person 0.94677323
person 0.95997024
person 0.9867084
person 0.9051542
person 0.91975814
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.320s + 0.036s (eta: 0:00:04)
diningtable 0.95899045
chair 0.92374146
chair 0.97388816
chair 0.98078156
chair 0.9137259
chair 0.9808818
chair 0.9359147
chair 0.96045995
person 0.9415532
bird 0.9237426
bird 0.913214
boat 0.919992
person 0.96193314
person 0.9279568
person 0.98866653
person 0.91333634
person 0.90099525
person 0.9113889
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.317s + 0.035s (eta: 0:00:01)
person 0.9834077
person 0.9643658
person 0.9560605
person 0.9554663
person 0.94851846
person 0.96549225
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.307s + 0.026s (eta: 0:00:41)
person 0.9818162
person 0.91779923
person 0.97875947
person 0.97709626
person 0.9133122
bicycle 0.9725652
person 0.9315583
person 0.9129215
bicycle 0.9746085
person 0.9344094
bird 0.9434174
bird 0.9001241
pottedplant 0.9438427
person 0.97033286
person 0.92841834
person 0.90259326
person 0.96038944
person 0.9654988
person 0.9867209
person 0.9274406
person 0.96598387
person 0.95251095
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.300s + 0.035s (eta: 0:00:38)
person 0.9125858
person 0.9068036
person 0.9639183
person 0.91734797
person 0.9731689
person 0.94097084
person 0.93647325
person 0.9806969
person 0.9644631
chair 0.9261342
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.322s + 0.037s (eta: 0:00:37)
car 0.915931
car 0.94985616
person 0.9475345
person 0.9610109
person 0.98141885
person 0.9221219
person 0.9482291
person 0.9643542
car 0.9821976
cow 0.9186925
person 0.9158734
person 0.957073
person 0.9224577
person 0.9116712
person 0.9805571
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.318s + 0.038s (eta: 0:00:33)
person 0.9780156
chair 0.93339515
person 0.9293382
person 0.9263278
person 0.95112246
person 0.92570764
person 0.9317551
person 0.9181634
person 0.93964905
person 0.9806512
person 0.9868728
person 0.97706956
person 0.96891934
person 0.97691786
person 0.90213716
bird 0.90623873
bird 0.99079025
person 0.9215567
bird 0.9516252
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.325s + 0.037s (eta: 0:00:30)
person 0.94498646
person 0.9706407
person 0.9825362
person 0.92262286
person 0.94688207
car 0.9504959
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.330s + 0.037s (eta: 0:00:27)
bird 0.9110861
bird 0.9601566
bird 0.9136508
person 0.9120285
person 0.9434471
person 0.9156154
person 0.9148127
person 0.9882053
diningtable 0.94819015
person 0.9720906
diningtable 0.97844267
person 0.9219636
person 0.93481475
chair 0.9425315
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.330s + 0.036s (eta: 0:00:23)
person 0.9128867
person 0.903995
person 0.92658466
person 0.9468558
person 0.9856736
bus 0.9472197
dog 0.9519249
diningtable 0.97351915
diningtable 0.92111933
chair 0.9114711
chair 0.98494697
chair 0.91231406
chair 0.97054696
chair 0.93552274
pottedplant 0.9027625
person 0.90534204
person 0.9758236
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.326s + 0.035s (eta: 0:00:19)
diningtable 0.9202153
chair 0.9097864
person 0.9473008
person 0.96625775
person 0.9545184
person 0.9360755
person 0.90389216
person 0.9446739
bird 0.95508176
person 0.9256238
person 0.96185726
person 0.96364933
person 0.9310704
person 0.9803623
person 0.9076638
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.325s + 0.036s (eta: 0:00:15)
person 0.9626069
person 0.962355
person 0.9030687
train 0.9584311
person 0.9569905
person 0.9624559
person 0.9469499
person 0.93844473
person 0.91173404
person 0.932314
person 0.928633
person 0.9047865
person 0.9558814
person 0.92945033
person 0.9924049
person 0.9741217
person 0.960951
person 0.9172521
person 0.9274144
person 0.9010802
person 0.9282291
tvmonitor 0.91711974
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.324s + 0.035s (eta: 0:00:12)
person 0.9635217
person 0.90497667
person 0.9441599
horse 0.96000665
person 0.9664864
person 0.92965025
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.323s + 0.035s (eta: 0:00:08)
person 0.97510135
person 0.9655843
person 0.92212784
person 0.9195361
person 0.95062923
person 0.9260428
person 0.9357233
person 0.90225905
person 0.9138517
person 0.9866153
person 0.9446941
chair 0.97206444
person 0.956127
person 0.90392923
person 0.94516736
person 0.9582858
person 0.90528244
person 0.9168829
car 0.94539607
car 0.917206
car 0.95133525
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.320s + 0.035s (eta: 0:00:04)
person 0.9679612
person 0.94339234
person 0.9621147
person 0.94869417
person 0.97350264
person 0.9602048
person 0.9204246
person 0.90401274
person 0.9262468
person 0.9881455
person 0.98159987
person 0.9181579
person 0.97811735
person 0.9275407
person 0.93741417
diningtable 0.93254846
person 0.91679364
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.315s + 0.035s (eta: 0:00:01)
person 0.9073573
diningtable 0.9133812
person 0.9702535
person 0.93952835
person 0.987377
person 0.9660708
person 0.958349
person 0.95805925
person 0.90238625
person 0.9064197
person 0.9074047
person 0.941024
person 0.96357065
person 0.98118114
person 0.9088777
person 0.9488399
person 0.95666426
person 0.955765
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.426s + 0.038s (eta: 0:00:57)
diningtable 0.9008175
diningtable 0.9261109
person 0.9603156
person 0.97591114
diningtable 0.9727032
person 0.9220306
person 0.9192975
person 0.90530825
bottle 0.9066465
person 0.977742
pottedplant 0.9178374
diningtable 0.9257634
person 0.9657576
diningtable 0.9055591
chair 0.94973594
chair 0.9658076
chair 0.921968
person 0.9165097
person 0.9056666
person 0.9313555
person 0.9957409
person 0.98081094
person 0.9333911
person 0.9664061
person 0.96490633
person 0.90436566
person 0.97055537
person 0.9592429
person 0.9374601
person 0.9359386
person 0.96502453
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.340s + 0.035s (eta: 0:00:42)
person 0.94613576
person 0.9859315
person 0.93303376
person 0.9704972
bottle 0.9373289
person 0.9359242
person 0.9422324
person 0.9528426
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.326s + 0.038s (eta: 0:00:37)
chair 0.93313724
bottle 0.9093839
boat 0.90022236
person 0.97223806
person 0.925747
person 0.93862635
person 0.93092513
person 0.95165604
person 0.9303047
person 0.96753895
person 0.94276583
bottle 0.90737367
bottle 0.9530119
person 0.9821714
person 0.95330226
person 0.949055
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.334s + 0.037s (eta: 0:00:34)
person 0.9660303
person 0.92049307
bird 0.9187078
person 0.9537315
person 0.90519863
person 0.9670669
person 0.97016174
person 0.9592677
person 0.90245646
person 0.9353083
person 0.90593994
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.328s + 0.036s (eta: 0:00:30)
person 0.9694062
person 0.9693123
person 0.95387125
person 0.9050241
person 0.9291566
person 0.9705384
person 0.92027736
person 0.9411041
person 0.9136446
person 0.9181926
person 0.93766785
person 0.9697771
person 0.9002509
person 0.9533982
person 0.9251293
person 0.90403277
bottle 0.95366174
chair 0.931535
boat 0.90485054
person 0.97333115
person 0.9546897
person 0.90184766
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.323s + 0.036s (eta: 0:00:26)
car 0.9806887
car 0.93526024
person 0.9255087
person 0.9770825
person 0.9097393
person 0.9085547
person 0.91955036
person 0.9343547
person 0.9647545
person 0.9227862
person 0.9159137
person 0.9567561
pottedplant 0.91620827
pottedplant 0.93218064
car 0.98483974
car 0.9383113
person 0.93158066
person 0.9878106
person 0.9541473
person 0.92138064
person 0.9369996
person 0.9859051
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.324s + 0.036s (eta: 0:00:23)
person 0.97974235
car 0.9593287
person 0.9501553
person 0.92467976
tvmonitor 0.926032
person 0.90425205
person 0.9307872
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.323s + 0.036s (eta: 0:00:19)
chair 0.9027001
person 0.97066057
person 0.94360244
train 0.9182506
train 0.96083814
bird 0.9107829
bird 0.9828174
person 0.9215326
person 0.9225058
bicycle 0.9224503
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.318s + 0.036s (eta: 0:00:15)
person 0.9692842
person 0.901618
person 0.93664527
person 0.9293494
person 0.9293922
person 0.9784037
person 0.9866475
person 0.9583284
person 0.9728042
person 0.98934424
person 0.9096511
person 0.95492595
person 0.96545726
car 0.95126784
person 0.9236464
person 0.96891093
person 0.9498155
person 0.9771304
person 0.9322365
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.316s + 0.036s (eta: 0:00:11)
person 0.9147071
person 0.94518036
person 0.92320293
chair 0.975271
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.317s + 0.035s (eta: 0:00:08)
person 0.98124206
person 0.93955636
person 0.9271313
person 0.96304643
person 0.9621214
person 0.9020613
person 0.92602146
person 0.90712917
person 0.9386937
person 0.92012405
person 0.97882026
person 0.90488255
person 0.90726537
boat 0.9031673
person 0.96668774
person 0.9231064
person 0.94618434
person 0.9107641
person 0.9581557
person 0.9025497
person 0.9250193
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.316s + 0.035s (eta: 0:00:04)
person 0.96728814
person 0.9733185
person 0.977994
person 0.9485673
person 0.9198991
person 0.930462
person 0.92444366
person 0.950324
person 0.9604054
person 0.9176434
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.318s + 0.035s (eta: 0:00:01)
person 0.9359215
person 0.97136277
person 0.92234236
bird 0.90395254
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.407s + 0.028s (eta: 0:00:53)
person 0.9410196
chair 0.9187838
chair 0.95781785
person 0.9583071
person 0.9261505
person 0.9535512
person 0.9398981
diningtable 0.93117195
person 0.9740995
person 0.9711275
diningtable 0.93545806
person 0.9525018
person 0.96805745
person 0.90544826
diningtable 0.911683
person 0.9288524
person 0.96502787
diningtable 0.96515423
diningtable 0.92741084
diningtable 0.9394663
person 0.9523991
chair 0.9142816
chair 0.9812398
person 0.9069383
person 0.9299537
person 0.9890919
person 0.92129797
person 0.9618238
person 0.9852519
person 0.91672933
person 0.92362267
person 0.9836031
person 0.9681496
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.326s + 0.034s (eta: 0:00:41)
person 0.93886685
person 0.95339316
pottedplant 0.93367064
person 0.90312916
bird 0.97191936
bird 0.94909346
bird 0.9189279
person 0.9673911
person 0.9208198
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.314s + 0.035s (eta: 0:00:36)
person 0.95025504
person 0.90895426
person 0.96212566
person 0.9695901
person 0.97612816
person 0.9475832
bottle 0.9048246
bottle 0.95184684
bottle 0.90159523
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.316s + 0.035s (eta: 0:00:32)
person 0.90762395
person 0.9146298
diningtable 0.908662
pottedplant 0.9318763
chair 0.93947214
chair 0.9160841
person 0.97599834
person 0.93998665
bottle 0.91054666
bottle 0.90814686
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.313s + 0.035s (eta: 0:00:29)
person 0.96233714
person 0.9603527
person 0.9481469
person 0.9948119
person 0.9422436
person 0.9407298
person 0.92007047
person 0.97939235
person 0.9109098
person 0.95765895
person 0.9727941
person 0.91486835
person 0.935247
person 0.93748087
person 0.992044
person 0.9402083
person 0.97356737
person 0.92561287
person 0.96151555
person 0.98376274
person 0.9656212
person 0.9269978
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.307s + 0.035s (eta: 0:00:25)
person 0.9641953
person 0.9710679
person 0.9033905
diningtable 0.98365086
chair 0.9158457
chair 0.94773036
chair 0.97884494
chair 0.9873661
aeroplane 0.9130566
person 0.9665672
person 0.98131543
person 0.9311847
person 0.9151299
person 0.911357
person 0.9470535
person 0.9736222
car 0.92789656
car 0.98019904
car 0.9717937
car 0.95845586
bird 0.9581094
chair 0.93075776
chair 0.92972773
person 0.9445119
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.307s + 0.035s (eta: 0:00:21)
person 0.9837947
person 0.93460315
person 0.9061486
person 0.9815383
person 0.958143
person 0.9896359
person 0.93637323
person 0.9253776
person 0.9406354
person 0.99024045
person 0.93829954
person 0.9068352
horse 0.96237653
person 0.91180414
person 0.9054699
person 0.91965556
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.306s + 0.034s (eta: 0:00:18)
person 0.95412046
person 0.9449618
chair 0.9752601
chair 0.9210198
chair 0.95765495
pottedplant 0.9260019
pottedplant 0.9221256
pottedplant 0.94505376
person 0.9417724
person 0.90136904
person 0.92126703
person 0.93622065
person 0.95876676
person 0.9388235
person 0.9321357
person 0.90098786
person 0.90201586
person 0.9549417
bicycle 0.91549015
person 0.9110788
person 0.97970355
person 0.90122706
person 0.94094497
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.305s + 0.034s (eta: 0:00:14)
person 0.9881566
person 0.91852576
person 0.90202975
person 0.91565406
person 0.9589031
person 0.9851215
person 0.94040996
person 0.9768678
person 0.9777765
person 0.9717523
person 0.9375685
person 0.9203504
person 0.9141881
bus 0.9548798
person 0.90550584
person 0.9471461
person 0.9389707
person 0.9644914
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.308s + 0.034s (eta: 0:00:11)
person 0.91176885
person 0.9681082
person 0.91237706
person 0.9343607
chair 0.9130313
chair 0.9486832
tvmonitor 0.9019636
chair 0.9709709
chair 0.9327127
chair 0.94070345
person 0.9476962
person 0.96941334
person 0.9563482
person 0.9772939
person 0.9201887
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.305s + 0.035s (eta: 0:00:08)
motorbike 0.91566545
motorbike 0.95187503
person 0.9001587
person 0.9784581
horse 0.9572989
horse 0.90240586
person 0.9025747
bicycle 0.95852214
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.305s + 0.035s (eta: 0:00:04)
person 0.92402035
person 0.9588904
person 0.98092103
person 0.94904536
person 0.9640484
person 0.9840004
person 0.94874704
person 0.9552054
person 0.942079
chair 0.9601341
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.305s + 0.035s (eta: 0:00:01)
person 0.90472025
person 0.9863107
person 0.98571
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 85.497s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1190  286  282 ... 1014 1013 1012]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2499
INFO voc_eval.py: 171: [126 127  71 517  70 245 457 293 271 130 138 128 487 570 388  73 398 547
 575 494 461 573 568  74 515 577 386  72 558  86 483 134 274 136 574 585
  90 462 579 535 569 135  75 590 259  97  81 132 383 571 390 523 302 466
 384  78  92 139 527 560 129 549  93 473 294 304  76 246 476 567 251 144
 467 472 553 519 465 443 103 548 591 478 550 141  19 152 145 382 488 303
  43 524 471 393 295 546 249 250 243 415 285 247 348 526 389 429 555 300
  80 253 584 581 463 539  22 105 509 418 468 387  21 275 150 286 551 434
 310 147 107 441 307 522 518 140 164 269 276 131 521 273  41 464 309 481
 582 563 392 587 354 407 165 552 258 343 199 424 201 545 208  89 220 477
 151  37 444 576 470  25  28 414 244 284 529 588  34 562   2 333 336 137
 349 260 148 410 495 442 311 252 420 146 425  79 580 319 298 427 308  18
 578 572  42 556 525 396  44  33 264 277 583 301 323 325 437  84 559 542
 538  56 341 381 254 149 540 278 449 493 289 195 394 257 203 533   5 142
  16 248  94  98 586 435 561 397 326 106 313 133 411  91 564 474 454  88
 423 198 180 413 104 530 281 498 512 270 100  77 322 189  30 406 543 436
 510 589   1  46 479  20  24 345 266 256 534 209 297 455 446 181 486 516
 143 267 409  96 531 428  68 385 272  50 459  82 200 202 283  38 344 403
 405 178 371   0 306 163 520 321 287 224 282 337 168 192 101 314 196 193
 447 452 430  66 431 432 438  35 190 541 565 395  87  61 491 339 426 185
  32  26 123 118  27  62  55 492 329 445 456 419   3 288  47 204 182  17
   8 197  83  29 340 532 157 191 458 299 400 296 255 238 317  12 350 170
 417 225 187 316   6 484 475 117 324 482 291 318 360 507 422 448  15 194
 399 102 167 169 305 353 268 557 544 416 312  67 237 513 453 359  64 412
 433 113 496 188 229 469  23 361 536 554  31 365 401 380  13 342 450 158
 508  36  51 211 175 213 451  95 173 402 171  11 172 320 124 485 207   7
 355 111 212 362  58 379 206 367 217 408 537  53 166 236 290  60   4 263
 421 210  14 216 528 261 262 115 489  57 497 215 161 231 346 356 240 391
 205 110  63 186 357 351 155 404 156 378 328 370 154 460 184 327 315 179
  69 369 490   9  52  85  45 162 125 108 440 221 177 480 219 218 368 352
  59 183 335  54  10 366 159 439 358 347 153 174 112 176 109 330 114 160
 228 514 363 331 292 338  49 222 265  48 214  65 119 116 373 120 372 500
 332 511  99 227 242 506 375 234 226 223 235 241 501 377 376 233 230 503
 502 374 334 592 122 499 239  39 279 504 505 232 121 280 364 566  40]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3977
INFO voc_eval.py: 171: [ 645 1713 3236 ... 3476 3230 3234]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1941
INFO voc_eval.py: 171: [ 637  671  747 ... 2272 2380  121]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2368
INFO voc_eval.py: 171: [413 377 590 319  12  21 668 369 665 291 376 589 583 414 643 671 321 342
 608 560 385 420 617 297 685 148 493 845 674 563 604 333 770 704 844  18
 330 323 584 561 324 632 417 673 296 292 587 339 681  25 754 670 857 370
 644 667 740 337  16  22 687 680 683 298 719 378  20 846 338 379 629 388
 529 585 567 416 691 320 344 307 335  74 847 628 384 505 484 666 742 273
 418 562 294 648 325 851 512 694 855 224 301 601 262 675 753 605 383 528
  13 341 672 646 322 380 762 614  73 181 209 313 311 708 690 387 496 346
 849 312 618  46 696 747 627 419 596 692 566 506 328 486 121 635 372 591
 502 183 588 729 816 776 139 707 602 259 203 830 702 711 433 212  15 336
  26 718 710 184 514 571 540 717 218 728 679 485 586 686 268 600  19 631
 253 688 409 220  45 609 116 701 854 859 306 415 153 223 755 211 154 716
 343 440 300 264 713 633 815 780 813  53  17  75 175 698 179 789 164 594
 174 619  84 508 329  44 381 630 700 738 274 371 568 647 327 709 715 310
 624 448 412 340 564  24 157 266 592 482 205 269 676 134 874 399 759 677
 243 334 650 149 615 603 814 612 732 565 821 570  30 219 531 431 332 437
 429 817 173 123 606 106 761 326 460 536 706 684 623 510 373  47 829 598
 382  33 272 634  83 331 245 669 208 386 811 254 108 599 689  72 545 865
 293 693 863 249 302 767 240 176 305 459 539 189 812  85 226 730 120 315
 511  48 695 569 185  66 597 733 345 494 858 705 607 303 777 678 140 347
  23 535 651 663 138 132 760 703 237 714 544  62 295 103 504 102 595 682
 117 487 119 270 786  14 402 411 159 225 161 474 248 593 275 503 699 610
  82 861 860 739 625 712 613 785 775 202  27  58 137 368 213 410 621  91
 122 231 842 101  36 758 611 263 850 206 875 271 791 204 723 308 258 697
 118 236 156 724 636 616   5 498  57 542 799 284 495 843 622 507 807 257
 620 221 626 790 316 513 541 180 111 277 165 500 787 432 741  42 509 426
 348 822 222 743 731 499 400 191  63 455 876  43 358 757 744 408 530 878
 784 267 242   1 144 136 247 309  56 188 374 115 299 430 461 462 105 207
 304 124 768 126 114 652 125   6 746 172 765 200  78  52 192 720 155  97
 537 104 826 806  49 783 107 869 771 151  90 778 476  35 828 575 182 190
 158 856 256 538 532 375 543 428 241 177 853 401 252 645 232 517 848 244
 427 877 278 772 187 483 788 150  50 152 773 831 764 265 558 795 774 260
 350 228 439 210 501  65 552 766 534  28 748 434 280 823 135  95 479 199
   2 868 480 818 745 769 282 477 162 481 170 794 261 870 163 255 109 133
 443  94 749 779 871  31 444 639 167  81 357 518  98 756  29 251 808 456
 435 436 230 227 281 554 475 574 246 533  51 169 110 458 734 664 463 867
 866 186 166 171 763  77  80  38 792 735 250 234 488 653 168 852 450 128
  89 438 553 395 827 555 314 490 649 810 497 367  55  93  67 793 557   0
 473   4  69 283  96 178 736 279 100 113 470 160 519 556 492   8 147 873
 573 197 582 637 286 824  99 390 363   3 546 737 559 359 781  61 276 662
 216 722 579 392 491  40  60 146 578 489 638 442 454 391 469 457 520 819
 349 581 825 820 360 550 751 289 112 576 362 449 145 215 361 797 549  39
  86 864 721 478 580 577 143 809 229 551 526 235 404 452 862 472 441  34
 801  68 547  41 872 453 425 800 548 238  54  32 405 468  76 527 642 290
 656 515  79 394 285 658 403 366 393 364 365 130 516 655 317 641 752  37
   9 288 233 725 451 640  70 406 798  64 396  92 318 356 214 198 287 838
 726  11 839 782  88 804 471 397 522 750 196 727 351 572  10 239 355 354
  71  87 407 389 127  59 521 466 423 141   7 467 836 660 661 523 352 840
 841 837 796 201 835 525 353 447 803 464 805 832 802 524 398 217 422 195
 445 193 654 833 421 424 834 142 131 446 194 657 129 659 465]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3284
INFO voc_eval.py: 171: [442 174  90 175  92 443  88  75  95  99  97 464  96  73  91 446 373  74
 118 182  25 408 211 104 101  72  77 410  78  42 415 399 420  84 256 394
 166 268  94  80 372 142 278  31 387 450 447 403  43 366 271 374 165 299
  52 400  87  51 200 105 190 368 167 239 267  82 158 219  85 487 341 433
 115  76 141  41 204 226 117 439  83 398 296 244 107 326 337  11 208 259
 315 357  86  45 266  79  13 114 201 279 199 116 289 214  81 342 338 468
 198 122 153 252 272 346  29 397 379 343 360 402 213 179 386 138 269 334
 369  55 258 406 254 422 383 356 425 385 189   8 146 251 340  24 322 102
 419 180 215 413 157 291 395 416 151 441 231 120 196 430 438 119 333 325
  46 384 466 362 288 265 263 233 328 364 367 318 249 229 202 290 382 396
 295 257 129 358  30 171  26 139 152 121 391 421 205 437 283 225  16 361
 127 427 230 335 227   4 261 302 377 148 336 426  34 301 133 432 125 203
 449 365  38  50 359  93 221 140 111 467 381 319 300 320 126 255 475 486
 137 273 414 209 176 164 112 124 423 100 280 207 363  56 168 228 113 212
 130 434 485 390 477 314  32 317   0  89 150  28 277 181 461 435 106 285
 405 172 163  35 159 388 236  98 404   5 206 234 393 348 401  44 476 250
 339 264 282 327 428 210 238 407 412 128 240   3 417 479 424 108 156 440
 253 216 155 349  54  40 123 245  64 262 329 409 143 144 243 147 375 376
 217 313 459 292 323  27 149 184 232 284 429 470 281 431 145 330  69  65
 380 297 222   7 160 247 392 474 480 170 311 445 186 260 370  57 274  10
 270   6 378 286 173 248 332 131 331 481 418 411 457 309 109 237 448 444
 235 347 354 321 154   9  36 316 276 344 103 298 454 293 324 110 197 169
 471 275 287  17 456   1 132 177 389  39 472 220 223  37   2 436  33 465
 345 161 294 312 162 191 218  58  53 185 469 371  47  22 242 310 473  70
 246 193  60 462 241 224 306 451  19 458  20 484 303 351 187  59  63  49
  14 304 305 350 183 308 452 307  67 134 353  61 483  23 455  18 482 136
 192  71 178 195 188  15 194  12 463 460  48 453  68 135 478  21  66  62
 352 355]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4242
INFO voc_eval.py: 171: [1581  717 1516 ... 1495 1709  117]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2879
INFO voc_eval.py: 171: [ 29 264  66 111 157 317  67  19 315 112 288  24 121 265  68 156 256 316
 202 128 287 155 266  26 296 232 260 113 291 211  76 201  86 289  52 125
 136 250 233  97  48  50 241  96  16 120 263 140 105 146 118  71 174 172
 234 192 178 307 191 154 297  42  20  77 108  27  94  65 127  73 227 219
 205  82 293  17 175 119 124  98  18 167 295 200 255 116 244  28 251 160
  21  55  22 114 286 166 217  75 258 194  78 237 204  83 173  54 168 133
 242 190 292  23 235 257  93  51 153 203  88 197 294 216 159  85 215 158
  89 290 311 130  57 308  79 262 314 122 246 283  56 126 148 309  53 220
 238  25 239 147 195   5 184 151 152 226 224  47 193 106 236  70 240 141
  91 243 162 129 267  87 134 139 223 131 138  90 115 171  49 245 259 165
 218 270  84 285 225 135 110  44 310 312 280 318 149 222  45  14 137 117
  69  10 302  95 104 209 196 248 142  46 253 247 313 214 298  41  92 252
 276  99 109   8   3 206 107   0 183  39  35  43 123  15 199 230 281 103
 208 145  30   7 221 102 213 268 269 282 277   2 132  34 261 182 210 170
 249 278 100   9 284  61   6 169 271 176 231  36 101   1   4  38  60  12
  31 161 198 163 300 181  59 164 207 275  74 303 179 272 279 187  32 299
 274 305 212 306 228  33  58  13 301  64 150  80  81  62 185 143 304 229
 177 180  63  72 189  11 254  40  37 273 144 188 186]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0977
INFO voc_eval.py: 171: [5861 2529  712 ... 4385  293 4386]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3111
INFO voc_eval.py: 171: [332 261 198 444  59 204 281 386 270  50 703 257 764 260 269 452 282 256
 413 259 756 450 650  71 224  78 732 333 410 197 617 638 157 447 336 199
 200 382  65 182 755 564  74  55  54 137 179 203 258 231 454 448 206 375
 277 449 758 616 128 126 228 445 272 208 443  70 383 367 263 459 265 565
 171 760 441 759 323  61  69 651  33 442  66   7  68 275  21 113 743 267
 225 761 541 421 566  72 403 196 634 569 568 538 458 285 728 620 456 708
 522 207 245 630 387 446 248 394 646 308   6 618 619 740 212 729 359 151
 264 455 344 457 763 707 768 268 559 145 723  49 534 520 341 289 202 322
 201 653 427 218  58 636 361 222 639 393 475 290 205 635 177 412 346 736
 641 727 230 735 419 276 274 739 519  11 772 542 143 572 246 674 672 733
 241  41 350 628 280 757 381 738 219 183 111 704 266 384 420 667 422 335
 451 278 186 239 279 178 730  51 220 411 567 622 440 147 347 670 666 213
 765 334 293 423 287  15  64 244 392 211 652 561 429 311 574 414 466 654
 185 545 175 288 562 139 181 125 637 124 710 152 570  91 737 339 130  80
 223 417 348 726 324  48  10 284 318  29 741  30 262 521 187 358 240 496
 657 640 774 388 131 366 539 237 409 127 770 180 129 416 385 316 627 425
 453 233 424 153 283 724 747 418 238 532 766 767 722 174  46 105 428 509
 217 250 751 415 360 184 714 523 247 713 391 698 166 563 315 718  67 165
 752 642  76 656  81 629 398 380 133 210 176 624 711 271  47 286 721 426
 686 460 161 390 325 578  22 273 313 340 338 227 725 337 369 530 560 497
 690 746 314 576 663 107 785 511 144 787 110 535 688  86 540  95 215 159
 479 242 101 396   8 771  31 544 731 543 668 507 706 343 330 702 593 742
  16 762 294 482 400 434 209 786 364 575 499  90 136 291 525 342 554 769
 138 317 106 253 715  27 312 633 160 631 744 118 573 243 399 626  45 484
  42 585 328 571 671 508 353 295 734 379 675 500 684 404  13 658 461  14
  24 783 292 754 162 389 587 673 537 108 719 320 116 221 577 483 546  56
 645  23 514  92 648 102 229 462  53 189 750 748 536 776 623 492 775 329
 669 745 547 155  34 661 490 701 753 158 749 597   0 172 788 649 687 498
 371 214 397 614 156 401 402 465 527 659 695   9 395 321 103 605 481  26
 632 588  94 518 319 524 692 141 372 611 195 773 513 232 226  36  32 363
 309 216  25 104 109 135 696  60 255 368  20 528  79 140 700 331 326 251
 712  88 533 664 644 134 613  35 553 119 236 596 610 777 438 254 512  19
 489 132 112 697 486 252 606 173  84  63 327 123 148 647 168 117   2 678
 376 689 115 602 526 682  28 235 234 594 506 600 436 595  77 365 612  93
 374 662 529 432 531 167  73 683 163 550 676  52 694 249 370 146  38 784
  40 705 191  37  82 493 586 716  18 494  12 655 433 362 149 437 192 598
 699 558 473 154 194 643 495  17 485 188 351 579 121 601 693 170 356  85
  83 470 604 169 150 305 164 680 120 463 599 491 114 503 505 488 607  97
  62 665  87  43 621 603  89 377 480  96 487  75 679 608 556 615 354 510
  39 430 190 681 504 310 660  44 685 677  57 100 549 472 431 555 548 469
 552 717 405 378 299 551 515 122 501 307 467 300 435 609 557 407 502 297
 306 406 709   3   1 355 478   5 464 193 439 625 408 471 591   4 302 474
 516 782 477 720 373 691 296 349 476 352 357 592 304 468 298 301 778 517
 781 142 303  98 580 589  99 345 780 590 779 583 582 581 584]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2101
INFO voc_eval.py: 171: [2900 1280 1333 ... 2802 2803 2312]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3269
INFO voc_eval.py: 171: [345 410 348 849 445 501 376 355  49 482 465 448 628 549 145 351 502  63
 408 443 160 780 143 540  46 627 506 280 150  50 159 202 513  41 377 522
 851 154 237 845 742 357 276 802  48 127 794 413 409 449 141 462  65 203
  45 803 526 452 776 152 775 146 478 839  40 517 270 854 346 467  62 791
 819 434  51 375 842 372 219 442 830 808 356 470  82 625 148 626 765  53
 247 738 777 542 847 646 271 236 224 778 162 225  68 582 533 515 464  64
 550 416 474 546 248 484 243 666 860 505 486 421 417 602 358 858 826 283
 153 850 261 239 437 347 144 722 463 223  54 831 197 674 451 645  44 414
 848 125 415 471 205 212 238 450 475 433 673 436 520 545 616 151  52 338
  61 664 296 781  84 252 279 824 610 619 783 825 326 660 739 827 192 779
  31 663 707 352 227 591 809 131 812 586 617 706 220 799 601 511 763 764
 790 734 430 215 158 156 600 650 259 608 447 439 161  67 198 547  69 265
  85 213 195 383 773 229 412 380 282 228 866 749 349 431 432 440 544 840
 129 411 514 419 204 816 381 226 810 373 147 637 157 424 708 206 788 149
 805 675  47 130 142 766 320 135 194 435 164 852 606 744 857 217 126 622
 806 817 864  56  59 163 553 634 344 438 745 337 295 422 785 425 250 596
 230 838 597 444 313 624 309 274 829 647 116 262 350 704 176 644 267 815
 110 750 272 684 699 112 804 512 853 481 811 289 155 268  15 353 441 140
 589 290 572 489  42 786 193 741 541 294 869  66 128 334 524 132  87 196
 245 281 632 244 676 697 618  91  60 573 726 813 284 134 774 384 303 681
 828 782 232  30 468 603  25 649  89 792 124 519 479 835 285  57  58 222
 420 273 727 182  93 354 729 234 233 122 241 607  22 861 740 671 293 789
 793 278 246 834  19 277 623 385 863 137  86 428 665 117 571 275 746 620
 568 328 382 859 784 670 595 249 677 680 682 302 583  14 214 288 231 360
 113 111  55 552 476 613 832  88 114 612 669 251 210 216 555 787 528 611
 567 101  12 426 446 301 120 543 654 365 621 343 702  29 460 253 218 429
  32 751 712 703 374 315 221 725 548 390 672 537 314 755 605 480 308 254
 700 735 822 557 527 551 736 865 306   4 683 570 731 269 292 554 818 211
 102 688 732 841 418 266 821 184 820 118 652 862 532 823 814  18 264 298
 327 843 235 648 556  90 833 728 477 689 335 427  34 636 855 678 685 407
 730 263  83 844  92 569 329 662 690 846 518 401 733 635 185 856 693 260
 362 323 640 593 123 371 119 287 574 508 695 359 186 133 633   3 104 867
 868 807 459  70  20 340  26 400 588 679 300 507 321 667 747 870   9 317
 659 585  94   5 304 180 604  81 319   1 594 173 311  28 737 538 641 361
 503 563 242   2 394 258 609 525 598  43   0 339 531  78 686 516 240 336
 487 187 257 255 599 115 291 769 797 297 307 800 561 363 318 752 299 324
 105 209 286 332  79 497 694 614 364 493 534 615 753 325 453 743 490 316
 485 310 369  39  38 629 639  27  95  35 456  13  76 178 166  23 716 423
 121 396 798 564 138 539  99 643  98 172 638 710 687 366 760 189 495 136
 342 587 535 305 758  17 536 455 466 201 167 651 657 312  33 165 504 454
 207 559 642 368 658 483 558  24 256 521 498 139 181 458 179 796  73  77
 748 406 399 473 402 713 691 529 756 175  96 592 403 367 795 378 801 492
 653 404  21  97 491 103 717 190 578 469 575 488 579 655  74 715 457  72
 584 472  80 174 656 461 177 183 510 581 580 530 560 322 100  75 661 754
 108 388  71 590 705 200 494 761  16 208 711 509 698 701 191  37 577 188
  36 692 565 762 759 199 668 109 523   6 405 330 566 709 389 496 630 386
 724 379 576  11 107 770 370 757  10 772 718 631 341 723 171 721   8 562
 714 331 333   7 720 387 391 719 170 106 168 169 398 771 768 767 499 500
 397 395 392 393 836 837 696]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2138
INFO voc_eval.py: 171: [132 949 437 ... 567 767 768]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3039
INFO voc_eval.py: 171: [ 47 275 262 142 147  26  46 151  48  51 143 282 117  72  34 159  68  31
  78 269 145  59  57 189  56 250 168 115 270  73 264 161  33  53  20 198
 172 205 193 130 271  36 272 253 241  25 285 162  32 254 244 263  30  77
 266 279 148 116 286 150 156 283   6 197 154  52  17  82  61 165 215 114
  50 144  22 278 196  84  62 184 225 146 191  99  75 113  85 204 141  60
 195 158 267 287  42  18 243  88  10 232 268  35  97 261  66 108 212 242
 166 126  23 218 265 255 152  27  19  65 167  54  67  80  58 245 185 163
 201  95 134  79 112 149 125  70 246 186  12 220  38   4 231 248 240  29
   0 259 235  93 155  21 273 284  98  14  39 194 110 153  64 252 131 199
   5 157  41  76   8  90 139  16 221  11  71 274  94 105 207 138 137  37
  28 229 217 280  81  92 188 219 256 237 210 169  24 102 249 203 135  69
 103 177 251  15 187 133 164 181  96  86 140 101   2   3  49 214 170 100
   9 173 109 119 227 192 281 277 106  87 209 121 107  40   7 123 182  45
 224  83 174 236  55 234 171  91  74  13 124 176  89 183 175 111 216 211
 104 128 238 178 200 213 258 208 206 180  43   1 118 127 223 222 202 230
 160 190 122  63 276 226 179 247 233 228 129 120 257 239 136  44 260 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4981
INFO voc_eval.py: 171: [ 8556 13986  1051 ...   769 10856 10858]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5393
INFO voc_eval.py: 171: [2507  713  259 ... 2708 2709 1827]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4171
INFO voc_eval.py: 171: [ 65 320 540  33 340 549  66  56 135  40 543  49 344 544 336  44 324 621
 541 159 352  34  46 484 224 138 334 553 349 545 258 318 161 347 554 548
 127 413  36 652 141 289 351 419 640 546 312  70 371 429 622 148 147 328
  67 165 338 542 149 140 425 355 160 231 630 230 492 323 631  60 496 229
 272 125 567 626 414 325 628 162 341 617  51 124 563 632 485 157 256 223
 316 634 418 468 643 339 259 357 568 322 624 167  39 531 625 143 353 221
 235 627 502 557 237 270 422 342 494 333 168 654 503 132  45 197 421 644
 370 327 142 372 137 520 495  43 620 321 317 271 294 635 238 376 257 335
  64  91  62 649 166 319 136  96 415 226 498 655 483 133 579  92 233 222
 416 290 651 350 428 642 681 354 236 678 126 637 154 315 332 348 572 343
 512 248 260 313 426 234 145 564 508 146 648 164 152 139 129 326 314 513
 569 618 439 269 175 284 330 417 369 623 144 614 515 573 641 345 638 170
 566 528 267 225 288 329 633 412 346 131 689 679 201 209 375 331 676 268
 153 639 645 337 128  78 552 532 677 636 420 558  42 615 647 489 130 646
 227 309 169 490  50 550 360 274 585 202 163 603 497 470 650 571 455 467
 507 653 486 101 102 232 299  14 574 198 510 264 501 383 204   5 359 511
 629  27 616  63 203 195  55 447  35  69 373 469 565 109 273 384  38  24
 158 581 356 547 682 361 386 275 619 525 208 189 249 556 174 122 493 196
 262 657 184 121 261 199 362  32 358  95 228 305 482 280 171   0 265 246
  94 521  90  93 516 200 691  23 523 263 191  15 509 686  71 250 279 423
 382 120 398 680 656 307 390  47  97 276 266  37 471  22 559 480 662 277
 446 365 177 134 658 291 180 524  13 522 432  87 377 687   8 444 156 560
 217 576 487 306  30 172  48 526  61 433 112 123 473 450  79 451 441 514
 108 186 106 561 251  53 253  77  81 454 582 111 185 506 593 438 464 602
  84 683 562 427 105  25 304 173 517 215 551 440 688 298 609 188 211   2
 465  20 466 491 442 462 690 607   1 283 478 430 100 519  16 661 216  57
 692 583 518  17 287 659 213 374 604 684 586 527 481 613 400   6   7 178
 685 580 310 693 254 214 410 403 555 449 500  58 608  18 453 244 529 577
  73  76  31  68 378 110 107  74  19  41 605 578 282 479 286  21 533 570
 660 488 285 293  80 505 448  75 297  52 474 187 404  72 278 116 311 114
 460  29   3 113 424 210 606 252  88 379 183 530 601 452 292 504 431 499
  54 435  59 590 437 296 391 255 476 399 219  26 295   4  98  28 155 308
 443 281 364 194 380 193 212 366 600 118 409 434 205 597 367 436 406  12
 363 381  86  82 117 119 220 594 388 589 537 592 667 612 610 591 405  83
  99 587 411 596 206 179 575  89 477 588  85   9 584 475 115 665  10  11
 218 240 181 669 368 535 395 598 104 385 595 392 536 611 241 457 599 239
 192 402 242 389 207 176 300 245 396 668 247 190 666 303 472 675 243 401
 397 534 387 394 393 407 151 182 673 672 150 463 458 302 664 408 301 538
 456 671 539 459 103 674 461 670 663 445]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3039
INFO voc_eval.py: 171: [ 39   4  43 113 179 173 269  44 118 330 183   7  46 177 225  48 148 152
 180  12 185 326 146 149  38  56 333 271 142 171 119 176 222 267 212 147
 133 175 220 226 264 265 235 141  95 268  75 120  73 239 329  94  45 270
   6 116  54  99 244  55 153 112  11 143 154 221 321 165  22 184 174 137
 121  53 317 261 297 203 236 126  40 318 301 178 166 224 172 228 234 100
 181  70  67 223  42 206   5 150 157 111 182  93 260 151 124  90  97   9
  47  13 241 238 314 328 312  98 242 251  84 122 320 332  25 237 257  14
  91 135 240 279 114  68 248 243  96  41 324 227 199 323 144 117 202 331
 299  85  78  59 210 161 311 233  79  92  69 252  57 194 334  24 107 250
 191 305 258 298 284 316 219  26 285 218 187  74 109 214   8 139 110  80
 115 106 125 186 302  10 274 230 158 201 209 313  89 247  77 304 123  71
 259 295 108 246 140  72 315  58 145  31  51 325 306 231 289 266 215 253
 327 213 319 217 254 262 300 308  37 136  28  83 192 249  27 280 293  23
  60 335 288 207 134 204 162 287 322 167 208 105  50  82 229  29 164 211
  49 196 216  52 255 286 307 138 104  76 189   2  30  21 273 132 296 127
 281 256   3   0 245   1 188 193  81 200 290 160 272  17 102 130 197 198
 294 291 190 159 101 283 195 232 292 169 155 275  18  34 128 129 168 163
  20 276 103  19 263  16  33  15  66 131 282  35 170 156  64 278 277  32
 309 303 310  65  62  36  61  63  86 205  88  87]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1311
INFO voc_eval.py: 171: [637 378 644 120 649 316  86 207 390 288 126  80  82 400 172 891 641 642
 289 318 315  89 122 184 393 129 212 799  98 292 210 795 504  87 384 535
 208 635 379 191  88 451 639 657 672 449 296 102 206 452 124 906 317 121
 321 190 899 893 301 332 291 179 828 802 509 381 290  92 463 383 522 539
 876 847 901 838 706 169 399 216 139 140 203 218 833 905 900 638 295 525
 453 326 542  66 385 487 430 863  46 743 320 105  21 117 217 305 174 382
 843 809 141 307 197 678 877 319 727 707 194 664 656 896 801 454 109 797
 830 177 507  84 198 456 276 904 529 259 440 110  90 173 170 832 436 434
 815 345 185 221 752 728 188 229 101  74  47 621 299  22 508 394 538 171
 878 749 874 822 907 524 199 894 324 302 209 181 717  36 912 303 888 187
 168 263 737 176 552 733 837  69 710 244 460 133 883 196 445 294 214  77
 892 241 455  70 450 486 510 107 895 834 781 778 247  45 825 213 226 459
 868  60 867   3 186 540  91 541 544 448 909  97 713 813 380 389 370  55
 716 846 700  19 183 703  96 709 329 182 498 261 708 898  99 257 600 687
 578 309 636 683 756 526 506 886  20 925 640 875 918 333 162 180 482 395
  61 588 433 831 205 396 865 465 404 429 751 826 779 848 314 114 322 373
 300 730 910 104 138 457 870 827 349 513 437 484 769  54 311 676 734 812
 398  75 233  34 175 387  44 232 605 626 462 464 818 397 371 178 441  41
  79 873 679 885 908 260 235 924 149 493 517 548 816   6 757 200 339 353
  38 903 431 103 274 285 249 505  76  68 428 352 860 804 879 607 100 864
 559 740 845 855 690 271 252 659 911 528 614 523 461 146 927 262 821 665
 467 297 684   8  85 118 819 128 211 546 161 106 376 742 654 127 116 166
 228 189  59 807 220 313 530 248 476 246  78 775 167 887 250 573 618 264
 754 442 518 853 686 685 902 726 534 917 543 495 823 193 351 604  48 421
 884 215 889 350 881  40 572  10 741 201 238 325 598 688 633 237 735 142
 836 824 236  94 489 304 731 115 258 753 164 466 225 681 695 563 240 897
 219 725 763 854 254 844 131 869 527 872 306 402 439 622 699 327  39 514
 851  58 202 438 575 871 242 458 612  43 890 560 808  57 401 566 227 432
 550 158 272 601 310 587 192 473 772 714 627   0  42 603 806 388 926 729
  15  93 648 599 279 705 842 374 856 602 620  83 786 787 520 346 554  50
  37 340  64 419 820 567 545 792 391 721 608 817  62 111 745 630 427 277
 496 195 119 392 386 551 928 435 342 923 852 680 882 736 500 157  24 577
 331 755 766 283 767 355 234 492 267 576 689 677 239 682 135 308 372  12
 569 414 624 516 558 536 835 785   4 585 278 720 789 619 782 768 143 293
 770 744 243 343 420 850 711 780 712 298 255 584 273 788 611 579 866 477
 591 606  56 553  35  63 282  33 245 275 356 377  65 443 574 732 702 344
 783 447 704 719 580 312 739 418 646 841 375 478 280 609 694 610 634 698
 363 839 750 675 499  49 701 268 113 204 422 765 159 112 796 284  95 108
 483 557  32 488 880 746 615  23 362 784 512 475  71 805 625  81  73 403
 913 269  18 549 777 748 803 571   9 814 416 323 564  25 583 230 515 565
 472 251 531 444 365   2 790 590 586  16 793 722 338 231 581 281  72 920
 471 840 556 446  17 348 469 570 568 555 810 265 747 861  67 773 417 653
 670 253 501 354 776 774 723 491 337 919 613 256 360 490 738 165 623 532
 593 667  53 468 502 800 798  11 811 696 266 411 503 697 410 794 470 547
 519 145 474 494 829 364 724 582  13 481  51 408 156 596 592 485 335 425
 914 369   7 715   5  14 155 368 286   1 497 589 791 154 669 424 521 771
 136 718 764 617 533 511 479 537 595 849 862 645 674 693 366 287 426 367
 594 423 480 347 762 160 357 328 406 628 359 658 163  31 643 358 413 412
 761  52 361 921 330 148 152 222 137 857 631 916 415 561 405 655 562 632
 922 629  30 662 597 663 334 616 123 692  26 760 858 859 759 224 150 151
 758 407 661 336 341 409  29 147 691 223 132 671 651 270 134 125 130 915
 652 144 647 153 666  28  27 673 650 668 660]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2854
INFO voc_eval.py: 171: [424 252 596 173 437 300 136 445  24 471   1  37 480 500 256 323 353 477
 482 325  39 400 441 254 250 351 422 126 352 124 472 579 312 301 553  57
 354 265 609 425  68 255 536 106  71 326 129 232  26   3 448 598 319 442
 521  17 434 178 331 479 342 181 306 350 322  43 586 597 617 154 626 375
 228 359 101 607 369 612 261 378 150 444 537 310 373 108 222 109 447 475
 208 137 324 211  19 266 488  74  70 225 439 144 160   8  51 559 304 507
 467 308 207 534 473 383 275 486 379 476 102 236 465 615 327 399 366  58
 363 360 397  40  28 187 368 245 119 631 334 162 216  16 161 485 621 487
  47  56 431  45 176 128  84 601 302 409 594 303 316   6 210 474 515 429
 259 505 289 580 478 347 545  41 605 307   9 309  72  53 317 483 226  29
 118 143 504 100 215 572 562 563 374 340 230  49 212 177 520 314 311 436
 332 214 402 630 320 218 380 125 171 622 430 217 481   2  38 158 267 190
 131 213 618 174 116   5 193 172 583 582 385 611  44 120 229  89 227 115
 556  92  75 357 358 438 367 516 398 105  30 274 484 550  12 408 183  27
  46 337  90 551 123 362 600 305 498 625 283 557 401 508 186 566 295  23
 610  18 403 418 179 613 565 455  13 111 620 286 627 345 548 443 457  32
  99 130 209 159 313 153  55 321 541 543 502  87 531 170 517 538 200 564
 599 145 616 524 558 287 239 415 535 364 282 279  97 142 280 499 406 491
 278 628 623  94 389 361 527  93 576 614 365 608 624 503 241 219 148 501
 113 456 192 336 571 110  33 221 381  85 281  82  54 155 519 335 284 206
 506  63 587  78 619 258 544 589  91 220 315 370 318 386 223 569 147 371
 522 453  42 138 264 533 423 518 277 459 546  61 547 276 114 634 127 372
 180  76 356 288  77 103 377 512 451  80 117 542 257 270 433 552 188 454
 231 426 570  35 575 412 382 166  60 333 384 355 440 376 122 204 133 449
 243 203 629 271  86  50 157  34 591  52 633 107 460 285   4 393 290 435
  31 141 592 466 260  36 328 235 593 452 525  15 432  59 294 341 273 339
  11 149 146 104  64 165  81 578  69 244 632 468 121 156 343 330 555 233
   7 606 588 414 139 489 112 427  22 132 549 152 490 561 532  88 568 463
 539 298 151  25 530 191 240 199 540 428 140 291 529 396 450 198 135 560
 134 293 242 185 577 407 528 511 411 182 297 329  83 413 237 446  95 224
  67 251 299 168 184 410 262 405 523 167 567 201  96 296  66 391 164 469
  10 234 416 263 175 554  73 344 253 169 585 346 392 526 394 202 338 163
 348  79 573 513 590 390 189 404 574  62 272 417 509  21  98  65 238 462
   0 388 510 292 349 514 249 584 395 387 494 248 458 603  20 604 461 247
 602 269  48 464 492 493 195 419 268 581 197 420 595 205 246 497 495  14
 421 196 194 496 470]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3370
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3047
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.250
INFO cross_voc_dataset_evaluator.py: 134: 0.398
INFO cross_voc_dataset_evaluator.py: 134: 0.194
INFO cross_voc_dataset_evaluator.py: 134: 0.237
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.424
INFO cross_voc_dataset_evaluator.py: 134: 0.288
INFO cross_voc_dataset_evaluator.py: 134: 0.098
INFO cross_voc_dataset_evaluator.py: 134: 0.311
INFO cross_voc_dataset_evaluator.py: 134: 0.210
INFO cross_voc_dataset_evaluator.py: 134: 0.327
INFO cross_voc_dataset_evaluator.py: 134: 0.214
INFO cross_voc_dataset_evaluator.py: 134: 0.304
INFO cross_voc_dataset_evaluator.py: 134: 0.498
INFO cross_voc_dataset_evaluator.py: 134: 0.539
INFO cross_voc_dataset_evaluator.py: 134: 0.417
INFO cross_voc_dataset_evaluator.py: 134: 0.304
INFO cross_voc_dataset_evaluator.py: 134: 0.131
INFO cross_voc_dataset_evaluator.py: 134: 0.285
INFO cross_voc_dataset_evaluator.py: 134: 0.337
INFO cross_voc_dataset_evaluator.py: 135: 0.305
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 6499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.410s + 0.026s (eta: 0:00:54)
person 0.9699949
person 0.96355414
person 0.9922889
person 0.9606143
person 0.9731915
person 0.92604244
person 0.93787026
person 0.98225313
person 0.9449346
person 0.911008
person 0.98157054
person 0.93870795
bottle 0.9224204
bottle 0.91970026
bird 0.95145535
person 0.98249835
person 0.9180609
person 0.94427216
person 0.9035401
person 0.90492874
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.324s + 0.034s (eta: 0:00:40)
person 0.94445175
person 0.9190363
person 0.9600253
person 0.9458381
person 0.9806055
person 0.9474679
person 0.9677547
person 0.98177344
person 0.93954164
person 0.921388
person 0.9276618
person 0.93156046
person 0.90594757
person 0.9225908
person 0.92058307
person 0.9266747
person 0.9750856
person 0.91194844
person 0.98017305
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.325s + 0.035s (eta: 0:00:37)
person 0.92882204
person 0.91256744
person 0.9558176
chair 0.91430235
person 0.97226936
person 0.91333956
person 0.95501685
person 0.96600336
chair 0.9656882
chair 0.93727183
chair 0.9146258
person 0.90298504
person 0.97907114
person 0.98794556
person 0.9286635
person 0.9040022
person 0.9768397
person 0.9916004
person 0.98143035
person 0.91231966
person 0.9079778
person 0.9082557
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.331s + 0.037s (eta: 0:00:34)
bird 0.93871456
person 0.92743367
person 0.9212712
person 0.9938148
person 0.9565895
person 0.9745905
person 0.9799201
person 0.90885144
person 0.9594768
person 0.9658549
person 0.90548384
person 0.91806746
person 0.96967983
person 0.94044155
person 0.95870924
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.332s + 0.036s (eta: 0:00:30)
diningtable 0.9012883
diningtable 0.9392395
chair 0.9636027
chair 0.9658829
chair 0.9381033
chair 0.9571921
pottedplant 0.91825575
pottedplant 0.9146905
pottedplant 0.91060126
pottedplant 0.9045943
pottedplant 0.943683
pottedplant 0.9381217
pottedplant 0.9295372
person 0.9657146
person 0.9828128
person 0.92112833
person 0.9796681
person 0.949071
person 0.9889303
person 0.91490054
person 0.90335345
person 0.9123938
person 0.9205756
person 0.92123353
person 0.9731503
person 0.9545
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.325s + 0.035s (eta: 0:00:26)
person 0.9568301
person 0.9229424
person 0.90496105
person 0.9201044
person 0.9463502
person 0.9132401
person 0.95325935
person 0.9363737
person 0.9286751
bird 0.9226741
person 0.93579537
person 0.9876402
person 0.917028
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.321s + 0.035s (eta: 0:00:22)
pottedplant 0.9035471
person 0.96217984
person 0.93258804
person 0.91084063
person 0.95381
person 0.9179966
person 0.95860064
diningtable 0.90901333
chair 0.98269606
diningtable 0.9609724
chair 0.9167252
chair 0.9482206
chair 0.98064363
chair 0.92754996
bird 0.9930728
person 0.9020286
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.324s + 0.035s (eta: 0:00:19)
car 0.9286168
car 0.9418953
car 0.91378725
person 0.98848104
person 0.9003448
person 0.9740956
person 0.9296511
horse 0.9742071
horse 0.9138299
horse 0.9206913
person 0.93784845
person 0.9533362
person 0.96822906
person 0.9182392
person 0.94150984
car 0.9458087
car 0.9644434
bicycle 0.9611093
bicycle 0.93058544
person 0.9375232
person 0.9167802
person 0.97320265
chair 0.9180474
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.323s + 0.035s (eta: 0:00:15)
person 0.97426087
person 0.95885247
person 0.97347826
person 0.9158179
person 0.91236603
person 0.9885719
person 0.98636115
person 0.9432258
person 0.9165002
person 0.96422654
person 0.96331114
person 0.9911408
person 0.90698206
person 0.9667223
person 0.9112249
person 0.98771983
person 0.97994393
pottedplant 0.9123668
bus 0.91280484
person 0.9899282
person 0.9616574
person 0.9411961
person 0.95406944
chair 0.94615334
person 0.9056572
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.326s + 0.036s (eta: 0:00:12)
motorbike 0.9640105
person 0.97475094
person 0.9110744
person 0.9149054
person 0.9563241
person 0.96680516
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.324s + 0.035s (eta: 0:00:08)
person 0.9043484
boat 0.98196214
boat 0.94803786
boat 0.91813576
person 0.99032485
person 0.9373693
person 0.98555356
person 0.9181585
person 0.9207996
person 0.9889663
person 0.9749606
person 0.9100575
person 0.97466415
person 0.97655475
person 0.91280586
person 0.94856626
car 0.9505298
person 0.93902
person 0.9468841
person 0.95990336
person 0.98673147
person 0.905907
person 0.9201669
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.329s + 0.036s (eta: 0:00:05)
diningtable 0.95924926
chair 0.92407703
chair 0.9738998
chair 0.9807726
chair 0.9142206
chair 0.98097426
chair 0.9013405
chair 0.93615353
chair 0.9603471
person 0.94185627
bird 0.92381775
bird 0.91386944
boat 0.9199448
person 0.9619756
person 0.9283
person 0.98871255
person 0.9136758
person 0.90176535
person 0.91197103
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.325s + 0.035s (eta: 0:00:01)
person 0.98342746
person 0.96427166
person 0.9561317
person 0.9554665
person 0.94865894
person 0.96565664
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.545s + 0.029s (eta: 0:01:11)
person 0.98188615
person 0.9181743
person 0.97881293
person 0.97711253
person 0.91354394
bicycle 0.97253877
person 0.9317303
person 0.9135154
bicycle 0.9746415
person 0.9348482
bird 0.94348496
bird 0.9002996
pottedplant 0.94364554
person 0.9703481
person 0.9285696
person 0.9025692
person 0.9605547
person 0.9656595
person 0.98677385
person 0.9272843
person 0.96601003
person 0.9525802
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.337s + 0.034s (eta: 0:00:42)
person 0.91266483
person 0.90667915
person 0.9639435
person 0.91746676
person 0.9733689
person 0.94146717
person 0.9363593
person 0.9807483
person 0.96441346
chair 0.92596984
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.346s + 0.036s (eta: 0:00:39)
car 0.91592586
car 0.9501196
person 0.94743633
person 0.96117306
person 0.9814968
person 0.92277336
person 0.94845337
person 0.96449775
car 0.9822473
cow 0.91979355
person 0.9157312
person 0.9572367
person 0.9226503
person 0.91210073
person 0.98070735
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.340s + 0.037s (eta: 0:00:35)
person 0.97809315
chair 0.9337485
person 0.92957884
person 0.92602414
person 0.95121026
person 0.92583984
person 0.9320896
person 0.9181233
person 0.9398617
person 0.980667
person 0.9869004
person 0.97709084
person 0.9689978
person 0.97705334
person 0.9030652
bird 0.90578294
bird 0.99080455
person 0.9216543
bird 0.9516775
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.333s + 0.037s (eta: 0:00:31)
person 0.94530606
person 0.97078013
person 0.98258036
person 0.92242116
person 0.9472448
car 0.9505298
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.330s + 0.037s (eta: 0:00:27)
bird 0.9108593
bird 0.96001333
bird 0.91366255
person 0.91270304
person 0.9436635
person 0.9164217
person 0.9148756
person 0.98826176
diningtable 0.94839853
person 0.9721522
diningtable 0.9786227
person 0.92264724
person 0.9351203
chair 0.94269997
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.334s + 0.036s (eta: 0:00:23)
person 0.9136883
person 0.90480614
person 0.9276508
person 0.94748855
person 0.9857298
bus 0.94703287
dog 0.95202607
diningtable 0.9736312
diningtable 0.92191494
chair 0.9117889
chair 0.9849926
chair 0.9120981
chair 0.97073066
chair 0.9356684
pottedplant 0.90255314
person 0.9050218
person 0.97600216
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.331s + 0.036s (eta: 0:00:19)
diningtable 0.92055774
chair 0.9102963
person 0.9475255
person 0.9663509
person 0.9548903
person 0.9361688
person 0.90460366
person 0.9447639
bird 0.9548678
person 0.9257529
person 0.96196026
person 0.9637678
person 0.9313946
person 0.980354
person 0.9082349
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.329s + 0.035s (eta: 0:00:16)
person 0.962649
person 0.9627386
person 0.9148022
person 0.9031226
train 0.9583937
person 0.95698017
person 0.96237576
person 0.94678926
person 0.93891895
person 0.91258764
person 0.9316067
person 0.9285795
person 0.90491974
person 0.9560017
person 0.9288443
person 0.9924545
person 0.97409105
person 0.9606619
person 0.91720206
person 0.92742527
person 0.9016177
person 0.92900455
tvmonitor 0.9172495
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.330s + 0.036s (eta: 0:00:12)
person 0.96369386
person 0.90530944
person 0.9443503
horse 0.9600165
person 0.9665541
person 0.9299519
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.327s + 0.035s (eta: 0:00:08)
person 0.9751222
person 0.96550345
person 0.92215633
person 0.9195471
person 0.9510943
person 0.9266351
person 0.93588096
person 0.902515
person 0.91441226
person 0.9866508
person 0.94520366
chair 0.972202
person 0.9561309
person 0.904728
person 0.94489825
person 0.9583698
person 0.90539384
person 0.9173244
car 0.9452809
car 0.91745204
car 0.9515536
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.325s + 0.035s (eta: 0:00:05)
person 0.96783227
person 0.94355905
person 0.9622561
person 0.94874364
person 0.9737135
person 0.9604001
person 0.92087775
person 0.90486616
person 0.92682534
person 0.9881499
person 0.98155046
person 0.91769964
person 0.97818154
person 0.92769474
person 0.9376454
diningtable 0.93287647
person 0.900972
person 0.9170785
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.320s + 0.034s (eta: 0:00:01)
person 0.90754527
diningtable 0.91277224
person 0.97044003
person 0.93986696
person 0.98735005
person 0.96589893
person 0.95830584
person 0.95842475
person 0.9039675
person 0.90823466
person 0.91571534
person 0.94097006
person 0.9634309
person 0.9810783
person 0.9094635
person 0.94899166
person 0.9566078
person 0.955861
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.471s + 0.028s (eta: 0:01:01)
diningtable 0.90126735
diningtable 0.92667586
person 0.9602521
person 0.975973
diningtable 0.9729646
person 0.92215824
person 0.919094
person 0.90607285
bottle 0.9067096
person 0.9778441
pottedplant 0.91774344
diningtable 0.9265968
person 0.9658057
diningtable 0.90596265
chair 0.9496726
chair 0.9658801
chair 0.922302
person 0.91757977
person 0.9060573
person 0.9317395
person 0.9957209
person 0.9808092
person 0.9337659
person 0.96634954
person 0.965033
person 0.90445864
person 0.9705983
person 0.95945776
person 0.9378428
person 0.93574846
person 0.965151
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.321s + 0.029s (eta: 0:00:39)
person 0.94644386
person 0.9859283
person 0.9331418
person 0.970588
bottle 0.93725765
person 0.9362112
person 0.9427015
person 0.9530681
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.323s + 0.030s (eta: 0:00:36)
chair 0.9328192
bottle 0.9094169
boat 0.9004313
person 0.9724061
person 0.93893784
person 0.9328542
person 0.9313545
person 0.93087894
person 0.967792
person 0.94315785
bottle 0.90765643
bottle 0.9531578
person 0.98217255
person 0.95308745
person 0.94912845
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.332s + 0.033s (eta: 0:00:34)
person 0.9662368
person 0.9207996
bird 0.91826534
person 0.95374024
person 0.9055594
person 0.96731514
person 0.9702717
person 0.9594156
person 0.90276814
person 0.9356528
person 0.90649813
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.334s + 0.033s (eta: 0:00:30)
person 0.969454
person 0.9692858
person 0.95373493
person 0.905491
person 0.92880017
person 0.9705833
person 0.92035604
person 0.9410893
person 0.91382146
person 0.9186243
person 0.93752134
person 0.96980536
person 0.900501
person 0.9534308
person 0.9250625
person 0.9044232
bottle 0.9536652
chair 0.93146944
boat 0.90470904
person 0.97331786
person 0.9548233
person 0.90264803
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.331s + 0.033s (eta: 0:00:26)
car 0.9807227
car 0.93521947
person 0.9256728
person 0.97716343
person 0.9097598
person 0.90927356
person 0.9198146
person 0.9345962
person 0.9648222
person 0.9228859
person 0.9164733
person 0.9569507
person 0.9172689
pottedplant 0.9157263
pottedplant 0.93199736
car 0.9848935
car 0.9381608
person 0.9318503
person 0.9878066
person 0.954578
person 0.9232052
person 0.9369466
person 0.9859213
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.329s + 0.034s (eta: 0:00:23)
person 0.97975624
car 0.9595294
person 0.9502755
person 0.92505276
tvmonitor 0.9261385
person 0.9046508
person 0.9334417
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.328s + 0.034s (eta: 0:00:19)
chair 0.90266764
person 0.9707244
person 0.9443663
train 0.9189536
train 0.9610071
bird 0.9109308
bird 0.9828818
person 0.9217729
person 0.92249906
bicycle 0.9219805
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.328s + 0.035s (eta: 0:00:15)
person 0.9694494
person 0.9015412
person 0.9365735
person 0.9292611
person 0.9294434
person 0.97836477
person 0.9727904
person 0.9867071
person 0.95847154
person 0.9893059
person 0.9103446
person 0.9550579
person 0.9653728
car 0.95158356
person 0.9236356
person 0.96894395
person 0.949917
person 0.97713894
person 0.93279755
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.325s + 0.035s (eta: 0:00:12)
person 0.9149699
person 0.94526565
person 0.9234452
chair 0.975255
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.327s + 0.034s (eta: 0:00:08)
person 0.9812859
person 0.9400904
person 0.92777383
person 0.9632996
person 0.96214366
person 0.9021494
person 0.9261271
person 0.90718216
person 0.93922466
person 0.9204381
person 0.97885865
person 0.90539265
person 0.9075783
boat 0.90321755
person 0.96686625
person 0.92365336
person 0.9466163
person 0.9111985
person 0.95815635
person 0.9029243
person 0.92544407
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.327s + 0.034s (eta: 0:00:05)
person 0.96732104
person 0.97342277
person 0.977999
person 0.9487796
person 0.9194455
person 0.93047893
person 0.9245161
person 0.9504347
person 0.96054596
person 0.91813165
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.328s + 0.035s (eta: 0:00:01)
person 0.9361482
person 0.9713585
person 0.92384464
bird 0.9029521
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.467s + 0.034s (eta: 0:01:02)
person 0.9412567
chair 0.91902316
chair 0.9580045
person 0.958335
person 0.92591524
person 0.9535512
person 0.9402279
diningtable 0.9314494
person 0.9742042
person 0.9713172
diningtable 0.9360913
person 0.95287234
person 0.9681704
person 0.9060288
diningtable 0.9117452
person 0.9295849
person 0.9649645
diningtable 0.9654515
diningtable 0.9280165
diningtable 0.93984467
person 0.952791
chair 0.9146411
chair 0.9813333
person 0.9069641
person 0.9303641
person 0.9890906
person 0.92096066
person 0.96171665
person 0.98525244
person 0.91661197
person 0.92370766
person 0.98361087
person 0.9682074
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.342s + 0.035s (eta: 0:00:43)
person 0.93907684
person 0.95374197
pottedplant 0.9336337
person 0.9033528
bird 0.97188985
bird 0.948944
bird 0.9189903
person 0.96738386
person 0.92081267
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.330s + 0.040s (eta: 0:00:38)
person 0.95031583
person 0.90935916
person 0.9621945
person 0.96972847
person 0.9762803
person 0.947298
bottle 0.90506566
bottle 0.95199835
bottle 0.9021387
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.331s + 0.038s (eta: 0:00:34)
person 0.9082148
person 0.91544193
diningtable 0.9084727
pottedplant 0.931723
chair 0.9396405
chair 0.9161052
person 0.9759193
person 0.9403432
bottle 0.91039824
bottle 0.90846455
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.326s + 0.038s (eta: 0:00:30)
person 0.96235734
person 0.9604351
person 0.9480639
person 0.9948042
person 0.942347
person 0.94068414
person 0.9201883
person 0.97941786
person 0.911333
person 0.9577593
person 0.972959
person 0.9140433
person 0.9151624
person 0.9351193
person 0.93787426
person 0.99203616
person 0.940132
person 0.97379535
person 0.9255804
person 0.9613953
person 0.98376936
person 0.96582854
person 0.92712045
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.320s + 0.038s (eta: 0:00:26)
person 0.94792956
person 0.96437794
person 0.97097373
person 0.90432274
diningtable 0.9837997
chair 0.9160664
chair 0.9478337
chair 0.978997
chair 0.9873947
aeroplane 0.91284597
person 0.96662825
person 0.9813797
person 0.9316337
person 0.9157669
person 0.9116351
person 0.947209
person 0.9736222
car 0.9266564
car 0.9802452
car 0.971816
car 0.9585521
bird 0.95822346
chair 0.9310507
chair 0.9298316
person 0.9447767
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.320s + 0.037s (eta: 0:00:22)
person 0.9838859
person 0.93434674
person 0.9069236
person 0.98147905
person 0.9582498
person 0.98963284
person 0.9365635
person 0.92576563
person 0.9407065
person 0.9902507
person 0.93838876
person 0.9072481
horse 0.96240866
person 0.9120406
person 0.9054114
person 0.9196561
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.314s + 0.036s (eta: 0:00:18)
person 0.95445037
person 0.9453146
chair 0.9752308
chair 0.92125136
chair 0.9574724
pottedplant 0.9257171
pottedplant 0.9222304
pottedplant 0.9451991
person 0.94837165
person 0.90200186
person 0.9214496
person 0.9366666
person 0.9589087
person 0.938679
person 0.93222314
person 0.90116376
person 0.9025881
person 0.95515734
bicycle 0.9150912
person 0.9111443
person 0.97973067
person 0.9015613
person 0.9414525
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.313s + 0.036s (eta: 0:00:15)
person 0.9881422
person 0.9184652
person 0.9021787
person 0.9159494
person 0.9590023
person 0.9852046
person 0.9403225
person 0.9768747
person 0.9778269
person 0.9717704
person 0.9374554
person 0.9208648
person 0.91477674
bus 0.9550393
person 0.905718
person 0.94744295
person 0.93912727
person 0.96467066
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.313s + 0.036s (eta: 0:00:11)
person 0.91108924
person 0.9679612
person 0.91305244
person 0.93466324
chair 0.9130073
chair 0.94869214
tvmonitor 0.9019569
chair 0.9710541
chair 0.93213075
chair 0.9405958
person 0.94817674
person 0.9695858
person 0.956491
person 0.9773376
person 0.91978383
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.311s + 0.036s (eta: 0:00:08)
motorbike 0.91563356
motorbike 0.95178777
person 0.90051454
person 0.97835165
horse 0.9568955
horse 0.9020517
person 0.9007262
person 0.9031606
bicycle 0.95865226
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.311s + 0.036s (eta: 0:00:04)
person 0.9242141
person 0.95892894
person 0.9809428
person 0.9490802
person 0.9641706
person 0.9840846
person 0.9492644
person 0.955583
person 0.9423998
chair 0.96004874
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.312s + 0.036s (eta: 0:00:01)
person 0.9052252
person 0.98636127
person 0.9857283
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 85.906s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1180  284  280 ... 1004 1001 1000]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2494
INFO voc_eval.py: 171: [126 127  71 511  70 243 451 290 269 130 138 128 480 564 383  73 393 541
 569 487 455 567 562  74 509 381 571  72 552  86 476 134 272 136 568 579
  89 456 573 529 135 563  75 584 257  97  83 132 378 565 386 517 460 301
 379  78 295  91 139 521 129 554 543  92 467 303  76 244 470 249 561 144
 461 466 547 513 438 459 103 542 473 585 544 141  19 294 152 145 481 377
 312  43 518 297 465 390 291 247 540 248 241 245 283 410 343 520 385 424
 549 299  80 251 575 457 578 533  22 105 503 382 462 413  21 150 273 545
 284 429 147 516 107 436 512 140 164 267 515 131 458  41 274 557 581 307
 271 576 388 349 165 402 546 256 199 338 420 201 539 471 310 208  88  37
 319 151 570 464 439 220  28  25 523 242 409 282 582 556   2  34 488 137
 344 331 328 148 258 437 407 489 308 250 415 146 574  79 419 316 306 422
  18 566 572 550  42 519 391 305 262  44  33 300 321  84 432 577 553  56
 536 532 275 252 149 336 534 376 486 276 286 444 195 255 203 142 527  16
   5 246 580  93  98 555 430 392 468 309 133 106  90  87 558 405 524 198
 104 448 180 418 279 408 506 492 268 100  77 320 537  30 188 401 431  96
 504  46 583   1 472  20  24 293 264 528 254 340 296 209 441 479 143 510
 449 181 265  95 404 525 380  68 423 399 270  50 453  81 200 202 302 397
 281  38 339 400 366 514   0 178 163 318 285 280 101 311 196 191 332 224
 168  66 425 446 193 426 442 427  35 433 535 559 384 484 189  61 421 334
 118  32 123 185  26  62  55  27   3 450 440  82 485 197  17 204  47 414
 182   8 335 253  29 452 526 395 298 190 157 292 236  12 345 314 170 187
 313 412 477 225 469   6 322 475 117 102 501 443 394 417  15 315 194 167
 355 288 266 551 169 304 348 235  67 411 507 538 406 428 447  64 354 192
  65 113  23 490 463 396 356 360 375  31 530 228 502 158 548  36  13  51
 445 337  94 211 175 213  11 171 172 317 173 124 350 478 357 207 212 111
 398   7  58 374 206 531 362 234  53 287 166 416 217  60 403 210 261   4
 522  14 260 259 216 115  57 491 482 341 230 215 161 205 238 387 351  63
 110 186 155 324 352 373 365 346 156 454 154 323 483  69 183  85 389   9
  52 364 179 125 435  45 108 162 177 221 474 218 363 219 184  59 347  54
  10 361 330 342 159 434 174 353 153 160 114 176 109 227 112 325 508 263
 289 326 358  49 222 333  48 119 327 368 116 214 494 120 367 505  99 240
 233 370 500 223 226 239 371 495 232 372 229 497 369 329 496 586 122 493
 237 277  39 498 499 121 231 278 359 560  40]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3964
INFO voc_eval.py: 171: [ 645 1713 3237 ... 3478 3235 3230]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1940
INFO voc_eval.py: 171: [ 643  676  753 ... 3267  123 2280]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2406
INFO voc_eval.py: 171: [408 372 580 317  12  21 657 364 654 371 289 579 573 409 633 660 320 598
 340 551 380 415 609 295 674 147 487 834 663 554 594 693 331 760 833 329
  18 319 574 322 552 623 662 412 294 290 336 577 670 744  25 659 846 634
 656 365 729  16  22 676 338 669 672 296 708  20 373 835 374 383 621 520
 575 558 680 411 318 305 334  74 836 619 379 499 478 655 731 271 413 553
 638 292 326 323 505 683 844 222 299 592 333 260 595 664 378 743 519 839
  13 339 661 321 636 375 752 606  73 179 311 207 309 697 382 679 490 838
 310 608  46 685 737 618 414 587 681 557 327 500 480 120 625 367 581 181
 496 578 718 696 766 138 257 201 593 691 819 700 428 210  15 335 707 699
  26 507 562 182 706 531 216 668 717 675 479 576 266 591 622 251  19 677
 404 218 600  45 115 690 843 304 410 152 221 745 153 209 341 705 434 298
 702 262 624 804 770 802  53  17  75 173 687 162 177 779  85 610 172 585
 502 328 620 376  44 850 689 801 727 272 559 637 366 698 325 308 599 442
 704 615 337  24 555 407 264 156 267 665 133 582 476 203 394 666 749 241
 862 640 332 148 607 803 604 556 561 805 810 721  30 426 217 522 424 122
 432 105 806 596 751 324 171 527 695 454 673 368 818 503 614  47 377 589
  33 270  84 330 243 658 381 800 107 206 252 590 852 854 678 536  72 682
 247 758 291 300 303 238 174 453 530 187 224 119  86 560 504 719  48 183
 684 313 588  66 342 847 694 722 488 301 597 767 667 139 343  23 526 641
 692 137 131 652 535 703 293 750 235 586 102  62 101 498 671 116 481 118
  14 268 776 397 603 223 468 159 583 246 584 406 688 497 273 849  83 602
 848 616 728 701 775 200 605  27  58 136 765 363 211 121  92 613 405 831
 229 100  36 601 748 261 840 204 306 781 712 863 686 269 202 117 256 155
 234 626 713   6 492  57 533 789 489 501 617 612 832 282 255 796 611 780
 506 219 314 178 532 110 163 494 427  42 777 275 730 344 421 811 732 493
 720 220 395 189  63 449 864  43 354 747 240 733 866 521 403 265   1 774
 143 245 307 135 369 186 114  56 297 425 104 302 205 455 456 757 123 113
 125   5 642 124 735 170  78  52 755 154 528 198 190 103  96 815 709  49
 858 795 773 106 150 761  91 768 470  35 817 845 565 180 523 188 157 254
 529 423 534 370 239 396 842 635 175 249 509 837 230 242 865 762 422 276
 149 151 185 778 477  50 820 763 263 549 754 764 785 258 495 433 226 346
 756 525 738 543 208  65 429  28  97 278 812 134 474   2 197 475 759 734
 857 807 471 160 280 168 259 161 859 784  95 253 108 437 739 132  31 860
 769 629 165  82 438  98 250  29 746 353 510 469 450 430 279 431 797 545
 225 244 228 564 524  51 452 109 856 855 232 723 164 167  38 457 753 653
 184 169  77 482 248 643 166 724  80 782 444  90 841 127 390 544 816 639
 362 484 491 799 312 546  94  55 783 548  67   0 176 467  69 277   4 281
 511 725 158 547 112  99 464 486   8 563 146 861 736 572 195 627 284   3
 813 358 385 537 274 550 771 726 387  61 214  40 569 145 711 651 386 485
  60 568 436 483 463 448 628 451 512 473  81 808 571 345 814 809 287 357
 541 144 566 111 741 356  39 853 787 540 443 213 355  87 570 710 472 142
 567 798 513 542 227 399 233 435 851  34 466 446 791 447 420  68  41 538
 790 539 283  32  54 236  76 518 632 646 288 462 400 508  79 648 389 361
 388 359 398 360 645 631   9 286 445 742 714  37 315 231 630  70 391 788
 316 352 401 196 285  64  93 828  11 212 715 827 793 772  89 392 514 740
 465 716 194 347  10 351 237  71 384 402 350  88  59 126 517 460 418 461
 649 140   7 650 825 129 348 829 830 826 824 199 516 786 441 349 130 792
 821 458 794 515 215 393 417 193 128 822 644 439 191 419 416 823 141 440
 647 192 459]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3307
INFO voc_eval.py: 171: [435 171  91 172  93 436  89  76  96 100  98 457  97  74  92 439 367  75
 119 179 402  26 105 102  73  78 404  79 211  43 409 393 414  85 251 388
 163 210 263  95  81 366 443 381 440  32 141 273 397  44 360 266 368 162
 294  53 394  88  52 197 106 187 362 164 235 262  83 156  86 478 116 426
 335  77 218  42 432 140 118 201 222 392  84 108 320 291 240 331  11 205
 254 351  87 309  46 260  80  13 115 198 196 274 117 209 336 284 332  82
 460 195 267 151 123 340  30 373 247 391 337 176 396 380 208 354 264 137
 400  56 328 363 253 212 249 416 377 350   8 186 419 379 334 144 246 316
 103 177 413 407 155 286 410 389 193 434 227 149 121 120 423 327 431  47
  22 319 283 378 356 458 259 257 229 225 312 361 358 244 322 199 298 376
 285 252 168 390 352  31 129 290  27 122 150 385 415 138 202 430 221 277
  16 355 127 421 226 223 329 371 256   4 296 420  35 146 442 125 330 359
  51 133 200 353 425  39 112 375  94 215 313 139 459 314 268 126 295 408
 173 477 250 206 466 136 113 124 101 161 417  57 275 204 357 114 384 224
 165 130 427 468  33 308 476 178  29 311 454  90 428   0 148 107 280 169
 272 399 160 157 382  36 232 398  99 203  45   5 387 342 230 467 245 395
 278 333 258 321 234 207 401 406   3 128 236 109 411 154 470 248 433 418
 153  65 343  55  41 241 261 369 142 452 403 323 370 145 239 317 213 287
 307  28 181 279 147 462 424 228 143 276 422  66  70 324 374 292 386 465
 158 217   7 438 167 471  58 265 269 372 170 255  10 183 326 364 243 281
 131 325   6 450 441 472 412 405 341 110 437 303 231 233 348 152 315 310
  37   9 271 288 447 338 104 293 318 111 166 463 194 270  17  40 282   1
 174 449 464 132 383  38  34 219 289 429 159   2 339 214 305 300 182  48
 188  59 365 461 306  54  71 190  23 304 238 216  61 242 301 455 444 220
  19 451 237  20  60 475  25 345 184 299  64 297 180  50  14 344 302 445
 347 134  68  62 474  24 448  18 473 189 135  72 175 191 185  15 456 192
  12 453  49  69 446  21  67  63 469 346 349]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4222
INFO voc_eval.py: 171: [1591  719 1525 ... 1503  118  100]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2862
INFO voc_eval.py: 171: [ 29 261  64 107 153 316  65  19 314 108 287  24 117 262  66 152 253 315
 199 124 286 151 263  26 295 229 257 109 290 198 208  74  83 288  52 121
 132  94 247 230  48  93 238  50  16 116 260 136 101 142 114  69 170 168
 231 189 174 306 188 150 296  42  20  75 104  27  91  63 123  71 202 224
 216  80  17 292 171 115  95 120 163  18 112 197 294 252 241 248  21 156
  28  55  22 214 110 285 162  73 191 255  76 234  81 201 169  54 164 129
 239 187 291  23  90 232 254  51 200 149 212 155  85 213 293  82 194 154
  86 289 310 126  56 118 307 243  77 259 282 313 144 122  53 308 235  25
 217 236 192   5 181 147 143 148 223 221  47 190 233 102  68  88 237 137
 240 158 265 125  84 135 130  87 127 134 111 220 161  49 167 256 242 215
 268 131 284 222 106 309  44 279 311 219 317 145 113  45  14  67 133  10
 301  92 100 206 245 193 211 250 312 138 244  89  46 297 249  41 275 105
  96   8   4 103 203   0 180 119  35  39  43  15 227 280 196  99 141 205
  30 210 218   6  98 266 267 281   2 276 128  34   3 166 258 178 207 277
  97 246   9 283  60 269 172 165  36 228  38   1   7  12  59  31 157 159
 195 299 177 160  58 274  72 204 302 270 175 278 209  32 184 298 273 304
 305  33 271 225  13  62  57  78 146 300  79  61 182 139 173 226 303  70
 176 264 186  11 251  40 179  37 272 140 183 185]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0977
INFO voc_eval.py: 171: [5853 2519  711 ... 4512 4378 4379]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3095
INFO voc_eval.py: 171: [332 260 197 446  60 203 283 387 274  51 699 257 762 259 273 262 284 454
 256 414 452 754 646  72 270 223  79 730 333 411 263 196 614 635 449 336
 158 198 199 383  66 183 753 564  75  56  55 180 139 202 230 258 450 456
 205 376 280 451 447 128 227 756 613 130 207 276 445 384 368  71 461 268
 565 570 172 758 443 757 323  62  70 539 647 444  33   7 278  67  21 113
  69 271 422 224 759 741 404  73 195 566 631 726 460 286 617 458 523 704
 540 206 388 448 247 244 395 627 267 642 309   6 615 616 727 211 738 266
 360 153 265 345 457 459 761 703 766 272 559  50 148 720 521 535 322 341
 290 201 200 649 394 632 217 362 428  59 476 221 291 636 204 178 344 413
 420 733 734 637 575 724 346 633 229 277 737 520  11 117 770 264 571 245
 145 668 543 670 731 240 282  42 625 351 755 218 736 382 269 184 421 385
 335 453 423 663 238 127 700 187 728 412 179 567  52 219 442 150 424 334
 666 619 662 763 243  16 294 213  65 393 210 312 561 572 415 573 186 648
 430 467 650 289 545 562 176 706 182 126 568 339 154 634 735 142  92 222
 132 723 418  81  49 324 285  10 318 349 739 188 347 522 239  29  30 261
 653 498 772 359 389 133 417 236 367 129 410 181 386 768 316 425 131 232
 624 455 721 426 155 237 419 745 764 533 175 719  47 765 480 511 429 106
 185 749 416 216 249 361 524 710 578 167 246 709 563 315 392 166 694 715
 146  68 750 638  77 652 626  82 399 209 136 381 288 177 621 718  48 707
 391 275  22 162 338 325 462 281 427 682 226 337 340 313 314 722 560 686
 531 744 499 576 370 215 659 785 147 783 108 512 287 684 111 536 279 541
 160  96 397  87 241 102   8 769 729 544  31 343 509 664 740 592 330 702
 760  17 208 574 401 295 698 484 784 501 365 342 435 554 767 526 292 542
 137  90 317  27 140 253 107 120 711 161 242 742 630 623 400 628 486 667
 584 569 328 732  44 510 502 725 296 671 355  14  15 380 680  24 654 405
 752 163 463 390 293 781 116 538 586 320 716 669 109 220 577 485  23 546
  57 641 748 515 644 103 228  93  54 746 537 190 329 743 620 774 773 159
 665  34 494 751 547 156 173 747 657 786 212   0 492 697 372 645 398 500
 640 683 599 402 528 157 611 403 396 655 321 466  26 629 691 319 104 231
 483 525 587  95 519 134 688  32 596 468 194 514 608 771 364 143 225 310
 373  36  25 214 138 369 255 692  61 529 105 110  20 141 331 534 708 696
  80 326 250  35 121 553 235 660 610 595  89 607  19 775 135 513 491 112
  12 252 174 251  85 488 440 693  64 169 327 125 643 603 115 233 527  28
   2 674 685 234 678 593 151 377  78 609 508 481 366 438 164 530 433 168
 594  43 598 375 532 658  74  94 550  53 679 248 672 149 690 165  39 254
 371  41  13 701 192 782 713  37 585 651  83 496 439 434 495 712 695 363
 597 193 558 639  18   9 171 497 487 352 189  86 689 123 601 357 170  84
 152 676 114 122 118 306 661 464 505 493 604  98 490  63 600  91  88 507
  76  45 618 556 482 675 378  97 489 354 602 605 677 311 612 431 506 656
 191  40  46 673 681 101 474 549 472  58 548 432 555 714 552 406 551 379
 503  38 516 300 119 124 469 436 308 348 504 606 301 437 408 557 298 407
   3 705   1 307 470 356   5 479 441 622 409 465 473 590 303   4 475 517
 780 478 717 374 687 350 297 477 353 358 305 299 591 471 779 776 518 302
 304 144  99 579 588 778 100 589 777 582 581 583 580]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2066
INFO voc_eval.py: 171: [2908 1284 1339 ... 2809 2810 2318]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3284
INFO voc_eval.py: 171: [346 411 349 848 446 503 377 356  49 484 467 449 628 145 552 352 504 409
  63 444 161 779 542 143  46 627 280 508 150  50 160 204 515  41 378 524
 850 154 358 844 276 238 741 801  48 127 414 793 410 451 141 464  65  45
 528 205 802 152 775 454 774 146 480 838 270 519  40 347 853 469  62 790
 818  51 435 376 841 373 221 443 472 357 829  82 807 625 148 626  53 764
 247 737 776 544 271 648 846 225 163 237  68 777 226 517 584 535 466 553
  64 549 476 486 417 507 666 248 488 243 859 422 418 359 603 857 283 825
 153 438 240 262 849 348 722 465 164 144  54 224 830 675 199 453 646  44
 415 847 416 473 125 207 239 214 452 477 434 437 674 548  52 617 522 665
 200 151 297  84 338 780  61 252 610 279 823 620 782 824 326 661 738 827
 664 193  31 778 353 709 229 593 808 811 588 618 131 513 222 708 798 789
 762 733 602 763 159 431  67 157 217 259 448 162 653 440 608  69 550 601
 265 196 772 215 384 282 381 231 413 230 864 748 432 441 516 350 546 433
 129 839 412 382 815 420 206 227 158 809 374 710 638 147 208 787 149 425
 142 804 676 130  47 195 320 765 436 165 135 743 607 851 647 805 816 126
 855 622 344 219 556  59 634  56 439 784 744 426 295 250 232 337 423 837
 156 274 445 598 314 310 116 272 624 649 706 828 351 645 263 110 177 267
 814 749 685 599 700 514 112 803 483 289 354 852 867 155  15 140 268 810
 670 442 591 575  42 491  66 290 543 740 785 194 294 128 526 868 132  86
 334 197 198 281 245 698 244 677 619  60  91 576 725 632 284 385 304 134
 773 826 812 682 470 781 482 521  30 124 791 285  89 651 604 834  25  57
 273  83 355  58 421  93 183 726 234 235 728 242 122 739 788 223 860 792
 672 278  22 833 246 293 137 386 623 277  19 862 429 275 574 117 745 783
 621 328 383 611 571 858 863 671 585 249 678 303 597 683 681 361 233  14
 288 555 478  87 113 614 216 111  55 114 613 669 831 558 212 447 302 251
 218 570 786 612 427 120  12 530 655 101 545 343 704 462 366 430 253 450
  29  32 220 713 375 705 316 547 724 551 750 539 391 673 606 734 315 529
 481 701 309 754 307 254 821  85 865 554   4 730 573 684 269 735 557 840
 817 102 292 689 213 419 654 731 345 185 118 819 820 260 534 299 813 861
 822 866 264 559 296 842  90 650 327 236  18 832 479 729 679  88 428 690
 335 843  34 686 727 636 854 408  92 520 845 663 652 691 402 572 694 323
 732 329 186 635 261 641 856 363 123 266 510 595 287 696 119 372 577 360
 187 637 133 104   3 806  70 461 340 401 590  20 667  26 321 301 680 746
 869 509 605 596 587   5   9 318  94 660   1  81 319 181 174 305 312 258
  28 736 540 362 642 339 527 609   2 394 505  43 565 533   0 687 241 600
 518  78 188 257 336 768 255 115 308 799 489 298 364 796 291 300 633 324
 751 332 105 563 286 211  79 615 499 695 380 495 487 616 365 455 536 752
 325 317 311  39 742 370  38  27 629 640  35  95 492 458  13 179  76 167
  23 424 121 138 797 397 717 566  98 541  99 639 173 644 136 688 712 589
 367 759 497 537 468 190 538 342  17 457 757 658 306 203 168  33 456 166
 313 561 643 659 560 506 369 209  24 485 256 475 500 139 407 182 523  77
 747  73 180 460 795 400 403 692 176 755 714 531 404 368  97 405  96 794
 800 493  21 103 471 718 191 494 490 581 474 656  74 580  72 586 512 716
 459 594  80 175 184 178 583 657 100 582 532 662 463 562 322  75 753 592
 202 389  71 379 108 707 760  16 496 210 511 703 192 699 579 189  36  37
 693 761 668 201 758 567 109 569 406 525   6 330 396 711 390 702 498 630
 387 723 107  11 578 568 371 769 756 771 719  10 341 631 721 331   8 228
 172 564 715 720 333 388   7 171 106 170 169 399 770 767 766 501 502 398
 395 393 835 392 836 697]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2165
INFO voc_eval.py: 171: [129 946 432 ... 564 768 769]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3041
INFO voc_eval.py: 171: [ 47 275 262 142 147  26  46 151  48  51 143 282 117  72  34 159  68  31
 269  78 145  59  56 189  57 250 168 115 270  73 264 161  53  20 199 172
 206 130 194 271 272  36 254 242  33  25 285 162  32 256 245 263  30  77
 266 279 148 116 286 150 156 283   6 154 198  52  17  82  61 165 216 114
  50 144  22 278 197  84  62 184 226 146 191  99 113  76  85  60 205 141
 196 158 267 287 244  18  88  10 268  35 233  97 108  66 261 213 243 166
  43 219 126  23 265 255 152  65  19  27  54 167  67  80  58 246 202 134
 163 185  95  79  70 112 125 149 247  12 186   4 221  38 232 241 253  29
   0 259 236  93 155 273  21 284  98  39  14 195 110 153  64 131 252 200
 157   5  41  75   8  90 139  16 274  11  71 222  94 105 138 208  37 218
  92 280 230  28 137  81 238 188 220 249 102 204 211 169  24 257 103 135
 177  69 251 187  15 133 164  96  86 181 140   2 101   3  49   9 215 109
 100 173 192 170 119 106 228 277 281 210  87 121 107  40   7 123  45 182
 225 174  83 235  91 237  13  55 171 176  74 124  89  42 111 183 175 217
 104 212 178 214 239 201 128 207 209 180  44   1 118 203 223 224 231 190
 127 160 122  63 193 179 276 227 234 248 229 129 120 258 240 136 260 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4999
INFO voc_eval.py: 171: [ 8563 13992  1048 ... 10870 10869 10871]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5398
INFO voc_eval.py: 171: [2511  258  715 ... 2709 2707 1829]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4152
INFO voc_eval.py: 171: [ 64 315 539  32 335 548  65  54 131  39 542  48 339 543 331  43 319 618
 540 155  33  45 349 483 220 134 329 552 345 544 254 313 157 342 553 547
 408 124  35 137 649 284 346 414 637 545 307  69 367 144 424 619 143 323
  66 541 145 161 333 136 420 156 351 227 226 627 318 491 628  58 495 225
 268 566 623 410 320 625 158 336  50 122 614 562 629 484 153 219 252 632
 311 465 334 413 640 255 353 567 317 621 163 139  38 529 622 218 348 501
 624 231 556 417 233 337 493 164 651 328 129  44 502 416 194 641 322 366
 138 133 449 518 494 368 617  42 316 312 258 289 633 267  63 234 330 253
  90  60 372 162 647 314 132  95 222 497 652 482 578 130 229  91 411 285
 347 650 423 350 639 675 232 672 123 327 634 150 310 344 571 642 510 338
 141 409 256 563 421 244 230 308 142 646 506 160 135 511 148 321 126 568
 309 434 265 325 172 279 620 615 140 412 572 513 365 611 166 565 638 340
 526 635 324 263 283 221 407 631 128 341 673 206 683 198 636 326 370 264
 643 149 671 125 332 531 551 343  77 630 415 557  41 488 612 645 644 127
 223 165 304  49 489 356 269 549 199 583 496 159 601 464 467 452 485 648
 570 505 266 228 100 101  14 294 508 573 195 500   5 201 378 355 626 509
  62  27 613  53 200 192 443  68  34 564 466 369 379 108 580  24 676 352
 154  37 546 358 270 381 616 523 205 186 245 121 555 170 492 654 261 357
 120 193 257  94 180 196 354 481 224 275 300   0 260 242 167  93  89 519
  23  92 197 514 685 521 259 680 188 507  16  70 274 184 418 119 377 246
 674 393 302 653 385 262  46  96  36 558 468 173  22 271 272 659 361 442
 655 520 479 522  13 439 177 286 428  86 373 681 559 575   8 486 214 152
 524 168 301  30 471 112  47 427 446  59 107  78 105 249  76 436 512 560
 182 447 247 433 451 581 111 181 504  80 561  83 422 677 600 591 104 461
 212 169 515 299  25  28 607 550 435 682 185 490 293  20 209 462 437   2
 477 459 463   1  15 278 605  99 684 425  17  55 582 517 657 516 686 213
 210 602 282 584 525 371 678   7 609 395 405 480 250 478 211   6 554 579
 398 445 305 679 176  56 450 240 527  18 499 606  75  67 110  19 576 374
 603  72 109 577 532 569 106  40 277  21  31  73 487 281 656 292 444 280
 288  79 470 503  51 183  74 399  71 472 419 114 457 273 306 530 116   3
 113 207 375 604 528  87 426 248  52 498 430 599 448  57 287 179 441  61
 588 251 291 432 475 216 394  26 386 303 438  97   4 360 290  29 151 191
 276 376 362 190 598 208 117 429 363 404 595 202 401 431  12  85 592 359
 217 118  81 383 587 536 590 663 400 610  82 175 406 589  98 585 574 476
 203  88 594 586  84   9 115 236 661 473 215  10 665  11 390 178 534 364
 103 474 596 593 380 237 387 608 535 454 189 235 597 384 397 204 238 295
 241 391 174 243 664 187 298 469 239 662 396 171 533 392 670 382 389 147
 388 402 668 667 460 146 297 296 455 403 660 537 453 456 538 666 458 102
 669 658 440]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3043
INFO voc_eval.py: 171: [ 41   4  45 116 183 177 271  46 121 330  48   7 186 181 227  50 151 184
 155  13 187 326 149 152  40  58 333 273 145 175 180 122 224 269 214 150
 136 179 222 266 228 267 237  98 144  78 270 123  76 241 329  97  47 272
   6  56 119 102 246  57 156 146 157  11 168  23 321 115 223 178 140 124
  55 317 298 205 263 238 129  42 318 302 169 182 176 226 103 236 230  73
  70 225  44 208 160 153   5  96 154 114 185 262 127  93 100   9  49 240
 243  14  12 314 312 328 101 244  87 253 320 332 259 125  26 239  15 138
  94 117 242  71 280 250 245 324  99  43 229 201 204 323 120 147 331 300
  88  81  61 164 212 311 235  82  95 196  72  59 254 334  25 110 193 252
 299 260 305 316 285 221  27 189 286 220 112  77  62 142 216   8 118  83
 109 113 128 188  10 276 161 249 313 203 232  80  92 304 211 261 126 296
  74 143 111 248  75 315  53 148  60 306  32 325 290 233 327 215 217 219
 255 319  38 264 256 308 268 139 301 294  29  24 194 251  86 289  63 281
 209  28 335 206 165 322 210 170 108 137 288  85 167  52 231  30 213 198
 287 307  51  79  54 141 218 257 107 191  31  22   2 275 135 130   3 297
 282   0 247 258 190   1 195 291 202  84 274 163  18 133 200 105 199 295
 292 192 104 162 284 234 197 293 172  39 158 131  19 277  35 106 132 166
  21 171 265  20  34  17  36  16  69 134 283 173 159 174 279 278  67 309
  33 303 310  68  65  37  64  66  89 207  91  90]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1317
INFO voc_eval.py: 171: [637 380 644 120 649 318  86 208 392 289 126  80  82 401 172 891 641 642
 290 320 317  89 185 122 394 801 129 213  98 293 211 797 506  88 386 537
 209 635 381 192  87 452 639 657 672 451 297 102 207 454 124 319 121 323
 191 900 906 892 303 334 180 292 829 805 511 291  92 464 385 524 541 876
 848 839 706 903 400 169 217 139 140 204 219 834 905 638 296 527 544 328
 387 455  65 489 863 432  46 322 103 728 745 307 117  21 174 218 383 844
 141 811 198 309 678 321 877 195 707 664 656 895 804 456 109 831 798 177
 509 199  84 904 531 458 260 277 170  90 173 110 442 833 436 438 347 222
 186 816 189 754 729 907 231  47  72 101 622 301 510  22 395 171 878 540
 553 898 874 751 200 526 823 893 304 326 717 182 210  36 305 888 188 911
 739 168 176 838 734 710 552  69 197 462 133 245 295  67 883 215 105  77
 447 242 457 894 512 107 835 453 488 783 780 214 826 248  45 228 461 868
  59 867 187  91 223 546 264 543   2 814 450 802  96 713 391 382 372  54
 847 716 700 703  19 331  97 184 183 709  99 500 899 708 262 258 636 687
 601 311 579 683 758 528 508 886 640  20 924 162 875 917 396 181  60 484
 435 832 589 206 397 865 467 405 431 324 849 781 827 316 300 114 909 753
 138 104 375 870 351 459 515 828 439 486 771  53 399 736 313  74 813 676
 235 389 175  44  34 627 398 465 234 373 606 819 466 885  79 179  41 149
 873 519 908 237 261 923 550 443 495 679   6 342 201 817 759 902 433 355
 286 274 542 250  38 507  75 430 100 879 860 354  73 864 608 690 742 846
 562 253 910 271 659 148 530 926 525 615 463 263 665 684 896 118 469 799
 298 822  85   8 212 106 128 654 116 378 161 548 820 127 178 230 190 810
 166  58 574 856 247 221 744 619 478 315 167 532 249 251 901 777  78 520
 887 756 444 686 726 854 536 545 685 554 497 916 194 353 824  48 605 889
 423 884 216   9 573 202 881 352 743  40 142 688 327 633 239 599 837  76
  93 825 737 732 115 238 491 468 259 306 897 695 227 335 220 755 164 255
 565 241 681 845 725 765 131 308 869 623 529 516 441  39 403 872 852 871
 699 460  57 329 402 440 576 243 203 229 434  43 613 890  56 272 567 475
 158 193 774 312 602 588 809 727 628   0 390  42 925 604 648 714  94 280
 730 757 843 600  15 521 603 705 621 789 556  63  83  37 348 376 788 421
  50 821 721 343 547 111 384 794 631  61 609 196 393 568 747 498 437 388
 278 429 345 680 927 119 818 882 853 502 333 738 284 922  24 578 157 577
 357 768 494 769 689 677 236 267 135 560 415 625 143 538 518 570 374 240
 682 310  12 720 294 836 279 770 586 244 851 787 784 620 746   4 772 782
 422 790 791 711 256 346 585  62 273 299 592 855  55 612 276 555 580 607
 479  64 806 246 712 358 379 283 733 866 445  35 575 646 704  33 785  68
 741 702 449 314 719 581 420 842 480 611 694 634 377 840 752 113 698 610
 365 501 281  49 675 767 701 424 205 268 485 159 112 285 275 559 490 748
 108 880  95 514  70 786  32 616 626 364 808 477  81  23  18 750 404  71
 912 779 269 551 807 572  10 815  25 325 418 302 566 446 367 533 584 591
 517 474 232 252 792 341 233 587 795  16 582 282 841 722   3 919 558 472
 350  17 571 448 557 569 861 749 265 419 775 653  66 670 254 356 257 339
 493 362 614 723 778 776 503 492 740 918 624 803 667 731 165 504 594 534
 800 470 812  52 735 696  11 266 412 505 697 473 549 145 476 411 796 471
 522 724 539 830 366 496 409  13   7 483 156  51 913 583 427 487 337 597
 715 371 593   1 155 370  14   5 590 287 499 793 523 669 426 773 154 718
 136 618 766 645 513 850 535 862 596 674 481 288 482 693 368 425 369 595
 349 428 561 160 359 407 330 764 658 643 361 360 629 163  31 414 413 340
 363 763 152 137 332 920 147 857 632 224 655 416 915 406 563 564 921 598
 630 662 617 663 123 336 859 692 761 226  26 762 858 150 151 408 760 661
 338  29 410 146 344 417 671  30 225 132 691 270 651 134 125 130 144 652
 914 647 153 666  28  27 673 650 660 668]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2874
INFO voc_eval.py: 171: [422 250 594 172 435 298 136 443  24 469   1  37 478 498 254 321 352 475
 480 323  39 398 439 252 248 350 420 351 126 124 470 577 310 299 551 353
  56 263 610 423  67 253 473 535 106 324  70 129 230  26   3 446 595 440
 520  17 317 178 432 329 477 340 180 304 349  43 320 584 598 618 154 374
 627 226 358 101 606 368 613 259 377 150 442 372 308 536 220 108 445 109
 474 206 137 322 209  19 264 486  73  69 223 437 144 160   8 302  50 557
 601 465 506 306 205 533 472 382 273 378 484 234 325 102 616 365 463 397
   9 362 359  57 395  40 367  28 185 243 332 119 632 162  16 214 161 483
 622  46  55 485 176  44 600 429 128  83 407 301 592 300 314 208   6 514
 427 257 287 578 505 476 345 544  41 305 307  52  71 315 481 224  29 502
 143 118 213 100 373 570 338 560 228 561  48 177 210 471 519 309 312 434
 330 212 216 400 318 631 623 597 379 215 125 479 170 428  38   2 265 158
 131 620 188 173 211 116 191 171   5 612 384 580 581 227 120 114  92 225
  89 554 357  74 436 356 366 515 396 105  30 549 271 482 406 182  12  27
  45 335 361  90 550 599 123 303 496 626 555 507 281 399 564 293  18 184
 611 416  23 401 174 562 453 614 111 621 455  13 343 284 547 628 441 207
  32  99 130 311  54 319  87 153 159 500 169 530 542 516 540 609 198 563
 537 145 523 617 596 556 414 237 285 363 277 534 280  97 278 142 404 497
 360 629 276 489 624 526 364  94 387  93 574 625 608 615 501 113 239 217
 454 190 334 499 110 148 569  33 219 380  86 279 518 333  81 204  53 504
  62  77 282 256 155 543 585 619 313 218  91 587 316 385 521 221 147 369
 370 567  42 451 262 545 138 458 517  60 532 274 275 421 546 115 503 179
 127 371 635 355  75 286  76 103 376 449 511 117  79 255 268 431 541 452
 186 229 568  35 573 424 166 410 381  59 331 383 354 438 375 133 447 202
 241 201  85 122  34 630  51 157 457 589 269 634 107  49   4 283  31 391
 258 590 141 326 288 233  36 464 524 450  15 339 591 430 433 337  58 149
 272 292  11 146 242 104  63  68 165  80 576 291 633 466 121 341 156 553
 231 328   7 607 487 586 412 139 112 152 548  22 488 132 425 531 559  88
 566 538 461 296 529 151 238 197 539  25 189 426 140 528 289 448 394  84
 196 183 240 558 135 134 405 181 575 510 527 409 411  82 295 327 235  95
 444  66 297 260 222 249 167 408 168 403 522 565 389  96 199 294  10  65
 467 175 164 232 261  72 413 342 251 344 552 390 583 336 392 163 200 525
  78 346 571 512 402 588 187 572 388  61 415 270  21 508  98 236  64 460
   0 509 290 347 513 582 247 393 386 492 605 456  20 246 604 459 602 348
 245 603 267  47 462 490 491 417 193 579 195 266 203 418 593 244 495 493
  14 419 192 194 494 468]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3369
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3049
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.249
INFO cross_voc_dataset_evaluator.py: 134: 0.396
INFO cross_voc_dataset_evaluator.py: 134: 0.194
INFO cross_voc_dataset_evaluator.py: 134: 0.241
INFO cross_voc_dataset_evaluator.py: 134: 0.331
INFO cross_voc_dataset_evaluator.py: 134: 0.422
INFO cross_voc_dataset_evaluator.py: 134: 0.286
INFO cross_voc_dataset_evaluator.py: 134: 0.098
INFO cross_voc_dataset_evaluator.py: 134: 0.310
INFO cross_voc_dataset_evaluator.py: 134: 0.207
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.216
INFO cross_voc_dataset_evaluator.py: 134: 0.304
INFO cross_voc_dataset_evaluator.py: 134: 0.500
INFO cross_voc_dataset_evaluator.py: 134: 0.540
INFO cross_voc_dataset_evaluator.py: 134: 0.415
INFO cross_voc_dataset_evaluator.py: 134: 0.304
INFO cross_voc_dataset_evaluator.py: 134: 0.132
INFO cross_voc_dataset_evaluator.py: 134: 0.287
INFO cross_voc_dataset_evaluator.py: 134: 0.337
INFO cross_voc_dataset_evaluator.py: 135: 0.305
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 6999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.508s + 0.030s (eta: 0:01:06)
person 0.9701863
person 0.96366984
person 0.99233514
person 0.96074617
person 0.9732833
person 0.92622066
person 0.9384721
person 0.98239154
person 0.94522464
person 0.9112548
person 0.9815985
person 0.93873435
bottle 0.9221494
bottle 0.91955334
bird 0.9513194
person 0.98251706
person 0.9181446
person 0.9445121
person 0.9032953
person 0.90488786
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.317s + 0.034s (eta: 0:00:40)
person 0.944602
person 0.91968656
person 0.96035516
person 0.9483459
person 0.9806043
person 0.9476384
person 0.96760714
person 0.9818233
person 0.9396526
person 0.9215073
person 0.9270773
person 0.93166393
person 0.9060534
person 0.9227053
person 0.9207779
person 0.92688537
person 0.9753406
person 0.91250795
person 0.98012483
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.321s + 0.035s (eta: 0:00:37)
person 0.92887247
person 0.9124226
person 0.95618975
chair 0.91454905
person 0.9723556
person 0.91432816
person 0.957322
person 0.96628064
chair 0.9656574
chair 0.93719506
chair 0.9147161
person 0.9036352
person 0.9790926
person 0.98794764
person 0.92824054
person 0.9048167
person 0.9769597
person 0.9915986
person 0.9814051
person 0.9116034
person 0.9083829
person 0.90846974
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.324s + 0.035s (eta: 0:00:33)
bird 0.93912023
person 0.9275901
person 0.9212725
person 0.9938227
person 0.9567975
person 0.9745008
person 0.97983426
person 0.9083924
person 0.95953435
person 0.9659371
person 0.9050345
person 0.9184178
person 0.9696512
person 0.94068855
person 0.9586269
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.323s + 0.036s (eta: 0:00:30)
diningtable 0.90181273
diningtable 0.93904686
chair 0.9637007
chair 0.9658978
chair 0.9381156
chair 0.9573298
pottedplant 0.91823846
pottedplant 0.914623
pottedplant 0.9108141
pottedplant 0.9045938
pottedplant 0.9437203
pottedplant 0.9382528
pottedplant 0.9292107
person 0.965739
person 0.9828414
person 0.9208544
person 0.97972625
person 0.94904554
person 0.9889894
person 0.91518384
person 0.9035439
person 0.91230875
person 0.92116475
person 0.92170656
person 0.9734374
person 0.95470923
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.322s + 0.036s (eta: 0:00:26)
person 0.95728916
person 0.923307
person 0.90580845
person 0.92005944
person 0.9468957
person 0.9139093
person 0.95336705
person 0.93653125
person 0.929103
bird 0.9227222
person 0.93642056
person 0.98764867
person 0.917021
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.314s + 0.036s (eta: 0:00:22)
pottedplant 0.9033076
person 0.9623027
person 0.93306625
person 0.91155833
person 0.9538349
person 0.9183541
person 0.9586296
diningtable 0.90972036
chair 0.98267394
diningtable 0.9613401
chair 0.9165285
chair 0.94836104
chair 0.9806877
chair 0.92719686
bird 0.99305975
person 0.9023442
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.312s + 0.035s (eta: 0:00:18)
car 0.9286903
car 0.94207895
car 0.91430587
person 0.9884587
person 0.90040743
person 0.9741203
person 0.9297471
horse 0.97407
horse 0.9137586
horse 0.92077607
person 0.9379361
person 0.95336074
person 0.96821475
person 0.9184377
person 0.94206053
car 0.945829
car 0.9642987
bicycle 0.9607839
bicycle 0.9307279
person 0.9373402
person 0.91717947
person 0.973327
chair 0.9182402
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.311s + 0.035s (eta: 0:00:15)
person 0.97425395
person 0.95895755
person 0.9735971
person 0.9164752
person 0.91268677
person 0.9005427
person 0.9885889
person 0.98637474
person 0.943373
person 0.9163031
person 0.96419185
person 0.96317136
person 0.9911548
person 0.9076368
person 0.9666963
person 0.91146904
person 0.9877834
person 0.9799509
pottedplant 0.91213554
person 0.9899087
bus 0.9127536
person 0.9615384
bus 0.90778315
person 0.9413858
person 0.95431757
chair 0.94640785
person 0.9056611
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.312s + 0.035s (eta: 0:00:11)
motorbike 0.96375597
person 0.9748191
person 0.91093636
person 0.9152551
person 0.9565715
person 0.9671002
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.312s + 0.035s (eta: 0:00:08)
person 0.90524054
boat 0.9819896
boat 0.94813323
boat 0.9182803
person 0.990348
person 0.93743896
person 0.9855342
person 0.91821355
person 0.9198618
person 0.9890071
person 0.9749391
person 0.91028196
person 0.97486854
person 0.9766456
person 0.9132039
person 0.9485575
car 0.9505603
person 0.93961066
person 0.9470131
person 0.9598957
person 0.9867605
person 0.9066621
person 0.92054254
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.315s + 0.036s (eta: 0:00:04)
diningtable 0.9594728
chair 0.92436194
chair 0.9738874
chair 0.98074543
chair 0.9146462
chair 0.9810524
chair 0.90151155
chair 0.9363595
chair 0.960224
person 0.94218624
bird 0.92398494
bird 0.9144773
boat 0.9199171
person 0.96204257
person 0.9286931
person 0.98875797
person 0.9141249
person 0.90246093
person 0.91253614
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.315s + 0.035s (eta: 0:00:01)
person 0.98345554
person 0.96422565
person 0.9562423
person 0.95547813
person 0.948806
person 0.965844
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.420s + 0.065s (eta: 0:01:00)
person 0.981979
person 0.9186384
person 0.97887576
person 0.97713816
person 0.9137917
bicycle 0.97248185
person 0.93188936
person 0.9140778
bicycle 0.974686
person 0.93527293
bird 0.9435359
bird 0.90049773
pottedplant 0.9434733
person 0.9682694
person 0.92873627
person 0.90259445
person 0.9607267
person 0.96582866
person 0.9868227
person 0.92717046
person 0.9660424
person 0.9526449
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.308s + 0.037s (eta: 0:00:39)
person 0.9128098
person 0.90668494
person 0.96398765
person 0.9176332
person 0.9735928
person 0.9419203
person 0.9362652
person 0.9807989
person 0.9643967
chair 0.92573166
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.313s + 0.035s (eta: 0:00:36)
car 0.9159089
car 0.95037174
person 0.94729984
person 0.96128905
person 0.98158777
person 0.92342657
person 0.9487386
person 0.9646833
car 0.9822895
cow 0.9210235
person 0.91560197
person 0.9574329
person 0.92289764
person 0.9125582
person 0.9808811
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.311s + 0.035s (eta: 0:00:32)
person 0.97816193
chair 0.934069
person 0.9298349
person 0.9258208
person 0.95130515
person 0.9534067
person 0.9324513
person 0.9181268
person 0.9400453
person 0.9806826
person 0.9869365
person 0.977126
person 0.9691139
person 0.97719985
person 0.903893
bird 0.90533006
bird 0.99081284
person 0.9218152
bird 0.95169735
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.312s + 0.035s (eta: 0:00:29)
person 0.9456564
person 0.97093236
person 0.9826324
person 0.922352
person 0.9476389
car 0.9505603
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.317s + 0.035s (eta: 0:00:26)
bird 0.91059786
bird 0.95986384
bird 0.9136743
person 0.9134756
person 0.9439341
person 0.91721237
person 0.9149796
person 0.9883178
diningtable 0.94849616
person 0.9722412
diningtable 0.97877127
person 0.92337674
person 0.9354809
chair 0.94283366
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.322s + 0.035s (eta: 0:00:22)
person 0.9145532
person 0.90583265
person 0.9289923
person 0.948158
person 0.9857939
bus 0.94682133
dog 0.9521839
diningtable 0.9737254
diningtable 0.92264205
chair 0.912069
chair 0.9850263
chair 0.9118084
chair 0.970892
chair 0.9357591
pottedplant 0.9023802
person 0.90476215
person 0.97617775
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.321s + 0.034s (eta: 0:00:19)
diningtable 0.9207647
chair 0.91073966
person 0.9478035
person 0.96647215
person 0.9553008
person 0.93636996
person 0.9053324
person 0.9448862
bird 0.95468676
person 0.92596096
person 0.96208835
person 0.9639154
person 0.93175614
person 0.98035663
person 0.9088623
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.320s + 0.034s (eta: 0:00:15)
person 0.9627375
person 0.9631729
person 0.9156963
person 0.9032211
train 0.95837766
person 0.9570291
person 0.96232665
person 0.9466677
person 0.93937945
person 0.9134178
person 0.9310328
person 0.928616
person 0.9050424
person 0.9561343
person 0.92819124
person 0.99249965
person 0.9740689
person 0.960392
person 0.9173835
person 0.92747325
person 0.9021772
person 0.9296819
tvmonitor 0.9174206
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.322s + 0.034s (eta: 0:00:12)
person 0.9638911
person 0.90568095
person 0.9445337
horse 0.96006763
person 0.9666443
person 0.9303325
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.322s + 0.036s (eta: 0:00:08)
person 0.9751721
person 0.9654249
person 0.92215174
person 0.91962314
person 0.9515579
person 0.92719996
person 0.93610203
person 0.9028412
person 0.91496474
person 0.9866968
person 0.94570524
chair 0.972313
person 0.9561524
person 0.905559
person 0.9446769
person 0.95847285
person 0.9055313
person 0.9178431
car 0.94518685
car 0.91772115
car 0.9517708
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.320s + 0.035s (eta: 0:00:04)
person 0.96771395
person 0.9437336
person 0.96239775
person 0.94882345
person 0.97393453
person 0.960619
person 0.92140406
person 0.90574116
person 0.92748135
person 0.9881654
person 0.98151106
person 0.91732675
person 0.9782541
person 0.9279088
person 0.93788576
diningtable 0.9331652
person 0.9011417
person 0.9173911
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.315s + 0.035s (eta: 0:00:01)
person 0.90778863
diningtable 0.91202974
person 0.9706147
person 0.9401967
person 0.9873297
person 0.96575814
person 0.95826644
person 0.9588466
person 0.905573
person 0.909203
person 0.91649896
person 0.94098413
person 0.96337557
person 0.98099315
person 0.91000587
person 0.9491551
person 0.95655537
person 0.95597076
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.434s + 0.047s (eta: 0:00:59)
diningtable 0.9015509
diningtable 0.9271554
person 0.96020687
person 0.97604305
diningtable 0.97319275
person 0.92228913
person 0.91892326
person 0.9068395
bottle 0.90665257
person 0.9779473
pottedplant 0.91770756
diningtable 0.9273176
person 0.9658747
diningtable 0.906337
chair 0.94957066
chair 0.9659237
chair 0.922507
chair 0.90024704
person 0.9185687
person 0.90654105
person 0.93217236
person 0.99570185
person 0.98082525
person 0.93408644
person 0.9663657
person 0.96516955
person 0.9045986
person 0.9706623
person 0.95969427
person 0.93825084
person 0.9355967
person 0.9652825
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.302s + 0.032s (eta: 0:00:38)
person 0.94678813
person 0.98593754
person 0.933318
person 0.9706848
bottle 0.937078
person 0.9365136
person 0.9431958
person 0.9533126
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.310s + 0.032s (eta: 0:00:35)
chair 0.93244374
bottle 0.90926945
boat 0.90064585
person 0.9725892
person 0.9392945
person 0.9331475
person 0.93179876
person 0.9314642
person 0.9680475
person 0.94353485
bottle 0.9078142
bottle 0.95325243
person 0.9821806
person 0.9529735
person 0.94918257
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.320s + 0.033s (eta: 0:00:33)
person 0.96644866
person 0.9211527
bird 0.91780084
person 0.9537651
person 0.90594894
person 0.96757954
person 0.97039795
person 0.95959264
person 0.9031223
person 0.94947803
person 0.9070985
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.322s + 0.034s (eta: 0:00:29)
person 0.9695007
person 0.9692526
person 0.953611
person 0.9059562
person 0.92845863
person 0.9706379
person 0.9205043
person 0.9410797
person 0.914024
person 0.91913563
person 0.93738127
person 0.9698546
person 0.90078974
person 0.95347327
person 0.92497903
person 0.90482783
bottle 0.95363593
chair 0.93135816
boat 0.9045656
person 0.9733276
person 0.95497996
person 0.90359
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.322s + 0.033s (eta: 0:00:26)
car 0.98075885
car 0.93518597
person 0.9258933
person 0.9772455
person 0.9098149
person 0.90996057
person 0.920113
person 0.9348528
person 0.9648982
person 0.9230183
person 0.9169937
person 0.95714015
person 0.9176713
pottedplant 0.91524094
pottedplant 0.93183386
car 0.9849546
car 0.93804395
person 0.9320975
person 0.9878055
person 0.95501345
person 0.92487234
person 0.9369591
person 0.98595387
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.323s + 0.033s (eta: 0:00:22)
person 0.9797791
car 0.95972943
person 0.95039237
person 0.9254465
tvmonitor 0.92621225
person 0.9051003
person 0.93365407
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.325s + 0.034s (eta: 0:00:19)
chair 0.9025019
person 0.9707855
person 0.94508344
train 0.91964024
train 0.9611885
bird 0.91103715
bird 0.98293555
person 0.92202526
person 0.9224937
bicycle 0.92148644
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.322s + 0.034s (eta: 0:00:15)
person 0.9696201
person 0.90158236
person 0.93656087
person 0.92918926
person 0.92946833
person 0.9783132
person 0.97281253
person 0.98676527
person 0.95861524
person 0.98925936
person 0.911017
person 0.9551984
person 0.96532017
car 0.95190966
person 0.9689839
person 0.9119659
person 0.95006454
person 0.97716296
person 0.93341416
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.321s + 0.035s (eta: 0:00:12)
person 0.91526055
person 0.9454065
person 0.92370117
chair 0.97520953
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.322s + 0.035s (eta: 0:00:08)
person 0.9813374
person 0.9406263
person 0.92848945
person 0.9636041
person 0.9621797
person 0.90228957
person 0.9262196
person 0.90729594
person 0.93975306
person 0.92075366
person 0.97890294
person 0.9059453
person 0.9080162
boat 0.9032384
person 0.96705204
person 0.9243258
person 0.94705737
person 0.9117596
person 0.9581671
person 0.90335006
person 0.9259005
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.319s + 0.035s (eta: 0:00:04)
person 0.96738225
person 0.9735304
person 0.978036
person 0.94902116
person 0.91912556
person 0.93056583
person 0.9247076
person 0.9506064
person 0.9607095
person 0.9185934
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.319s + 0.035s (eta: 0:00:01)
person 0.9364057
person 0.97139263
person 0.92513233
bird 0.90194577
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step6999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.449s + 0.040s (eta: 0:01:00)
person 0.9415023
chair 0.9191872
chair 0.9581431
person 0.9584038
person 0.92578965
person 0.95363605
person 0.94061
diningtable 0.93157506
person 0.9743346
person 0.97151834
diningtable 0.93667424
person 0.95325977
person 0.9683026
person 0.96198124
person 0.9066104
diningtable 0.9117061
person 0.93040985
person 0.9649564
diningtable 0.9657087
diningtable 0.92854124
diningtable 0.9401636
person 0.9531899
chair 0.9148487
chair 0.9813985
person 0.9070103
person 0.93077934
person 0.9890838
person 0.9205938
person 0.9615982
person 0.98525584
person 0.916493
person 0.9237684
person 0.98361796
person 0.96827835
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.329s + 0.035s (eta: 0:00:41)
person 0.9393358
person 0.95411915
pottedplant 0.9336241
person 0.9039592
bird 0.971865
bird 0.9488067
bird 0.91908276
person 0.9673963
person 0.92083836
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.320s + 0.034s (eta: 0:00:36)
person 0.95037174
person 0.9098301
person 0.96226937
person 0.9698905
person 0.97641796
person 0.94708824
person 0.90079284
bottle 0.90521896
bottle 0.95211506
bottle 0.9025946
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.314s + 0.034s (eta: 0:00:32)
person 0.908886
person 0.91635925
diningtable 0.9082139
pottedplant 0.93159056
chair 0.9397457
chair 0.9160797
person 0.9758518
person 0.94070566
bottle 0.9101262
bottle 0.90867287
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.315s + 0.034s (eta: 0:00:29)
person 0.96240985
person 0.96052957
person 0.9479993
person 0.9947988
person 0.942497
person 0.9406418
person 0.92030615
person 0.9794527
person 0.9118954
person 0.95791376
person 0.97314
person 0.9146683
person 0.915452
person 0.9350188
person 0.9382594
person 0.9920312
person 0.94005466
person 0.97402835
person 0.9256103
person 0.96134645
person 0.9837928
person 0.96608615
person 0.92731565
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.311s + 0.034s (eta: 0:00:25)
person 0.9480032
person 0.964586
person 0.9709033
person 0.9052633
diningtable 0.9839352
chair 0.91626143
chair 0.9478875
chair 0.9791312
chair 0.98741156
aeroplane 0.9126708
person 0.96668756
person 0.9814478
person 0.9321158
person 0.9164201
person 0.9118726
person 0.94737726
person 0.97361696
car 0.9254425
car 0.9802981
car 0.9718312
car 0.9586673
car 0.9002897
bird 0.9583402
chair 0.93127215
chair 0.92985636
person 0.94507706
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.313s + 0.035s (eta: 0:00:22)
person 0.9839803
person 0.934112
person 0.907733
person 0.981426
person 0.9583377
person 0.9896305
person 0.93676555
person 0.92621124
person 0.9407894
person 0.99026686
person 0.9384907
person 0.9076206
horse 0.96244335
person 0.91231287
person 0.9054003
person 0.91969955
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.309s + 0.036s (eta: 0:00:18)
person 0.9547891
person 0.94565076
chair 0.9751728
chair 0.92144674
chair 0.9572665
pottedplant 0.9254647
pottedplant 0.9223721
pottedplant 0.94536155
person 0.9485032
person 0.9026906
person 0.92163604
person 0.9371212
person 0.9590477
person 0.9385396
person 0.9322932
person 0.9013419
person 0.90320015
person 0.9553972
bicycle 0.9146502
person 0.9112565
person 0.9797744
person 0.9020638
person 0.94193476
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.312s + 0.035s (eta: 0:00:15)
person 0.9881263
person 0.918392
person 0.90235287
person 0.916227
person 0.9591274
person 0.98529506
person 0.9402746
person 0.97690374
person 0.97789586
person 0.9718041
person 0.9373838
person 0.9216201
person 0.9156043
bus 0.9551634
person 0.9059247
person 0.94773304
person 0.9392954
person 0.9648627
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.313s + 0.035s (eta: 0:00:11)
person 0.91050774
person 0.9678467
person 0.9137342
person 0.93501264
chair 0.91289645
chair 0.94864714
tvmonitor 0.90190876
chair 0.97111154
chair 0.9314979
chair 0.9404186
person 0.9486786
person 0.9697742
person 0.95669675
person 0.97739196
person 0.9195217
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.311s + 0.035s (eta: 0:00:08)
motorbike 0.9156059
motorbike 0.95169413
person 0.9010144
person 0.9330432
person 0.9782738
horse 0.9564885
horse 0.9017514
person 0.901815
person 0.90381336
bicycle 0.9587445
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.311s + 0.035s (eta: 0:00:04)
person 0.92443216
person 0.9589657
person 0.9809849
person 0.9491698
person 0.96431744
person 0.98418045
person 0.9498364
person 0.95603055
person 0.9427714
chair 0.95989513
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.313s + 0.035s (eta: 0:00:01)
person 0.90575534
person 0.9864238
person 0.9857491
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 84.739s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1169  281  278 ...  994  997  992]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2502
INFO voc_eval.py: 171: [126 127  71 510  70 243 450 289 269 130 138 128 479 563 383  73 393 540
 568 486 454 566 561  74 508 381 570  72 551 475  86 134 136 567 274 578
  89 455 572 528 135 562  75 583 257  97  83 132 564 386 379 516 459 378
 300  78 294  91 139 520 129 553 542  92 466 302 244  76 469 249 560 144
 460 465 546 512 271 458 438 103 541 472 543  18 141 293 152 480 145 312
 377  43 517 296 464 390 290 247 248 539 241 245 282 410 519 343 384 424
 298 548  80 251 574 456 532 577  22 382 461 502 105 413  21  37 150 544
 429 283 147 515 436 107 511 140 163 514 267 131 556 457 580  41 388 306
 575 272 349 273 164 545 402 256 338 199 201 470 420 538  88 309 151 318
 208 463 569 439  27 220 584 522 281 242   2 409 555 581  34 137 487 258
 148 344 331 328 437 250 488 307 407 146 415 573  79 419 315 305 571 565
  19 422 549  42 518 392 262 304  44  33  84 299 552 320 432 576 252  57
 535 531 533 149 336 485 285 195 376 275 444 255 203 142 526  16 246 391
  98 579   5  93 554 467 430 133  87  90 308 106 523 405 557 104 198 278
 418 448 179 505 268 536 491 408  77 100 319  30  47 431 401  96 187 503
 471 582   1 264 292  24  20 527 254 340 143 478 509 295 209 441 180 265
 449 404 380 524  95  68 423 270  51 399  81 452 200 397 202 301  38 400
 280 339 513   0 162 365 177 284 322 101 279 196 190 310 332 224  66 167
 425 426 192  35 534 446 442 433 427 558 483  61 421 188  32 385 118 123
  26  25  56 334  82   3 184  62 484 197 440  48 525  17 204 253   8  29
 451 181 395 414 335 297 156 314 291 169 345  12 186 476 313 189 235 468
 225 412 474   6 321 117 394 102 316 417 266  15 500 550 443 166 193 287
 354 506 168  67 234 348 303 411 537  64 447 406 353 428 489 113 191 396
  65 355 462 529  23 375  31 359 501 157 547  94 227  36  13  52  28 445
 337 213 211 317 174 170 124  11 171 172 477 350 356 207 111  46 212 398
   7 530 374 206  54 361 416 233 217 165 286 521  60   4 261  14 210 260
 114 216 259 481 403 490 160 229  58 387 215 341 205 238 351  63 352 185
 110 346 373 364 324 155 311 482 372 453 389  85 323 154  69 182   9 125
 363 435  45  53 178 176 161 473 108 194 221 347 183  59 362  55 218 219
 434 330 342  10 360 173 159 158 325 112 153 115 109 175 507 226 326 357
 263 288  50 222 333  49 119 214 327 367 116 120 504 366 493  99 223 499
 369 232 240 239 370 494 371 231 228 496 368 329 585 495 122 492 236  39
 276 497 498 121 237 230 277 559 358  40]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3956
INFO voc_eval.py: 171: [ 644 1714 3237 ... 3475 3235 3230]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1918
INFO voc_eval.py: 171: [ 646  679  756 ...  125 2278 2282]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2406
INFO voc_eval.py: 171: [402 366 574 311  12  21 651 648 358 365 284 573 567 403 654 627 314 592
 333 545 374 409 602 290 669 313 141 480 548 830 657 589 688 324 757 829
  18 322 568 546 316 616 656 289 406 285 329 571 665 739  25 653 843 628
 650 359 724  16 670 331  22 664 667 291 703 367 368 377  20 831 514 613
 552 569 675 405 471 312 299 327  69 832 611 373 492 649 726 265 547 632
 407 287 317 678 499 294 216 840 587 326 590 255 658 372 513 738 835  13
 332 655 315 369 630 747 600 305  68 172 200 303 692 376 483 615 674 834
 304  42 680 732 610 408 676 581 551 320 493 473 361 620 114 575 489 174
 572 690 713 761 132 194 252 814 588 686 695 203  15 422 328 694 702 501
 556 175 701 210 663 525 712 671 570 586 260 472 614 246 672  19 398 212
 594 109 685 839 298 404 147 148 334 215 202 428 740 583 700 293 697 617
 799 257 797 765  50  17  70 163 170 156 682 773  80 166 579 370 495 612
 321 847 684 722  43 796 553 631 266 360 693 302 319 434 699 593 330  24
 401 549 261 259 151 127 660 576 388 661 470 196 468 859 743 236 142 634
 325 601 598 798 555 800 550 805 716 516 211  29 420 116 426  99 418 746
 801 521 318 691 446 668 362 813 497 371 606  44  33 584 264  79 238 323
 652 375 795 199 247 101 585 851 849 754 242 530 677 673  67 297 295 286
 233 524 167 445 218 180 554 113  81 498 176  45 679 335 844 714 296 582
 689 481 307  61 591 717 662 762  23 133 520 337 635 618 687 131 698 125
 529 288 745 646 580 230  97 491  96  57 110 666 474  14 112 391 262 608
 597 336 460 241 217 577 846 153 683 578 400 490 267 845  78 607 596 723
 696 770 599 193  54  26 130 357 204 760 399 115 223 605  87 826 595  95
  35 836 744 195 256 197 861 300 681 768 263 775 707 150 251 111 229 619
 708   6 485  53 609 527 783 494 482 774 603 827 604 250 500 277 791 213
 487 157 526 421 171 308 104 771 496 725 338 415 270 727 806 636 486 715
 214 389 182 441  58 862  31  41 268 348 235 742 659 728 515 137 397   1
 769 301 240 292 179 363  52 129 108 753 419 447 448 198 117   5 107 118
 165 119 730 149 750  73  49 522  91 191 183  46  98 751 855 704 810 100
 144 790 756  86 463 763  34 517 812 841 173 559 152 181 249 523 417 364
 234 629 528 838 168 390 244 833 503 224 863 237 416 143 178 758 145 271
 815 772 469  47 779 759 253 543 488 749 258 427 519 220 733 340 752  27
 201  60 537 273   2  92 423 466 807 128 190 467 802 729 828 755 854 462
 154 275 155 856 162 225 254 778  90 248 102 734 431 857 764 126 624 159
  77 432  30  93 245 741 504 347 461  28 442 424 425 792 274 239 539 219
 558 518  48 853 852 444 103 158 161 748 718 227 164  72 243 177 475  37
 637 449 647 719 436  75 776 160 837  85 121 538 384 633 540 811 484 306
 356 477 794  51  89 777   0 542  62  64 169 272   4 276 459 541 456 505
 106   8 720  94 479 860 140 731 557 566 188 280 621 146 808   3 352 379
 544 531 721 269 766 139 381  56  39 706 208 380 563 440 478 645 476 455
 622  76 562 430 443 465 506 339 565 804 803 809 535 282 351 138 534 736
 850 350 435 105 560  82 781  38 349 206 705 564 464 536 136 561 793 507
 221 222 393 228 458 438 848 429 858 785 414 532  63  40 439 533 784 842
 283 278 626 640 231  32 642 355 454 512 502  74  71 394  83 383 353 392
 382 354 639 625 281 437  36 737 309 709   9  65 385 623 226 782 310 395
 346 189  59 823  88  11 822 205 767 710 386 788 457 508 735 711 341 187
 279 345  10 232 396  66 344 378  55 120 511 207 433 452 412 643 453 134
   7 820  84 644 342 122 824 825 821 192 510 819 780 787 343 124 816 450
 789 786 387 509 209 123 184 411 817 186 638 410 135 413 818 641 185 451]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3332
INFO voc_eval.py: 171: [432 169  93  92 170 444  90  77 100 456  96  75  94 435 107 362  76 118
 178 397  26 103  74  79 399  80 209  44 404 388 409  86 106 383 208 248
 260 161  82 361 436 376  32 270 101  45 140 392 355 263 363 160 437 290
  54  89  53 389 195 185 357 162 233 259  84  87 154 476 331 421 115  78
 217  43 428 200 117 139 221 387 108  85 316 287 236 327  11 204  88 251
 348  47 305 257  13  81 194 197 114 281 116 332 193  83 271 459 264 328
 336 149 122  30 368 386 174 244 333 391 207 375 261  57 350 324 136 395
 358 250 211 411 246 346 372 205   8 184 330 374 413 312 142 243 175 408
 279 402 153 405 283 384 191 120 226 147 431 119 323 418  22 427 352  48
 315 373 280 227 224 256 457 254 356 308 198 354 318 241 294 371 385 166
 347  31 282 249 286  27 148 121 128 380 410 201 220 426 137 272  16 351
 126  97 416 225 325 253 366 222 210 439   4 292 415 144  35 326 132  52
 349 124 199 112 196  40 110 420 265 304 370 310 309 138 458 214 125  95
 403 171 247 414 291 475 465 123 102 111 159 135  58 203 353 163 379 113
 223 129 467 474 176 422  29  33  91 307 423 276   0 146 394 167 377 269
 158 155  98  36 105 230 393  46 202  99   5 338 382 466 390 229 242 329
 238 255 274 232 206 317 401   3 127 396 234 406 412 430 152 245 468  37
 339 237  66 151  55 450  42 258 364 365 313 398 429 319 212 303 143 179
 284  28 448 275 145 461 419  71 228 273 141  67 417 381 464 369 320 434
   7 262 470 156 266 165  59 168 216 181 367  10 252 240 359 130 277 438
 321 322 433 407   6 109 150 337 311 344 231 400 268   9  38 306 104 445
 285 334 289 314 471 192 164 267 462 447 440 172 288 451   1  41  17 131
 278 463 378  39 218  34 157   2 335 425 300 213 180  60 296 186 460  49
  56 299 360 424  23  72  62 235 188 302 215 239 452 441 449 297 219  61
  19 453 182 341  14 295 293  65 340  51 177 442  21 298  63 343  69 133
  24 472 446 473  18 301  25 469  73 187 134 173 454 189 183  15 455 190
  12  50 443  70  64  68  20 345 342]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4235
INFO voc_eval.py: 171: [1595  717 1528 ... 1726  117   98]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2858
INFO voc_eval.py: 171: [ 29 264  64 108 156 316  65 314  19 109 288 118  24 265  66 155 256 203
 315 125 154 287 266  26 297 232 260 110 213 291 202  74  83 289 122  52
  95 233 250 133  48  94 241  50 117  16 263 137 102 144 115  69 173 171
 192 234 177 191 307 153 296  42  20  75 105  92  27 206  63 124  71 227
 219  17 293  80 174 116  96 113 121  18 166 201 295 255 244 251  21 159
  22 217  55  28 111 165 286  73 258 194 238  76 205  81 172 167  54 242
 130 190 292  91  23 235 257  51 215 204 152 158 216  85  82 294 197  86
 157 290 127 311  56 119 308 262 246  77 146 283 313 123  53 309  25 211
 220 195 239 150 151 226   5 145 184 193 224  47 236 103  88  68 240 268
 243 138 161  84 126  87 131 128 136 164 135 112 223 237  49 170 259 245
 218 132 271 310 107 285  44 225 312 280 222 114 317  45 147  14 139  67
  93  10 302 134 101 248 196  89 247 253 140  46  41 252 277 298  97 106
   8  90   4 104 120 183 207   0  35  39  43  15 230 100 281 214 143  30
 209   6 269 221  99 270 282  34 129   1   3 261 169 181  98 278   9 272
 249 284  60   2 168 175 199 231  36   7  38 160  12  59  31 198 162 212
 163 180 300  58 200 210  72 276 208 273 303 279 178  32 187 275 299 306
 305  33 228  62  57  78  13 301  79 148  61 141 185 304  70 229 176 267
 179 149  11 189 254 182  40  37 274 142 186 188]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1014
INFO voc_eval.py: 171: [5848 2514  706 ...  289 4374 4375]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3080
INFO voc_eval.py: 171: [334 263 199 449  60 205 288 390 277  52 260 699 765 276 262 265 289 457
 259 415 455 757 645  73 273 224 729  79 335 414 266 198 613 338 452 634
 200 201 159 385  66 184 756 565  75  57  55 181 233 203 261 453 141 459
 206 284 378 454 230 450 130 759 612 208 448 280 132 370 271  71 566 464
 571 173 446 761 760 325 447  70  62 646 542   8 115  22 282  34 422  67
 225 274 762  69 741  72 407 197 567 630 655 388 725 616 463 291 527 461
 207 543 391 704 451 250 398 270 247 626 204 641   7 312 726 614 211 615
 269 735 347 268 363 154 462 460 764 703 769 275 560  51 720 525 149 539
 295 343 202 396 217 479 648 365 389 222 431 631  59 296 635 179 654 346
 426 576 730 636 731 724 281 632 734 232 349 119 267  12 524 773 327 573
 146 248 243 545 733 669 287  43 218 758 732 272 354 384 624 241 337 185
 456 386 423 664 129 727 188 568 180 151 445  53 424 336 220 700 618 671
 663 766 246 298  17 315  65 416 562 572 187 214 212 736 574 470 647 649
 563 547 433 706 569 294 341 177 183 128 633 155 223 134 738 723  92 290
 742 144 326 419  50  11  81 321 737 352 526 421  30 108 189 775 264 242
 652  31 392 350 502 362 418 239 428 413 135 319 182 369 131 240 235 743
 387 721 156 771 133 623 420 429 767 458 748 226 176  48 537 483 719 768
 425 432 739 186 514 752 417 528 252 666 249 364 168 579 710 564 167 394
 318 147 753 709  68 427 715 694 651 637  77 293  82 210 625 402  49 178
  23 340 620 383 718 278 163 229 707 138 339 328 682 285 465 316 430 342
 317 722 747 577 686 535 561 503 660 216 787 148 785 107 397 372 109 292
 516 112 684 283 228 286 279 161 544   9  96 113 400 728 244 772 102 546
  87 591 344 332  32 512 740 665 702 763 209  18 488 786 575 345 505 299
 770 404 367 530 320 297 698  28 555 438 162 106 122  90 256 245 711 139
 142 744 403 570 629 490 627 667 622 331 583  16 513  45  24 653 506  15
 755 466 408 164 681 382 358 118 323 783 393 668 585 716 541 578 221 489
  25 640 548 519  58 751 643 749  56 777 103 231 746 540 619 160 754  35
 191  93 776 157 174 498 750 549 788 213 658 395 496   1 401 374 504 644
 111 745 697 639 219 683 532 598 234 399 405 158 324 656 469 610 406 322
 529 628 487  27 691 104 136 595 586 471 774 523 688 196 227  33  95 366
 518 607  26 313 215 375  37 533  21 140 692  61 258 333 105 371 110 253
 329 708 143  80 123 538 662 696 554 594 238 609  36 606  13  20 114 778
 175  89 255 517 495  85 492 254 137 330 117 443 127 170 693 236 670  64
  29 674 642 531 602 685 678   3 592 237   0 486 165 379 593 597  78 169
 534 608 680 368 152 659 536 436 484  74 377  44 441 679 552  54 251 690
 150  94 257 166 672  40 373  42 701 193 784  14  38 650 584 713 442 596
 437 712 559 499 500 172 695  83 194 638  10  19 190 355 491 689 116 171
 501  86 600  84 153 125 120 509 467 676 124 360 661 494  63  88 599 497
 309 603  76  98 511 485 617 601 675 557 380  97  91 493 515 357  46 510
 677 314 611 657 604 435 192  41 673 477  47 101 551 434 556 475 550 553
 714 409 381 121  39 507 520 126 311 303 472 508 411 605 440 304 351 558
 439 301 410 705   4 473 482 310   2 359   6 195 444 621 412 468 589 476
 306 478   5 521 782 481 687 376 717 353 480 300 356 361 308 590 474 302
 779 305 522 781 307 145  99 580 100 780 587 348 588 581 582]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2076
INFO voc_eval.py: 171: [2895 1279 1331 ... 2794 2795 2307]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3282
INFO voc_eval.py: 171: [345 410 348 842 444 501 376 355  48 482 465 449 548 144 351 502 408  62
 621 442 159 772 540 142  45 279 506 451 148 158  49 512 204  40 522 377
 844 152 275 357 839 734 237 794  47 413 126 786 409 462 140  64 526  44
 205 795 150 768 767 145 477 832 269 516  39 346 467 847 783  61 811  50
 434 375 836 220 372 470 441  81 356 822 800 622 146  52 757 246 730 270
 542 769 642 840 224 162  67 225 520 579 770 236 464 549 533 546  63 484
 474 505 416 660 486 242 247 853 421 282 417 358 598 851 818 151 437 261
 239 347 843 715 463 143  53 163 823 223 669 198 640 450 471  43 415 841
 124 414 207 238 436 475 214 433 668 545  51 612 296  83 149 519 199 773
  60 337 251 278 605 775 615 816 326 655 817 659 731 658 820 352  30 192
 701 771 228 583 588 801 221 613 804 130 699 782 791 834 726 157 755  66
 430 155 756 160  68 258 647 217 603 595 547 439 264 597 281 195 764 230
 383 380 215 513 440 229 412 431 857 741 349 544 128 432 833 411 156 381
 419 206 226 808 147 373 702 208 632 802 424 141 780 797  46 670 129 194
 164 320 758 435 134 602 641 845 736 798 809 552 343 125 849  58 618 628
 219 777 438  55 179 250 425 737 161 231 422 336 294 273 831 271 154 313
 309 443 698 620 114 643 639 350 679 266 821 108 262 742 807 511 692 110
 796 353 139 481 288 593 447 267 664 860  15 846 586 570  41 153  65 803
 489 778 289 733 127 541 193 293 524  85 196 861 131 197 334 280 244 243
 671 690 614  59 571  89 718 283 626 480 468 676 805 819 765 303 518 284
 133 384  29 122 645 784  87 774 354 828  82  91 272  56 599  24 781  57
 233 719 234 182 420 241 785 732 277 120 721 245 666 827 222 136 292  22
 428 274 766 619 385  19 276 569 855 115 328 776 738 606 616 856 382 826
 852 675 248 566 476 672 677 665 232 580  14 302 551 814 360 592  86 287
 111 609 109 216 663 212 824  54 608 112 554 445 218 301 565 118  99 779
 649 607 249  12 427 528 342 617 543 460 448 429 446 365 252 696 374  31
  28 515 706 743 697 315 727 537 717 390 479 527 667 306 693 747 601 314
 253 308 568 858  84 813 550 678   5 100 553 268 723 810 835 648 423 344
 728 418 724 291 213 681 184 116 812 259 298 532 555 854 295 859 806 263
 815 837 327  88 644 235 825 478 673 838 848 722 683 517 680 335  90  18
 631  33 407 646 185 720 567 657 401 850 686 323 725 260 121 426 635 629
 329 286 362 265 132 508 371 688 630 359 572 117 590 102 186   4 339 799
  69 458 585 400 661 321 674 300 739  25 654 591 600 862  10  80 582   6
 507 180  92 319 317 173   2 525  27 304 604 257 729 338 361   3 311 636
  42 538 240 514 503 560 393 531 187  77   1 256 682 596 760 290 297 792
 324 788 307 113 363 594 254 318 487 332 627 299 211 744 687 497 485 285
  78 103 559 493 610 611 379 364 452 745 325 534 735 316  38 310 369   0
  93 634 623  37  75 490  13  34  26 455 119  23 166 789 561 396 137 790
 710 539 172 638  96 584 633 705 752 123  97 366 135 535 466 495 652 341
 454  17 750 189 536  21 203 453 305 557 637 653 556 312  32 176 167 165
 473 483 368 255 504 405 209 181  76 498 521 138 457 684 740 178 787 402
 399  72 748 707 403 529  95 367 469 174  94 404 190 101 492 793 510 711
 491 576 472  20 488 575 650 709  73  71 456 589  79 651 175 581 578  98
 656 461 322 577 530 183 746  74 558 177 587 202 388  70 700 106 494 210
 378  16 695 191 753 509 691 704 685 574 188 662  35  36 754 751 200 562
 107 459 406 523 330 395   7 703 564 694 389 201 496 624 716  11 386 370
 105 573 761 563 749 712 763 625 340 227   9 708 714 171 331 333 713 387
   8 170 104 169 168 398 762 759 499 500 397 394 392 829 391 830 689]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2108
INFO voc_eval.py: 171: [131 956 438 ... 570 776 777]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3039
INFO voc_eval.py: 171: [ 48 275 262 144 148  27  47 153  49  52 145 283 118  73  35 161  69  32
 269  79 146  60  57 190  58 250 170 116 270 264  74 163  54 200  21 174
 207 131 271 195 272  37 254 242  34  26 164 285  33 256 245 263  31  78
 266 280 149 150 117 152 286 158 284   7 156 199  53  84  18  62 167 217
 115  51  23 198 279  85  63 185 147 227 192 100 114  77  61  86 206 143
 160 196 267 244 287  19  89 268 109  36  11 233  98  67 243 214 168 261
  44 220 265 255 154  24 127  66  20  28  55 169  68  81  59 246 135 203
 165  71  96  80 113 151 186 126 247  39  13 241   5 232 222 187 253  30
   1 259  94 236 157 273  22 197  40 111 155  99  15  65 132 252 201 159
  43   6  76   9  91 141  17 274  12  72 223  95 106 140 209   0 219  82
 281  38 238 230  93 189 139  29 212 221 205 104 249 171  25 136 257 103
 179 251  70  16 188 134 166  97 142   3  87 182  50 110 102   4  10 216
 175 193 101 120 107 172  88 282 108 229 278 122 211  41   8 183 124  42
  83 176  46 226  92  14 235  56 237 178  75 173 125  90 112 184 177 105
 218 213 202 180 138 215 239 129 208 210 276   2 204 191  45 119 224 225
 128 162 231  64 194 123 277 228 181 234 248 130 121 258 240 137 260 133]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4981
INFO voc_eval.py: 171: [ 8580 14029  1061 ...   775 10890 10892]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5391
INFO voc_eval.py: 171: [2512  257  714 ... 2698 1948 1831]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4131
INFO voc_eval.py: 171: [ 65 315 537  33 335 546  66  55 323 132  40 540  49 541 332 342  44 319
 617 538 156  46  34 349 480 221 135 330 345 550 542 255 313 158 343 338
 552 545 125 407  36 138 647 284 413 637 346 543 307  70 366 145 618 423
 144 324  67 539 146 162 333 137 419 157 227 226 318 627 487 626  59 492
 225 269 564 622 409 320 624 159 336  51 123 613 560 628 481 154 220 631
 311 253 462 412 640 334 256 352 317 620 565 140 164  39 348 497 621 526
 219 231 623 554 416 233 337 165 649 490 329 130  45 415 500 196 642 139
 322 365 134 514 446  43 491 616 316 367 312 259 289 632  64 268 331 234
  91 254 645 163  61 133 371 314  96 495 223 650 229 131 576  92 285 347
 410 675 648 639 350 422 232 672 151 310 124 328 633 344 506 641 569 339
 142 408 479 257 143 561 244 635 308 420 230 161 136 503 309 149 127 566
 321 507 433 266 326 141 619 411 174 614 279 570 509 167 563 610 340 638
 523 364 634 264 325 630 406 129 283 208 222 341 200 673 265 683 636 327
 671 150 643 369 126 549 529 629 414  77 555  42 484 644 611 128 224 304
 166  50 355 582 201 493 270 547 600 482 502 160 449 568 461 102 464 267
 101 646 228  15 294 571 485 499 197   6  63 625 203 354 377 505 612  28
 202 441  54  69 194  35 562 463 494 109 378 368 155 676  38  25 351 578
 271 527 544 520 615 553 245 380 122 207 188 171 121 261 652 489 357 195
 258  95 198 353 182 300 275 356 242 478   0 168  94 262  90 510 199 517
 518  24  93 685  17 190 504  71 260 680 417 392 186 274  47 302 384 651
  97 120 674 246  23 376 263 465  37 360 175 272 556 440 519 653 516 656
 477 286 179 438  14 427  87 557 372 573 169   9 681 153 468 301 522 215
 113 444  48 426 108 508 106 435  31  60   1  78 558 249  76 184 445 448
 432 112 559 579  80 183 247 170  83 677 421 105 599 213  26 590 299 459
 511 682 606 460 548 434  29  21 293 210   3  16 486 187 474 278 436 604
 684 100   2 457 373  18  56 580 513 424 512 686 282 211 583 214 654 521
 601 608 678 577 370 551 488 475 250 212 443   8   7 404 240 447 305 394
 178  57 679 605 524  19 397 498 567  75 111  68  20 574 476 530  22 602
  73 107  32 581 110 515 575  41 277 483 442 467  74 281 292 288 655  79
 280 501  52 185 469 398  72 117 115 418 528 306 454 114   4 525 273 374
 209 248  88 496 658 425 439 603 291 429  53 598 287 181  58  62  27 251
 587 393 431 385 472  98   5 303 437  30 217 359 276 193 290 152 361 192
 428 597 375  85 118 252 403 362 204 400 430 218 662 594  13 358 591 119
  86 586  81 382 534 589 661 399 609  82 405 177 588 584  99 473 205 593
  89 572 585  84  10 659 470 236 116  11 216 363 180  12 532 104 664 670
 471 595 389 237 592 379 386 607 172 235 450 533 191 383 596 396 173 238
 206 241 295 176 390 243 663 189 298 466 239 660 395 391 381 669 531 387
 148 388 667 401 666 458 147 402 452 296 657 297 535 453 536 665 451 456
 668 103 455]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3174
INFO voc_eval.py: 171: [ 42   3  46 118 186 277 180  47 123 337  49   6 189 184 232  51 154 187
  13 158 333 191 155 152  41  59 279 148 178 339 183 124 229 275 218 153
 139 182 273 227 272 233  99 243  79 147 276 125  98 336  77  48 247 278
   5  57 121 103 252  58 159 160 149  10 171 328  23 117 228 181 143 126
  56 304 324 209 269 244 131  43 325 308 172 185 179 105 241 231 235  74
  11  71  45 230 212 163   4 156  97 268 157 188 116 129 101  94   8 249
  50 246 321  12 319 335 102 250  88 200 259 327 265 127  26 245 141  14
 248  95 119 100 256  72 286 251 331  44 205 234 208 338 150 122 330 306
  89  15 104  82  62 167 318 216 240  96  73  83 260  60  25 197 258 341
 305 112 266 312 323 291 193  27 340 226 292 224 114  63 145  78 220   7
 120 130 111  84 190 192 309 115   9 282 164 207 320  81 255 311 215 237
  93 146 113 267  75 302 128 254  76 151 322 242  54 313 332 296  32 223
  61 334 219 221 326  38 238 315 261 307 142 300 270 274 262  29  64  24
 257 295 198  87 213 287  28 168 294 329 210 110 173 214 140 236 170  86
 202  53 217 314  30  80  55 144  52 293 222 263 109 195  31  22   1 137
 281 132 303 288   0   2 253 264 194 225 199 206 297  85 166 280  18 204
 107 135 203 301 196 298 106 165 290 201 239  40 175 133 299 108 161 283
  36  34 134  19 174  21 169  20 271  17  16 176 136 138  70 289  35  39
 177 285 162  68 284 310 316  33 317  69  66  37  65  67  90 211  92  91]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1316
INFO voc_eval.py: 171: [630 374 637 115 642 317  81 203 386 287 121  76 395  78 167 887 634 635
 288 319 316  83 180 117 388 797  89 124 208  93 289 793 206 499  82 380
 530 204 628 375 188 632 446 666 650 445 294  98 202 448 119 318 116 187
 896 902 888 300 333 175 291 801 825 504 290  86 458 517 379 534 872 835
 844 899 701 394 164 134 212 135 200 214 830 901 631 520 293 381 326 537
 449  63 482 859  44 426 321  99 112 723 740 169  19 213 840 377 136 807
 194 308 322 673 305 320 873 191 702 658 649 891 800 450 827 794 105 502
 195 172  80 900 524 256 165 452 274 168  84 436 829 861 432 430 181 217
 185 724 812  94 750 903 226  45  70  97 298  20 615 503 389 166 874 533
 546 894 196 747 870 889 324 519 819 712 177 205 884 184 303  34 735 907
 163 171 834 705 730 545  67 193 879 128  65 456 239 210  74 237 441 890
 451 831 505 102 481 779 209 447 776 244  43 822 223 455 864 312  57 810
  85 539 536 218 183 863 258   2 798 444 708 385 376  52 367 843 711 698
  92 329 178  17 179 695  95 895 629 703 704 259 493 254 682 310 678 594
 572 521 754 501 633 882  58 390  18 921 157 871 176 913 828 429 477 391
 582 201 860 460 311 399 845 323 425 315 905 297 109 133 777 749 100 370
 866 103 453 347 508 433 767 732  51 479 393 809 671  72 170 230 144 392
 620 383 229 459  33 881 512 461 599 815 904 368  42 174  39 257 488 232
 543 869 920 437 674 197  73 338 898   5 755 427 351 824 813 271 535 246
 284 500  36 875 153 424  96  71 737 855 858 842 350 906 601 249 685 143
 268 555 523 652 113 892 923 236 457 518 679 608 659 295 795 463 647 101
 207 122 818   7 372 123 173 156 541 111 816 897 225  56 186 806 567 851
 216 161 612 242 525 773 247  75 314 245 513 162 471 883 752 741 681 680
 438 538 849 911 721 490 529 190 547 823 820 349  46 417 598 198 211   8
 885 880 739 877 182 348 566 137  38 626 833 683 261 592 325 727 234 821
  87 110 483 255 893 233 304 733 462 301 160 215 222 251 260 751 558 690
 126 841 761 676 616 720 522 302 306 435 865 868 509 224 396  55 847 434
 397  37 454 199 867 694 327 428  41 569 805 886  54 606 332 269 722 670
 560 770 152 189 468 595 621 581   0 384  40 922 839 597 641  88 593 709
 753  14 614  61 785 277 344 596 725 514 700 549  35 371 784 624 415  79
  48 106 540 378 716 339 192 790 602 387  59 743 817 561 382 431 491 341
 675 423 275 878 495 114 924 282 331 848 919 734 353 814 571 130 487 570
 765 672 138 409 764 618 511 677 684 231 235 553 563 309 154 264 369 531
 715 832  11 292 238 766 276 613 252 780 342 783 742 579   4  60 787 786
 416 768 778 706 270 850 802 585  53 296 605 600  62 273 472 241 548 639
 578 573 728 862  32 354 343 568 699 707 738 697  66 373 280 781 337 439
 414 313  31 443 240 714 473 838 574 627 748 604 836 689 669 603  47 418
 108 693 494 265 107 763 361 278 744 696 272 149 876 478 307 283 484 104
 552 507  22  91 804  30  90  77  68 609 782 360 619  21 243 470 746 908
  69  16 803 398 775 565 266 544   9 299 411 559 811 577 227  23 248 336
 510 584 440 467 363 526 837 717 788 791  15 580 575 279   3 346 917 550
 281 465 228 551 564 745 664 646 412 442 771 856 562  64 250 253 772 352
 718 486 262 496 607 358 736 774 485 617 914 497 799 726 729 464 661 731
 498 527 808 158 587 796  50  10 691 263 406 466 140 405 469 542 515 692
 826 719 792 403 532 576 489   6 151  12 421  49 909 476 335 362 590 480
 586   1 366 365 492 710 583  13 285 131 420 789 663 516 769 713 912 150
 611 762 506 638 846 668 857 528 589 474 475 688 364 345 286 422 419 588
 554 328 651 355 622 401 357 413 760 155 356 636  29 408 159 407 359 132
 759 141 330 147 625 852 648 410 219 400 557 556 916 655 591 623 118 657
 915 610 334 918 853  24 687 145 654 221 756 757 402 758 854 142 146  27
 340 404  28 665 686 127 220 267 645 120 129  26 644 139 125 148 640 910
 667  25 660 643 662 653 656]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2878
INFO voc_eval.py: 171: [423 251 598 174 436 299 137 444  23 471   1  37 480 500 255 322 352 477
 482 324  39 397 440 253 350 249 421 127 351 125 472 581 311 300 554 353
 264  57 614  68 424 254 474 538 107 325  71 130 231  31   3 441 599 447
 523 180  16 330 433 318 479 341 182 305 349 321  43 588 602 622 155 374
 631 227 358 102 610 368 617 377 260 372 443 151 309 539 221 446 109 110
 476 207  50 323 138  18 265 210 488  74  70 438 224  26 145 303 161   8
 604 561 466 307 509 206 475 536 382 378 274 486 326 235 365 103 464 620
   9 396 362 359  40 367 395  28  58 333 187 244 120 636 163  15  24 215
 162 626 485  46  56 487 178  44 605 129  84 430 409 596 302 315 301 209
 517 428   6 258 288 582 478 507 346 546 306 308  41  53 316  72 225  29
 483 505 213 119 373 339 101 574 229 179  48 564 565 473 522 146 211 313
 435 331 214 217 627 481 319 379 400 601 429 216 635 126 172  38   2 266
 624 190 159 175 132 403 117 212 193 173 616   5 384 584 585 228 557 121
 115  93  75 310  90 357 356 437 366 518 106 552 184 408 484 272  11 336
  27  45 361  91 603 553 498 304 124 630 398 510 282 558  17 568 615 294
 186 417 401 176  22 454 456 566 625 618 343 112  12 550 442 208 632 100
 312  32 285 131  55  88 171 502 320 533 160 519 402 154 544 542 200 613
 526 540 600 567 621 503 559 238 415 226 278 363 286 537 281 279  98 499
 491 277 360 628 633 144 529 364 629  95 504 612 578 387  94 218 335 114
 619 111 192 240 455  33 501 573 149 508 220 411 380 521 205 506 334 280
  87  54  82 257 623  78 156 283 317 314 219 148 385  63 545  92 524 370
 222 369 589 591 571  42 139 452 263 547 458 275 142 520 422 116 181 535
  61 549 276 371 128 355 639  77  76 104 287 450 118 376  80 514 256 269
 543 453 432  49 230 188 577  35 381 425 167 572 383 354  60 439 375 332
 448  34  85 134 242 202 203 123 634  52   4 593  51 270 457 108 158  30
 638 327 259 391 234 284 594 143 451 290 527  36 340 465  14 431 338 595
 150 434 147 243  59 293  10 273  81 105  64  69 580 166 292 468 122 399
 342 637 232 329 556   7 157 489 153 413 113 140 590 611 490 551 534  21
 426 133 563 570 548  89 532 199 297 462 239 152 141 191 541 427  25 185
 531 394 449  86 198 136 562 241 183 135 407 530 513 410 579 412  83 296
 328 445 406 236 223  96 261 298 168 169 250 569  67 525 405 201  97 389
 295 177  73 344 165 233 414 469  66 262 252 337 555 345 560 170 390  79
 587 528 164 392 467 347 575 515 289 404 189 592 576 388 416 271  62 511
  20 237  99  65 461   0 512 291 516 348 586 248 393 493 386 609  19 247
 459 460 608 606 246 607 268 496 463  47 418 492 195 267 419 597 197 204
 497 245 494 583  13 420 194 196 470 495]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3363
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3052
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.250
INFO cross_voc_dataset_evaluator.py: 134: 0.396
INFO cross_voc_dataset_evaluator.py: 134: 0.192
INFO cross_voc_dataset_evaluator.py: 134: 0.241
INFO cross_voc_dataset_evaluator.py: 134: 0.333
INFO cross_voc_dataset_evaluator.py: 134: 0.424
INFO cross_voc_dataset_evaluator.py: 134: 0.286
INFO cross_voc_dataset_evaluator.py: 134: 0.101
INFO cross_voc_dataset_evaluator.py: 134: 0.308
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.211
INFO cross_voc_dataset_evaluator.py: 134: 0.304
INFO cross_voc_dataset_evaluator.py: 134: 0.498
INFO cross_voc_dataset_evaluator.py: 134: 0.539
INFO cross_voc_dataset_evaluator.py: 134: 0.413
INFO cross_voc_dataset_evaluator.py: 134: 0.317
INFO cross_voc_dataset_evaluator.py: 134: 0.132
INFO cross_voc_dataset_evaluator.py: 134: 0.288
INFO cross_voc_dataset_evaluator.py: 134: 0.336
INFO cross_voc_dataset_evaluator.py: 135: 0.305
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 7499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.433s + 0.026s (eta: 0:00:56)
person 0.9703513
person 0.96375185
person 0.99237365
person 0.96085674
person 0.9733627
person 0.9263305
person 0.93903553
person 0.9825152
person 0.9454909
person 0.9114539
person 0.98161674
person 0.93874305
bottle 0.92194146
bottle 0.9194321
bird 0.95113766
person 0.9825231
person 0.91820955
person 0.94469595
person 0.9030604
person 0.90481865
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.326s + 0.038s (eta: 0:00:41)
person 0.94473124
person 0.9202891
person 0.96065253
person 0.94845754
person 0.9805896
person 0.94780767
person 0.9675194
person 0.9818556
person 0.93967223
person 0.9215591
person 0.92643464
person 0.9316924
person 0.9061289
person 0.9228138
person 0.920901
person 0.9270065
person 0.97554636
person 0.91301167
person 0.98005944
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.327s + 0.038s (eta: 0:00:37)
person 0.9288728
person 0.9122357
person 0.9565204
chair 0.9147971
person 0.97240925
person 0.9151843
person 0.9575058
person 0.9665471
chair 0.9656216
chair 0.93712354
chair 0.91481334
person 0.90427506
person 0.97909534
person 0.98794216
person 0.92773926
person 0.9055954
person 0.97706133
person 0.9915902
person 0.98136157
person 0.9108078
person 0.9519973
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.330s + 0.038s (eta: 0:00:34)
bird 0.93950886
person 0.9276794
person 0.92125124
person 0.9938259
person 0.95699054
person 0.9744006
person 0.9797594
person 0.9078703
person 0.9595642
person 0.96598715
person 0.9044764
person 0.91869473
person 0.9695895
person 0.94083434
person 0.9585105
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.333s + 0.038s (eta: 0:00:31)
diningtable 0.9022963
diningtable 0.9388089
chair 0.9637889
chair 0.96590906
chair 0.9381118
chair 0.95746136
pottedplant 0.9181445
pottedplant 0.9144652
pottedplant 0.91092813
pottedplant 0.904496
pottedplant 0.9437
pottedplant 0.93832576
pottedplant 0.92882276
person 0.96575856
person 0.9828652
person 0.9205569
person 0.979785
person 0.94901305
person 0.9890428
person 0.9154365
person 0.9037313
person 0.9121415
person 0.9217292
person 0.922154
person 0.9737205
person 0.95490694
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.329s + 0.038s (eta: 0:00:27)
person 0.9577052
person 0.92360324
person 0.9064841
person 0.9198112
person 0.9474371
person 0.9144438
person 0.953457
person 0.9367006
person 0.92947054
bird 0.9226657
person 0.9370323
person 0.98765427
person 0.917006
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.327s + 0.037s (eta: 0:00:23)
pottedplant 0.9030148
person 0.9623677
person 0.9334931
person 0.91226155
person 0.953826
person 0.9186732
person 0.95863867
diningtable 0.9103701
chair 0.9826418
diningtable 0.96167463
chair 0.9163037
chair 0.9484774
chair 0.98072827
chair 0.92680985
bird 0.9930426
person 0.90254444
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.325s + 0.037s (eta: 0:00:19)
car 0.9287184
car 0.942259
car 0.9148142
person 0.9884298
person 0.9004253
person 0.97412914
person 0.92980814
horse 0.973898
horse 0.9135891
horse 0.9207816
person 0.937957
person 0.953359
person 0.96818227
person 0.91856295
person 0.94257176
car 0.94587237
car 0.964172
bicycle 0.9604648
bicycle 0.9308715
person 0.9371649
person 0.9175596
person 0.97343653
chair 0.918451
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.323s + 0.036s (eta: 0:00:15)
person 0.9742309
person 0.9590373
person 0.9737008
person 0.91709006
person 0.91298795
person 0.90113926
person 0.98859465
person 0.9863689
person 0.94346887
person 0.97133756
person 0.91591316
person 0.9641089
person 0.99116325
person 0.90823543
person 0.96663564
person 0.91164654
person 0.987837
person 0.9799182
pottedplant 0.9118308
person 0.98988193
bus 0.91275954
person 0.96139634
bus 0.9072364
person 0.9414862
person 0.9545115
chair 0.94668114
person 0.90556
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.321s + 0.036s (eta: 0:00:12)
motorbike 0.96349585
person 0.97486115
person 0.910752
person 0.91555303
person 0.9568113
person 0.967383
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.320s + 0.036s (eta: 0:00:08)
person 0.9060938
boat 0.98200226
boat 0.94818854
boat 0.91836923
person 0.99036473
person 0.93748015
person 0.98551315
person 0.91826564
person 0.9188492
person 0.98904467
person 0.97491646
person 0.9105052
person 0.97506714
person 0.9767079
person 0.9135721
person 0.94848144
car 0.95058906
person 0.9401337
person 0.94711363
person 0.95984405
person 0.98678094
person 0.9073952
person 0.9208965
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.322s + 0.036s (eta: 0:00:05)
diningtable 0.95967233
chair 0.9246328
chair 0.97386724
chair 0.980719
chair 0.9150651
chair 0.98112166
chair 0.9016898
chair 0.93656117
chair 0.96009386
person 0.9424846
bird 0.92393374
bird 0.9150245
boat 0.9198414
person 0.9620707
person 0.9290497
person 0.9887911
person 0.91449094
person 0.9030826
person 0.91308224
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.321s + 0.036s (eta: 0:00:01)
person 0.983472
person 0.9641387
person 0.95632493
person 0.9554743
person 0.94892466
person 0.9660089
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.431s + 0.026s (eta: 0:00:56)
person 0.98204595
person 0.9189976
person 0.9789225
person 0.97713506
person 0.9139901
bicycle 0.9724371
person 0.93200016
person 0.91461575
bicycle 0.9747356
person 0.9356721
bird 0.94353783
bird 0.9005999
pottedplant 0.9432448
person 0.9683517
person 0.9288492
person 0.90243065
person 0.9608874
person 0.96597534
person 0.9868657
person 0.927001
person 0.9660432
person 0.9526782
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.320s + 0.035s (eta: 0:00:40)
person 0.9127651
person 0.90654075
person 0.96399975
person 0.91772515
person 0.9737969
person 0.9424017
person 0.9361073
person 0.98083544
person 0.9643885
chair 0.9254693
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.327s + 0.035s (eta: 0:00:37)
car 0.915896
car 0.9506345
person 0.94717586
person 0.9614176
person 0.981658
person 0.92408156
person 0.94896954
person 0.9648366
car 0.9823263
cow 0.92213607
person 0.9154423
person 0.95760375
person 0.9230978
person 0.9129284
person 0.98102736
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.326s + 0.034s (eta: 0:00:33)
person 0.978233
chair 0.93438786
person 0.93006206
person 0.9255534
person 0.95138747
person 0.95349914
person 0.93277097
person 0.91808534
person 0.94020945
person 0.9806846
person 0.9869595
person 0.9771459
person 0.9691263
person 0.9773536
person 0.9046992
bird 0.9049144
bird 0.99081504
person 0.92192453
bird 0.9516811
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.324s + 0.035s (eta: 0:00:30)
person 0.9459691
person 0.9710714
person 0.98267186
person 0.92216027
person 0.9479869
car 0.95058906
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.327s + 0.035s (eta: 0:00:26)
bird 0.9102696
bird 0.9596823
bird 0.9135915
person 0.9141549
person 0.9441442
person 0.9180165
person 0.9150285
person 0.9883676
diningtable 0.94857496
person 0.97230566
diningtable 0.97890925
person 0.9240095
person 0.9357851
chair 0.94294316
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.331s + 0.035s (eta: 0:00:23)
person 0.9153611
person 0.90667087
person 0.9301577
person 0.9487663
person 0.9858472
bus 0.9466148
dog 0.9523413
diningtable 0.9737936
diningtable 0.92333657
chair 0.9123434
chair 0.98505026
chair 0.9114977
chair 0.97104925
chair 0.93584275
pottedplant 0.9021155
person 0.9045288
person 0.90037537
person 0.97634786
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.327s + 0.035s (eta: 0:00:19)
diningtable 0.92091304
chair 0.9111726
person 0.9480863
person 0.9665719
person 0.9556903
person 0.93670195
person 0.9450171
bird 0.95445865
person 0.9260741
person 0.9621801
person 0.96401745
person 0.93206507
person 0.9803422
person 0.90943694
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.325s + 0.035s (eta: 0:00:15)
person 0.9627977
person 0.90005654
person 0.9635485
person 0.9165757
person 0.9032574
train 0.9583396
person 0.9570516
person 0.9622361
person 0.94649166
person 0.9398146
person 0.91425085
person 0.93026215
person 0.92855966
person 0.9051297
person 0.9562628
person 0.9274641
person 0.99254155
person 0.97404647
person 0.96010375
person 0.9006653
person 0.9174983
person 0.92746997
person 0.9027125
person 0.9303726
tvmonitor 0.9175488
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.327s + 0.035s (eta: 0:00:12)
person 0.9640613
person 0.90599364
person 0.9446565
horse 0.9600713
person 0.966708
person 0.93063325
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.325s + 0.035s (eta: 0:00:08)
person 0.975205
person 0.9653208
person 0.9221312
person 0.91965294
person 0.9520128
person 0.92774457
person 0.93622804
person 0.90307224
person 0.9154358
person 0.98672825
person 0.9461759
chair 0.9724186
person 0.95616686
person 0.9063472
person 0.94441736
person 0.9585449
person 0.9000489
person 0.90560764
person 0.9183234
car 0.9450923
car 0.918002
car 0.951987
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.323s + 0.034s (eta: 0:00:04)
person 0.9675698
person 0.9438704
person 0.9625305
person 0.94887185
person 0.9741356
person 0.9608048
person 0.921819
person 0.90659547
person 0.9280682
person 0.9881733
person 0.9814617
person 0.91682345
person 0.9783095
person 0.9280309
person 0.93811613
diningtable 0.9333914
person 0.90122795
person 0.917634
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.317s + 0.034s (eta: 0:00:01)
person 0.93805516
person 0.90810806
diningtable 0.91110724
person 0.97079945
person 0.9405123
person 0.98730123
person 0.9655867
person 0.9582147
person 0.9592449
person 0.9070757
person 0.91011614
person 0.9172582
person 0.94092417
person 0.96322125
person 0.9808842
person 0.9105381
person 0.94928247
person 0.9564968
person 0.95605105
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.409s + 0.029s (eta: 0:00:54)
diningtable 0.9017937
diningtable 0.9275845
person 0.9601263
person 0.97609746
diningtable 0.9734074
person 0.92241013
person 0.9186915
person 0.9075707
bottle 0.9066614
person 0.9780317
pottedplant 0.91759557
diningtable 0.928005
person 0.96590894
diningtable 0.90665025
chair 0.9494399
chair 0.9659534
chair 0.92269653
person 0.9309228
chair 0.9007973
person 0.9069383
person 0.9325613
person 0.9956768
person 0.9808178
person 0.93437016
person 0.96629834
person 0.96527964
person 0.90468234
person 0.970707
person 0.95990294
person 0.93860924
person 0.93539214
person 0.96540236
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.317s + 0.031s (eta: 0:00:39)
person 0.9470727
person 0.9859331
person 0.93342566
person 0.9707743
bottle 0.93694603
person 0.93679214
person 0.94365203
person 0.9535566
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.311s + 0.034s (eta: 0:00:35)
chair 0.9320601
bottle 0.90926045
boat 0.9007933
person 0.97273785
person 0.9395958
person 0.9334149
person 0.93220955
person 0.93201244
person 0.96829814
person 0.9439015
bottle 0.9079889
bottle 0.9533799
person 0.98218
person 0.9528017
person 0.9492269
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.319s + 0.035s (eta: 0:00:33)
person 0.9666477
person 0.92142725
bird 0.9172218
person 0.95376337
person 0.90634626
person 0.96782637
person 0.9704973
person 0.95975596
person 0.90338224
person 0.9496746
person 0.90766686
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.324s + 0.035s (eta: 0:00:30)
person 0.9695389
person 0.9692118
person 0.9534393
person 0.9064081
person 0.92809343
person 0.9706731
person 0.92057
person 0.94103634
person 0.9141766
person 0.9195256
person 0.9371916
person 0.96988297
person 0.9010261
person 0.95349544
person 0.9248696
person 0.90518457
bottle 0.953635
chair 0.93125737
boat 0.9043475
person 0.97332305
person 0.95512044
person 0.9043741
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.323s + 0.035s (eta: 0:00:26)
car 0.98078984
car 0.9350863
person 0.9260728
person 0.97731084
person 0.9098423
person 0.91061836
person 0.9203766
person 0.9350639
person 0.9649451
person 0.92308074
person 0.9174972
person 0.957323
person 0.91801345
pottedplant 0.91468656
pottedplant 0.93161273
car 0.9850133
car 0.9378769
person 0.9323333
person 0.98779476
person 0.9554341
person 0.92646176
person 0.93691295
person 0.9859746
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.321s + 0.035s (eta: 0:00:22)
person 0.97978437
car 0.9599249
person 0.9504692
person 0.92578137
tvmonitor 0.9262867
person 0.905489
person 0.9338165
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.323s + 0.035s (eta: 0:00:19)
chair 0.90235853
person 0.97082675
person 0.9457864
train 0.9203565
train 0.9613733
bird 0.91112494
bird 0.9829829
person 0.92220795
person 0.92248
bicycle 0.9210205
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.323s + 0.035s (eta: 0:00:15)
person 0.9697744
person 0.9014769
person 0.9364709
person 0.92906016
person 0.92947733
person 0.97825134
person 0.9728318
person 0.9868233
person 0.9587533
person 0.9892097
person 0.9116645
person 0.9553095
person 0.96523184
car 0.95222163
person 0.96900696
person 0.9101138
person 0.9501701
person 0.97716683
person 0.93394107
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.322s + 0.035s (eta: 0:00:12)
person 0.9155191
person 0.94551146
person 0.9239647
chair 0.97515476
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.321s + 0.035s (eta: 0:00:08)
person 0.98137534
person 0.9411691
person 0.92917955
person 0.9638505
person 0.96218985
person 0.9024278
person 0.926306
person 0.9073284
person 0.94027805
person 0.9210244
person 0.97893244
person 0.9064649
person 0.90838605
boat 0.90318316
person 0.9672248
person 0.92491955
person 0.9474646
person 0.912228
person 0.9581467
person 0.9037124
person 0.92632353
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.320s + 0.035s (eta: 0:00:04)
person 0.96741706
person 0.9736067
person 0.9780556
person 0.949229
person 0.91866434
person 0.93058527
person 0.9248598
person 0.9507181
person 0.96084505
person 0.9190419
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.320s + 0.035s (eta: 0:00:01)
person 0.9366324
person 0.9713887
person 0.9265102
bird 0.90085405
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.422s + 0.028s (eta: 0:00:55)
person 0.9417351
chair 0.91934663
chair 0.9582741
person 0.9584047
person 0.92560375
person 0.95365906
person 0.9409484
diningtable 0.93168616
person 0.97443706
person 0.9717076
diningtable 0.9372165
person 0.9536276
person 0.9684101
person 0.96218723
person 0.90713984
diningtable 0.9116299
person 0.93115616
person 0.9790887
diningtable 0.9659282
diningtable 0.9290455
diningtable 0.94042635
chair 0.9150866
chair 0.981472
person 0.9070064
person 0.93117434
person 0.9890681
person 0.92014897
person 0.9614768
person 0.9852454
person 0.9163444
person 0.92377514
person 0.98361844
person 0.9683367
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.339s + 0.039s (eta: 0:00:43)
person 0.9395458
person 0.9544254
pottedplant 0.93355733
person 0.90433395
bird 0.97180283
bird 0.94864213
bird 0.91913563
person 0.96739423
person 0.9208295
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.331s + 0.037s (eta: 0:00:38)
person 0.9504049
person 0.9102056
person 0.96232474
person 0.9700171
person 0.97655404
person 0.94677156
person 0.9014733
bottle 0.905436
bottle 0.9522429
bottle 0.9030935
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.324s + 0.037s (eta: 0:00:33)
person 0.90949667
person 0.9171778
diningtable 0.90790457
pottedplant 0.93138415
chair 0.9398456
chair 0.91604203
person 0.97577935
person 0.9411087
bottle 0.90995544
bottle 0.9089448
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.322s + 0.036s (eta: 0:00:30)
person 0.96242756
person 0.96060145
person 0.9479349
person 0.9947902
person 0.94261
person 0.9405792
person 0.92038983
person 0.9794687
person 0.91239995
person 0.95802075
person 0.97330385
person 0.9152113
person 0.9157589
person 0.9348671
person 0.9386186
person 0.99202067
person 0.9399194
person 0.9742443
person 0.9255186
person 0.9612463
person 0.98380023
person 0.96631014
person 0.9273945
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.317s + 0.035s (eta: 0:00:26)
person 0.9480541
person 0.96477413
person 0.90239173
person 0.9708067
person 0.9061348
diningtable 0.98405415
chair 0.91648316
chair 0.9479169
chair 0.9792596
chair 0.9874226
aeroplane 0.9124705
person 0.9667351
person 0.9815091
person 0.93256295
person 0.9170154
person 0.9120686
person 0.9475148
person 0.9735965
car 0.9242745
car 0.98035973
car 0.97187793
car 0.9587992
car 0.90070784
bird 0.9583662
chair 0.9315305
chair 0.92988634
person 0.9453415
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.318s + 0.035s (eta: 0:00:22)
person 0.98406583
person 0.9338488
person 0.9084728
person 0.9813619
person 0.9583884
person 0.98961735
person 0.93693596
person 0.92658484
person 0.9408717
person 0.990274
person 0.93856806
person 0.90801907
horse 0.9624419
person 0.91256684
person 0.90533036
person 0.91967463
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.314s + 0.035s (eta: 0:00:18)
person 0.9551245
person 0.9459757
chair 0.97509944
chair 0.92164177
chair 0.9570597
pottedplant 0.92515177
pottedplant 0.9224184
pottedplant 0.9454749
person 0.94855106
person 0.9032976
person 0.9217904
person 0.93755317
person 0.95917064
person 0.9383637
person 0.93235046
person 0.9015382
person 0.903689
person 0.9555984
bicycle 0.91422933
person 0.9112856
person 0.9798039
person 0.9024752
person 0.94240004
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.314s + 0.035s (eta: 0:00:15)
person 0.9881023
person 0.91825557
person 0.90247744
person 0.9164447
person 0.9592058
person 0.98536867
person 0.9402086
person 0.9768958
person 0.9779394
person 0.97181916
person 0.93725556
person 0.92222726
person 0.91625816
bus 0.9552986
person 0.90606403
person 0.94798934
person 0.9394376
person 0.96501845
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.317s + 0.035s (eta: 0:00:11)
person 0.96770287
person 0.9143455
person 0.9353297
chair 0.912813
chair 0.9486328
tvmonitor 0.9018175
chair 0.97116023
chair 0.9308127
chair 0.94022745
person 0.94914275
person 0.9699388
person 0.9568432
person 0.977434
person 0.91917026
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.316s + 0.035s (eta: 0:00:08)
motorbike 0.9155748
motorbike 0.9515909
person 0.9014381
person 0.9332502
person 0.97817403
horse 0.9560379
horse 0.90134954
person 0.9027731
person 0.90435636
bicycle 0.9588611
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.314s + 0.035s (eta: 0:00:04)
person 0.92457217
person 0.95897806
person 0.98101497
person 0.94924027
person 0.9644446
person 0.98426074
person 0.9503504
person 0.95642036
person 0.9430883
bird 0.90000874
chair 0.9597373
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.315s + 0.035s (eta: 0:00:01)
person 0.9062734
person 0.9864811
person 0.9857589
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 86.072s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1165  282  279 ...  991  988  986]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2240
INFO voc_eval.py: 171: [125 126  70 507  69 241 447 286 267 129 137 127 476 560 377  72 387 565
 537 483 451 563 558  73 505 567 375  71 548 471  85 133 135 564 272 575
  88 452 569 525 134 559  74 580 255  96 131  82 380 561 513 456 373 372
 295  77  90 290 138 517 128 550 539  91 463 296 242 247  75 465 557 143
 457 462 543 509 433 455 269 299 102 468 538 540  18 140 289 151 477 144
 371 514  43 461 384 287 245 246 536 239 243 318 516 279 378 403 337 417
  79 293 249 529 453 571 545  22 574 302 376 458 499 104 407  21  37 149
  24 541 146 423 512 280 139 508 106 431 511 162 130 264 577 553 454  41
 382 301 572 270 542 343 163 271 421 254 396 332 195 466 197 535 414  87
 307 460 150 566 204 519 434 581   2 216 278 552 240 578 404 136 484 256
 147 322 338 325 248 432 304 485 145 401 570 410  78 413 562 292 300 568
 546  19  42 386 515 260 298  44  83 549  32 294 314 426 250  57 532 148
 573 530 528 482 330 282 191 422 273 199 253 439 370 141 464 244 385  16
 523 576 551  97   5 424  92  86 132  89 306 105 520 554 399 103 194 276
 533 502 412 265 177 445 488  76  99 313  29  47 402 425 500 467 395  95
 579 186   1  20  25 524 252 266 334 142 475 506 291 263 436 178 205 374
 521  94 398  67 446 416  80  51 391 196 393 449 394 198 262 333  38 510
   0 316 161 359 281 175 192 100 188 277 326 185  65 418 166 189 220 531
 419 555  34 443 427 420 437 480  30  61 117 415  81  26 328   3 187  56
 268 379  62 122 481 193 182 303  36  48 251 200 435 522  17   8 448 179
  28 329 389 339 408  12 184 310  33 472 473 168 155 288 309 233 470   6
 405 221 101 388 116 311 315 411 497 547  15 305 503 438 190 165 348 167
 284  66 232 342 406 534 297  63 400 444 390 347  64  93 486 112 349 526
 459  23 369  31 353  35 544 498  52 156  27 224 312 441 440 209 207 123
  11 170 169 331 442 173 171  46 344 203 474 208 350 110 527 368 392 202
 355   7  54 283 231 409 164 259   4 518 213  60 206 258 113 257 478 159
 487  58 226  14 212 201 335 430 381 211 397 345 236 346 479 340 154 367
 109 358 183 308 450 383  84 153 317 124  68 357 180  45   9  53 174 176
 469 107 160 341 181  59  13  55 356 217 336 214 429 215  10 324 354 157
 158 172 504 108 111 152 114 223 261 319 285 351  50 327 321  49 218 428
 320 118 362 490 210 501 115 361 119  98 238 229 364 496 219 222 230 237
 365 491 228 366 225 493 363 582 492 323 121 360 489 234  39 274 494 495
 235 227 120 275 352 556  40]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3955
INFO voc_eval.py: 171: [ 640 1707 3227 ... 3470 3220 3225]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1967
INFO voc_eval.py: 171: [ 642  673  752 ...  125 2278 2282]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2389
INFO voc_eval.py: 171: [399 363 572 307  12  21 648 645 355 362 280 571 565 400 624 651 310 588
 329 543 371 404 599 286 666 309 139 477 546 826 654 586 685 755 320 825
 318  18 566 544 312 614 285 653 401 281 325 569 662 737 650  25 838 625
 647 721 356 667  16 327  22 661 664 403 287 700 374 365  20 364 512 827
 550 611 567 672 468 295 308 323  67 828 609 370 490 646 723 262 545 629
 402 283 675 313 497 290 215 836 584 322 587 655 511 369 252 736 831 328
  13 652 311 366 627 597 745 301 170 199  66 299 689 373 613 671 480 830
 300 677  42 608 730 673 578 549 316 491 470 358 112 573 172 487 570 687
 710 193 130 810 249 692 759 683 585 202  15 417 324 691 699 499 554 698
 209 173 660 523 601 668 583 568 709 257 469 244 612 669  19 591 211 396
 682 107 835 294 144 145 330 424 201 581 214 738 697 289 694 615 797 254
 763  49 795  17 168  68 154 680 161  78 771 164 367 493 610 317 577 681
 842 551 628 794 719  43 263 357 690 298 430 315 696 326 590  24 258 149
 256 547 398 125 658 657 385 465 741 574 234 467 195 854 631 141 598 321
 595 796 553 798 548  29 802 210 713 415 514 114 421  97 744 413 519 799
 688 314 665 809 359 442 495 368 604  33 580 236  77 261 649 319 372  99
 793 245 198 846 844 752 582 240 528 674 293 670 291 231 282 522  57 165
 217 179 552 441 111 496  79  44 174 331 292 839 676 478 686 579 711 589
 659 760 714 518 303  23 131 632 333 616 684 284 527 129 695 123 228 743
  94 489  95 108 643  56 663  14 471 110 388 606 259 594 332 457 239 216
 841 575 679 151 576 488  63 264 605  76 397 840 593 720 693 768 192  26
 596  53 203 353 128 758 113 603 395  93 592  85 822 221 832  35 194 742
 678 296 196 253 148 176 856 773 766 260 704 109 248 617 227   6 705 482
  52 607 525 492 772 781 479 600 823 247 602 498 416 789 273 483 212 524
 155 169 769 102 304 494 722 725 633 266 410 334 803 712 484 181 437 857
 213  41 386  31  58 344 265 233 740 656 513 135 726   1 394 297 767 238
 288 360 178 127 751 106  51 414 115   4 443 197 444 116 105 117 163 728
 748 520  48 146  71 182 850  89 749 190  45 142  96 806 788  98 701 754
  84 460 761 515  34 150 837 808 171 246 557 521 180 626 412 526 361 834
 166 232 241 387 501 222 829 143 858 411 235 756 147 177 811 486 267 250
  46 466 770 517 757 777 541 747 423 731 750 255 336  27 535  90 269 463
 200   2 418 126 464 804 753 189  64 824 152 727 800 459 849 153 271 251
 160 851 776 223 243 100  88 732 427 157 762 621 852 124  75  91  30 242
 739 428 458 502 438  28 343 420 270 419 218 537 237 790 556 848  47 516
 847 101 440 156 746 225 715 159  37 162 634  70 472 175 432 445 644 833
  73 158 774  83 716 630 536 140 302 538 381 119 481 422 807 352 474 775
 792   0  87  50 272 268 485   5 540 539 167 456 453 503 855  92 476   8
 104 555 138 717 729 724 187 564 276 618   3 348 805 376 542 529 764  60
 378  55  39 718 137 703 561 207 377 436 439 642 452 426 475 473 560  74
 619 462 563 504 347 335 845 136 801 533 278 734 431 346 532 558 779 103
  38  80 702 345 562 205 505 534 559 134 461 220 219 791 226 390 455 425
 853 434 843 435 409  40 783 530  61 782 531  32 274 229 279 623 637 351
 500  81 639  69 510  72 451 380 391 379 389 622 349   9 706 636 433 350
 735 620  62  36 305 277 382 306 224 780 188 392 342 819  86 818  11 204
  59 383 707 454 765 733 506 785 275 354 186 708 337 341  10 230  65 393
 375 340 118  54 509 206 429 448 786 407   7 449 132 641 640  82 816 338
 820 821 120 191 817 778 815 339 508 122 446 812 787 784 208 507 384 121
 185 183 813 406 635 405 408 133 814 638 184 447 450]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3332
INFO voc_eval.py: 171: [433 171  95  94 172 445  90  80 102 457  98  78  96 436 109 363  79 120
 180 398  26 105  77  92  82 400 210  45 405 389 410  88 108 384 209 261
 163 249  85  83 362 437 377  46  32 103 271 393 143 356 264 364 438 162
 291  55  91  54 390 196 187 358 164 233 260  86  89 156 476 331 117 422
  81 218  44 429 201 110 119 388 222 142  87 317 288  11 327 237 205 252
 349  48 258 116 195 306 198  13  84 332 281 118 265 194 333 460 272  30
 337 328 151 124 369 176 387 245 208 334 392 376 262  58 146 351 396 359
 251 139 412 212   8 247 347 206 186 373 330 375  24 313 177 145 414 244
 409 280 155 406 403 192 385 284 149 122 432 227 121 353  49 282 428 324
 228 316 225 419 374 257 357 199 458 309 255 242 295 372 355 319 168  31
 386 348 283 250 381 131  27 150 123 287 411 202 427 221  16 273 140  99
 352 129 367 417 254 440 211 223 226 325 416 293   4 147  35 197  53 326
 266 350 114 200  41 127 135 112 371 310 311 305 173 459 141 421 128 404
 292 113 126 215 104 475 248  97  59 466 415 161 138 380 354 165 204 224
 468 115 132 178 423  29  33 474 424 308  93 277   0 148 169 395 157 100
 160 378  36 270  47 232 107 394 203 383 467 339 275   5 256 101 243 391
 329 230 239 234 207 318 130   3 402 397 407 235  43 413 469 246 154 431
 340 125  37 451  56 153  67 238 365 259 430 366 213 314 320 399 449 181
 304 285  28 276 462 420  73  68 229 144 274 465 418 370 382 435 263 321
 167  60 267 183 470 158 170   7  40 368  10 217 241 439 253 360 133 323
 278 434 322 111 338 408 152   6 231 345 312   9 446 307  38 269 401 286
 106 335 315 290 268 193 441 166 448 174 452 463 471 289 464  42  17   1
 379 279 134  34  39 159 426 336 214 301 219   2 182 297  61  50 461 300
 361 190  63  57 188  74 236 425 303  22 216 442 240 453 298 450  62  25
 220  20 454 342 294 184 296 341 179  66  14  52 299 443  71  19  64 344
 136  23 472 447 473  18 302 189 175  75 455 185 137 191 456  15  12 444
  51  72  21  76  70  65  69 343 346]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4241
INFO voc_eval.py: 171: [1595  719 1530 ...  118   99  103]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2840
INFO voc_eval.py: 171: [ 29 264  64 108 156 317  65 315 109  19 289 118  24 265  66 155 256 203
 316 125 154 288 266  26 298 232 260 110 213 202 292  74  83 290 122  52
  95 233 250 133  94  48 241  50 117  16 263 137 102 144 115  69 173 171
 192 234 177 191 308 153 297  42  21  20  75 105 206  92  27  63  70 124
  17 219 227 294  80 174 116  96 113  18 121 166 201 255 296 217 244 251
  22 159  55  28 111 165 287  73  56 258 238 194  76  81 205 172 167  54
 243 130 190  91 293 215  23 257 158 235 152  51 216  85  82  86 295 197
 157 291 127 312 119 262 146 309 246  77  53 123 284 314 310 211  25 195
 239 220 151 150 226 145   5 184 193  47 236 224  88  68 240 103 268 242
 161 138  84 126  87 131 128 164 136 237 135 112  49 223 259 170 245 218
 132 271 311 286 107 225  44 114 313 281 222  45 319 147  14 139  93  10
  67 101 303 248 134 196 247  89 253 252 140  46 277 299  41  97 106 204
  90   8 120 104   4 183 207   0  35 100 214  43  39  15 230 282 143  30
 209  99 221 269 270   6 283 278  34   1   3 129 261 169 272 279  98 181
   9 318  60 285 249   2 175 168  36 231 199   7  38 160  12  31  59 198
 162 163 212 301 180  58 210  72 200 276 273 208 280 304 186 178  32 300
 275 307 306 228  33  78  62  57  13  79 148 302  61 141 185 305  71 229
 176 267 179 149  11 189 254 182  40  37 274 142 187 188]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1014
INFO voc_eval.py: 171: [5854 2514  703 ... 5600 4367 4368]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3088
INFO voc_eval.py: 171: [332 260 196 446  59 202 285 390 274  51 257 698 765 273 259 262 286 454
 256 414 452 757 644 270  72 333 221 729 413  78 263 611 195 336 197 449
 198 633 155 385 182  65 756 561  74  56 136 179  54 230 450 200 258 281
 203 447 228 451 128 378 609 759 205 445 277 268 371 562  70 567 461 129
 443 171 761 760 323 444  69 645 456 114  61 537   7 279  21 421 271 222
  33  66 762 612  71 741  68 406 194 563 654 388 629 725 522 288 460 458
 204 391 448 538 247 704 267 244   6 625 360 201 727 640 309 345 266 613
 735 265 150 541 459 457 764 703  50 272 769 556 400 520 720 145 199 534
 292 341 396 475 364 389 213 219 647 293 630 430  58 344 634 177 425 653
 730 572 635 724 731 278 734 631 117 229 264  10 347 519 325 773 569 245
 142 733 540 668 284 269 214 732 758 239  41 384 352 335 453 386 422 623
 615 183 127 663 726 178 186 564 334 423 442 147  52 217 614 662 617 243
 312 766 670  15  64 185 568 295 415 208 570 558 699  85 736 466 210 646
 706 648 565 339 544 432 559 291 175 181 126 151 220 632 287 723 738 132
 742   9 319  49 324 418 610 737 141  80 775 420 187 521 107 392  29 240
 350 261 651 417 427 317 348 498 236  30 235 232 412 180 387 130 133 743
 370 419 152 721 622 748 767 131 771 428 455  47 479 223 174 184 532 719
 424 739 752 431 768 509 523 665 416 248 246 575 165 163 143 560 363 394
 316 426 753 710  67 709 650 715  76 636 290 693 207 624  22  48 402 159
 367 275 176 337 338 326 619 718 315 542 226 314 340 429 282 681 383 707
 361 747 135 237 557 573 101 530 722 685 144 499 212 787 659 785 289 397
 106 108 280 283 225 511 157 683 276 111 372   8 399 539 728 112  95 342
 543 241 587 772 507 206 763 330 664 740 701 571  17  31 343 786 318 404
 484 770 296  27 501 366 158 525 294 437 551 120 253 242 697 313 105 666
 744 711 566  88 403 137 329 139 486  23 626 628 621 579 116  14  13 508
  44 502 652 755 407 462 160 321 680 115 382 783 393 581 356 574 716  24
 536 667 218 751 485  57 545 639 749 514 642 227  55 156 172 746 102 777
 238 153 528 535 618  34 776 754 750 788 494 209  90 189  92 395 500 374
 657   1 643 401 638 745 110 492 231 696 595 655 465 216 322 398 405 682
 154 320  26 524 592 607 483 627 690 134 467 582 687 103 774  32  91 518
 604 224 215 513  25 365 211  20  94 310 193 331 375  36 691 109  60 138
 255 104 533 140 121 327 250 708 234 591 695 550 113  35 661  19  11  79
 606 252 173 603 512 778  87 328 491 168 251 362  83 588 669  28  63 489
 692 673 125 526 677 684 641   3 440 233 161 368   0 599 167 482 529 605
 589 480  77 435 379 369 594 679 658 531 148 249 548  73 377 438  43  53
 254 162 146 527 689 678 671  93  39 373  12 580 593 170  42 713 702  37
 191 700 784 649 436  18 712 496 439 555 637 495 694 192  81 487 188 353
 169 688 497 118 123 597  84 675  82 149 463 660 504  16  62 122 596 583
  75 358 493 488  86 506  97 306 166 600 164 598 553  89 481 676 674 510
 355 616  96 490 505 380 434 656 608 601 311 590  45 672  40 190 473  46
 471 433 547 714 546 552 100 549  38 503 119 408 515 381 308 124 468 410
 301 300 602 554 349 409 298 469 705   4   2 478 357 307   5 441 411 620
 464 474 472 303 516 782 477 717 376 686 351 297 476 354 305 359 470 586
 299 517 779 781 304 302 346 576  98 584  99 780 585 577 578]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2081
INFO voc_eval.py: 171: [2886 1275 1328 ... 2790 2789 2307]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3280
INFO voc_eval.py: 171: [343 409 346 839 442 498 353 375  49 479 462 446 546 146 349 499 617 407
  63 440 160 767 538 144  45 277 503 448 149 159  50 509 203  41 519 376
 841 153 273 355 836 728 236 790  48 412 128 781 408 459  65 524  44 204
 151 791 763 147 762 474 142 829 267 164 513  40 344 464 844 778  62 808
  51 433 374 833 219 467 371  82 818 354 796 618  53 752 245 724 268 540
 764 637 517 837 163 223 162  68 224 576 547 461 544 235 765 531 481  64
 471 502 656 483 415 241 246 850 420 280 416 356 848 814 435 594 152 345
 259 238 710 460 840  54 145 165 821 198 665 521 222 635 447 468 414 838
  43 126 206 413 437 237 472 432 543 213 664  52 608  85 294 199 150 768
 516  61 335 601 276 611 250 770 650 324 813 811 654 653 725 816 350  31
 697 193 766 584 797 220 609 227 800 132 695  67 777 158 831 720 786 750
 429  69 156 599 257 545 642 438 751 279 216 262 591 593 759 196 510 379
 229 439 430 382 411 228 854 736 141 347 214 542 130 830 157 380 431 410
 205 418 225 698 207 628 804 148 372 143  46 793 423 775 798 131 666 195
 166 318 753 434 136 636 598 805 794 731 842 550 341  59 624 772 846 436
 127 735 614 249 424 218  56 230 161 269 421 271 582 828 292 334 694 616
 311 307 634 441 155 116 348 638 264 675 508 737 110 803 688 260 817 112
 197 792 351 659 477 286  66  42 444 569  15 857 265 154 843 773 486 590
 194 129 727 287 539  86 133 291 278 858 332 243  60 242 686 667 570 610
  91 478 713 281 465 301 282 515 760 815 672 383 622 801 135 779  89  30
 124 640 352  84 769 799 825  57 776  93 232 233 780  58 270 595 419 714
 183 122 726 240 275 715 244  24 662 824 139 272 221 761 427  21 290 568
 384 615 326 852 117 771 732  18 274 602 823 612 853 473 577 381 247 671
 661  88 673 849 668 231 565 300  14 359 605 658 588 111 285 113 114 820
 211 604 552 181 215  55 443 120 644 299 564 101 217  12 613 603 426 248
 428 774 541 549 526 340 445 457 364 251 692 373  32 693  28 807 738 512
 721 701 525 535 712 476 389 663 304 313 689 742 306 597 554 567 855 312
 252 548 832  87 674 717 266 643 806   5 102 342 551 417 718 289 212 810
 809 677 422 261 185 258 553 722 683 834 530 118 296 819 856 812 325 293
 802 669  90 851 822 475 639 234 716 835 676 845 514  83 679  92 406 400
 627  19  34 333 847 186 652 641 682 123 719 321 566 263 425 625 284 630
 684 361 327 104 134 370 505 119 571 626 586 358 187 729  70 795 337   4
 319 655 399 455 581 670 298 733 649  25 596 589 587 859 579   6  10  81
  94 522 180 317 504 315 175 723 256 600   2 360  27 336  47 631 302   3
 309 536 511 239 392 188 529 559 500  78 607   1 255 362 678 755 322 295
 330 253 783 592 115 288 787 316 623 297 305 210 482 484  79 105 283 494
 739 606 558 490 314 730 323 308 378 740 449 532 363  39 368   0  95 452
 178  76 629  35  26  13 121  38 619 487 784  22 140 785 395 580 168 174
 137 705 463 560 660 633 533 365  99  98 125 700 537 556 492 534 747 647
 190 339 202 745 451 450 789  17 303 648 310 632 555 169  33 470 167 404
 208 480 138  23 367 495 501  77 254 182 680 398 782 734 179 454 518  73
 401 527 402 702 403 743 466 366  97 176  96 103 191 788 469 507 489 357
 706 645 488 704 485  20  74 574 572  29 585 453  72 651 646 578  80 177
 100 575 528 320 458 557  75 583 201 184 741 387 491 696 108  71 691  16
 209 192 377 687 699 748 506 573 681 189 657  36 749  37 746 200 561 405
 394 456 109 328   7 563 520 690 493 388 620 523 711 385  11 369 756 107
 562 758 744 707 621 226 709 338 703 329 173   9 331 708 386   8 172 106
 171 170 397 757 496 754 497 393 396 826 391 390 827 685]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2110
INFO voc_eval.py: 171: [134 953 440 ... 572 778 779]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3037
INFO voc_eval.py: 171: [ 47 272 259 142 146  26  46 151  48  51 143 280 118  73  34 160  69  31
 266 144  79  60  56 189  57 169 116 267 261  74  53 162 247 197  20 204
 173 131 269 268 194  36 240  33  25 163 282 254  32 243 260  30  78 263
 277 147 249 148 117 150 283 281 156   7 154 191  52  84  17  62 167 214
 115  50  22 196 276  85  63 184 145 224 114 100  77  61  86 141 203 159
 195 264 242 284  89  18 265 109  35  11 231  98  67 241 211 187 258 217
 262  43 253 152  66  23 127  19  54 168  29 248  58  81  68 244 135 200
 113 164  71 166  80 126  96 149 245 185  38 239 230   5 219  28   1  59
  94 155 234 270   9  21  39 153 111  99  14  65 132 252 199 158  42 157
  76   6  10  91 271  16  72 139  13 220  95 106 138 206   0 216  82  37
 188 278 104 236  93  27 228 209 136 218 250 202  24 103 255 178 251  70
  15 186 134 165  97 140 110  87   3 102  49 181  12 213   4 192 174 101
 120 107 279  88 171  40 275 226 108 208 122   8  41 124 182  83  45 175
  92 223 233  55 177 235 172  75  90 112 125 176 183 170 105 210 179 215
 198 212 256 237 129 205 207 273 201   2 119  44 190 221 222 161 193 128
 229 123  64 274 225 232 180 246 227 130 121 238 137 257 133]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.4982
INFO voc_eval.py: 171: [ 8601 14064  1069 ...   783 10928 10930]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5386
INFO voc_eval.py: 171: [2510  256  710 ... 2706 1942 1824]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4130
INFO voc_eval.py: 171: [ 64 312 537  32 333 546  65  54 321 131  38 540  48 541 331 340  42 313
 317 617 538 153  44  33 348 481 219 134 329 344 550 542 252 310 155 341
 336 545 552 124 408  35 137 647 281 414 638 345 304 543  69 144 366 618
 424 143  66 322 539 145 160 136 154 420 225 316 224 627 488 626  58 223
 493 564 266 622 410 318 624 156 334  50 122 613 628 560 482 218 151 631
 465 308 641 250 413 351 253 139 315 162 620 565  37 498 347 621 527 217
 623 554 229 417 335 231 163 649 491 121 327 129  43 416 501  89 138 194
 165 320 515 364 133 643 448  41 616 492 314 309 633 256 286 365  63 330
 372 265 232 161 645 251  60 132 311  94 221 496 650 130 227  90 576 346
 423 282 676 411 648 349 640 307 673 123 230 150 634 326 328 343 642 507
 141 337 367 480 409 569 142 561 636 254 421 305 242 135 158 228 504 508
 566 306 126 434 319 148 140 325 263 570 412 619 172 614 276 563 510 338
 164 524 610 261 639 323 635 128 280 407 363 339 630 198 674 206 220 637
 262 324 684 672 149 332 125 629 549 530 342 415  76 555  40 485 127 611
 644 222 301  47 632 354 494 581 199 547 599 483 264 503 267 568  15 226
 101 100 464 451 646 291 486 467 571 157 499 195 625  62   7 353 201 378
 506 612  68  27  53 200 443 192  34 562 495 369 373 466  36 677 108 270
  24 152 350 379 544 268 528 521 578 120 553 615 381 205 243 186 169 258
 652 490 119 255 356 193 196  93 352 297 355 240 180 479   0  92 166 259
 518  88 511 519 197  23 686 188  91  17 505 257  70 681 418 394 184  45
  95 299 118 651 385  22 675 260 377 159 359 468 269  52 244 173 556 442
 517 520 656 653 429 177  86 478 557 439 283  14 167 112 682 370   4 471
 523 298 213 574 509  46 445 107 105 433 428  77 436   1  30 558 450 182
  59 247  75 573 446  20 111 559 181 598  79 103 245 422 168  82 211 462
 296 512  25 678 605 683 463 487 435 548  28   2  16 290 185 603 208 437
 476 460 685 513 275 371  55  18  99 425 579 209 514 687 401 279 591 654
 522 212 272 582 600 608 477 679 489 405 577 551 248 368   9   8 104 210
  56 238 604 525 449 302 567 500 398 110  67  19 680 176  39 531  21 109
 601 580  74  72  31 274 289 106 516 575 444 470 502 484 278 285 655 277
  78  73  49 183 472 114 397  71 116 303 419 113   3 271 529 526 440 374
 426 246 207 441 497  87 284 602 658 597  51 431 288 249 393 474 447  26
  61 586 179  57 358 386 432 438  29   5  96 215 300 287 191 273 190 360
  84 430 596 375 117 404 361 202 662  13 593 427 357 216 400 589  85 585
  80 535 383 661 588 399 609  81 175 406  97 583 587 475 572 203 584  98
 592  83   6  10 473 234 115 362 214  11 659 533 178 671 594 390  12 664
 380 590 235 387 189 606 233 452 170 534 595 384 204 236 239 396 171 391
 292 241 663 174 187 607 469 456 237 295 395 660 670 532 147 382 392 388
 389 402 102 668 146 461 667 403 293 657 455 294 536 666 453 457 459 669
 665 376 458 454]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3160
INFO voc_eval.py: 171: [ 43   3  47 120 188 276 182  48 125 334  50   6 191 186 232  52 156 189
  13 160 330 192 157 154  42  60 278 150 180 185 336 229 126 274 219 141
 155 184 272 227 233 271 101  80 275 243 127 149 100  78 333  49 247 277
  58   5 123 104  59 252 162  10 151 161 173 324  23 228 119 183 145  57
 128 304 210 268 244  44 134 307 174 181 187 107 241 322  75 231 235  11
  72 230  46 165 213   4 158  99 323 267 159 103 131 190 118  96   8 319
  51  12 317 246 249 332 106  89 201 250 259 264  25 245 129 143  14 248
 102 121  97 328 256  45  73 285 251 206 209 234 124 152 335 105 327 305
  91  15  83 169  63 316 217 240  74  98  90 260  61  84 325  26 198 114
 258 338 194 321 310 265  27 337 116 290 291 226 225 147  64  79 221   7
 122 133 326 113  85   9 193 117 281 166  82 208 318 255 309 237 216 148
 266 115 303 254  95  76 301 130  77 320 153 242 311 295  55 224  62 132
 329 331 220  33 313  39 299 222 238 144 269 261 273 306 262  65 294 257
  30  24  88 199 214 170  28 211 286 112 293 215 175 236 203 172 142 218
 312  81 292  87 146  31  53  54 223  56 111 196 139   1  32  22 280   2
 302 135 287   0 263 253 195 200 296 207 168  86 279  18 205 109  29 138
 204 300 197 108 297 289 167 202 239  41 177 110 298 136 163 282 137  37
  35 176  19  21  20 171 270  17 315  16 140 178 288  71  36 284  40 164
 179  69 283 308  34 314  70  67  38  66  68  92 212  94  93]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1311
INFO voc_eval.py: 171: [633 378 640 116 644 321  82 207 391 291 122  77 400 169  79 890 637 638
 292 323 320  84 182 118 393 800 125  90 212  94 293 796 210 502  83 208
 533 384 631 379 190 635 450 669 653 449 298  99 206 452 120 117 322 189
 898 904 892 304 338 295 177 805 829 507 294 461  87 520 537 383 875 839
 848 901 705 399 166 216 135 136 202 834 218 902 634 385 523 330 540 297
  64 453 485  44 863 430 325 100 113 844 171 727  19 381 217 744 137 196
 811 326 312 676 309 324 205 193 876 706 652 905 661 895 804 454 831 797
 106 505 197 903 174  81 527 259 167 170 278  85 833 865 440 436 220 183
 434 728 187 906  95 754 816  46  98 229 302  71  20 506 394 618 168 198
 877 536 549 893 328 751 873 522 823 716 179 209 886 739 307 186  34 173
 165 910 838 709 734 548 891 195  68  45 882 459 129 214  66 242  75 894
 240 455 445 103 835 484 783 213 508 780 247 451 826 227 458 814 867 316
  59 185 542 539 221  86 801 261 866   2 448 390 712  54 371 715 380 847
 702 632 897 334  93 180 181  96  17 707 698 262 257 708 496 314 685 681
 524 597 575 758 504 636 395  60 885  18 923 159 178 874 433 832 916 396
 480 849 864 327 204 585 315 404 463 319 134 429 301 781 908 110 101 753
 374 456 352 869 511 104 437 771 736 398  53 813 482  73 674 397 172 145
 623 233 388 232 515 462 907 884 602 372  33  42 464 260  39 819 176 235
 491 872 922 900 199 346  74 677   6 431 441 759 828 275 288 356 503 538
 249 817  36 878 155  97 862 741  72  43 428 846 909 355 144 252 859 604
 688 655 114 526 271 460 682 557 798 521 239 650 662 466 611 211 102 123
 299 899 112 175 822   8 376 124 158 544 228 188  58 820 810 777 516 570
 615 855 318 248  76 245 163 250 528 164 474 887 756 684 541 745 683 853
 192 550 725 914 532 442 493 827  47   9 354 421 200 601 824 743 888 880
 568 215 138 184 883 353 629 731  38 329 837 686  56 111 595 264 825 896
 258 237 308  88 486 236 305 254 737 219 263 465 845 693 619 162 226 306
 560 755 127 525 765 311 868 724 679 871 457 851 512 438 401 439 402 697
 809  37 432 331 870 201  57  41 609 572 926 889 272 673 774 337 598 191
 470 154 389 726 584   0 562 924 624  40 843 645 225  89 600  62 596 517
 713 617 333 757 788  35  13 107 704 599 729 552 281 419 375 194  49 627
 382 347 720 787 543  80 392 605  61 793 821 386 747 678 435 336 286 115
 279 427 494 563 498 925 881 139 852 345 738 571 573 574 131 769 675 621
 490 358 514 296 768 680 687 556 534 234 313 719 267 238 156 818 373 565
 836 413 280   7 770 746 616 241 255 420 789 806 582 349 784 786 790 642
 710 782 273 854  63   4 772  55 203 551 588 608 603 277 300 244 350 703
 576 475 581 732 359  32 742 711 418 700  67 718 284 785 377 344 443 317
 243 842 447 752 476 607  31 840 274 630 577 748 109  48 497 108 696 672
 387 606 692 422 310 767 268 365 276 699 487 341 555 282 879 481 808 105
 151  22  78  92 510  30  69 612 569 622  91 750 546 246 364 473  21  70
 911 303 269  16 807 403  10 779 547 340 567 415 561 815 580 513 587 251
 841 343 794 471 529 444 367 230  23 791  15 721 287 578 583 283 231   3
 749 920 285 468 566 553 649 416 860 564 775 256 554 253 666 446 489 701
  65 722 776 740 357 265 778 499 363 610 802 488 733 917 500 664 620 730
 812 735 467 799 590 160  52 501 530 803 694  11 143 409 545 410 266 518
 408 472 695 469 723 795 535 830 492 579 339 425 153  50 912 479 366  12
 483 589 495 593 714   1 289 369 370 667 519 586   5 132  14 717 915 424
 792 641 614 773 152 509 671 766 850 592 531 861 477 478 368 351 290 691
 423 654 625 362 360 591 332 426 639 157 764 417 406 361  29 412 411  51
 161 133 148 335 763 141 628 856 405 342 651 414 558 559 222 660 919 626
 658 119  28 613 594 921 918 146 857 760  24 140 690 761 657 407  27 147
 858 762 224 348 668 150 128 689 223 270 121 130 648  26 647 142 126 149
 643 913 663  25 670 656 665 659 646]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2879
INFO voc_eval.py: 171: [424 250 596 173 437 298 136 445  23 471   1  36 480 500 254 321 351 477
 482 323  38 398 441 252 349 248 126 422 350 472 124 579 310 299 352 553
 263 612  56  67 425 253 474 538 106 324  70 129 231  31 442 597   3 448
 179 523  16 328 434 479 317 181 340 348 304  42 320 600 586 620 154 630
 265 226 358 101 615 368 608 377 260 371 444 150 308 539 220 447 108 109
 476 206  49 322 137 256  18 366 209 488  72  69 439 223  28 302 143 602
   8 160 559 467 306 205 510 382 475 536 378 364 325 273 234 487 102 618
 465 359 361   9 397 367 332 396  39 186 244 119  57 635 162  15  24 161
 214 624  55 486  45 484 603 177  43  25 128 431  83 594 410 301 517 314
 208 429 300 258   6 580 478 287 508 345 546 307 305  40 315  52  71  29
 224 483 505 212 118 338 373 240 100 229 178 572  47 473 522 562 563 145
 312 210 329 213 436 216 625 481 599 430 401 379  89 318 215 171 125 634
  37 264 622 174   2 131 189 158 404 116 192 614 211 172 384   5 583 228
 582 120 555 114 309  74  92 357  88 355 438 365 518 105 551 183 409 485
 335  11 271  44 601  90 552 498 628 303 399 123  17 281 556 566 613 175
 185 418 293 457  22 402 623 455 342 564 616 111 207 549 443  12  99 311
 631 130 284  86  54 533  32 170 519 502 319 153 403 199 159 542 544 526
 557 565 503 619 540 611 237 225 416 276 362 278  97 285 280 537 360 491
 598 529 499 626 632 363 627 144 217 610  93 575  94 334 504 113 110 456
 191 629 388 239 617 501 509 219 148 571 412 380 279 507 204 521 333 257
  85  53 621  81 524 316 147 313  77 385 218  91 370 155 282 545 221 369
  62  27  41 547 138 453 180 569 589 587 274 459 141 520 423 115 386 372
 354  60 506 275 535 548 127 103  75  76 638 117 451 286 376  78 255 268
 514 454 543 574 433  35  48 375 187 230 381 166 353 426 570 440 383 356
 374 449  33  59 330  80 133 242 122 202 201  51 277 633 458   4  30 326
 637  50 591 269 107 259 157 233 592 392 452 339 283 527 142 337 466 289
 593 432  14  34 149 331 243 435 146 272  10  58  79 104 292  68 291 578
  63 165 341 468 636 121 400 489   7 151 554 227 156 139 609 588 414 112
 490 550 534  21 427 561 132 568 532 463 296 541 198 140 190 238 531 184
 152 450 428 395  26 241  84 182 197 408 135 560 530 134 577 411 513  82
 295 413 327 407 446 235  87 222  95 297 168 261 167 567 249  66 200 406
  96 294  73 176 525 390 232 343 164 262 415  65 344 251 336 469 169 558
 585 391 528 394 163 346 515 573 590 288 389 405 188 576 417  61 511 270
  20 236  98  64 462   0 512 290 516 347 584 393 387 493 607 606 460  19
 461 247 246 604 605 267 496  46 464 492 419 194 266 420 203 595 196 497
 245 581 494  13 421 193 195 495 470]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3331
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3038
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.224
INFO cross_voc_dataset_evaluator.py: 134: 0.395
INFO cross_voc_dataset_evaluator.py: 134: 0.197
INFO cross_voc_dataset_evaluator.py: 134: 0.239
INFO cross_voc_dataset_evaluator.py: 134: 0.333
INFO cross_voc_dataset_evaluator.py: 134: 0.424
INFO cross_voc_dataset_evaluator.py: 134: 0.284
INFO cross_voc_dataset_evaluator.py: 134: 0.101
INFO cross_voc_dataset_evaluator.py: 134: 0.309
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.211
INFO cross_voc_dataset_evaluator.py: 134: 0.304
INFO cross_voc_dataset_evaluator.py: 134: 0.498
INFO cross_voc_dataset_evaluator.py: 134: 0.539
INFO cross_voc_dataset_evaluator.py: 134: 0.413
INFO cross_voc_dataset_evaluator.py: 134: 0.316
INFO cross_voc_dataset_evaluator.py: 134: 0.131
INFO cross_voc_dataset_evaluator.py: 134: 0.288
INFO cross_voc_dataset_evaluator.py: 134: 0.333
INFO cross_voc_dataset_evaluator.py: 135: 0.304
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 7999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.419s + 0.026s (eta: 0:00:55)
person 0.97052145
person 0.9638462
person 0.9924164
person 0.9609506
person 0.97571975
person 0.9396136
person 0.98264366
person 0.9457456
person 0.9116995
person 0.9816374
person 0.93875766
bottle 0.92177
bottle 0.9193362
bird 0.95102894
person 0.98253745
person 0.9182398
person 0.94489086
person 0.9028342
person 0.904755
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.333s + 0.031s (eta: 0:00:41)
person 0.9448644
person 0.9208808
person 0.96095294
person 0.94858205
person 0.98058516
person 0.9480965
person 0.9674516
person 0.9818994
person 0.93972033
person 0.9216456
person 0.92581177
person 0.9317554
person 0.90623754
person 0.92292434
person 0.920983
person 0.9270857
person 0.975747
person 0.91352266
person 0.9800029
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.341s + 0.035s (eta: 0:00:39)
person 0.928853
person 0.91205853
person 0.9568106
chair 0.9381376
person 0.97246933
person 0.9160167
person 0.95765173
person 0.9668187
chair 0.9656004
chair 0.93704945
chair 0.9149233
person 0.9048807
person 0.9791069
person 0.9879382
person 0.9275109
person 0.9771607
person 0.9915814
person 0.9813239
person 0.952014
person 0.91063535
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.333s + 0.036s (eta: 0:00:34)
bird 0.93999654
person 0.92776984
person 0.9212431
person 0.9938305
person 0.95718044
person 0.97430134
person 0.9796927
person 0.9073622
person 0.9595947
person 0.96604407
person 0.90392923
person 0.9189333
person 0.96953815
person 0.9409668
person 0.95840365
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.334s + 0.036s (eta: 0:00:31)
diningtable 0.9028542
diningtable 0.9386222
chair 0.9638937
chair 0.9659358
chair 0.9381262
chair 0.95760417
pottedplant 0.91807526
pottedplant 0.91432476
pottedplant 0.9110683
pottedplant 0.9044141
pottedplant 0.9436867
pottedplant 0.9384092
pottedplant 0.92845625
person 0.9657764
person 0.9828864
person 0.9202928
person 0.9798243
person 0.9489792
person 0.9890977
person 0.90009993
person 0.9157074
person 0.9039193
person 0.9119344
person 0.92224413
person 0.9225708
person 0.9739982
person 0.9550987
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.329s + 0.036s (eta: 0:00:27)
person 0.95806605
person 0.9239022
person 0.9071694
person 0.91959447
person 0.9479497
person 0.9150143
person 0.9535545
person 0.936853
person 0.92978793
bird 0.9227486
person 0.9375971
person 0.987661
person 0.91696364
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.327s + 0.036s (eta: 0:00:23)
pottedplant 0.90279025
person 0.96242654
person 0.9339323
person 0.91297543
person 0.95383114
person 0.9190188
person 0.9586617
diningtable 0.9110623
chair 0.9826158
diningtable 0.962033
chair 0.91612554
chair 0.9486109
chair 0.9807811
chair 0.92644364
bird 0.9930322
person 0.9027364
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.325s + 0.036s (eta: 0:00:19)
car 0.92877096
car 0.9424396
car 0.91531277
person 0.98840874
person 0.9004976
person 0.97413063
person 0.929861
horse 0.9737718
horse 0.9135407
horse 0.9209365
person 0.9379899
person 0.9533607
person 0.96814597
person 0.91861486
person 0.9430882
car 0.94591045
car 0.9640443
bicycle 0.96017057
bicycle 0.93105334
person 0.9369433
person 0.91794837
person 0.97355235
chair 0.91867715
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.324s + 0.036s (eta: 0:00:15)
person 0.97420424
person 0.9591269
person 0.97380334
person 0.9177014
person 0.9132709
person 0.90172666
person 0.9885965
person 0.9863652
person 0.94355065
person 0.97116345
person 0.9154854
person 0.9639669
person 0.9911753
person 0.9088434
person 0.96658015
person 0.91186655
person 0.9878876
person 0.9798886
pottedplant 0.9115907
person 0.9898589
bus 0.91275847
person 0.9612754
bus 0.90667766
person 0.94154304
person 0.95475924
chair 0.9469613
person 0.9054701
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.323s + 0.036s (eta: 0:00:12)
motorbike 0.9632511
person 0.97489864
person 0.9105614
person 0.9158265
person 0.95702684
person 0.9676532
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.322s + 0.035s (eta: 0:00:08)
person 0.9067682
boat 0.9820404
boat 0.9482982
boat 0.9185201
person 0.990378
person 0.93751055
person 0.98548746
person 0.91826713
person 0.9178986
person 0.98908234
person 0.97490263
person 0.9107187
person 0.9752621
person 0.976752
person 0.9138957
person 0.94842964
car 0.95061153
person 0.94062567
person 0.94723
person 0.95981693
person 0.9868116
person 0.9081319
person 0.9212367
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.325s + 0.035s (eta: 0:00:05)
diningtable 0.9598881
chair 0.9249409
chair 0.97385186
chair 0.98070437
chair 0.91551405
chair 0.981199
chair 0.9018899
chair 0.93678313
chair 0.9599851
person 0.9427703
bird 0.92407215
bird 0.91563725
boat 0.9198027
person 0.96211
person 0.9293705
person 0.9888263
person 0.9148756
person 0.90364474
person 0.9135899
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.322s + 0.034s (eta: 0:00:01)
person 0.9834917
person 0.9640555
person 0.9564091
person 0.95547205
person 0.9490347
person 0.9661837
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.429s + 0.026s (eta: 0:00:56)
person 0.9821098
person 0.9193171
person 0.97897136
person 0.977132
person 0.9142107
bicycle 0.9724167
person 0.93211114
person 0.91516155
bicycle 0.9747772
person 0.93606925
bird 0.9436755
bird 0.9008678
pottedplant 0.94306016
person 0.9684349
person 0.9289504
person 0.902271
person 0.96104664
person 0.96612376
person 0.9869082
person 0.92682964
person 0.96605253
person 0.95272076
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.312s + 0.033s (eta: 0:00:39)
person 0.91264653
person 0.9064255
person 0.96401167
person 0.9178444
person 0.9739925
person 0.9429193
person 0.93596417
person 0.9808786
person 0.96439904
chair 0.92525196
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.321s + 0.033s (eta: 0:00:36)
car 0.9158824
car 0.9508885
person 0.9470934
person 0.96154535
person 0.9817254
person 0.9248445
person 0.9491547
person 0.96498525
car 0.98236644
cow 0.92337114
person 0.9152795
person 0.9577877
person 0.92332584
person 0.9132997
person 0.98117054
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.320s + 0.034s (eta: 0:00:33)
person 0.97830987
chair 0.93472195
person 0.9303071
person 0.92527163
person 0.9514709
person 0.9536033
person 0.9330889
person 0.91807437
person 0.94037336
person 0.98068786
person 0.9869818
person 0.97717345
person 0.969131
person 0.9774919
person 0.9054013
bird 0.9045495
bird 0.99082375
person 0.92206466
bird 0.95169556
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.320s + 0.034s (eta: 0:00:29)
person 0.94628507
person 0.97120416
person 0.9827085
person 0.9219514
person 0.9483382
car 0.95061153
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.320s + 0.034s (eta: 0:00:26)
bird 0.9100656
bird 0.9595731
bird 0.9135889
person 0.91485715
person 0.94437414
person 0.91872835
person 0.9150689
person 0.9884178
diningtable 0.9487001
person 0.9723726
diningtable 0.97905844
person 0.9397489
person 0.936078
chair 0.9430641
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.324s + 0.034s (eta: 0:00:22)
person 0.9159857
person 0.9074581
person 0.93122655
person 0.94933665
person 0.98590684
bus 0.94643956
dog 0.9525036
diningtable 0.97387874
diningtable 0.9241108
chair 0.9126377
chair 0.9850793
chair 0.9112347
chair 0.97122204
chair 0.9359425
pottedplant 0.9018602
person 0.90426093
person 0.9011075
person 0.97651744
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.321s + 0.034s (eta: 0:00:19)
diningtable 0.92108434
chair 0.9116188
person 0.9482621
person 0.96668243
person 0.9560311
person 0.9370736
person 0.94513
bird 0.95428336
person 0.9262234
person 0.96230865
person 0.9641382
person 0.93236893
person 0.9803367
person 0.91000056
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.321s + 0.034s (eta: 0:00:15)
person 0.9628555
person 0.9004053
person 0.96391064
person 0.9174576
person 0.90330154
train 0.95835906
person 0.9571121
person 0.96216595
person 0.94631636
person 0.9402531
person 0.9151012
person 0.929576
person 0.93172467
person 0.90523773
person 0.956395
person 0.92673224
person 0.9925787
person 0.9740155
person 0.95981747
person 0.90140736
person 0.91762054
person 0.9274506
person 0.9032452
person 0.9310562
tvmonitor 0.91767514
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.321s + 0.034s (eta: 0:00:12)
person 0.9642223
person 0.90657645
person 0.94477224
horse 0.96011966
person 0.9667946
person 0.9309407
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.320s + 0.034s (eta: 0:00:08)
person 0.9752381
person 0.9652196
person 0.92209387
person 0.91967493
person 0.9524646
person 0.9282779
person 0.93633276
person 0.9033103
person 0.9159261
person 0.9867675
person 0.94664675
chair 0.9725389
person 0.9561947
person 0.90710306
person 0.9441489
person 0.9586393
person 0.9008742
person 0.9056302
person 0.9187544
car 0.94500166
car 0.91825217
car 0.9521954
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.319s + 0.034s (eta: 0:00:04)
person 0.9674442
person 0.9440088
person 0.96266466
person 0.9489241
person 0.9743259
person 0.9609722
person 0.9222517
person 0.9003024
person 0.9074641
person 0.9286554
person 0.9881892
person 0.98142576
person 0.9163921
person 0.9783699
person 0.928189
person 0.9383326
diningtable 0.9336633
person 0.9012964
person 0.91789544
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.315s + 0.033s (eta: 0:00:01)
person 0.93827707
person 0.9082605
diningtable 0.9102723
person 0.9709282
person 0.9407776
person 0.9872788
person 0.9654199
person 0.9581895
person 0.95964694
person 0.90843946
person 0.9110001
person 0.9179938
person 0.9408888
person 0.96304554
person 0.980793
person 0.911093
person 0.9494127
person 0.9564401
person 0.95613265
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.347s + 0.041s (eta: 0:00:48)
diningtable 0.90208435
diningtable 0.9280307
person 0.96005577
person 0.9761574
diningtable 0.9736312
person 0.9370816
person 0.91847837
person 0.90830636
bottle 0.90669954
person 0.978119
pottedplant 0.91749847
diningtable 0.9286775
person 0.96595097
diningtable 0.9069578
chair 0.94937646
chair 0.9659954
chair 0.9228555
person 0.931462
chair 0.90135777
person 0.90735483
person 0.9329601
person 0.9956527
person 0.9808139
person 0.9345925
person 0.9662653
person 0.9653885
person 0.9047797
person 0.9707493
person 0.9600954
person 0.93897897
person 0.9408903
person 0.9655245
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.314s + 0.035s (eta: 0:00:39)
person 0.94735855
person 0.98593605
person 0.93356085
person 0.9708702
bottle 0.936851
person 0.9370705
person 0.9440851
person 0.95379347
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.326s + 0.035s (eta: 0:00:37)
chair 0.9316975
bottle 0.9092823
boat 0.90105087
person 0.97290593
person 0.93987465
person 0.93370146
person 0.9326178
person 0.932551
person 0.9685333
person 0.9442745
person 0.9012873
bottle 0.9081005
bottle 0.95351064
person 0.9821805
person 0.9526621
person 0.9492881
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.324s + 0.035s (eta: 0:00:33)
person 0.9668393
person 0.9216889
bird 0.91694933
person 0.95375866
person 0.90675676
person 0.9680665
person 0.9706063
person 0.95991987
person 0.90364724
person 0.9498725
person 0.90819466
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.324s + 0.034s (eta: 0:00:30)
person 0.9695765
person 0.96918505
person 0.95327693
person 0.9068995
person 0.9277508
person 0.9707065
person 0.9206514
person 0.9409995
person 0.91435206
person 0.9198957
person 0.9370214
person 0.96991473
person 0.901256
person 0.9535514
person 0.92475134
person 0.90548456
bottle 0.9536416
chair 0.9311507
boat 0.9041359
person 0.9733087
person 0.95526826
person 0.90512
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.326s + 0.034s (eta: 0:00:26)
car 0.9808216
car 0.9349942
person 0.92623067
person 0.97738373
person 0.909867
person 0.9112954
person 0.9206223
person 0.93526405
person 0.9650001
person 0.9231481
person 0.9179724
person 0.95749533
person 0.9183607
pottedplant 0.91411734
pottedplant 0.931408
car 0.9850723
car 0.93774736
person 0.9325422
person 0.9877895
person 0.9558578
person 0.9279064
person 0.9367985
person 0.98598415
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.322s + 0.034s (eta: 0:00:22)
person 0.97979045
car 0.96010774
person 0.9549971
person 0.9261184
tvmonitor 0.9263405
person 0.90584826
person 0.93399876
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.320s + 0.035s (eta: 0:00:19)
chair 0.90433216
person 0.970867
person 0.9464638
train 0.9570788
train 0.9581312
bird 0.91125757
bird 0.98304546
person 0.92240006
person 0.92248887
bicycle 0.92058724
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.318s + 0.035s (eta: 0:00:15)
person 0.96992314
person 0.9521811
person 0.92894185
person 0.92946875
person 0.9781723
person 0.9868739
person 0.9588572
person 0.97375995
person 0.9891605
person 0.95541364
person 0.9651571
car 0.9525015
person 0.96903867
person 0.9081379
person 0.95027256
person 0.97717184
person 0.9344448
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.318s + 0.035s (eta: 0:00:11)
person 0.91572005
person 0.94561934
person 0.924137
chair 0.9751091
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.318s + 0.035s (eta: 0:00:08)
person 0.9814171
person 0.9416126
person 0.92981976
person 0.9641041
person 0.96219265
person 0.9025835
person 0.9264158
person 0.907385
person 0.9407527
person 0.92128724
person 0.9789544
person 0.9069935
person 0.90878487
boat 0.9031624
person 0.9674058
person 0.9253851
person 0.9478595
person 0.9126951
person 0.9581488
person 0.9040883
person 0.926709
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.320s + 0.035s (eta: 0:00:04)
person 0.96745044
person 0.9736882
person 0.9780935
person 0.94946265
person 0.918184
person 0.9306451
person 0.92502403
person 0.9508527
person 0.9609854
person 0.9194785
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.320s + 0.035s (eta: 0:00:01)
person 0.9407117
person 0.97139615
person 0.92770797
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step7999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.418s + 0.038s (eta: 0:00:56)
person 0.9419775
chair 0.9195331
chair 0.9584183
person 0.9584114
person 0.9254573
person 0.95368254
person 0.9412914
diningtable 0.93184435
person 0.974538
person 0.97190386
diningtable 0.9377609
person 0.9539614
person 0.9685242
person 0.96240866
person 0.90767527
diningtable 0.91161734
person 0.9318995
person 0.97917366
diningtable 0.9661723
diningtable 0.929591
diningtable 0.9407354
chair 0.9153722
chair 0.98155344
person 0.90701735
person 0.9315718
person 0.9890532
person 0.9196809
person 0.9613618
person 0.9852368
person 0.9162009
person 0.92380077
person 0.9836173
person 0.96837866
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.325s + 0.034s (eta: 0:00:40)
person 0.9397551
person 0.95472544
pottedplant 0.93348783
person 0.904655
bird 0.9249874
bird 0.97176623
bird 0.9485103
bird 0.9079026
person 0.9673996
person 0.9208625
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.319s + 0.033s (eta: 0:00:36)
person 0.9504024
person 0.9104903
person 0.96234894
person 0.97015226
person 0.97668344
person 0.94648635
person 0.9021954
bottle 0.9056967
bottle 0.9523822
bottle 0.9036128
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.319s + 0.033s (eta: 0:00:33)
person 0.9101746
person 0.9180426
diningtable 0.907597
pottedplant 0.9311856
chair 0.9399732
chair 0.9160368
person 0.9757041
person 0.9414847
bottle 0.9097779
bottle 0.9092322
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.316s + 0.034s (eta: 0:00:29)
person 0.9624523
person 0.9606896
person 0.9478457
person 0.9947836
person 0.9427349
person 0.9405297
person 0.92049104
person 0.97949094
person 0.91289455
person 0.9581384
person 0.9734769
person 0.9157739
person 0.9160754
person 0.9347155
person 0.93895954
person 0.99201
person 0.9397764
person 0.9744479
person 0.92534196
person 0.96115595
person 0.98381907
person 0.9665326
person 0.9274712
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.314s + 0.034s (eta: 0:00:25)
person 0.9481147
person 0.96496606
person 0.9025542
person 0.9707299
person 0.9070107
diningtable 0.9841738
chair 0.91671985
chair 0.94796985
chair 0.9793888
chair 0.9874427
aeroplane 0.9123051
person 0.9667758
person 0.98157954
person 0.93302476
person 0.91760516
person 0.91231847
person 0.9476635
person 0.97357845
car 0.92305475
car 0.9804081
car 0.9719045
car 0.9589152
car 0.9011143
bird 0.9584549
chair 0.9317989
chair 0.95090204
person 0.94563127
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.314s + 0.035s (eta: 0:00:22)
person 0.98415035
person 0.93360096
person 0.9092562
person 0.9813053
person 0.9584493
person 0.98960334
person 0.9371093
person 0.926918
person 0.9409758
person 0.9902811
person 0.9386356
person 0.90842235
horse 0.9625155
person 0.91283333
person 0.90532386
person 0.9196585
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.311s + 0.034s (eta: 0:00:18)
person 0.95543146
person 0.9462774
chair 0.97503155
chair 0.92183906
chair 0.9568609
pottedplant 0.92485887
pottedplant 0.92247534
pottedplant 0.94560736
person 0.948598
person 0.9038888
person 0.92195016
person 0.9379599
person 0.9592883
person 0.90004015
person 0.93819886
person 0.9324102
person 0.90178937
person 0.9041873
person 0.955816
bicycle 0.91386527
person 0.9553322
person 0.97983885
person 0.9028613
person 0.9428594
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.316s + 0.035s (eta: 0:00:15)
person 0.98808086
person 0.9181167
person 0.9321209
person 0.9165882
person 0.9593161
person 0.985443
person 0.9401535
person 0.97688645
person 0.977991
person 0.97183335
person 0.93717915
person 0.92287457
person 0.9169792
bus 0.95544857
person 0.90619546
person 0.939563
person 0.9482405
person 0.9651733
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.315s + 0.034s (eta: 0:00:11)
person 0.9675744
person 0.9149375
person 0.93565226
chair 0.9126919
chair 0.94861853
tvmonitor 0.9017124
chair 0.971228
chair 0.9301292
chair 0.9400475
person 0.949611
person 0.9701104
person 0.9570019
person 0.97748715
person 0.9188639
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.313s + 0.035s (eta: 0:00:08)
motorbike 0.9155415
motorbike 0.95152247
person 0.9018044
person 0.9334308
person 0.97808903
horse 0.95565253
horse 0.9010771
person 0.903705
person 0.90486765
bicycle 0.9589509
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.312s + 0.034s (eta: 0:00:04)
person 0.9247297
person 0.95898676
person 0.9810487
person 0.9493218
person 0.96457547
person 0.9843461
person 0.9508777
person 0.95681995
person 0.94339365
bird 0.90043616
chair 0.95958143
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.312s + 0.035s (eta: 0:00:01)
person 0.90675557
person 0.9865406
person 0.9857692
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 85.867s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1161  283  280 ...  989  990  987]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2210
INFO voc_eval.py: 171: [123 124  69 505  68 236 445 281 127 262 135 125 476 558 375 385  71 563
 535 450 561 556 503  72 565 373  70 546 470  84 131 133 562 267 573  87
 451 458 132 523 567 557  73 578 250  95 455 129  82 378 559 511 454 370
 371 290  76  89 285 136 126 515 548 537 449  90 291 237 242 462  74 555
 461 541 507 431 264 294 468 536 139 101  18 538 137 148 284 477 141 369
 512  42 460 382 240 282 241 534 234 514 238 313 274 376 401 334  78 244
 415 288 569 528 452 543  22 572 297 466 374 456 497 405  21 103  37 146
  24 539 143 510 421 275 506 429 105 509 575 128 260 159 551 453 380  40
 296 570 540 265 160 419 266 341 249 394 192 328 464 194 533  86 302 412
 147 564 201 517   2 432 579 550 465 273 235 576 134 213 402 251 144 243
 320 317 335 430 299 483 142 568 399 409  77 411 560 295 566 287 384  41
 544  19 513 457 255 472 293  81  43 547 289 245  32 309 424 530 145  56
 527 526 571 277 188 326 420 196 268 248 437 138 383 239 368  16 549 574
  96 521 422  91   5  85  88 130 518 104 301 102 552 398 531 140 259 191
 500 271 410 174 308  46  75 486 443  30  98 467 423 498 393 400 577  94
   1 183  20  25 522 261 247 475 504 286 258 372 434 202 482 175 519  92
 396 414  66  50 389 444  79 193 447 331 391 392 195 257  38 329 508 311
   0 357 276 158 189  99 172 272 185 529  64 321 553  34 416 417 459 182
 186 480 163 217 418 425 441 435  29  60 115  80 413   3  26  55 263  61
 323 481 120 184 190 246 298  47  36 179 377 197 520 433 446  17   8 387
 181  33 473  28 337 324 176 305 471 165 304 406 469 283  12 228 386 152
 100   6 218 403 114 306 310 545 300 408 495  15 501 436 162 187 227 346
  65 164 279  62 340 404 532 292 388 397 442 345  63  93 524 367  23 347
  35 351 484  31 111 542  51  27 496 153 307 220  11 121 439 438 327 206
 166 167 170 440 204 200 474  45 342 168 348 448 525 205 109 366 199  53
 407 390   7 353 516 278 226 210  59 161 254   4 478 253 203 112 333  14
 252 428 149  57 379 485 156 209 222 198 330 479 231 208 343 336 356 395
 344 108 338 365 180 381 303  83 312 463  67 122 150 177   9 171 355  44
  52 173 157  13 106 354 339 332  54  58 178 427 214 211 319  10 212 352
 155 169 154 113 502 107 110 256 219 280 314 349  49 322  48 315 316 116
 426 215 325 359 360 207 499 488 117  97 494 225 216 362 233 232 363 224
 489 364 221 491 361 580 490 119 318 358 487 229 269  39 492 493 151 223
 118 230 270 350 554]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3964
INFO voc_eval.py: 171: [ 645 1713 3239 ... 3477 3232 3237]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1950
INFO voc_eval.py: 171: [ 635  666  747 ...  125 2274 2278]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2388
INFO voc_eval.py: 171: [401 365 573 309  12  21 649 357 646 364 282 572 566 402 625 652 312 589
 331 543 373 601 288 666 311 141 477 546 824 655 686 587 322 754 823 320
  18 567 544 314 287 654 615 403 283 327 570 662 736 651  25 837 648 626
 721 358 667 329  22  16 657 664 405 289 700 376 367 513  20 366 550 825
 612 568 672 297 468 325 310  70 826 372 610 490 647 724 630 545 265 404
 285 675 315 497 292 217 834 585 588 324 656 371 512 255 735 829 330  13
 653 313 368 628 599 744 303 172  68 202 301 690 375 616 671 480 828 302
 678  43 609 729 673 580 549 318 491 360 470 175 574 114 487 571 688 710
 693 196 808 132 252 684 758 205 586  15 326 418 692 699 499 554 698 211
 661 176 524 602 668 584 569 709 247 260 328 669 613  19 469 592 213 398
  31 683 109 833 296 146 147 332 424 582 204 216 737 697 291 695 614 795
 257  50 762 793 143 170  17 155 681 162  80 770 611 369 493 166 682 319
 841 551 629 578 792 719 266  44 300 359 691 430 317 591  24 261 127 259
 152 547 400 659 660 387 465 237 740  69 575 676 467 632 198 853 598 596
 323 794 553 796 548 515 212 416 800 713  99 422 743 116 520 414 689 797
 316 361 807 665 442 370 495 605  34 239 581  79 650 321 374 264 791 101
 248 201 845 843 751 243 583 529 295 674 293 670 234 523 284 219  58 552
 167 441 182 113 333  45 496 177 294  81 838 677 687 579 478 633  23 590
 519 759 133 711 714 305 335 617 685 286 528 131 696 125 230  97 742  96
 489 110  14 644  57 663 471 390 112 607 262 595 840 334 456 242 218 576
 680 153 606 488 839  64 577  78 267 399 694 720 594 767  27 195 597  54
 206 355 130 757 397 444 593 115  95 223 830 604  87  36 820 197 298 741
 150 179 199 855 256 772 765 704 263 111 251 618 165   6 482 705 229  53
 608 771 526 492 479 600 780 498 821 603 417 250 156 483 788 525 171 104
 768 276 214 494 634 306 722 725 411 336 801 484 437 712 269 856 184 236
 388  32  42 268 346 215  59 739 679 658 137   1 514 726 299 290 766 241
 396 362 750 108 181 129  52   4 415 117 107 443 118 119 200 728 521 747
  73 148  49  91 848 185  46 748 144  98 193 804 100 787 701 753  86 459
 760 516  35 151 835 806 558 249 183 522 174 627 413 168 235 244 832 527
 363 389 827 224 857 502 145 149 809 755 486 238 412 180 270 253  47 466
 518 769 776 756 541 746 423 730  26  28 749 258 338  92 535 272 233 463
   2 203 752 128 173 154 419  65 822 192 802 464 727 798 458 849 246 850
 274 775 254 225 161  90 731 102 622 427 158 851  77 761 245  93 457  30
 126 738 428 503  29 460 438 273 345 421 420 240 220 847 537 789 517  48
 557 846 157 440 103 745 227 472 715 160  38 635 163  72 178 432 831 445
  75 645 631  85 164 773 142 481 716 538 536 159 121 354 304 383 805 474
 774   0 485 790 271 169   5 540 539  51  89 275 455 452 854 504 476   8
 106  94 723 556 140 717 565   3 619 278 191 378 350 803 763 530 139 380
 379 703 718  40  56 562 542  61 475 439 210  76 451 436 426 643 620 462
 473 561 844 564 505 349 337 138 533 799 348 733 280  39 431 778 532 559
 105  82 347 702 563 534 136 208 506 461 560 221 222 228 393 454 852 425
 842 434 410 435  41 836 782 531 781  62  33 277 353 281 624 638  74 511
 231 640 500  83  71 394 382 381 450 623 352 351 501  37   9 637 706 433
 391 307 621  63 734 279 384 308 226 779 344 189 817  11 392  67  88 816
 764 385 732 453  60 207 507 707 356 339 784 190 708  10 343 232 555  66
 395 377  55 120 342 209 510 448 429 785 408   7 134 449  84 814 642 641
 813 340 818 122 819 815 194 777 341 509 446 124 810 786 783 508 386 811
 407 123 186 188 636 406 409 135 812 639 187 447]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3379
INFO voc_eval.py: 171: [438 171  96  95 172 450  91  81 103 462  99  79  97 441 110 367  80 121
 180 402  26 106  78  93  83 210  45 410 394 415  89 109 261 209 389 249
 163  86  84 442 366 381  46 104  32 398 271 143 264 360 368 443 162  54
 291  92  53 395 197 405 188 362 164 233  87 260  90 482 156 334 118 427
 218  82  44 434 111 201 393 120 222 142  88 320  11 330 288 205 237 252
 353  48 196 258 199  13 117  85 309 335 265 195 281 336 119 465  30 340
 373 331 176 272 151 125 392 208 337 380 397 245 262  57 363 146 400 355
 417 139 251   8 351 212 187 247 206 377 333 319  24 316 379 177 145 244
 419 414 280 411 192 155 408 390 284 123 122 149 437 227 357 282 228 327
  49 225 378 257 433 424 200 463 361 312 255 242 359 295 376  31 168 391
 322 386 352  27 283 416 150 124 131 250 287 202 432 221  16 273 100 140
 422 371 129 356 445 254 211 223 328 226 421 293  52 198  35 147 354 266
   4 329 314 113  41 127 115 173 375 313 135 308 128 464 409 105 141 426
 126 471 481  59 114 292 420  98 215 248 161 358 138 385 403 166 204 224
 178 116 473 132  29 428  33 480  58 311 429  94 277   0 157 148 169 383
 101 399 160  47 270  36 472 232 203 108 388 342 243 275 256 396 102 332
   5 239 230 234 321 207 130   3 412 407  43 235 401 436 154 246 474 456
 343 418  37  68 369 153 238 259  55 317 370 435 213 181 454 323 276 285
  74 425 404  28 467  69 274 144 440 387 470 374 229 263 423 267  60 167
 184 170 324   7 158 475  40 372  10 444 217 476 253 439 133 278 241 112
 326 325 364 341   6 152 269 349 452 315 231  38 286   9 310 107 406 318
 194 338 268 413 457 453 290 446 468 174 165 289  42 384   1 469 303  17
 279 134  34  39 307 339 159 219   2 304 183 382 431 214  61  50 306 297
 466 191  22 430  56  63  75 365 236 189 216 447 301 240 458 455 298  62
  25 220 479  20 347 305 345 459 294 296 185 344 179  14  67 182 302 448
  72  64  19 136 451  23 299 478 300  18 477 175  76 190 186 137 460 461
  15 193  12  66 449  51  73  21  77  65  70  71 348 350 346]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4242
INFO voc_eval.py: 171: [1595  715 1528 ...  100  119  104]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2752
INFO voc_eval.py: 171: [ 29 263  64 108 156 314  65 109  19 118 287  24 315 313 264  66 155 255
 203 125 154 286  26 265 296 232 259 213 110 202 290  74  83 122  95 288
  52 233 248  94  48 133 242 117  50  16 262 137 102 144 115  69 171 173
 192 234 191 177 306 153 295  42  21  20  75 105 206  92  27  63  70 124
  17 219 227  96 174 292  80 116 113  18 121 166 201 254 294 217 243  22
 249 159  55  28 112 165 285  73  56 257 237 194  76  81 205 172 167  54
 241 215 190 130  91 158 291 256  23 216 152  51 235  85  86  82 293 197
 157 289 310 127 119 261 146 246 307  53  77 123 282 312 308 211  25 238
 195 220 150 226 151   5 193 184 145  47 236 224 267  88  68 239 240 161
 103 138  84  87 131 126 164 128 136  49 135 111 258 170 223 132 245 218
 309 114 270 284 140 107  44 311 225 279 222  45 317  14  93 147  10  67
 101 139 301 244 247 134 196  89 252 275  46 251  41 204 297  97 120 106
  90   8 250 183 104   4 214 207 100  43  35   0 230  39 280  15 143  30
  99 268 269 209 221   6 276 281   3  34   1 169 129 260 271 181 277  98
   9 283 316  60 175   2 168 199 231  36   7  38  31  12  59 160 198 162
 212 163 180 210 299  58  71 200 274 272 208 278 302 186 298 178  32 273
  33 305 304 228  62  78  13  57  79 300 148  61 141 185  72 303 266 229
 176 149 179  11 253 189 182  40  37 142 187 188]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0868
INFO voc_eval.py: 171: [5832 2513  698 ... 4487 4349 4350]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3072
INFO voc_eval.py: 171: [329 256 194 442  58 200 282 387 270 253  50 269 762 695 255 258 283 450
 252 412 448 754 641 266  71 330 409 218 726  77 259 608 193 333 195 445
 196 630 154 382 287 180 558  64 753  73 177  55  53 228 136 446 198 254
 278 241 443 226 201 126 447 375 203 606 756 441 273 264 559 368 564  69
 439 457 127 169 758 757 320 440 112  68 452 642 765 419 534 276  21   7
  60 267 219  33  65 759  70 609 403 738 192 560  67 385 651 626 723 456
 454 202 388 444 263 535 701   6 357 199 725 342 637 306 622 262 261 610
 732 148 455 538 761 453 274  49 700 268 515 397 553 716 197 144 289 338
 393 386 471 531 361 216 211 290 644 524 341 426 418 627 175  57 631 650
 727 411 569 275 720 731 632 728 628 260 115  10 770 227 322 514 566 344
 242 141 730 237 281 665 265 537 221 755 729 449 332  40 381 383 420 612
 349 181 125 410 660 620 724 331 176 421 561 438 146 184  52 214 611 309
 240 658 183  15 763 413 565 567 614 667 206 555 292  63  84 462 703 733
 208 336 562 643 645 722 541 556 288 428 696 179 173 150 284 217 719 124
   9 129 739 629  48 321 735 607 734 416 772 516 389  79 185  29 423 415
 105 234 314 648 257 347 230 494 233  30 408 345 178 384 417 132 128 740
 151 717 764 367 744 619 424  46 475 130 768 172 182 451 220 736 715 749
 427 662 530 517 505 766 244 243 422 414 155 162 572 557 142 313 360 750
 391 706 286  66 647 705  75 711 633 521 205 690  22 335  47 316 621 334
 271 224 158 312 174 714 364 323 311 399 279 337 539 425 358 616 678 743
 527 235 380 707 570 554 718 495 784 285 134 682 143 782 394 656  99 210
 277 168 106 104 280 223 156 272 507 109 680   8 369 536 396 133 110 584
 339 204 238  93 540 737 503 769 661 327 568 698 760 340  17 783 315 480
  31 767 401 497  27 157 293 519 363 548 118 433 291 249 310 239 663 103
 741 563 326 694 708 482 400  23  87 576  14 114 721 618 623 139 137 625
  13 649 504 113 159 318 752 498 458  42 404 677 390 379  45 571  24 780
 578 215 481 747 712 636 533 664 353  56 542 745 135 510 236 170 247 742
 639 225  34 525  54 774 100 152 532 615 751 207 773 785 490 746 496 187
 748 654 392 371 398  91  89 229   1 635 640 592 529 488 402 108 319 652
 693 212 395 589 317 461 518 153 479 679  26 463 101 687 624 131 684 604
 579 771 222  32  25  20 509  90 328 601 362 191 307 209  94  36 372  59
 107 138 251 522 688 102 324 119 588 111 704 213 232 140  11  19 171 547
 692  35 659 248  78 603 508 775 600 325 359  28 166 246 487  82  86 585
 485 520  62 670 666 674 160 123 689 231 586 165 681 365   3 638 676   0
 526 478 602 436 591 476  76 366 431 596 245 376 655 528 374  72 545 161
 250 145  51 523 434 686 147 668 675 370  12  38  92 577  41 697 189  37
 646 432 590 781 699  18 634 435 709 552 190 691 492 491 167 483 186 116
  80 493 685 350  81  83 121 149 459 657  16 500 672 593 594  85  61 164
 580 163 120  74 484 595 489 597 673 502 501 506 355  95 352 671 477  88
 551 613 377 653 303 486 308 587 605 430 669 469 598  39  43  44 188 467
 429  98 710 543 549 546 544 117 499 405 511 305 378 122 464 407 550 599
 346 297 298 406 465 295 702   4 474 354 304   2   5 437 617 460 468 470
 300 512 473 713 779 373 683 472 348 294 351 302 356 466 296 583 513 776
 778 301 299 573 581  96 343  97 777 582 575 574]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2079
INFO voc_eval.py: 171: [2886 1278 1750 ... 2790 2791 2304]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3280
INFO voc_eval.py: 171: [343 410 346 840 445 501 353 375  50 481 464 448 443 549 146 349 620 502
 408  64 442 161 769 541 144  46 278 506 149 159  51 512 204 522  42 376
 842 153 274 355 837 731 237 792  49 413 128 783 461 409  66 527  45 205
 151 765 793 147 764 475 268 142 830 164 516  41 344 466 781 845  63 810
  52 435 374 834 469 220  83 371 819 354 621 798  54 753 269 727 246 520
 543 766 640 163 838 224 547 162  69 550 578 225 463 483 767 236 534 505
 473  65 659 485 416 242 247 851 281 421 417 849 356 437 152 815 596 345
 712 260 239 462 145  55 841 822 165 638 668 524 199 223 470 449 839 207
 415  44 126 414 439 238 434 474 546  53 667 214 611  86 295 150 200 770
 603 277  62 519 335 614 772 251 653 324 657 814 656 813 728 817 350 696
 700  32 194 586 221 612  68 799 768 802 228 698 132 158 779 723 832 429
  70 450 788 752 156 601 280 548 440 258 645 754 263 217 513 761 441 379
 197 431 595 230 593 141 382 739 229 855 412 347 545 130 157 831 380 215
 411 433 206 208 226 148 419 701 631 143 806  47 795 777 372 424 166 131
 196 669 800 436 318 755 136 639 600 796 734 807 553 843 341 438 774  60
 627 270 847 250 738 127 272 425 160 231 617  57 829 219 584 422 334 293
 637 155 307 311 619 116 444 348 641 265 678 511 805 691 110 740 351 112
 818 261 794 479 662  67 198  43 571 287  16 858 266 447 154 775 844 488
 544 129 542 195 730 288  87 279 133 292 859 480 244 572 613 332 592 689
 670 243  61 467  92 282 518 283 715 302 676 816 762  31  90 803 123 135
 780 352 643 383 625  85 778 296 771  94 782 801 826 233 729  58 716 234
 184 271 241 420 276  59 597 122 718 245 139 273 665  25 825 763 427 570
 291 326 618 384 773 117  22 604 222 853 735 824 615 275  19 248 854 476
  89 675 674 579 381 567 671 232 717 607 850  15 359 664 661 301 111 182
 555 606 286 590 212 821 113 114 120 216  56 647 446 566 300  13 218 102
 428 616 605 776 552 249 340 459 529 252 364 373  33 724 695 515 528 741
 697 704 809 478  29 538 304 608 389 666 569 833 306 692 313  88 745 551
 599 646 856 808 253 103 677   2 312 342   5 418 267 262 720 721 554 556
 812 680 811 290 186 423 835 213 533 118 259 325 857 686 820 430 804 294
 725 672  91 852 823 836 642 477 719 235 846  84 517  93 682 679 401 630
 407 655 187 333 644  35 426 848  20 568 722 321 124 285 685 687 714 264
 633 508 628 105 370 327 361 134 573 119 629 358 188 588  71 732 337   4
 319 797 658 583 400 457 598 652 673 299 736  26 589 591  82 525 581   6
 317  10 602 860 298 257 181 175 726  95 336 315 360 507  48 634  28 240
 303   3 309 539 514 189 503 532 561 256  79 610 392 757   1 362 322 785
 115 289 254 330 484 681 594 789 316 297 305 626  80 486 106 211 492 609
 560 284 742 733 308 314 743 451 323 535 363 378 368  40 178  96 454   0
 622  14 121 786  77 632  36  39  27  23 140 489 582 137 787 395 168 465
 663 174 636 562 703 365 536 558 707 100  99 540 125 650 537 749 432 453
 339 191 203 494 651 472 452  18 791 747 557 405 310 635  34 138 482 167
 367 169  24 209 183 255  78 456 497 521 180 683 784 399 504 496 403 737
 402  74 530 468 510  97 366 404 746 471 104 176  98 357 491 192 790 490
 648 706  75 708  21 576 487 654  30 574  73 587 455 649 580 177  81 101
 577 531 320 202 460 585 185 559 179 744 694 387  76 193 493  72 699 377
 108 210  17 750 509 702 690 660 684 575 751 190  37  38 406 201 748 394
 563 109 458 565   7 328 693 523 495 388 526 623 498 713 369  12 107 758
 385 564 760 709 624 329 338 711  11 227 705   9 173 386 710 331   8 172
 170 171 398 759 499 500 756 396 393 397 827 391 390 828 688]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2092
INFO voc_eval.py: 171: [132 943 433 ... 564 766 767]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3072
INFO voc_eval.py: 171: [ 46 270 257 142 146  25  45 151  47  50 143 278 118  72  33 160  68  30
 264 144  78  59  55 188  56 169 116 265 259  73  15  52 162 245 197 203
 131 267 173 266 193  35 238  32 163 252 280  31 241 258  16  29  77 261
 275 147 247 148 117 150 281 279 156   6 154 190  51  84  61 167 213 115
  49  21  42 196  83 273  62 145 183 114 223  60  76 100  85 159 141 195
 202 262 240  88 109 263 282  19  34  10  98 229  66 239 210  22 216 260
 186 256 251 152  65  14 127  53 246 168  28  67  80  57  18 242 135 200
  70  79 113 164 166 149  96 126  37 243  24 184 228 237   4  23 218  27
   1  58 155  94 232 268   8  20  38 111 153  99 132 158  64 250 199  41
 157   5  75   9  71  91  12  17 269 139 219 106  95 138 215 205   0 276
  81 234 104  36 187  93 226 208 136 201 217 248  26 178 249 253 103  89
  69 185 134 165  97 110 140  86   3 212 180  48 191 102  11 174  13 101
 120 277  87 107 108 274  39 225 207 171 122   7  40 124  82  92 181 175
  44 222 177  54 233 231 112 172  74 194  90 105 125 198 176 170 182 179
 209 214 211 254 235 204 129 271 206 189 119   2  43 220 221 161 192 128
 227 123 272 230  63 224 244 130 121 137 236 255 133]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5020
INFO voc_eval.py: 171: [ 8613 14077  1069 ... 10947   783 10949]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5386
INFO voc_eval.py: 171: [2513  255  708 ... 2701 2712 1828]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4127
INFO voc_eval.py: 171: [ 66 309 536  34 331 545  67  56 318 129  40 539  50 540 328 338  44 310
 313 615 537 151  46  35 346 480 215 132 326 342 550 541 249 307 154 339
 334 551 544 123 405  37 135 646 278 411 637 301 343 542  71 142 617 363
 421 141  68 319 538 143 159 134 152 417 221 312 220 626 487 362 625  60
 219 492 563 621 263 317 407 314 623 155 332  52 121 627 611 559 481 214
 630 149 463 305 640 137 247 349 410 250 619 160 497 564 344  39 620 526
 213 553 622 226 414 333 228 161 648 128 324 491 127 413  45  90 500 136
 514 316 163 131 191 642  43 446 614 365 311 490 306 632 253  65 283 364
 327 262 229 130 644 248  62 162 308  95 217 495 224 649 368  91 574 345
 420 279 675 647 304 639 408 347 672 122 227 633 148 323 325 506 340 641
 139 335 140 479 635 560 568 406 251 418 157 302 133 225 239 565 432 507
 138 125 303 315 504 322 146 260 569 618 409 273 169 612 562 509 336 523
 608 638 258 320 634 126 277 404 337 629 195 673 216 636 259 321 361 683
 147 628 671 329 330 549 124 529 412 341  77 554  42 484 609 643 218 631
  49 298 493 352 579 196 482 597 546 153 261  15 101 223 449 462 102 502
 567 264 485 645 288 498 465  64 156 624 198   7 351 192 616 505 375 610
  70 441  55 561  28 197  36 189 494 464  25 676 109  38 267 376 150 348
 520 265 527 543 552 576 120 613 202 240 377 166 183 222 255 119 252 651
 354 190  94 489 350 193 353 294 237 478 367   0  93 164 177 256  89 194
  33 517 510 518  24  17  92 185 685 680  72 503 254 415 181  47 296 391
  96 382  23 357 650 158 674 118 374 257 466  53 516 440 266 555 241 170
 437 519 427 652  87 556 174 655 280  14 477 113 522   4 681 369 469 572
 508  48 443 295 106 557 431 434 108 209 426   1 179 244 448 558  78  61
  31  21 112 571 444  76 178 104  83 596  80 165 419 207 511 460 242  26
 677 293 603 682 486  16 461 433 548 601   2 182 287 204 474  18 512 370
 436 272  58 684 458 577 100 521 205 653 513 686 422 276 269 398 580 606
 208 475 488 598 589 547 678 575 402 245   9   8 206 105 111  57 566  19
 524 602 235 299 447 173 679  69 395 530  22 110  32  41 476 499 286 599
  74 515  75 442 271 107  20 468 573 578 483 275 501 282 274 654 180  79
  51 470  73 115 114 416 525 366 394 300 116   3 268 203 423 528 371 243
 438 281 439 496 600 595 285  88  54 428 390 657 429 246 445  27  29  59
 584  63 472 430 176 383 356  97   5 188  30 211 270 284 297 187  85 358
 359 425 117 372 594 401 199 396 212 424 661 591 355  13  86 435 587 582
  81 607 534 380 660  98 586 397  82 172 403 583 581  84 473 200 585   6
  99 570 471 231  10 590 532 360 658 175 210  11 592 663  12 670 386 588
 232 378 186 384 230 533 604 450 167 381 593 236 201 233 393 168 289 388
 662 171 238 605 184 234 454 292 659 392 467 145 531 669 379 385 389 667
 387 399 103 666 459 144 453 290 656 400 291 535 451 455 665 668 457 664
 373 456 452]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3156
INFO voc_eval.py: 171: [ 43   3  47 121 190 280 184  48 126 338  50   6 193 188 236  52 191 158
  13 161 334 194 159 156  60  42 282 152 182 187 340 233 127 279 143 223
 186 157 277 237 231 276 101  80 128 246 151 100  78 337  49 252 281  58
 124   5 104  59 153 163 257  10 162 328 175  23 232 185 147 120 308  57
 129 213 273 247 135  44 311 176 189 107 183  75 245 326  11  46 235 239
 234 166  72 216   4  99 160 327 272 103 132 192 323 119  96   8  12 254
 321  51 251 106 336  89 203 269 264 255  25 250 145 130  14 253 102 122
  97  45 332  73 261 209 256 289 212 154 238 105 125 339  15  91 170 309
  83 331  63 320 225 244  98  90  74 329  61 265 200  84  26 263 248 343
 114 196 325  27 270 341 117 314 149  64 221 228 294 230 295 249  79 134
 123   7 330 113  85   9 167 195 118 285 211  82 260 322 313 241 150 220
 307 271 116 259  76 131 305  77  95 315  55 155 324 299 335 224 133  33
 303 317 333  39 146 274  62 226 242 310 266 218 278 267  65  30 262 298
  24 342 201 217 171  88 112 214  28 297 290 219 177 240 205 174 316 110
 222  81 144 148 296 227  31  87  56  53 198  54 141  32   1  22 284 136
 258   2 306 291   0 268 197 229 210 300 202 169  86 208 283  18 109  29
 139 207 115 304 199 301 108 168 293  41 204 243 206 137 111 179 302 286
  35  37 164 138 178  21 172  19  20 173  17 319 275 142  16 180  71  36
 140  40 292 288 181  69 165 287 312  34 318  70  67  38  66  68  92 215
  94  93]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1308
INFO voc_eval.py: 171: [382 641 636 115 325  80 207 396 293 121  76 404 168  78 898 643 639 294
 655 327 324  83 181 117 398 320 807 125  89 212  92 210 295 804 505  81
 208 536 637 389 383 188 642 453 656 452 300  97 206 119 455 116 326 187
 197 906 912  84 646 900 306 342 296 176 813 837 510 711 464  86 541 523
 847 388 883 856 909 403 165 216 135 134 842 218 910 640 390 526 334 544
 299  43  63 871 456 488 329 434 852  98 112 171 732 385  18 217 749 136
 195 315 330 312 681 328 192 205 913 884 712 662 903 457 811 838 805 105
 196 911 508 173 166 261 668 530 169 280 308 840 873 439 182 220 443 733
 438 914  45 760  93 812 304  19 230 509 824  70 399 623 167 199 885 901
 553 540 332 756 881 525 721 832 841 178 894 744 209 185 310 164 846 172
 714 918  33 739 899 194 552  67  44 890  82 128  65 214 462 244 902 458
 242  74 101 843 213 511 791 487 448 788 821 249 454 227 461 834 710 319
 543 875  58  85 184 546 808 222 263 874 221   2 451 395  53 638 717 375
 707 855 384 720  91 338 905 179  94 180 713  16 264 259 703 499 317 298
 690 686 527 400 764  59 579 602 507 893  17 177 930 882 437 158 401 198
 331 872 483 924 857 318 590 204 408 466 133 303 323 916  99 789 433 109
 759 378 459 440 514 741 356 877 778  52 103 144 485 402 170 820  72 679
 518 629 393 234 465 915 376 892 233 607 494 827 467  32  38 262 236 908
  41 175 350 880 689 200 929 435  73 277 836   5 290 682 360 506 542 765
 251 444 886  95 154 825 143 917 870 746  35  71  42 854 254 432 359 113
 609 693 867 529 273 463 687 561 524 806 100 907 241 122 211 664 469 616
 174 111 301 380   7 548 124 157 123 830 186  57 228 574 519 620 818 322
 785 250 531  75 828 863 252 247 648 162 163 477 895 762  96 545 191 688
 554 861 730 496 923 535 445   8 750 835  46 201 358 748 137 606 425 896
 831 572 183 215 888 839 891 634 357 735 110  55 845 266 333 691  37 904
 600 260 833 489  87 256 307 311 239 238 237 853 265 742 624 468 528 309
 219 226 564 698 772 761 161 314 405 460 879 406 729 876 515 817 684  56
 436 442 202 860 878 335 702  36  40 897 576 614 933 274 190 603 394 678
 441 341 473 781   0 931 731 102 153 589 651 566 851  61  88  39 229 605
 622 337 601 604 520 106 763 556 193 632 796 709 351 386 423 621  34  13
 718 725 734 379  60  48 547 397 283  79 801 391 610 795 752 683 340 288
 281 829 138 189 889 567 114 497 501 431 932 349 743 578 130 626 493 577
 575 680 362 776 692 560 297 517 775 844 537 685 569 240 316 724 377 417
 269 235 155 751 282 243 777 661   6 797 257 814 859 353 647 826 792 424
 587  62   4 790  54 798 794 593 862 275 779 715 302 736 279 708 354 555
 246 613 608 478 586 747 203 580 348 793  31 705 363  66 422 716 450 446
 723 245 321 850 286 757 479 381 612 753 635 848  30  47 677 108 276 392
 500 313 107 426 387 816 581 676 278 701 345 704 611 774 887 369 697 484
 104 150 270 490 284 559  21  77 627  90  68 755 573 513 617 248 550  29
 919 476  15  20  69 305 815 368 571 271 407 787   9 849 551 419 592 344
 758 253 516 584 347 565 802 823 371 474 447 926 231  22  14 532 799 289
 726 588 754 287 582 654 285   3 471 232 570 558 258 255 420 557 670 706
 568 782 868 492 361 783 449  64 727 745 367 615 502 625 786 737 267 809
 503 925 740 491 738 819 504 470 533 159 810 595 699  51 142 414  10 549
 475 268 413 728 521 539 700 412 472 343 803 495 822 583 482 152 920 429
  49  12 370 486 498   1 719 131 598 594 291 671 374 373  11 522 591 428
 722 619 784 800 675 780 512 773 151 858 538 663 869 657 481 597 480 355
 534 292 372 644 696 427 630 585 430 364 596 421 336 366 410 628 771 645
 156 365  28 416 132 160 415  50 147 339 770 927 633 140 418 864 649 409
 922 346 563 562 118 658 223 660 928 631  27 618 599 865 139 145 673 767
 766  23 695 411 768  26 866 146 225 769 352 672 149 127 694 665 224 272
 129 120 652 653  25 141 126 148 650 921  24 666 674 659 667 669]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2996
INFO voc_eval.py: 171: [420 251 594 174 433 297 136 441  24 468  37   1 477 497 255 320 350 474
 479 322  39 394 437 253 348 126 418 249 349 469 124 575 309 298 351 549
 610 264  57  68 421 254 471 535 323 107  71 129 230  32 438 180 595 444
   3 520  17 327 476 182 430 339 316 303 347  43 319 598 583 618 154 628
 226 357 102 613 367 375 370 261 606 440 307 150 443 536 219 109 110 473
 205 321  50 137 257 365  19 208  73  70 485 435 222  29 301 600 143 464
   8 305 204 160 555 507 381 472 533 363 324 377 232 273 484 616 103 462
 360 358 393 331   9 366  40 392 245 120  58 633 162 186  16 161  25 622
 213  56  46 483 481 601 178  44 128  26 592 300  84 406 427 514 207 313
 425 299 259   6 475 576 287 505 306 344 542 304  41 314  53  72  30 223
 502 211 337 480 372 239 119 101 179 229 568 470 519  48 145 558 328 559
 311 212 215 209 432 478 597 623 426 378 317 397 214  90 172  38 265 175
 620 125 632   2 131 189 158 117 612 400 192 173 210 383 228 551 121   5
 579 578 115  75 308  93 356 434  89 354 515 364 106 547 184 582 405 334
 482  45  11 271 599 548 495 626  91 302 395  18 123 611 454 562 281 552
 176 292 414 621 398 341  23 560 452 614 206 112 439 545 100 310  12 130
  55 171 530 516  87  33 629 284 499 318 199 399 153 523 159 540 538 500
 553 617 225 561 537 235 609 412 276 584 361 278  98 359 488 534 280 285
 526 624 362 596 496 630 625 333 216 111 114 608  94 453 571 191 144 501
 627  95 506 385 238 615 498 567 148 218 408 379 203 504 279 332 518  86
 258 315  54 147 619 521 217 312  82 384  78 369  28 220 282 155 541  92
 368  63 543 181  42 138 274 450 539 116 565 141 587 353 371 517 456 585
  61 419 503 544 127 532  77 275 104  76 118 447 636 376 256  79 286 268
 451 511 570 374 428 380 224  49  36 187 166 352 382  34 436 422 566 355
  81 445 373  60 242 329 133 201 325 277   4  31  52 260 631 455 635 108
 589 269 157  51 338 590 449 389 283 524 336 149 142 591 330 429  15 463
 288  35 146 243 431  80 105  10  59 291 272 574  69 466 340 165 396 290
 151 550 122  64 486 634 227   7 156 113 139 487 410 586 546 607 531 564
 557  22 132 423 529 140 198 190 460 295 185 236 446 424 183 528 391 152
 240 197  27  85 135 241 404 556 407 573 134 510 527 326 409  83 294 442
 403  88 234  96 221 262 168 167 296 563 200 448  74  67 402 250 177  97
 522 293 342 387 164 231 237 343 335 263 411 252 554  66 170 388 525 581
 163 390 465 345 512 569 588 188 401 386 572 413 270  62 233 508  21 244
  99  65 459   0 509 289 169 513 346 580 490 605 604  20 248 458 602 247
 457 603 267 493  47 461 415 489 194 266 593 202 196 416 494 246 577 491
  13 417 193 195 467 492  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3265
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3030
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.221
INFO cross_voc_dataset_evaluator.py: 134: 0.396
INFO cross_voc_dataset_evaluator.py: 134: 0.195
INFO cross_voc_dataset_evaluator.py: 134: 0.239
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.424
INFO cross_voc_dataset_evaluator.py: 134: 0.275
INFO cross_voc_dataset_evaluator.py: 134: 0.087
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.209
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.502
INFO cross_voc_dataset_evaluator.py: 134: 0.539
INFO cross_voc_dataset_evaluator.py: 134: 0.413
INFO cross_voc_dataset_evaluator.py: 134: 0.316
INFO cross_voc_dataset_evaluator.py: 134: 0.131
INFO cross_voc_dataset_evaluator.py: 134: 0.300
INFO cross_voc_dataset_evaluator.py: 134: 0.326
INFO cross_voc_dataset_evaluator.py: 135: 0.303
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 8499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.446s + 0.037s (eta: 0:00:59)
person 0.9705384
person 0.96386296
person 0.99242157
person 0.9609657
person 0.97572994
person 0.9396607
person 0.9826554
person 0.9457752
person 0.9117298
person 0.98164386
person 0.9387791
bottle 0.9217651
bottle 0.9193336
bird 0.95102316
person 0.9825433
person 0.91825587
person 0.944913
person 0.9028407
person 0.9047744
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.321s + 0.037s (eta: 0:00:40)
person 0.9448845
person 0.92093015
person 0.96097815
person 0.9486003
person 0.98059034
person 0.94812673
person 0.967453
person 0.98190683
person 0.93973315
person 0.9216636
person 0.92578834
person 0.9317705
person 0.90626067
person 0.9229454
person 0.9209978
person 0.9271069
person 0.9757657
person 0.9135688
person 0.9800046
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.325s + 0.039s (eta: 0:00:37)
person 0.92886657
person 0.9120708
person 0.9568382
chair 0.9381569
person 0.97247833
person 0.9160783
person 0.9576714
person 0.9668419
chair 0.96560174
chair 0.93704295
chair 0.9149346
person 0.90493625
person 0.9791124
person 0.9879408
person 0.9275602
person 0.97717094
person 0.99158365
person 0.98132807
person 0.95202625
person 0.91066676
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.326s + 0.038s (eta: 0:00:34)
bird 0.94001895
person 0.92778796
person 0.92125213
person 0.9938326
person 0.95720047
person 0.9743022
person 0.9796943
person 0.9073469
person 0.9596048
person 0.966055
person 0.9039092
person 0.9189666
person 0.969543
person 0.9409854
person 0.95840484
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.325s + 0.038s (eta: 0:00:30)
diningtable 0.90288377
diningtable 0.93860936
chair 0.9639021
chair 0.96594065
chair 0.93813384
chair 0.9576147
pottedplant 0.918079
pottedplant 0.91432536
pottedplant 0.9110836
pottedplant 0.9044181
pottedplant 0.9436921
pottedplant 0.9384197
pottedplant 0.9284406
person 0.9657858
person 0.9828924
person 0.9203013
person 0.97983414
person 0.9489893
person 0.9891043
person 0.9001828
person 0.9157374
person 0.903957
person 0.91194946
person 0.9222854
person 0.9226084
person 0.97401977
person 0.9551214
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.324s + 0.036s (eta: 0:00:26)
person 0.9580969
person 0.92393327
person 0.90722406
person 0.91960406
person 0.9479958
person 0.91505694
person 0.95357287
person 0.93688136
person 0.9298236
bird 0.9227551
person 0.93763846
person 0.9876649
person 0.91697323
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.321s + 0.035s (eta: 0:00:22)
pottedplant 0.90277946
person 0.96244013
person 0.93396986
person 0.91302717
person 0.9538436
person 0.91905385
person 0.95867187
diningtable 0.91109705
chair 0.9826147
diningtable 0.9620497
chair 0.91612077
chair 0.9486207
chair 0.9807858
chair 0.9264247
bird 0.99303234
person 0.9027711
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.319s + 0.035s (eta: 0:00:19)
car 0.928772
car 0.9424502
car 0.91534424
person 0.9884107
person 0.9005119
person 0.97413856
person 0.92987967
horse 0.9737631
horse 0.9135286
horse 0.9209364
person 0.93800765
person 0.9533705
person 0.9681493
person 0.91863257
person 0.94312793
car 0.9459147
car 0.96403784
bicycle 0.9601543
bicycle 0.93105656
person 0.9369448
person 0.91798955
person 0.9735671
chair 0.9186971
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.319s + 0.035s (eta: 0:00:15)
person 0.9742065
person 0.9591362
person 0.9738159
person 0.9177536
person 0.91330206
person 0.90177786
person 0.9885993
person 0.9863685
person 0.9435671
person 0.9711619
person 0.91548884
person 0.9639674
person 0.9911788
person 0.9088942
person 0.9665845
person 0.9118954
person 0.9878945
person 0.9798924
pottedplant 0.91158575
person 0.98986113
bus 0.9127549
person 0.96127594
bus 0.9066407
person 0.94155484
person 0.9547815
chair 0.9469815
person 0.9054838
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.320s + 0.035s (eta: 0:00:12)
motorbike 0.96323794
person 0.9749077
person 0.9105709
person 0.9158632
person 0.95705205
person 0.9676779
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.319s + 0.034s (eta: 0:00:08)
person 0.90682536
boat 0.98204136
boat 0.94830483
boat 0.91853005
person 0.99038166
person 0.937523
person 0.9854907
person 0.9182834
person 0.9178614
person 0.98908746
person 0.9749102
person 0.9107448
person 0.9752804
person 0.9767603
person 0.91393745
person 0.9484394
car 0.95061046
person 0.94066536
person 0.94724774
person 0.9598284
person 0.98681647
person 0.908188
person 0.9212701
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.323s + 0.035s (eta: 0:00:05)
diningtable 0.9598972
chair 0.9249607
chair 0.9738506
chair 0.98070425
chair 0.9155405
chair 0.9812036
chair 0.901908
chair 0.9367992
chair 0.9599796
person 0.9428037
bird 0.9240737
bird 0.91567093
boat 0.9198048
person 0.9621232
person 0.9294042
person 0.9888321
person 0.91491264
person 0.90370214
person 0.9136446
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.322s + 0.034s (eta: 0:00:01)
person 0.9834975
person 0.96405864
person 0.95642364
person 0.9554805
person 0.9490536
person 0.96620184
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.441s + 0.030s (eta: 0:00:58)
person 0.98211735
person 0.91934997
person 0.97897846
person 0.9771368
person 0.9142384
bicycle 0.97241443
person 0.93212867
person 0.9152087
bicycle 0.97478026
person 0.9361033
bird 0.9436734
bird 0.9008744
pottedplant 0.94305295
person 0.96844584
person 0.92897254
person 0.9022833
person 0.9610669
person 0.9661402
person 0.9869141
person 0.92683476
person 0.96606123
person 0.9527346
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.322s + 0.033s (eta: 0:00:40)
person 0.91266334
person 0.90643996
person 0.9640217
person 0.91787046
person 0.97401184
person 0.9429599
person 0.9359662
person 0.9808852
person 0.9644078
chair 0.9252506
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.343s + 0.034s (eta: 0:00:39)
car 0.915868
car 0.95089895
person 0.9470949
person 0.96156275
person 0.98173493
person 0.9248947
person 0.94917756
person 0.965002
car 0.98236775
cow 0.9234296
person 0.9152977
person 0.95780754
person 0.92335856
person 0.9133421
person 0.98118365
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.334s + 0.035s (eta: 0:00:34)
person 0.9783201
chair 0.9347466
person 0.93033135
person 0.92526835
person 0.9514833
person 0.953615
person 0.93312544
person 0.918094
person 0.9403945
person 0.98069257
person 0.98698723
person 0.9771792
person 0.96913904
person 0.977507
person 0.90546423
bird 0.90454304
bird 0.9908251
person 0.92208385
bird 0.95170027
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.334s + 0.035s (eta: 0:00:30)
person 0.9463142
person 0.97121936
person 0.9827152
person 0.9219532
person 0.948371
car 0.95061046
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.336s + 0.035s (eta: 0:00:27)
bird 0.9100573
bird 0.95957464
bird 0.91359794
person 0.91491586
person 0.9444009
person 0.9187844
person 0.9150883
person 0.9884242
diningtable 0.94870263
person 0.97238106
diningtable 0.9790647
person 0.93976295
person 0.9361045
chair 0.94307595
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.340s + 0.034s (eta: 0:00:23)
person 0.9160458
person 0.9075216
person 0.93130887
person 0.94937754
person 0.9859139
bus 0.94642335
dog 0.9525153
diningtable 0.9738817
diningtable 0.9241479
chair 0.91265845
chair 0.9850817
chair 0.9112277
chair 0.9712327
chair 0.9359508
pottedplant 0.90184927
person 0.90427506
person 0.90116054
person 0.9765326
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.338s + 0.034s (eta: 0:00:20)
diningtable 0.92109495
chair 0.91164976
person 0.94828665
person 0.9666963
person 0.9560567
person 0.93711066
person 0.94515055
bird 0.95428413
person 0.92624557
person 0.962325
person 0.9641535
person 0.93240106
person 0.9803413
person 0.9100523
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.334s + 0.034s (eta: 0:00:16)
person 0.962868
person 0.9004491
person 0.9639399
person 0.9175207
person 0.90332896
train 0.9583551
person 0.9571241
person 0.96216893
person 0.94631636
person 0.9402865
person 0.9151596
person 0.9295557
person 0.9317522
person 0.9052668
person 0.9564102
person 0.92669725
person 0.99258304
person 0.9740216
person 0.9598112
person 0.9014685
person 0.91764104
person 0.9274678
person 0.903292
person 0.9311073
tvmonitor 0.91768456
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.334s + 0.035s (eta: 0:00:12)
person 0.9642411
person 0.90663385
person 0.94479257
horse 0.9601165
person 0.9668072
person 0.9309717
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.335s + 0.035s (eta: 0:00:08)
person 0.9752466
person 0.96522266
person 0.92210966
person 0.9196948
person 0.95249933
person 0.92832565
person 0.93635315
person 0.903337
person 0.915964
person 0.98677295
person 0.9466805
chair 0.9725457
person 0.95620143
person 0.90716195
person 0.9441508
person 0.9586551
person 0.9009397
person 0.90564996
person 0.9188032
car 0.9449942
car 0.91827315
car 0.95221233
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.332s + 0.035s (eta: 0:00:05)
person 0.96744573
person 0.94403267
person 0.9626833
person 0.9489402
person 0.97434276
person 0.9609918
person 0.9222925
person 0.9003564
person 0.90753156
person 0.9287082
person 0.900055
person 0.98819226
person 0.9814275
person 0.9163799
person 0.9783788
person 0.92821103
person 0.9383608
diningtable 0.933679
person 0.9013165
person 0.9179257
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.325s + 0.034s (eta: 0:00:01)
person 0.93830574
person 0.90828943
diningtable 0.9102299
person 0.9709439
person 0.94080484
person 0.9872804
person 0.9654188
person 0.9581964
person 0.95967597
person 0.9085302
person 0.91106945
person 0.9180565
person 0.94090325
person 0.96305084
person 0.98079395
person 0.9111377
person 0.94942975
person 0.9564507
person 0.95614934
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.367s + 0.029s (eta: 0:00:49)
diningtable 0.90209943
diningtable 0.9280482
person 0.9600596
person 0.97616553
diningtable 0.9736414
person 0.9371058
person 0.91848147
person 0.9083631
bottle 0.9067054
person 0.9781296
pottedplant 0.9175015
diningtable 0.92871344
person 0.96596164
diningtable 0.90696186
chair 0.9493742
chair 0.9659988
chair 0.92287266
person 0.931504
chair 0.90139526
person 0.90739113
person 0.932994
person 0.99565315
person 0.98082
person 0.9346174
person 0.96626866
person 0.96540403
person 0.9048044
person 0.97075754
person 0.9601191
person 0.939013
person 0.9409021
person 0.9655416
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.320s + 0.035s (eta: 0:00:40)
person 0.9473885
person 0.98594034
person 0.9335813
person 0.9708827
bottle 0.9368469
person 0.9371006
person 0.9441196
person 0.95381504
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.312s + 0.035s (eta: 0:00:36)
chair 0.9316802
bottle 0.9092841
boat 0.9010601
person 0.97292286
person 0.9399065
person 0.9337314
person 0.9326566
person 0.9325929
person 0.9685536
person 0.9443104
person 0.9013822
bottle 0.9081057
bottle 0.9535194
person 0.9821846
person 0.9526645
person 0.94930536
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.323s + 0.035s (eta: 0:00:33)
person 0.9668589
person 0.92171335
bird 0.91692436
person 0.9537676
person 0.9067943
person 0.9680873
person 0.9706187
person 0.959935
person 0.90368295
person 0.9498975
person 0.90824646
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.323s + 0.036s (eta: 0:00:30)
person 0.96958524
person 0.96919066
person 0.9532804
person 0.90694535
person 0.92774594
person 0.9707159
person 0.9206742
person 0.94100726
person 0.9143803
person 0.9199304
person 0.9370218
person 0.9699229
person 0.90128565
person 0.9535672
person 0.924761
person 0.9055261
bottle 0.9536471
chair 0.931146
boat 0.90411496
person 0.973317
person 0.9552886
person 0.9051763
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.319s + 0.036s (eta: 0:00:26)
car 0.9808218
car 0.9349894
person 0.92625165
person 0.97739476
person 0.9098853
person 0.9113444
person 0.9206592
person 0.9352916
person 0.9650129
person 0.92316854
person 0.9180187
person 0.9575185
person 0.9184033
pottedplant 0.91409093
pottedplant 0.93139976
car 0.91995716
car 0.9850766
car 0.93774277
person 0.9325705
person 0.9877927
person 0.95589167
person 0.9279854
person 0.9368067
person 0.98598933
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.318s + 0.037s (eta: 0:00:22)
person 0.97979456
car 0.9601144
person 0.95501095
person 0.92615336
tvmonitor 0.9263498
person 0.9058905
person 0.9340231
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.320s + 0.037s (eta: 0:00:19)
chair 0.9043306
person 0.97087765
person 0.9465089
train 0.957085
train 0.95813215
bird 0.91126746
bird 0.98305076
person 0.9224289
person 0.9225074
bicycle 0.9205568
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.315s + 0.036s (eta: 0:00:15)
person 0.9699435
person 0.9522021
person 0.9289465
person 0.92948586
person 0.97817403
person 0.98688173
person 0.95887524
person 0.9737681
person 0.98916197
person 0.9554292
person 0.965161
car 0.95251596
person 0.9690489
person 0.9080301
person 0.950287
person 0.97717744
person 0.9344813
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.318s + 0.036s (eta: 0:00:12)
person 0.9157455
person 0.9456372
person 0.924168
chair 0.9751086
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.319s + 0.036s (eta: 0:00:08)
person 0.9814245
person 0.9416535
person 0.9298643
person 0.9641248
person 0.96219945
person 0.90260357
person 0.92643005
person 0.90740657
person 0.94079196
person 0.9213159
person 0.9789604
person 0.9070347
person 0.9088241
boat 0.90315294
person 0.967421
person 0.9254256
person 0.9478892
person 0.91274035
person 0.9581593
person 0.9041276
person 0.9267445
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.321s + 0.035s (eta: 0:00:04)
person 0.9674598
person 0.97370064
person 0.9780993
person 0.9494859
person 0.9181932
person 0.9306571
person 0.9250486
person 0.950876
person 0.9610049
person 0.9195202
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.319s + 0.035s (eta: 0:00:01)
person 0.9407355
person 0.97140336
person 0.9277939
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.382s + 0.039s (eta: 0:00:52)
person 0.94200253
chair 0.91954356
chair 0.95842814
person 0.9584221
person 0.92546284
person 0.9536951
person 0.9413203
diningtable 0.93185616
person 0.97455
person 0.9719203
diningtable 0.9377866
person 0.95399004
person 0.96853954
person 0.962429
person 0.9077244
diningtable 0.91160434
person 0.93194723
person 0.9791819
diningtable 0.9661817
diningtable 0.9296108
diningtable 0.94074583
chair 0.91539574
chair 0.9815599
person 0.90703374
person 0.931609
person 0.98905575
person 0.91966003
person 0.9613662
person 0.9852409
person 0.9162095
person 0.9238157
person 0.983622
person 0.9683899
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.333s + 0.036s (eta: 0:00:42)
person 0.9397764
person 0.9547556
pottedplant 0.9334908
person 0.90470076
bird 0.92499405
bird 0.9717691
bird 0.94850934
bird 0.9078847
person 0.9674074
person 0.92087597
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.326s + 0.036s (eta: 0:00:37)
person 0.9504162
person 0.9105289
person 0.9623612
person 0.9701666
person 0.97669876
person 0.94648343
person 0.9022565
bottle 0.90571517
bottle 0.9523946
bottle 0.90364015
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.323s + 0.036s (eta: 0:00:33)
person 0.9102257
person 0.9181056
diningtable 0.9075776
pottedplant 0.9311801
chair 0.93998086
chair 0.9160421
person 0.97570646
person 0.9415209
bottle 0.9097779
bottle 0.90924627
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.322s + 0.035s (eta: 0:00:29)
person 0.9624632
person 0.96070325
person 0.9478575
person 0.99478453
person 0.94275284
person 0.9405367
person 0.9205072
person 0.9794978
person 0.91293555
person 0.9581561
person 0.9734929
person 0.9158283
person 0.91610944
person 0.93472373
person 0.9389938
person 0.99201196
person 0.9397804
person 0.9744655
person 0.925344
person 0.9611605
person 0.9838251
person 0.9665568
person 0.9274943
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.317s + 0.035s (eta: 0:00:26)
person 0.94812703
person 0.96498704
person 0.9025764
person 0.9707317
person 0.90707475
diningtable 0.9841787
chair 0.91673684
chair 0.94797575
chair 0.9793977
chair 0.987444
aeroplane 0.9122902
person 0.96678746
person 0.9815881
person 0.93306065
person 0.9176502
person 0.9123567
person 0.94768465
person 0.97358465
car 0.9229884
car 0.980411
car 0.9719059
car 0.9589237
bird 0.9584645
chair 0.9318178
chair 0.95090824
person 0.9456568
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.317s + 0.035s (eta: 0:00:22)
person 0.98415875
person 0.93359554
person 0.9093174
person 0.9813059
person 0.9584617
person 0.9896063
person 0.93713415
person 0.9269548
person 0.9409921
person 0.990285
person 0.93865955
person 0.9084732
horse 0.96251214
person 0.9128601
person 0.90534633
person 0.9196809
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.314s + 0.035s (eta: 0:00:18)
person 0.9554594
person 0.9463062
chair 0.97502977
chair 0.9218576
chair 0.95685107
pottedplant 0.92484736
pottedplant 0.9224873
pottedplant 0.9456207
person 0.9486149
person 0.9039424
person 0.92197573
person 0.93799466
person 0.95930666
person 0.90009767
person 0.93820155
person 0.93242884
person 0.9018207
person 0.9042348
person 0.9558374
bicycle 0.9138378
person 0.9553401
person 0.9798473
person 0.9029065
person 0.94290054
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.318s + 0.036s (eta: 0:00:15)
person 0.988084
person 0.9181242
person 0.9321361
person 0.91661143
person 0.9593312
person 0.98545134
person 0.94015795
person 0.9768917
person 0.977999
person 0.97184294
person 0.93718624
person 0.92292446
person 0.9170295
bus 0.95545244
person 0.90622747
person 0.9395814
person 0.94826853
person 0.9651913
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.322s + 0.035s (eta: 0:00:12)
person 0.96757454
person 0.91497743
person 0.93568105
chair 0.91268563
chair 0.94862
tvmonitor 0.90171355
chair 0.97123444
chair 0.9300975
chair 0.94004184
person 0.9496468
person 0.97012895
person 0.9570207
person 0.97749704
person 0.9188652
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.318s + 0.035s (eta: 0:00:08)
motorbike 0.9155268
motorbike 0.9515157
person 0.90185004
person 0.9780899
horse 0.95563513
horse 0.9010687
person 0.9037779
person 0.904918
bicycle 0.95895857
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.316s + 0.035s (eta: 0:00:04)
person 0.9247555
person 0.95899916
person 0.9810564
person 0.94933975
person 0.96459234
person 0.98435473
person 0.95091665
person 0.9568503
person 0.9434225
bird 0.90047413
chair 0.959577
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.315s + 0.035s (eta: 0:00:01)
person 0.9068024
person 0.9865479
person 0.9857736
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 87.296s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1156  282  279 ...  985  986  983]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2210
INFO voc_eval.py: 171: [123 124  69 505  68 236 445 281 127 262 135 125 476 559 375 385  71 564
 536 450 562 557 503  72 566 373  70 547 470  84 131 133 563 267 574  87
 451 458 132 524 568 558  73 579 250  95 455 129  82 378 560 511 454 370
 371 290  76  89 285 136 126 515 549 538 449  90 291 237 242 462  74 556
 461 542 507 431 264 294 468 537 139 101  18 539 137 148 284 477 141 369
 512  42 460 382 240 282 241 535 234 514 238 313 274 376 401 334  78 244
 415 288 570 529 452 544  22 573 297 466 374 456 497 405  21  37 103 146
  24 143 540 510 421 275 506 429 105 509 576 128 260 159 552 453 380  40
 296 571 541 265 160 419 266 341 249 394 192 328 464 194 534  86 302 412
 147 565 201 517   2 432 580 551 465 273 235 577 134 213 402 251 144 243
 320 317 335 430 299 483 142 569 399 409  77 411 561 295 567 287 384  41
 545  19 513 457 255 472  81  43 293 548 289 245  32 309 424 531 145 528
  56 527 572 277 188 326 420 196 268 248 437 138 383 239 368  16 550 575
  96 522 422  91   5  85  88 130 518 104 301 102 553 398 532 140 259 191
 500 271 410  46 174 308  75 486 443  30  98 467 423 498 393 400 578  94
   1 183  20  25 523 261 247 475 504 286 258 372 434 202 482 175 519  92
 396 414  66  50 389 444  79 193 447 331 391 392 195 257  38 329 508 311
   0 357 276 158 189  99 172 272 185 530  64 321 417 416 554  34 459 163
 480 182 186 425 418 217 441 435  29  60  80 115 413   3  26  55 263 120
 481 190  61 184 323 246 298 179  47  36 377 197 521  17 433 446 387   8
  33  28 324 181 176 473 337 471 304 305 165 469 406  12 228 283 152 386
 100   6 403 218 114 306 310 546 495 408 300  15 501 436 162 187 346 227
 279  65 164 388  62 533 292 340 404 442 345 397 367 525  93  63  35 484
  23 347 351  31 111 543  27 496  51 153 307 220 121  11 327 166 206 167
 438 439 204 440  45 342 474 168 170 200 526 348 448 366 109 205 199  53
 407 353 278 226 390 516   7   4  59 254 210 161 112 253 478 203 428 252
 333  14 485 149 222 209 379  57 156 330 198 208 479 231 343 336 395 356
 344 338 108 180 520 381 365 463 303  83 312  67 122 150 177 355   9 171
  52  44 173  13 157  54 106 354 332  58 339 427 178 214 319  10 211 352
 155 212 113 154 169 107 110 502 219 256 280 314 349  49 322 316 315  48
 426 116 215 325 207 499 359 360 488 117  97 216 362 233 494 225 232 363
 489 364 224 221 491 361 318 119 490 581 358 487 229 269  39 492 493 118
 230 223 151 270 350 555]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3964
INFO voc_eval.py: 171: [ 645 1713 3241 ... 3479 3234 3239]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1949
INFO voc_eval.py: 171: [ 636  667  748 ... 2274 2275 2279]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2388
INFO voc_eval.py: 171: [365 401 573 309  12  21 649 357 646 364 282 572 566 625 402 652 312 589
 331 543 373 601 288 666 311 141 477 546 824 655 686 587 322 754 823 320
  18 567 544 314 287 615 654 403 283 327 662 570 736 651  25 837 626 648
 721 358 667 329  16  22 657 664 405 289 700 376 367  20 513 366 550 825
 612 568 672 468 297 325 310  70 826 372 610 490 647 724 630 545 265 404
 285 675 315 497 292 217 834 585 324 588 656 371 512 255 735 829 330  13
 653 313 368 628 599 744 303 172 202  68 301 690 616 375 671 480 828 302
 678  43 609 729 673 580 549 318 491 360 470 175 574 114 487 571 688 710
 693 196 808 132 252 684 758 205 586  15 418 326 692 699 499 554 698 211
 661 176 524 668 602 584 569 709 260 247 613 328 669 469  19 592 398 213
  31 683 109 833 296 147 146 332 424 582 216 204 737 697 291 614 695 795
 257  50 762 793 143 170  17 155 681 162  80 770 611 369 493 166 682 319
 841 551 629 578 792 719 266  44 300 359 691 430 317 591  24 261 127 547
 152 259 659 660 400 387 465 237 740 575 676 467  69 853 198 632 598 323
 596 794 553 796 548 515 212 416 800 713  99 743 422 116 520 414 689 797
 316 361 807 665 495 442 370 605 239  34 581  79 650 321 264 374 248 101
 791 845 843 201 751 243 583 529 295 674 670 293 234 523 284 552  58 219
 167 182 441 177 333 113 294 496  45 838  81 677 687 478 579 633 759 590
  23 519 133 711 305 714 335 617 685 286 528 131 696 125 230  97 110 742
 489  96  14 644  57 663 471 390 112 607 262 334 595 840 456 242 576 218
 680 153 488 606 839 577  78  64 267 399 694 594 720 767 195  27 597  54
 206 355 130 593 757 397 115 444 604 223  95 830 820  87  36 197 298 179
 741 150 199 256 855 263 772 765 111 704 618 251   6 165 705 229 482  53
 608 771 526 479 600 492 780 498 603 821 417 250 483 788 156 525 104 768
 171 214 494 276 634 306 722 725 411 336 801 269 484 712 437 184 856 236
  42  32 388 215 346 268  59 739 658 679 137 514   1 299 726 290 766 241
 396 362 750 181 108  52 129 117 415   4 443 107 118 200 728 119 521 747
  49 148  73 848  91  46 185 748 144  98 193 804 753 100 701 787  86 459
 760 516  35 151 835 806 249 558 522 174 183 627 235 244 832 168 413 363
 419 389 527 224 827 857 145 502 149 809 755 412 486 238 180 253 270 518
  47 466 769 541 756 776 423 746 749 730 258  26  28 338  92 272 233 535
   2 203 463 154 420 128 752 173 464 822  65 802 192 798 727 458 849 850
 246 775 254 274 161 225 102  90 731 622 427 851 158 761  77 245 457  93
  30 126 738 428 503  29 460 438 273 345 421 240 220 847 537 789 517  48
 557 846 157 103 440 745 227  38 635 160 715 472  72 163 178 432 831 445
  85 164 773  75 631 645 536 538 142 481 159 716 354 121 383 304 805 474
 774   0 485 790 169   5 540 539  89  51 271 275 452 854 455 504   8 106
  94 476 556 140 723 717   3 565 619 191 278 378 350 803 763 530 139 380
 542 379 703 718  40  56 562  61 475 439 210  76 451 436 426 643 620 462
 473 561 844 564 505 349 337 138 533 799 348 733 280  39 431 778 532 559
 105  82 347 702 563 534 136 208 506 461 560 221 222 228 393 454 852 425
 842 434 410 435  41 836 782 531 781  62  33 277 353 281 638 624  74 511
 231 500  83  71 640 394 382 381 450 623 352 351 637 501  37   9 706 391
 433 307 734 279 384 621  63 779 308 226 344 189 817  11  88 392  67 816
 385 764 732 453  60 207 507 707 356 339 784 190 708  10 343 555 232  66
 395 377 120 342  55 209 510 448 429 785 408   7 134  84 449 814 642 641
 813 340 818 819 122 194 815 509 777 341 446 124 810 786 783 386 508 123
 188 407 186 811 636 406 409 135 812 187 639 447]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3379
INFO voc_eval.py: 171: [438 171  96  95 172 450  91  81 103 462  99  79  97 441 110 367  80 121
 180 402  26 106  78  93  83 210  45 410 394 415  89 109 261 209 389 249
 163  86  84 442 366 381  46 104  32 398 271 143 264 360 368 443 162  54
 291  92  53 395 197 405 188 362 164 233  87 260  90 482 156 334 118 427
 218  82  44 434 111 201 393 120 222 142 320  88  11 330 288 205 237 252
 353  48 196 258 199  13  85 117 309 335 265 195 281 119 336 465  30 340
 373 176 151 125 272 331 208 392 380 397 337 245 262  57 146 363 400 355
 417 139 251   8 351 206 212 247 187 377 333  24 319 379 316 177 145 244
 419 414 280 192 411 155 408 390 284 123 149 122 437 227 225 357  49 228
 327 282 257 378 433 424 200 463 361 255 312 242 359 295 376 168  31 391
 352 386 322 124 150  27 283 131 250 416 287 202 432  16 221 273 100 140
 422 129 371 445 356 254 211 223 328 226 293 421 198  35  52   4 354 147
 329 266 314  41 115 127 113 135 173 375 313 308 128 141 409 464 105 426
 126 114 481 471  59 215 248  98 420 292 161 138 358 385 403 473 166 204
 224 178 116 132  29 428  33 480  58 311 429  94 277   0 157 148 169 383
 101 399 160  47 270  36 472 232 203 108 388 342 243 275 256 396 102 332
 230 239   5 234 207 321 130   3 412 407  43 235 401 436 154 246 474 456
 343 418  37  68 369 153 238 259  55 317 370 435 213 181 454 323 276 285
  74 425 404  28 467  69 274 144 440 387 470 374 229 267 263 423  60 167
 184 170 324   7 158 475  40 372  10 444 217 476 253 439 133 278 241 112
 326 325 364 341   6 152 269 349 452 315 231  38 286   9 310 107 406 318
 194 338 268 413 457 174 453 290 446 468 165 289  42 384   1 469 303  17
 279 134  34  39 307 339 159 219   2 304 183 382 431 214  61  50 306 297
 466 191  22 430  56  63  75 365 236 189 216 447 301 240 458 455 298  62
  25 220 479  20 459 347 305 345 294 296 185 344 179  14  67 182 302 448
  72  64  19 136 451  23 299 478 300  18 477 175  76 190 186 137 460 461
  15 193  12  66 449  51  73  21  77  65  70  71 348 350 346]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4241
INFO voc_eval.py: 171: [1592  713 1526 ...  119  104  100]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2753
INFO voc_eval.py: 171: [ 29 263  64 108 156 314  65 109  19 118 287  24 315 313 264  66 155 255
 203 125 154 286 265  26 296 232 259 110 213 202 290  74  83 122  52  95
 288 233 248  94  48 242 133 117  50  16 262 137 102 144 115  69 171 173
 192 234 191 177 306 153 295  42  21  20  75 105 206  92  27  63  70 124
  17 219 227  96 174 292  80 116 113  18 121 166 201 254 294 217 243  22
 249 159  55  28 112 165  73 285  56 257 237 194  76  81 205 172 167  54
 241 215 190 130  91 158 291 256  23 216 152  51 235  85  86  82 293 197
 157 289 310 127 119 261 146 246 307  53  77 123 282 312 308 211  25 238
 195 150 226 151 220   5 193 184 145  47 236 224 267  88  68 239 240 161
 103 138  84  87 131 126 164 128 136  49 135 111 258 170 223 132 245 218
 309 114 270 284 140 107  44 311 225 279 222  45 317  14  93 147  10  67
 101 139 301 244 247 134 196  89 252 275 251  41  46 204 297  97 120 106
  90   8 250 183 104   4 214 207 100  43  35   0 230  39 280  15 143  30
  99 268 269 209 221   6 276 281   3  34   1 169 129 260 271 181 277  98
   9 283 316  60 175   2 168 199 231  36   7  38  31  12  59 160 198 162
 212 163 180 210 299  58  71 200 274 272 208 278 302 186 298 178  32 273
  33 305 304 228  62  78  13  57  79 300 148  61 141 185  72 303 266 229
 176 149 179  11 253 189 182  40  37 142 187 188]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0868
INFO voc_eval.py: 171: [5836 2512  698 ... 4351  288 4352]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3068
INFO voc_eval.py: 171: [329 256 194 442  58 200 282 387 270 253  50 269 762 695 255 258 283 450
 252 412 448 754 641 266  71 330 409 218 726  77 259 608 193 333 195 445
 196 630 154 382 287 180 558  64 753  73 177  55  53 228 136 446 198 254
 278 241 443 226 201 126 447 375 203 606 756 441 273 264 559 368 564  69
 439 457 127 169 758 757 320 440 112  68 452 642 419 765 534 276  21   7
  60 267 219  33  65 759  70 609 403 738 192 560  67 385 651 626 723 456
 454 202 388 444 263 535 701   6 357 199 725 342 637 306 622 262 261 610
 732 148 455 538 761 453 274  49 700 268 515 397 553 716 197 144 289 338
 393 386 471 531 361 216 211 290 644 524 341 426 418 627 175  57 631 727
 650 411 569 275 720 731 632 728 628 260 115  10 770 227 322 514 566 344
 242 141 730 237 281 665 221 265 537 755 729 449 332  40 381 383 420 612
 349 181 125 410 660 620 724 331 176 421 561 438 146 184  52 214 611 309
 240 658 183  15 763 413 565 567 614 667 206 555 292  63  84 462 703 733
 208 336 562 643 645 722 541 556 288 428 696 179 173 150 284 217 719 124
   9 129 739 629  48 607 321 735 734 416 772 516 389  79 185 415  29 423
 105 234 314 648 257 347 230 494 233  30 408 345 178 384 417 132 128 740
 151 717 764 367 744 619 424  46 475 130 768 172 182 451 220 736 715 749
 427 662 530 517 505 766 244 243 422 414 155 162 572 557 142 313 360 750
 391 706 286  66 647 705  75 711 633 521 205 690  22 335  47 316 621 334
 271 224 158 312 174 714 364 323 311 279 337 539 425 358 616 678 399 527
 743 707 235 570 554 718 380 134 495 784 143 285 682  99 394 782 656 210
 168 277 104 106 280 156 223 272 507 109 680   8 369 536 396 133 110 584
 339 204 238  93 540 737 503 769 661 327 568 698 760 340  17 783 315 480
  31 767 401  27 497 157 293 519 363 548 118 433 291 249 310 239 663 563
 103 741 326 694 708 482 400  23  87 576  14 114 721 618 623 139 137 625
  13 649 504 159 318 752 498 458 113 404 677  42 390 379  45  24 780 571
 578 215 712 481 747 664 636 353 533 135 236 510  56 542 745 170 247 525
 742 639 225  34  54 774 100 152 532 615 751 207 773 785 490 746 496 187
 748 654 392 371 398  91  89 229   1 635 640 592 529 488 402 108 319 652
 693 212 395 589 317 461 518 153 479 679  26 463 101 687 624 131 684 604
 579 771 222  32  25  20 509  90 328 601 362 191 307 209  94  36 372  59
 107 138 251 522 688 102 324 119 588 111 704 213 232 140  11  19 171 547
 692  35 659 248  78 603 508 775 600 325 359  28 166 246 487  82  86 585
 485 520  62 670 160 666 674 123 689 231 586 165 681 365   3 638 676   0
 526 478 602 436 591 476  76 366 431 596 245 376 655 528 374  72 545 161
 250 145  51 523 434 686 147 668 675 370  12  38  92 577  41 697 189  37
 646 432 590 781 699  18 634 435 709 552 190 691 492 491 167 483 186 116
  80 493 685 350  81  83 121 149 459 657  16 500 672 593 594  85  61 164
 580 163 120  74 484 595 489 597 673 502 501 506 355  95 352 671 477  88
 551 613 377 653 303 486 308 587 605 430 669 469 598  39  43  44 188 467
 429  98 710 543 549 546 544 117 499 405 511 305 378 122 464 407 550 599
 346 297 298 406 465 295 702   4 474 354 304   2   5 437 617 460 468 470
 300 512 473 713 779 373 683 472 348 294 351 302 356 466 296 583 513 776
 778 301 299 573 581  96 343  97 777 582 575 574]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2079
INFO voc_eval.py: 171: [2886 1278 1750 ... 2304 2790 2791]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3280
INFO voc_eval.py: 171: [343 410 346 840 445 501 353 375  50 481 464 448 443 549 146 349 620 502
 408  64 442 161 769 541 144  46 278 506 149 159  51 512 204 522  42 376
 842 153 274 355 837 731 237 792  49 413 128 783 461 409  66 527  45 205
 151 765 793 147 764 475 268 142 830 164 516  41 344 466 781 845  63 810
  52 435 374 834 469 220  83 371 819 354 621 798  54 753 269 727 246 520
 543 766 640 163 838 224 547 162  69 550 578 225 463 483 767 236 534 505
 473  65 485 659 416 242 247 851 281 421 417 849 356 437 152 815 596 345
 712 260 239 462 145  55 841 822 165 638 668 524 199 223 470 449 839 207
 415  44 126 414 439 238 434 474 546  53 667 214 611  86 295 150 200 770
 277 603  62 519 335 614 772 251 653 324 657 814 656 813 728 817 350 696
 700  32 194 586 221 612  68 799 768 802 228 698 132 158 779 723 832 429
  70 450 788 752 156 601 280 548 440 258 645 754 263 217 513 761 441 379
 197 431 595 230 593 141 382 739 229 855 412 347 545 130 157 831 380 215
 411 433 206 208 226 148 419 701 631 143 806  47 795 777 372 424 166 131
 196 669 800 436 318 755 136 639 796 600 807 734 553 843 341 438  60 774
 270 627 847 127 738 250 272 425 231 617 160  57 219 829 422 584 293 334
 637 308 312 155 619 348 444 116 641 265 511 678 805 691 740 110 818 351
 112 794 261 479 662  67 198  43 571 287 858  16 447 266 154 775 844 488
 544 195 129 730 542  87 288 279 292 133 859 480 244 613 332 592 572 689
 243 670  61 467  92 282 715 518 302 283 676 816 762  90 135 123 803  31
 352 643 383 780 625  85 296 778 826  94 771 233 782 801 276 234 716 241
 271 184  58 729 420 245  59 597 122 718 139 273 665 825  25 763 427 570
 326 291 618 384  22 222 117 773 604 853 735 824 615  19 275 476 854 248
 579 674  89 675 381 232 717 850 671 607 567  15 359 664 661 301 111 182
 555 606 286 590 212 821 113 114 120 216 647 446  56 566 300  13 218 102
 428 616 605 776 552 249 340 459 529 252 364 373  33 724 695 515 528 741
 697 704 305 809 478  29 538 608 389 666 569 833 307 692 314  88 745 551
 599 646 856 808 253 103 677   2 313 342   5 418 267 262 720 721 554 556
 812 680 811 290 186 423 835 213 533 118 259 325 857 686 820 430 804 294
 725 672  91 852 823 836 642 477 719 235 846  84 517  93 682 679 401 407
 848 655 188 333 644  35 630 426 124  20 722 321 568 285 685 687 714 264
 187 633 508 628 105 134 370 327 361 573 119 629 358  71 732 588 583 658
 319 337   4 400 457 797 673 299 652 598 736 589 525 591  82  26  10 317
 581   6 602 860 298 257 181 175 726  95 507 316 360 336  48 634  28 303
   3 539 240 310 514 190 561 532 503 256 392 610  79 362 785   1 757 322
 681 330 594 289 484 789 254 115 626 297 306  80 486 211 106 560 742 492
 284 609 451 733 315 743 309 323 363 378 535 368  40 178  96 454   0  77
 786  36 622 121 632  14  27  39  23 140 582 489 168 395 137 787 663 465
 636 562 174 703 558 100  99 536 365 707 540 537 203 749 494 650 125 432
 339 453 557 651  18 747 452 791 472 311 635 405 482  34 304 138 169 167
  24 367 183 255  78 209 497 180 784 521 683 399 403 496 737 456 504 468
  74 402 530 404 510 366 746  97 176 104 471  98 790 357 491 192 706 490
 648 708  75 487 654 576  21 574  73  30 455 649 587 189 101 580  81 177
 577 460 202 531 585 320 179 744 185 559 694  76 387 493 699  72 193 377
 108  17 210 702 750 690 660 509 575 684 751 748  38  37 191 406 201 458
 109 394 563 328   7 693 565 523 388 495 498 526 623 713  12 369 758 107
 385 760 564 709 227  11 711 329 338 624   9 173 705 710 331 386   8 172
 171 170 398 759 500 499 756 396 393 397 827 391 390 828 688]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2092
INFO voc_eval.py: 171: [133 940 433 ... 564 763 764]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3075
INFO voc_eval.py: 171: [ 46 269 256 141 145  25  45 150  47  50 142 277 118  72  33 159  68  30
 263 143  59  78  55 187  56 168 116 264 258  73  15  52 161 244 196 202
 130 266 172 265 192  35 237  32 162 251 279 240  31 257  16  29  77 260
 274 146 246 147 117 149 280 278 155   6 153 189  51  84  61 166 212 115
  49  21 195  42 272  83  62 144 182 114 222  60  76 100  85 158 140 194
 201 261 239  88 109 262 281  19  34  10  98 228  66 238 209  22 185 215
 259 255 151 250  65  14 126  53 245 167  28  80  57  67  18 241 134 199
  70 113  79 148 163 165  96 125 242  37  24 183 236 227   4  23 217  27
   1 154  58  94 231 267   8  20 111 152  38  99 131  64 157 249 198  41
 156  75   5   9  91  71  17 268  12 138 218  95 106 137 214 204 275   0
  81 233 104 186  36  93 135 247 216 225 207 200  26 177 252 103 248  89
  69 184 133 164  97 110 139  86   3 102 179  48 190  11 211  13 173 101
 120  87 107 276 108 273  39 206 224 122 170   7  40  82 123  44 174  92
 180 221 176  54 232 230 112 193 171  74  90 169 124 105 181 197 175 208
 213 178 210 253 234 203 128 270 205 188   2  43 219 220 119 160 127 191
 226 271 223 229  63 243 129 121 136 235 254 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5020
INFO voc_eval.py: 171: [ 8618 14084  1069 ...   783 10954 10956]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5390
INFO voc_eval.py: 171: [2514  255  708 ... 1946 2702 1829]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4121
INFO voc_eval.py: 171: [ 66 308 535  34 330 544  67  56 317 128  40 538  50 539 327 337  44 309
 312 614 536 150  46  35 345 479 214 131 325 341 549 540 248 306 153 338
 333 550 543 122 404  37 134 645 277 410 636 342 300 541  71 141 616 420
 362 140  68 318 142 537 158 133 151 416 220 311 219 625 486 624 361  60
 218 491 562 262 620 406 316 313 622 331 154  52 120 626 610 558 480 213
 148 629 462 304 136 639 246 409 348 249 618 159 563 496 343  39 619 525
 212 552 621 225 413 332 160 227 647 127 323 490 126 412  45  90 499 135
 513 162 315 130 190 641  43 445 613 364 489 310 305 631 252 282  65 363
 326 261 228 129 643  62 247 161  95 307 216 494 223 648  91 367 573 419
 344 674 278 646 407 303 346 638 226 671 121 147 322 632 505 324 640 339
 138 334 139 478 634 559 567 405 250 132 301 417 156 224 564 506 238 431
 503 124 137 314 302 321 145 259 617 568 408 168 272 611 561 508 335 522
 607 257 637 319 633 125 628 276 336 403 194 672 635 258 215 320 360 627
 682 670 146 548 328 329 528 411 123 340  77 553  42 608 483 642 217 630
  49 351 297 492 578 195 481 545 596 152  15 260 448 566 222 484 101 501
 102 263 461 644 497 287 464 623 155  64   7 197 350 191 615 504 609 374
  70 560 440  55  28 196  36 188 493 463  25  38 266 109 675 149 375 347
 542 264 526 519 575 551 119 612 239 201 165 182 376 254 221 251 650 118
 189 488  94 353 349 192 352 293 236 366 477   0 255 176  93 163 516  89
 193  33 509 517  24  17 184 684  92  72 679 502 253 414 180 390  47 295
  96  23 381 649 117 157 673 356 256 465  53 373 439 169 554 515 240 265
 518 436 426 651 173 112  14 555 279 654  87 476 368   4 521 680 468 571
 442 507  48 294 430 556 433 106 208 108   1 447 425 243 178 557  78  61
  21  31  76 570 443 177 418 595  80 104 206 164  83 510 459 292 241  26
 681 676 602 485  16 460 432 547 600   2 181 473 203 286  18 369 435 511
 271  58 457 576 100 683 652 520 204 512 685 275 421 397 579 268 207 588
 597 605 474 487 546 677 574 401   8   9 105 244 205 565  57  19 111 234
 523 601 298 446 172  69 394 678 529  41 110  32 285 598 475 498  22 270
 572 467  20  74 107 441  75 514 577 274 482 500 179 653 273 281  79 469
  51  73 114 415 299 524 113 393 365 115 267   3 370 437 527 202 495 242
 422 599 280 438 594 389  54  88 656 284 427 583 444  59 245  27 471 428
  29  63 355 175 382 429  97  30 187   5 269 283 210 296 357 186  85 358
 424 371 116 593 400 198 395 211 660 590 423 354  13 434 586  86 581 533
 659 379 606  81  98 585 171 402  82 396 580 582  84 199 472 584  99   6
 569 230 589 470  10 657 531 359  11 209 174 591 385 662 669  12 231 377
 587 383 185 229 532 603 232 200 166 449 235 380 592 392 288 167 387 170
 237 661 183 604 233 291 453 391 658 466 530 144 378 668 384 388 666 386
 103 398 665 452 143 458 289 655 399 290 534 664 450 454 456 667 663 451
 455 372]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3156
INFO voc_eval.py: 171: [ 43   3  47 121 190 280 184  48 126 338  50   6 193 188 236  52 191 158
  13 161 334 194 159 156  60  42 282 152 182 187 340 233 127 279 143 223
 186 157 277 237 231 276 101  80 128 246 151 100  78 337  49 252 281   5
  58 124 104  59 153 163 257  10 162 328 175  23 232 185 147 120 308  57
 129 213 273 247 135  44 311 176 107 189 183  75 245 326  11  46 235 239
 234 166  72 216   4  99 160 327 272 103 132 192 323 119  96 321   8  12
 254  51 251 106 336  89 203 264 269 255  25 250 145 130  14 253 102 122
  97  45 332 261  73 209 256 289 212 154 238 105 125 339  15  91  83 170
 309 331  63 320 225 244  98  90  74 329  61 265 200  84  26 263 248 343
 114 196 325  27 270 341 117 314 149  64 221 228 294 230 295 249  79 134
 123   7 330 113  85   9 167 195 118 285 211  82 260 322 313 241 150 220
 307 271 116 259  76 131  95 305  77 315 155 324  55 133 299 303 335 224
  39 317  33 333 274  62 146 226 310 242  65 278 266 218 267 298  30 262
  24 342 201 217 171  88 112 214  28 297 290 219 177 240 205 174 316 110
 222  81 144 148 296 227  31  87  56  53 198  54 141  32   1  22 284 136
 258   2 306 291   0 268 197 229 210 300 202 169  86 208 283  18 109  29
 139 207 115 304 199 301 108 168 293  41 204 243 206 137 111 179 302 286
  35  37 164 138 178 172  19  21  20 173  17 319 275 142  71  16 180  36
 140  40 292 288 181  69 165 287 312  34 318  70  67  38  66  68  92 215
  94  93]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1308
INFO voc_eval.py: 171: [382 641 636 115 325  80 207 396 293 121  76 404 168  78 898 643 639 294
 655 327 324  83 181 117 398 320 807 125  89 212  92 210 295 804 505  81
 208 536 637 389 383 188 642 453 656 452 300  97 206 119 455 116 326 187
 197 906 912  84 646 900 306 342 296 176 813 837 510 711 464  86 541 523
 847 388 883 856 909 403 165 216 135 134 842 218 910 640 390 526 334 544
 299  43  63 871 456 488 329 434 852  98 112 171 732 385  18 217 749 136
 195 315 330 312 681 328 192 205 913 884 712 903 662 457 811 838 805 105
 911 196 508 173 166 261 668 530 169 280 308 840 873 439 182 220 443 733
 438 914  45 760  93 812 304  19 230 509 824  70 399 623 167 199 885 901
 553 540 332 756 525 881 721 832 841 178 894 744 209 185 310 164 846 172
 714 918  33 739 899 194 552  67  44 890  82 128  65 214 462 244 902 458
 242  74 101 843 213 511 791 487 448 788 821 249 454 227 461 834 710 319
 543 875 546  58  85 184 808 222 263 874 221   2 451 395  53 638 717 375
 707 855 384 720  91 338 905 179  94 180 713  16 264 259 703 499 317 298
 690 686 400 527 764  59 579 602 507 893  17 177 930 882 437 158 401 198
 331 872 483 924 857 318 590 204 408 466 133 303 323 916  99 789 433 109
 759 378 459 440 514 741 356 877 778  52 103 144 485 402 170 820  72 679
 518 629 393 234 465 915 376 892 233 607 494 827 467  32  38 262 236 908
  41 175 350 880 689 200 929 435  73 277 836   5 290 682 360 506 542 765
 251 444 886  95 154  35 917 825 870 143  71 746 854 254  42 432 359 693
 113 609 867 273 529 687 463 806 907 524 561 100 122 664 241 211 469 616
 174 111 301 380   7 548 124 157 123 830 186  57 228 574 519 620 818 863
 322 785 250 531  75 828 252 247 648 162 477 163 895 762  96 545 191 688
 554 861 923 496 730 535   8 750 445 835 896  46 201 358 748 137 572 606
 425 831 215 888 183 839 891 634 357 735 110  55 845 266 333 691  37 904
 600 260 833 489  87 256 307 311 238 237 853 265 239 742 624 468 528 309
 219 226 564 698 772 761 161 314 405 817 460 879 406 729 515 684 876 860
  56 436 442 202 878 335 702  36  40 897 576 614 933 274 190 341 781 603
 394 678 441 473 102   0 931 731 153 651 589 229 566 851  61  39  88 605
 601 622 337 604 556 632 709 763 193 796 520 106 621 351 423 386 725  34
  13 718 547 734  60  48 379 801 397 283  79 795 610 391 683 752 189 340
 138 829 288 281 114 889 567 497 501 431 349 932 743 130 578 626 776 680
 575 362 577 493 517 297 560 692 844 685 537 775 569 377 724 417 235 240
 316 269 243 751   6 777 282 155 661 814 797 859 647 353 257 424 587  62
 826 792 798 794 790 779 275   4  54 593 862 736 354 279 613 708 302 608
 246 555 715 478 747 580 203  31 348 723 586 363 705 793 450 757 716 446
  66 422 245 321 479 286 850 381 848 612  30 753 635  47 677 276 108 500
 387 107 313 426 278 392 676 701 581 816 697 887 369 484 704 774 611 345
 490 104 559 150 270  77  21 284 755 573 513  68  90 627 248 617  29 550
 919 476 368  15  69 305 815  20 271 571 407   9 787 758 849 592 344 551
 584 419 565 516 253 347 371 926 474  22 823 802 231 447 532 289 799 726
  14 582 588 287 754 654 285   3 471 232 557 258 255 558 420 570 670 568
 782 706 868 492 727 745 783 449 361  64 367 615 737 625 786 502 925 738
 740 267 491 809 503 819 470 533 159 504  51 810 595 699  10 142 414 475
 413 268 549 539 521 700 728 412 472 343 803 495 822 583  12 920 370 482
 429  49 152 486   1 498 373 598 131 291 671 594  11 719 374 522 428 591
 722 784 800 675 619 512 780 773 151 858 538 663 597 869 355 481 657 480
 534 292 644 372 630 585 427 696 366 336 430 364 596 421 410 365 156 771
 628 645 416  28 132 415 160  50 147 770 339 927 633 140 649 418 864 563
 346 409 922 562 658 118 223 631 660 928  27 618 599 865 673 139 145  23
 767 695 766 146 768 411  26 866 769 225 352 672 149 127 694 665 224 272
 129 120  25 141 653 652 126 148 674 650 666  24 921 669 667 659]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2996
INFO voc_eval.py: 171: [420 251 593 174 433 297 136 441  24 467   1  37 476 496 255 320 350 473
 478 322  39 394 437 253 348 126 249 418 349 468 124 574 309 298 351 548
 609 264  57  68 421 254 470 323 534 107  71 129 230  32 438 180 444 594
   3 519  17 327 475 430 182 339 316 303 347  43 319 597 617 582 154 627
 226 357 102 612 367 375 370 261 605 440 307 150 443 219 535 109 110 472
 205 321  50 137 257 365  19 208  73  70 484 435 222  29 301 599 143 464
 305   8 160 204 506 554 381 471 532 363 324 377 232 273 483 103 615 462
 358 360 366 331 393   9 392  40 245 120  58 632 162 186  16 161  25 621
 213  56  46 482 480 600 178  44 128  26 591 406 513 300  84 427 207 313
 425 259 299   6 474 575 287 306 344 504 541 304  41 314  53  72  30 223
 501 479 337 211 372 239 119 101 179 229 567 469 518  48 145 557 328 212
 311 558 209 215 432 477 622 596 426 378 317 397 214  38 172 175  90 619
 265 125 631   2 189 131 158 117 400 611 192 173 210 383 228 550 121   5
 577 578 115  75 308  89 356 434  93 354 514 364 106 546 184 581 405 334
 481  45  11 271 598 547 494 625  91 395 302 123  18 610 454 561 176 551
 281 292 414 620 398 341  23 559 452 613 206 112 544 439 100 310  12 130
 171 529  55  87 515  33 628 284 498 318 199 399 153 522 537 539 159 499
 552 225 616 560 235 536 608 412 276 361 583 278  98 359 525 280 533 285
 487 623 595 629 362 495 624 216 333 607 453 111 114  94 191 144 500 570
 626  95 505 497 238 614 385 566 218 148 408 203 379 503 279 332 517  86
 258 315  54 147 618 520 217 312 384  82  78 369  28 220 282 155 540  92
 368  63 542 181  42 138 274 450 538 116 564 141 586 353 371 516 456 584
  61 419 502 543 127 531  77 275 104  76 118 447 635 376  79 256 286 451
 268 510 569 428 374 380  36  49 224 187 166 352 355 382 422 436 565  34
 373 445  81  60 242 329 133 201 325  31 277  52   4 455 260 630 634 108
 588 269  51 589 338 449 157 283 389 523 336 149 142 330 590  15 463 429
 243 288  35 146  80 431 105  10 291  59 272 573  69 340 465 165 290 396
 485  64   7 549 151 122 633 227 156 139 113 545 585 606 410 486 530 556
 563  22 132 423 528 198 190 140 460 295 185 236 527 424 446 183 391 152
 240  27 404  85 197 135 241 555 134 407 526 509 572 326  83 294 409 442
 403 234  88  96 221 262 296 168 167 448 200 562 177  74 402  67 250  97
 342 521 293 231 237 164 387 411 335 263 343 553 252 170  66 388 580 524
 163 390 345 511 568 587 401 188 386 571 413 270 507  62 233  21  99 244
  65 459   0 508 512 289 169 346 579 604 489  20 603 458 248 601 247 457
 602 267 492  47 461 415 488 194 266 416 592 202 196 246 493 576 490  13
 417 193 195 466 491  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3330
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3033
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.221
INFO cross_voc_dataset_evaluator.py: 134: 0.396
INFO cross_voc_dataset_evaluator.py: 134: 0.195
INFO cross_voc_dataset_evaluator.py: 134: 0.239
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.424
INFO cross_voc_dataset_evaluator.py: 134: 0.275
INFO cross_voc_dataset_evaluator.py: 134: 0.087
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.209
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.502
INFO cross_voc_dataset_evaluator.py: 134: 0.539
INFO cross_voc_dataset_evaluator.py: 134: 0.412
INFO cross_voc_dataset_evaluator.py: 134: 0.316
INFO cross_voc_dataset_evaluator.py: 134: 0.131
INFO cross_voc_dataset_evaluator.py: 134: 0.300
INFO cross_voc_dataset_evaluator.py: 134: 0.333
INFO cross_voc_dataset_evaluator.py: 135: 0.303
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 8999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.493s + 0.037s (eta: 0:01:05)
person 0.9705556
person 0.96387935
person 0.9924264
person 0.9609801
person 0.9757397
person 0.939708
person 0.98266715
person 0.94580483
person 0.9117602
person 0.9816497
person 0.9388002
bottle 0.9217602
bottle 0.91933227
bird 0.95101935
person 0.9825485
person 0.91827226
person 0.9449344
person 0.9028462
person 0.90479326
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.339s + 0.037s (eta: 0:00:42)
person 0.9449027
person 0.9209795
person 0.961004
person 0.94861853
person 0.98059547
person 0.94815713
person 0.9674547
person 0.9819142
person 0.93974704
person 0.92168194
person 0.92576456
person 0.93178636
person 0.9062833
person 0.922966
person 0.9210103
person 0.9271272
person 0.9757849
person 0.91361517
person 0.980006
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.346s + 0.036s (eta: 0:00:39)
person 0.92888117
person 0.9120836
person 0.9568646
chair 0.93817675
person 0.9724874
person 0.91613966
person 0.95769
person 0.9668647
chair 0.9656032
chair 0.9370358
chair 0.91494536
person 0.90499145
person 0.97911763
person 0.9879431
person 0.9276098
person 0.9771812
person 0.99158585
person 0.98133206
person 0.9520386
person 0.9106978
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.343s + 0.037s (eta: 0:00:35)
bird 0.94004333
person 0.9278067
person 0.92126024
person 0.99383473
person 0.9572203
person 0.9743025
person 0.97969556
person 0.907331
person 0.95961547
person 0.9660652
person 0.90388715
person 0.9189976
person 0.96954757
person 0.94100267
person 0.95840603
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.343s + 0.038s (eta: 0:00:31)
diningtable 0.9029131
diningtable 0.9385957
chair 0.96391076
chair 0.9659452
chair 0.93814075
chair 0.957625
pottedplant 0.91808057
pottedplant 0.9143238
pottedplant 0.91109633
pottedplant 0.90441996
pottedplant 0.94369566
pottedplant 0.9384279
pottedplant 0.92842287
person 0.965794
person 0.98289835
person 0.92030716
person 0.9798442
person 0.9489998
person 0.98911077
person 0.9002659
person 0.9157687
person 0.9039928
person 0.911964
person 0.92232597
person 0.9226453
person 0.97404116
person 0.9551438
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.341s + 0.038s (eta: 0:00:27)
person 0.95812637
person 0.92396396
person 0.90727985
person 0.91961443
person 0.94804174
person 0.9151006
person 0.9535913
person 0.9369093
person 0.9298575
bird 0.92276466
person 0.93767834
person 0.98766893
person 0.91698205
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.334s + 0.037s (eta: 0:00:23)
pottedplant 0.9027682
person 0.96245337
person 0.93400645
person 0.9130782
person 0.95385605
person 0.9190882
person 0.9586814
diningtable 0.9111329
chair 0.9826127
diningtable 0.9620667
chair 0.91611594
chair 0.948631
chair 0.98079
chair 0.9264065
bird 0.99303246
person 0.90280426
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.331s + 0.037s (eta: 0:00:19)
car 0.92877394
car 0.94246185
car 0.915378
person 0.9884123
person 0.90052533
person 0.9741461
person 0.9298981
horse 0.97375363
horse 0.9135148
horse 0.9209356
person 0.9380253
person 0.9533806
person 0.96815187
person 0.91864824
person 0.9431682
car 0.94591916
car 0.9640313
bicycle 0.9601384
bicycle 0.93106043
person 0.936944
person 0.9180313
person 0.97358185
chair 0.91871786
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.329s + 0.037s (eta: 0:00:16)
person 0.97420835
person 0.9591453
person 0.9738283
person 0.91780597
person 0.9133326
person 0.9018296
person 0.9886021
person 0.9863716
person 0.9435843
person 0.97116053
person 0.9154913
person 0.9639666
person 0.9911821
person 0.908945
person 0.966589
person 0.91192365
person 0.9879012
person 0.9798962
pottedplant 0.91157854
person 0.9898634
bus 0.9127511
person 0.9612762
bus 0.9066017
person 0.9415651
person 0.954803
chair 0.9470017
person 0.90549785
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.328s + 0.037s (eta: 0:00:12)
motorbike 0.9632237
person 0.97491676
person 0.9105796
person 0.9159016
person 0.95707726
person 0.9677023
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.328s + 0.037s (eta: 0:00:08)
person 0.90687793
boat 0.9820426
boat 0.9483117
boat 0.9185407
person 0.99038506
person 0.9375349
person 0.9854938
person 0.9183
person 0.91782427
person 0.98909247
person 0.97491765
person 0.9107679
person 0.97529864
person 0.976768
person 0.9139799
person 0.9484487
car 0.9506104
person 0.94070184
person 0.94726485
person 0.95983946
person 0.986821
person 0.90824354
person 0.92130333
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.330s + 0.037s (eta: 0:00:05)
diningtable 0.9599066
chair 0.9249812
chair 0.9738495
chair 0.9807042
chair 0.9155681
chair 0.98120856
chair 0.90192723
chair 0.93681514
chair 0.95997345
person 0.94283706
bird 0.9240795
bird 0.91570693
boat 0.91980726
person 0.96213615
person 0.92943776
person 0.98883784
person 0.91494906
person 0.9037579
person 0.91369736
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.328s + 0.036s (eta: 0:00:01)
person 0.98350257
person 0.96406156
person 0.95643836
person 0.9554886
person 0.9490724
person 0.96621966
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.392s + 0.026s (eta: 0:00:51)
person 0.9821249
person 0.91938335
person 0.9789854
person 0.97714114
person 0.9142655
bicycle 0.97241217
person 0.93214595
person 0.91525555
bicycle 0.9747833
person 0.936137
bird 0.9436723
bird 0.90088445
pottedplant 0.9430446
person 0.9684562
person 0.92899406
person 0.9022965
person 0.96108735
person 0.96615654
person 0.9869199
person 0.92683935
person 0.96606964
person 0.9527479
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.290s + 0.035s (eta: 0:00:37)
person 0.9126796
person 0.90645516
person 0.96403116
person 0.91789716
person 0.97403145
person 0.9430006
person 0.93596786
person 0.9808921
person 0.9644161
chair 0.92524916
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.317s + 0.037s (eta: 0:00:36)
car 0.9158545
car 0.95090973
person 0.9470959
person 0.9615799
person 0.9817442
person 0.92494375
person 0.94920087
person 0.965019
car 0.9823695
cow 0.92349213
person 0.9153162
person 0.9578278
person 0.9233913
person 0.9133854
person 0.98119694
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.326s + 0.037s (eta: 0:00:34)
person 0.97833055
chair 0.93477154
person 0.9303549
person 0.92526376
person 0.9514949
person 0.9536263
person 0.93316156
person 0.91811246
person 0.94041574
person 0.9806973
person 0.98699236
person 0.9771852
person 0.96914744
person 0.97752225
person 0.90552646
bird 0.90453786
bird 0.9908268
person 0.9221026
bird 0.9517072
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.327s + 0.037s (eta: 0:00:30)
person 0.9463435
person 0.97123456
person 0.9827216
person 0.92195696
person 0.9484035
car 0.9506104
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.326s + 0.037s (eta: 0:00:26)
bird 0.91005087
bird 0.9595768
bird 0.9136108
person 0.9149747
person 0.9444271
person 0.9188376
person 0.91510665
person 0.9884306
diningtable 0.9487053
person 0.9723891
diningtable 0.9790709
person 0.9397774
person 0.9361303
chair 0.94308835
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.333s + 0.036s (eta: 0:00:23)
person 0.91610646
person 0.9075876
person 0.93139744
person 0.94941926
person 0.9859207
bus 0.9464071
dog 0.95253175
diningtable 0.9738844
diningtable 0.9241859
chair 0.91267943
chair 0.9850839
chair 0.91121906
chair 0.97124386
chair 0.9359594
pottedplant 0.9018365
person 0.904289
person 0.90121245
person 0.97654676
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.327s + 0.036s (eta: 0:00:19)
diningtable 0.9211058
chair 0.9116797
person 0.94830936
person 0.9667104
person 0.95608157
person 0.9371483
person 0.94517064
bird 0.95428693
person 0.9262685
person 0.9623409
person 0.9641697
person 0.9324328
person 0.9803459
person 0.9101045
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.329s + 0.036s (eta: 0:00:16)
person 0.9628805
person 0.9004948
person 0.9639693
person 0.9175835
person 0.90335685
train 0.95835173
person 0.95713645
person 0.9621721
person 0.94631547
person 0.9403196
person 0.9152179
person 0.9295361
person 0.9317807
person 0.9052957
person 0.95642525
person 0.9266576
person 0.9925874
person 0.97402704
person 0.9598032
person 0.90153295
person 0.91766137
person 0.92748445
person 0.90333956
person 0.9311587
tvmonitor 0.91769445
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.331s + 0.035s (eta: 0:00:12)
person 0.9642585
person 0.9066918
person 0.9448098
horse 0.9601131
person 0.9668196
person 0.9310027
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.330s + 0.035s (eta: 0:00:08)
person 0.975255
person 0.96522516
person 0.92212456
person 0.91971457
person 0.95253414
person 0.92837346
person 0.936373
person 0.90336376
person 0.9160021
person 0.9867785
person 0.94671386
person 0.900059
chair 0.9725524
person 0.9562076
person 0.907221
person 0.94415176
person 0.95867044
person 0.9010049
person 0.9056696
person 0.9188516
car 0.9449859
car 0.918296
car 0.95223063
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.326s + 0.035s (eta: 0:00:05)
person 0.9674463
person 0.9440562
person 0.9627017
person 0.948956
person 0.9743594
person 0.96101165
person 0.92233235
person 0.90041053
person 0.9075995
person 0.9287621
person 0.9001302
person 0.98819554
person 0.9814291
person 0.9163662
person 0.9783875
person 0.92823386
person 0.9383884
diningtable 0.9336951
person 0.9013368
person 0.9179549
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.321s + 0.034s (eta: 0:00:01)
person 0.9383326
person 0.908315
diningtable 0.9101841
person 0.9709581
person 0.94082916
person 0.987282
person 0.9654168
person 0.9582027
person 0.9597052
person 0.9086171
person 0.9111391
person 0.9181177
person 0.9409176
person 0.96305704
person 0.9807944
person 0.91118085
person 0.9494475
person 0.95646113
person 0.9561658
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.466s + 0.043s (eta: 0:01:03)
diningtable 0.9021143
diningtable 0.9280654
person 0.96006334
person 0.9761736
diningtable 0.9736516
person 0.9371296
person 0.9184837
person 0.9084178
bottle 0.9067114
person 0.9781399
pottedplant 0.9175025
diningtable 0.92874956
person 0.9659724
diningtable 0.9069658
chair 0.9493723
chair 0.96600235
chair 0.9228901
person 0.93154484
chair 0.90143365
person 0.9074264
person 0.93302757
person 0.9956534
person 0.9808261
person 0.9346419
person 0.96627206
person 0.96541923
person 0.90482867
person 0.97076595
person 0.9601428
person 0.93904823
person 0.94091374
person 0.96555847
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.332s + 0.032s (eta: 0:00:41)
person 0.9474184
person 0.9859446
person 0.9336006
person 0.9708947
bottle 0.9368445
person 0.93713087
person 0.94415367
person 0.95383555
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.337s + 0.034s (eta: 0:00:38)
chair 0.9316625
bottle 0.90928817
boat 0.9010699
person 0.97293955
person 0.9399386
person 0.933761
person 0.9326946
person 0.93263406
person 0.9685732
person 0.9443468
person 0.9014786
bottle 0.908111
bottle 0.9535281
person 0.9821884
person 0.9526664
person 0.94932276
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.334s + 0.035s (eta: 0:00:34)
person 0.96687865
person 0.92173845
bird 0.916902
person 0.9537758
person 0.9068316
person 0.96810794
person 0.97063106
person 0.9599505
person 0.90371937
person 0.9499224
person 0.9082991
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.324s + 0.035s (eta: 0:00:30)
person 0.96959364
person 0.9691965
person 0.9532833
person 0.90699095
person 0.92774016
person 0.97072494
person 0.92069644
person 0.9410145
person 0.9144081
person 0.9199643
person 0.9370218
person 0.9699313
person 0.90131265
person 0.95358324
person 0.92476946
person 0.90556794
bottle 0.953653
chair 0.9311393
boat 0.9040955
person 0.97332484
person 0.9553093
person 0.9052267
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.321s + 0.035s (eta: 0:00:26)
car 0.98082286
car 0.93498504
person 0.92627156
person 0.9774061
person 0.9099044
person 0.9113942
person 0.920696
person 0.93531907
person 0.9650253
person 0.9231886
person 0.91806364
person 0.95754135
person 0.91844535
pottedplant 0.914063
pottedplant 0.9313901
car 0.9199403
car 0.98508066
car 0.93773764
person 0.93259853
person 0.9877956
person 0.95592606
person 0.9280647
person 0.9368127
person 0.9859941
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.318s + 0.035s (eta: 0:00:22)
person 0.9797982
car 0.9601207
person 0.95502365
person 0.9261883
tvmonitor 0.9263601
person 0.9059326
person 0.9340468
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.318s + 0.035s (eta: 0:00:19)
chair 0.90432644
person 0.97088754
person 0.9465536
train 0.95709246
train 0.95813346
bird 0.91127884
bird 0.9830566
person 0.9224578
person 0.92252517
bicycle 0.92052376
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.317s + 0.035s (eta: 0:00:15)
person 0.969964
person 0.95222336
person 0.92895144
person 0.92950183
person 0.9781757
person 0.98688936
person 0.9588928
person 0.97377586
person 0.98916304
person 0.9554444
person 0.9651657
car 0.9525307
person 0.96906
person 0.9079204
person 0.9503009
person 0.9771831
person 0.9345181
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.315s + 0.035s (eta: 0:00:11)
person 0.9157686
person 0.94565374
person 0.9241969
chair 0.97510785
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.317s + 0.035s (eta: 0:00:08)
person 0.9814316
person 0.9416908
person 0.9299078
person 0.96414655
person 0.9622059
person 0.9026224
person 0.92644304
person 0.9074278
person 0.9408305
person 0.9213438
person 0.97896636
person 0.9070752
person 0.90886474
boat 0.903143
person 0.9674355
person 0.92546326
person 0.9479188
person 0.91278636
person 0.95817
person 0.904167
person 0.92678005
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.318s + 0.035s (eta: 0:00:04)
person 0.9674693
person 0.97371316
person 0.9781049
person 0.9495094
person 0.9182034
person 0.93066764
person 0.92507124
person 0.95089936
person 0.9610244
person 0.91956294
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.317s + 0.035s (eta: 0:00:01)
person 0.9407606
person 0.97141075
person 0.92787325
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step8999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.479s + 0.034s (eta: 0:01:03)
person 0.9420272
chair 0.9195545
chair 0.9584386
person 0.95843244
person 0.92546743
person 0.9537075
person 0.94134927
diningtable 0.9318667
person 0.9745619
person 0.9719367
diningtable 0.9378128
person 0.9540186
person 0.9685544
person 0.9624495
person 0.9077748
diningtable 0.9115898
person 0.9319947
person 0.9791901
diningtable 0.96619076
diningtable 0.92962986
diningtable 0.94075584
chair 0.91541874
chair 0.9815656
person 0.90704834
person 0.93164617
person 0.9890587
person 0.9196366
person 0.9613696
person 0.9852447
person 0.9162164
person 0.9238308
person 0.98362654
person 0.9684009
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.350s + 0.037s (eta: 0:00:44)
person 0.93979716
person 0.95478606
pottedplant 0.93349147
person 0.90474796
bird 0.9250037
bird 0.9717727
bird 0.94850975
bird 0.90787107
person 0.9674145
person 0.92088777
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.344s + 0.036s (eta: 0:00:39)
person 0.95042866
person 0.91056657
person 0.96237224
person 0.9701808
person 0.9767139
person 0.9464797
person 0.90231776
bottle 0.9057356
bottle 0.9524084
bottle 0.90366894
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.343s + 0.036s (eta: 0:00:35)
person 0.91027683
person 0.91816634
diningtable 0.90755796
pottedplant 0.93117267
chair 0.93998927
chair 0.9160478
person 0.9757078
person 0.9415566
bottle 0.9097781
bottle 0.90926105
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.340s + 0.036s (eta: 0:00:31)
person 0.96247405
person 0.9607169
person 0.9478683
person 0.9947857
person 0.94277126
person 0.9405419
person 0.92052203
person 0.97950464
person 0.9129777
person 0.95817375
person 0.9735091
person 0.9158836
person 0.91614336
person 0.9347317
person 0.93902814
person 0.99201363
person 0.9397838
person 0.97448325
person 0.9253457
person 0.9611637
person 0.98383105
person 0.9665818
person 0.92751735
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.335s + 0.036s (eta: 0:00:27)
person 0.9481384
person 0.965008
person 0.9025977
person 0.9707333
person 0.90713894
diningtable 0.9841838
chair 0.9167535
chair 0.9479822
chair 0.9794072
chair 0.98744524
aeroplane 0.912275
person 0.96679896
person 0.98159695
person 0.93309593
person 0.91769534
person 0.9123939
person 0.94770503
person 0.9735903
car 0.92292345
car 0.9804143
car 0.9719077
car 0.95893365
bird 0.95847654
chair 0.9318372
chair 0.95091456
person 0.9456814
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.334s + 0.036s (eta: 0:00:23)
person 0.9841675
person 0.93358815
person 0.9093782
person 0.9813059
person 0.95847327
person 0.98960936
person 0.93715864
person 0.92699194
person 0.941008
person 0.9902886
person 0.93868256
person 0.9085216
horse 0.96250826
person 0.9128831
person 0.9053674
person 0.9197006
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.327s + 0.035s (eta: 0:00:19)
person 0.955486
person 0.9463359
chair 0.9750278
chair 0.9218757
chair 0.9568403
pottedplant 0.92483413
pottedplant 0.92249787
pottedplant 0.94563246
person 0.9486315
person 0.90399665
person 0.92200094
person 0.93802977
person 0.95932466
person 0.9001569
person 0.93820417
person 0.93246
person 0.90185016
person 0.90428215
person 0.9558585
bicycle 0.91380847
person 0.95534647
person 0.97985554
person 0.9029515
person 0.9429419
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.327s + 0.035s (eta: 0:00:15)
person 0.9880867
person 0.91813076
person 0.93215054
person 0.9166335
person 0.9593453
person 0.98545945
person 0.9401615
person 0.97689635
person 0.9780067
person 0.97185224
person 0.93719274
person 0.92297566
person 0.9170804
bus 0.9554565
person 0.9062594
person 0.9396002
person 0.9482965
person 0.96520954
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.326s + 0.035s (eta: 0:00:12)
person 0.9675744
person 0.9150177
person 0.9357098
chair 0.91267914
chair 0.9486212
tvmonitor 0.9017152
chair 0.9712408
chair 0.9300652
chair 0.9400353
person 0.9496834
person 0.97014755
person 0.95703983
person 0.9775069
person 0.9188665
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.323s + 0.035s (eta: 0:00:08)
motorbike 0.91550934
motorbike 0.9515079
person 0.90189594
person 0.97809064
horse 0.9556155
horse 0.901058
person 0.903852
person 0.9049696
bicycle 0.9589656
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.324s + 0.035s (eta: 0:00:05)
person 0.9247806
person 0.9590107
person 0.98106444
person 0.9493568
person 0.96460843
person 0.9843635
person 0.9509564
person 0.9568814
person 0.94344914
bird 0.90051395
chair 0.95957196
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.321s + 0.034s (eta: 0:00:01)
person 0.9068477
person 0.98655534
person 0.9857781
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 87.317s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1156  282  279 ...  985  983  986]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2210
INFO voc_eval.py: 171: [122 123  69 504  68 235 444 280 261 126 134 124 475 558 374 384  71 563
 535 449 561 556 502  72 565  70 372 546 469  84 130 132 562 266 573  87
 450 457 523 131 567 557  73 578 249  95 454 128  82 377 559 510 453 369
 370 289  76  89 284 135 125 514 537 548 448  90 290 236 461 241  74 555
 460 541 506 430 263 293 467 536 138 101  18 538 136 147 283 476 140 368
 511  42 459 381 281 239 240 534 233 513 237 312 273 375 400 333  78 414
 243 287 451 528 569  22 572 543 296 465 373 455 496 404  21  37 103 145
  24 539 142 509 420 274 505 428 105 508 575 127 551 259 158 452 379  40
 295 570 540 264 159 418 265 340 248 393 191 327 463 193 533  86 301 411
 146 564 200 516   2 579 431 550 464 272 234 576 133 211 401 250 143 242
 316 319 334 298 429 141 482 568 398  77 408 410 560 286 294 566 383  41
 544  19 512 254 456 471  43  81 292 547 288 244  32 308 423 530 527 144
  56 526 571 187 276 419 325 195 267 247 436 137 382 367  16 238 574 549
  96 521 421  91   5  85  88 129 517 300 104 552 102 531 397 139 258 270
 190 499 409  46 307 173 442  75 485  98  30 422 466 497 399 392 577  94
   1 182  20 522  25 246 260 474 503 285 433 257 371 481 201 174 518 395
 388  92  50 413  66  79 443 192 446 330 390 391 194 256  38 328 507 310
   0 356 275 157 188  99 171 271 184 529  64 320 416  34 553 458 162 181
 185 415 479 216 417 424 440 434  60  29  80 114 412  55   3  26 262 189
 183 480 119  61 322 245 297 178  36  47 196 376 520 432 445  17 386   8
  33  28 323 175 336 470 472 180 164 304 303 405 468  12 227 282 100 385
 151 545 402 113 217   6 305 309 299  15 494 407 500 161 435 163 345 278
 226  65 186 387 403 339 532  62 291 441 344 396 366 524  93  63 483 346
 350  23  35  31 110 542  27  51 495 306 152 219 165 326  11 120 205 166
 437 438 439 203 473 169  45 167 199 341 447 347 525 204 365 108  53 198
 406 277 352 225 389 515   7 160   4 253  59 202 111 252 209 477  14 427
 332 251 148 378  57 484 208 221 155 329 197 478 207 230 335 342 394 343
 355 107 337 364 519 380 179  83 462 311 302 121 149  67 176 354  52 170
  44   9 172  54 156 106  13 338  58 331 353 426 177 213 318 210  10 154
 112 351 153 168 109 501 255 218 279 348 313  49 212 321  48 315 314 425
 214 115 215 324 359 358 206 498 487 116  97 232 224 361 493 231 362 488
 223 363 220 490 360 580 317 118 489 357 486 228 268  39 491 492 150 117
 222 229 269 349 554]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3964
INFO voc_eval.py: 171: [ 644 1713 3239 ... 3477 3237 3232]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1929
INFO voc_eval.py: 171: [ 638  669  750 ...  122 2276 2280]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2387
INFO voc_eval.py: 171: [365 402 575 309  12  21 651 357 648 364 282 574 568 403 627 654 312 591
 331 545 373 603 288 668 311 141 479 548 826 657 589 688 756 322 825 320
  18 569 546 314 287 656 617 404 283 327 664 572 738 653  25 839 650 628
 723 358 669 329  22  16 659 666 406 289 702 376 367 515  20 366 552 827
 614 570 674 470 297 325 310  70 828 372 612 492 649 726 632 547 265 405
 285 677 315 499 292 217 836 587 590 324 658 371 514 255 737 831 330 655
  13 313 368 630 601 303 746 172 202  68 301 692 618 375 673 482 302 830
 680  43 611 731 675 582 551 318 493 360 472 576 175 114 489 573 690 712
 695 196 810 132 252 686 760 205 588  15 326 419 694 701 501 556 700 663
 211 176 526 604 670 586 571 711 260 247 671 328 615 471  19 594  31 398
 213 685 109 835 296 147 146 332 425 584 204 216 699 739 291 697 616 797
 257  50 764 143 795 170  17 155 683 162  80 772 369 613 495 166 319 684
 843 553 631 580 794 721 266  44 300 359 693 431 317 593 261  24 127 259
 152 549 400 661 662 387 467 237 742 678 469 577  69 198 855 634 600 323
 598 796 555 798 550 517 212 802 417 715  99 423 116 745 522 415 691 799
 809 316 361 667 370 443 497 607 239  34 583  79 652 374 321 264 793 101
 248 845 201 847 753 243 585 295 531 676 293 672 234 525 284 554 219  58
 167 182 442 113 177  45 333 498 294  81 840 679 689 480 581 635 592  23
 761 521 133 713 305 716 335 619 687 286 530 698 131 125 230  97 110 744
  96  14 491  57 665 646 473 390 112 609 262 597 842 334 458 578 242 682
 218 153 608 490 841 579  64  78 267 696 399 596 722 769 195  27 599  54
 206 355 130 595 115 445 759 397 223 832  95 606  36  87 822 197 298 743
 150 179 256 199 857 111 706 263 774 767 251 620 165   6 484 707 229  53
 610 773 528 494 602 481 782 500 605 823 250 418 485 790 156 527 770 171
 104 214 636 276 496 724 306 727 412 336 714 438 486 803 269 858 184 236
 388  42  32 268 215 346  59 741 681 660 516 137   1 728 299 768 290 241
 396 362 752 181 108   4  52 129 416 117 444 107 118 730 119 200 148 523
 749  49  73 850  91  46 185 750 144 193  98 806 100 755 789 703  86 461
 762 518  35 151 837 808 249 560 524 174 183 629 235 244 834 414 168 363
 420 529 389 224 829 859 504 149 145 811 757 488 413 238 180 253 270 520
 468  47 424 543 771 758 778 751 732 748 258  26  28 338  92 272 233 537
   2 203 465 154 421 128 754 173 466 824  65 804 192 800 729 460 852 851
 246 777 254 274 161 225 102  90 733 624 428 853 158 763  77 459 245  93
  30 126 740 429 505  29 462 439 345 422 273 401 240 220 849 539 791 519
  48 559 848 157 103 441 747  38 717 474 160 637 227 163 178  72 433 833
 446 647 633 164  85 142  75 775 483 540 538 159 718 121 354 383 304 807
 476 776   0 487 792 542 275 271  51 169   5  89 541 856 457 454 506  94
   8 478 106 140 725 558 719   3 567 191 621 278 378 805 350 532 380 139
 765 379 720 705 544  40  56  61 564 440 477 210 427 453 437  76 645 622
 464 475 563 846 566 507 801 349 337 138 535 534 561  82 348 280 735 780
 432  39 105 565 347 704 536 136 208 508 463 562 222 221 228 456 393 854
 426 435 844 411 436  41 838 533 784  62  33 783 277 640 281 353 626 642
 231  71 502  83 513  74 394 381 382 452 352 351 625 639 503 391  37 708
   9 434  63 307 623 279 736 384 308 781 226  11 344 189 819 818  67  88
 392 766 385  60 455 734 207 356 709 786 339 509 190 710 343  10 557 232
 395  66 377 120 342  55 209 512 430 787 449 409  84   7 450 134 644 816
 643 340 815 821 820 122 817 194 779 511 341 124 447 812 788 785 386 510
 188 813 186 123 408 638 407 135 410 814 187 641 448 451]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3380
INFO voc_eval.py: 171: [438 171  96  95 172 450  91  81 103 462  99  79  97 441 110 367  80 121
 180 402  26 106  78  93  83 210  45 410 394 415  89 109 261 209 389 249
 163  86  84 442 366 381  46 104  32 398 271 143 360 264 368 443 162  54
 291  92  53 395 197 405 188 362 164 233  87 260  90 483 156 334 118 427
 218  82  44 434 111 201 393 120 222 142 320  88  11 330 288 205 237 252
 353  48 258 196  13 199  85 117 309 335 265 195 281 119 336 470  30 340
 176 373 331 151 125 272 392 208 337 397 380 245 262  57 363 400 146 465
   8 139 417 251 355 351 187 247 212 206 377 333 319  24 316 379 177 145
 244 419 414 192 280 155 411 408 390 284 149 122 123 227 437 357 225 327
 228  49 282 433 257 378 200 361 424 463 255 312 242 359 295 376 168  31
 391 386 322 352 124 150 416  27 131 250 283 287 202 432 221  16 273 140
 100 422 371 129 356 445 254 211 223 328 226 293 421  35  52 198 354 266
   4 329 147 127 113 314  41 115 313 135 173 375 308 128 409 141 426 464
 105  59 472 126 114 482 248 292 420 215  98 161 358 469 138 385 403 204
 474 166 116 224 178 132  29 428  33  58 481 277 311 429  94 467   0 157
 169 399 148 383 101 160  47 270  36 232 473 203 108 388 243 275 342 256
 332 396 102 239 230   5 234 207 321 130   3 235  43 412 407 401 154 436
 475 246 343 418 456  68  37 153 238 259 369  55 317 435 370 213 181 454
 323 276  74 404 285  28 425  69 274 144 374 387 229 440 471 263 267 423
  60 184 167 170 158 324  40   7 476 372 444  10 217 439 253 477 112 133
 241 278 364 325 326 341   6 152 269 231 349 452 315 286   9  38 107 310
 406 268 194 318 338 446 453 457 466 290 413 174 165 289 303  17  42 384
 468   1 279  34 134  39 339 159 307 214 304 219   2 183 382 431  61  50
 306 297 191  22 365 430  56  63  75 189 236 447 216 458 301 240 298 455
  62 220  25  20 480 459 305 347 294 345 185 296 179  14 182 344  67 302
 448  64  72  19  23 136 451 479 299 300  18 478  76 175 186 190 137 460
 461 193  15  12 449  66  73  51  21  65  71  77  70 348 350 346]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4241
INFO voc_eval.py: 171: [1592  713 1526 ...  104  100  119]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2752
INFO voc_eval.py: 171: [ 29 264  64 108 156 314  65 109  19 118 287  24 315 313 265  66 155 256
 203 125 154 286 266  26 296 232 260 110 213 202 290  74  83 122  52  95
 288 233 249  94  48 242 133 117  50  16 263 137 102 144 115  69 171 173
 192 234 191 177 306 153 295  42  21  20  75 105 206  92  27  63  70 124
  17 219 227  96 174 292  80 116 113  18 121 166 201 255 294 217 243  22
 250 159  55  28 112 165  73 285  56 258 237 194  76  81 205 172 167  54
 241 215 190 130  91 158 291 257  23 216 152  51 235  85  86  82 293 197
 157 289 310 127 119 262 146 246 307  53  77 123 282 312 308 211  25 238
 195 151 150 226 220   5 193 184 145  47 236 224 268  88  68 239 240 161
 103 138  87  84 131 126 164 128 136  49 135 111 259 170 223 132 245 218
 309 114 271 284 140 107  44 311 225 279 222  45 317  93  14 147  10 139
 101  67 301 244 247 196  89 134 253 252 275  46  41 297 204  97 120 106
  90   8 251 183 104 214   4 207  35   0 100  43 230  39 280  15 143  30
  99 209 269 270 221   6 281 276  34   1   3 169 129 261 272  98 181 277
 316 283   9  60 248 168   2 175 199 231   7  36  38  59  31  12 160 198
 162 212 163 180 299 210  58  71 274 200 273 278 208 302 186 298 178  32
 305  33 304  78  62 228  57  13  79 300 148  61 141 185  72 303 229 267
 176 149 179  11 254 182 189  40  37 142 187 188]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0868
INFO voc_eval.py: 171: [5834 2510  697 ... 4489 4351 4352]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3064
INFO voc_eval.py: 171: [329 256 194 441  58 200 282 386 270 253  50 269 761 694 255 258 283 449
 252 411 447 753 640 266  71 330 408 218 725  77 259 607 193 333 195 444
 196 629 154 381 287 180 557 752  64  73 177  55  53 228 136 445 198 254
 241 278 442 226 201 446 126 203 375 605 755 440 273 264 558 368 563 438
  69 456 127 169 757 756 320 439 112  68 641 451 764 418 533  21 276   7
  60 219 267  33  65 758 608  70 402 192 737 559  67 384 650 625 722 455
 453 202 387 443 263 534 700   6 199 357 342 724 636 621 306 262 609 731
 261 148 454 537 760 452 274  49 699 552 268 514 396 715 197 144 338 392
 289 385 470 530 361 216 211 290 643 341 523 425 417 626  57 175 630 726
 410 649 568 275 719 727 631 730 260 627 115  10 769 227 322 513 565 344
 242 237 729 141 281 221 664 265 536 754 728 448 332  40 419 380 382 611
 349 409 181 125 619 659 723 331 437 176 560 420 184 146  52 214 610 309
 240 657 183 762  15 564 566 613 412  63 666 206 554 292 461  84 702 732
 336 642 208 561 644 540 721 555 288 427 695 179 173 150 718 217 124 284
   9 738 129 628 606 321  48 734 733 415 771 388  79 515 414 185 234  29
 314 105 422 647 257 347 493 230 233  30 178 345 407 383 416 132 128 151
 716 739 367 763 743 618 423  46 182 172 474 767 130 450 220 714 735 748
 426 516 661 765 529 504 421 155 413 244 243 162 556 571 142 313 360 749
 390 286  66 705 646 704 632 710  75 520 205 689  22 620 316  47 335 334
 312 271 158 224 174 364 323 713 279 311 337 398 615 538 358 424 677 742
 526 379 706 717 235 569 553 494 143 285 783 681 781 210 134  99 655 393
 277 168 106 104 280 272 223 156 679   8 109 506 369 395 535 339 110 133
 583 204 539  93 238 502 759 736 768 697 660 327 567  17 315 782 340  27
 400  31 479 766 496 157 293 518 291 118 363 432 547 310 239 662 249 562
 103 740 693 326  23 481 399 707  87  14 575 114 720 139 617 622 137 624
 113 648 503  13 751 457 318 159 497 403 676  42 378 389  24 779 577 570
  45 711 480 215 746 353 532 635 663 236 135 744 509 541  56 247 170 524
 638 225 741 773 784 207  54  34 750 531 772 100 152 614 745 489 495 747
 229  91  89 397 371 187 391 653   1 634 639 487 528 591 394 108 401 651
 319 460 212 588 317 692 153 517 678 131  26 478 623 101 462 686 603 578
 683 222  32 770  25 328 508 600 362  90  20 209 307 191  94 372  36 521
  59 138 251 107 119 687 324 102 213 587 111 232 140 703 171  11  19 546
 691 658  35 248  78 602 599 774 507 325  28  82 359 584 246 166  86 486
 160 665 669 484 519  62 673 123 585   3 688 680 165 365 231 637  76 675
   0 590 477 601 435 525 430 595 366 475 654 245 527 376 374 161 250  72
 544 522 145  51 433 685  12 667 674 147 370  92  38 576  41 589  37 189
 431 645 696 698 633  18 780 551 708 434 690 190 491 490 186  80 116 482
 167 350 492 684 121  16 458 499 149  83 671 656  81 593 592  61  85 483
  74 120 163 579 164 594 596 672 488 505 352  95  88 500 476 501 355 670
 612 485 550 303 377 652 668 308 586 429 604 468 597  43  39  44 188 466
 428 543 542 548 709  98 545 117 404 498 510 122 305 406 463 549 598 405
 346 298 297 464 295 473 701   4 354   2 304   5 436 616 459 467 469 300
 511 778 472 712 373 682 471 294 348 302 356 351 296 465 582 512 775 777
 299 301 343  96 580 572  97 776 581 574 573]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2079
INFO voc_eval.py: 171: [2890 1280 1334 ... 1887 2307 2794]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3276
INFO voc_eval.py: 171: [343 410 346 840 445 501 353 375  50 481 464 448 443 549 146 349 620 502
 408  64 442 161 769 541 144  46 278 506 149 159  51 512 204 522  42 376
 842 153 274 355 837 731 237 792  49 413 128 783 461 409  66 527  45 205
 151 765 147 793 764 475 268 142 830 164 516  41 344 466 781 845  63 810
  52 374 435 834 469 220  83 371 819 354 621 798  54 753 269 727 246 520
 543 766 640 163 838 224 547 162  69 550 578 225 463 483 767 236 534 505
 473  65 485 659 416 242 247 851 281 421 417 849 356 437 152 815 596 345
 712 260 239 462 145  55 841 822 165 638 668 524 199 223 470 449 839 207
 415  44 126 414 439 238 434 474 546  53 667 214 611  86 295 150 200 770
 277 603  62 519 335 614 772 251 653 324 657 814 656 813 728 817 350 700
 696  32 194 586 221 612  68 799 768 802 228 158 698 132 779 723 832 429
  70 450 788 752 156 601 280 548 440 258 645 754 263 513 217 761 197 379
 441 431 230 595 141 593 382 739 229 855 412 347 545 130 351 157 831 380
 215 411 433 208 206 148 226 419 701 631  47 143 806 795 777 372 424 166
 131 196 669 800 436 318 755 136 639 600 796 807 734 843 341 553 438 774
  60 270 627 847 127 250 738 272 425 617 231 160  57 829 219 584 422 293
 334 637 348 312 619 308 155 116 444 641 265 678 511 805 740 110 691 818
 112 794 261 662 479 198  67  43 571 287  16 858 266 447 775 154 844 488
 544 542 195 129 730 288  87 279 133 292 480 859 592 244 613  61 332 572
 670 243 689 467  92 283 282 518 302 715 676 816 762 803  90 135 123 352
  31 780 383 643 625  85 296 778 233 782 771 826  94 801 276 420 271 241
 729 234  58 716 184 122  59 139 245 597 718 273 665  25 825 427 763 570
 326 384 618 291 604 117 773 222  22 853 735 824 615 275 854 476 248  19
 675 674 381  89 579 232 717 607 850 671 359 661 567 664 301  15 182 111
 286 590 555 606 212 821 113 114 120 216 647 446  56 566 300  13 218 102
 428 616 605 776 552 249 340 459 529 252 364 373  33 724 695 515 741 697
 528 704  29 305 809 478 538 608 389 666 569 833 307 692  88 745 551 599
 314 646 856 808 253 103 677   2 313 342   5 418 267 262 720 721 554 556
 812 680 811 290 186 423 835 213 533 118 259 325 857 686 820 430 804 294
 672  91 725 852 823 836 642 477 719 235 846  84 517  93 682 679 401 407
 848 655 188 333 644 321  35 630 426 124  20 722 568 285 685 687 714 264
 187 633 509 628 105 134 370 327 361 573 119 629 358  71 732 588 583 658
 319 337   4 400 457 797 673 299 652 598 736 589 525 591  82  26  10 317
 581   6 602 860 298 257 181 175 726  95 507 316 360 336  48 634  28 303
   3 539 240 310 514 190 561 532 503 256 392 610  79 362 785   1 757 322
 681 330 594 289 484 789 254 115 626 297 306  80 486 211 106 560 742 492
 284 609 451 733 315 743 309 323 363 378 535 368  40 178  96 454   0  36
  14 121 632 622  77 786  23  27  39 582 140 489 137 787 395 663 465 168
 562 636 174 707 703 536 540 365 558 650  99 100 125 432 749 203 494 537
 339 453 452 747 472 791 651 557  18 635 311 405 482 138  34 304 367 169
 167  24 255 183  78 209 784 497 180 521 683 399 403 496 737 456 504 468
  74 402 530 404 510 366 746  97 176 104 471  98 790 357 491 192 706 648
 490 708  75 487 654 576  21 574  73  30 455 649  81 587 189 101 580 177
 577 320 460 202 531 585 559 744 179 387  76 694 185 193  72 493 699 108
 210 377  17 750 660 702 690 684 575 748  38 751  37 191 406 201 563 458
 109 394 565 693 523 328   7 388 508 495 526 623 498 713 369  12 385 758
 107 760 564 709 624 329 338 711 227  11 705 173   9 710 331 386   8 172
 171 170 398 759 500 499 756 396 393 397 827 391 390 828 688]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2092
INFO voc_eval.py: 171: [133 942 433 ... 566 765 766]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3074
INFO voc_eval.py: 171: [ 46 269 256 141 145  25  45 150  47  50 142 277 118  72  33 159  68  30
 263 143  59  78  55 187  56 168 116 264 258  73  15  52 161 244 196 202
 130 266 172 265 192  35 237  32 162 251 279 240  31 257  16  29  77 260
 274 146 246 147 117 149 280 278 155   6 153 189  51  84  61 166 212 115
  49  21 195  42 272  83  62 144 182 114 222  60  76 100  85 158 140 194
 201 261 239  88 109 262 281  19  34  10  98 228  66 238 209  22 215 185
 259 151 250 255  65  14 126  53 245 167  28  80  57  67  18 241 134 199
  70 113  79 148 163 165  96 125 242  37  24 183 236 227   4  23 217  27
   1 154  58  94 231 267   8  20 111 152  38  99 131  64 157 249 198  41
 156  75   5   9  91  71 268  17  12 138 218  95 106 137 214 275 205   0
  81 233 104 186  36  93 135 247 216 225 200  26 204 177 252 103 248  89
  69 184 133 164  97 110 139  86   3 102 179  48 190  11 211  13 173 101
 120  87 107 276 108 273  39 122 224 170   7  40  82 123  44 174  92 180
 221 176 208  54 232 230 112 193 171  74  90 169 124 105 207 181 197 175
 213 178 210 253 234 203 128 270 206 188   2  43 219 220 119 160 127 191
 226 271 223 229  63 243 129 121 136 235 254 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5020
INFO voc_eval.py: 171: [ 8619 14085  1070 ... 10955 10957   784]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5385
INFO voc_eval.py: 171: [2514  255  708 ... 1946 2702 1829]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4125
INFO voc_eval.py: 171: [ 66 307 534  34 329 543  67  56 316 128  40 537  50 538 326 336 308  44
 311 613 535 150  46  35 344 478 214 131 324 340 548 539 248 305 153 337
 332 549 542 122 403  37 134 644 277 409 635 299 341 540  71 141 615 361
 419 140  68 317 142 536 158 133 151 415 220 310 219 624 485 360 623  60
 218 490 561 262 619 405 315 312 621 154 330  52 120 625 609 557 479 213
 628 148 461 303 638 136 347 246 408 249 159 617 495 562 342  39 618 524
 212 551 620 225 412 331 160 227 646 127 322 489 126 411  45  90 135 498
 162 512 130 314 190 640  43 444 612 363 309 488 304 630 252 282  65 325
 362 261 228 129 642  62 161 247 306  95 216 493 223 647 366  91 572 418
 343 645 673 278 345 406 302 637 631 226 121 670 147 321 504 323 338 639
 138 139 333 477 566 404 558 633 250 416 156 300 132 224 505 238 563 430
 301 313 124 502 137 145 320 616 259 567 407 168 272 610 560 507 334 606
 521 636 257 632 318 125 627 276 402 335 194 671 258 215 634 319 359 681
 626 669 146 327 328 547 410 123 527 339  77 552  42 607 482 641 217  49
 629 350 491 296 577 195 480 544 595  15 260 152 102 101 500 460 483 263
 565 447 222 496 463 643 286  64 622 197 349   7 155 191 614 503  70 608
 373 439 559 196  28  55  36 188 492  25 462 674 109  38 266 346 149 374
 264 518 541 525 574 550 611 119 201 239 182 165 375 221 254 649 118 251
 189 487 352  94 348 292 351 192 236 365 476   0 176 255  93 163 193  33
 515  89 508 516  24 184 683  17  92  72 501 678 253 413  47 389 180 294
  96 648 355  23 117 157 672 380  53 464 256 372 169 514 438 517 553 265
 240 425 435 650  14 653 279 554  87 112 173 475 467 367 679   4 520 506
 441 293  48 429 570 108 208 432 106 555 424 243 178 446   1 556  78  21
  31  61  76 569 442 177 104 164  80 417 206  83 594 601 241 509 675  26
 458 291 680 484 459 431 546  16   2 599 181 285 472 203 434  18 368 510
  58 456 271 575 100 682 651 519 511 204 684 420 275 268 578 396 604 486
 596 587 207 473 545 400 573 676   8   9 105 244 205  57 111 234 564 522
  19 600 445 393 172  69 297 677  41 110  22 597 497 528 284  32 576 513
 474 270 107  20 440 481  74 571 466  75 499 274 281 179 652 273  51 468
  79 114  73 364 113 392 523 298 414 267 115   3 369 242 202 494 526 436
 421 280 598 437 593  88 426 388 283  54 655  59 470 427 443 582  63  27
 245  29 354 175 381 428 187  97  30   5 269 210 295  85 423 356 186 357
 116 370 399 592 198 394 589 422 353 211  13 659 585 433  86 580  81 605
 532 378 658 584  98 171  82 401 395 579 581  84 199 471 583   6  99 568
 469 230 588  10 358 656 530 174 209 590  11 668  12 661 384 586 376 231
 185 229 531 602 382 200 448 232 591 379 166 235 391 287 386 167 170 660
 237 183 233 603 452 290 390 465 657 144 667 529 377 383 387 385 665 397
 103 457 664 143 451 288 398 654 289 533 449 453 663 666 455 371 454 450
 662]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3156
INFO voc_eval.py: 171: [ 43   3  47 121 190 280 184  48 126 338  50   6 193 188 236  52 191 158
  13 161 334 194 159 156  60  42 282 152 187 182 340 233 127 279 143 223
 186 157 277 237 231 276 101  80 128 246 151 100  78 337  49 252 281   5
  58 124 104  59 153 163 257  10 162 328 175  23 232 185 147 120 308  57
 129 213 273 247 135  44 311 176 107 189 183  75 245 326  11  46 235 239
 234 166  72 216   4  99 160 327 272 103 132 192 323 119  96 321   8  12
 254  51 251 106 336  89 203 264 269 255  25 250 145 130  14 253 102  97
  45 122 332 261  73 209 256 289 212 154 238 105 125 339  15  91  83 170
 309 331  63 320 225 244  98  90  74 329  61 265 200  84  26 263 248 343
 114 196 325  27 270 341 117 314 149  64 221 228 294 230 295 249  79 134
 123   7 330 113  85   9 167 195 118 285 211  82 260 322 313 241 150 220
 307 271 116 259  76 131  95 305  77 324 315 155  55 133 299 303 335 224
  39 317 333 274  33  62 146 226 310 242  65 278 266 218 267 298  30 262
  24 342 201 217 171  88 112 214  28 297 290 219 177 240 205 174 316 110
 222  81 144 148 296 227  87  56  53  31 198  54 141  32   1  22 284 136
 258   2 306 291   0 268 229 197 210 300 202 169 208  86 283  18 109  29
 139 207 115 304 199 301 108 168 293  41 204 243 206 137 111 179 302 286
  35  37 164 138 178 172  19  21  20 173  17 319 275 142  71  16 180  36
 140  40 292 288 181  69 165 287 312  34 318  70  67  38  66  68  92 215
  94  93]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1308
INFO voc_eval.py: 171: [382 641 636 115 325  80 207 396 293 121  76 404 168  78 896 643 639 294
 655 327 324  83 181 117 398 320 806 125  89 212  92 210 295 803 505  81
 208 536 637 389 383 188 642 453 656 452 300  97 206 119 455 116 326 187
 197 904 910  84 646 898 306 342 296 176 812 835 510 710 464  86 541 523
 845 388 881 854 907 403 165 216 135 134 840 218 908 640 390 526 334 544
 299  43  63 869 456 488 329 434 850  98 112 171 731 385  18 217 748 136
 195 330 315 312 680 328 192 205 911 882 711 901 662 457 810 836 804 105
 909 196 508 173 166 261 668 530 169 280 308 838 871 439 182 220 732 443
 438 912  45 759 811  93 304  19 230 509 822  70 399 623 199 167 883 899
 553 540 332 755 525 879 720 830 839 178 892 743 209 185 310 172 164 844
 713 916  33 738 897 194 552  67  44 888  82 128  65 214 462 244 900 242
 458  74 101 841 213 511 790 487 787 448 820 249 454 227 461 832 709 319
 543 873 546  58  85 184 807 222 263 872 221   2 451 395  53 638 716 375
 706 853 384 719 338  91  94 903 179 180 712  16 264 259 702 499 317 298
 689 685 527 400 763 579  59 602 507 891  17 177 928 880 437 158 401 198
 331 855 204 922 483 870 318 590 408 466 323 303 133 914 109 433 788  99
 758 459 378 356 514 740 875 440 144 777  52 103 819 402 170 485 678  72
 629 518 913 393 465 234 233 890 376 607  32  38 262 825 494 467 236 906
 175  41 688 878 350 927 200 435 834 277  73   5 251 360 542 681 506 290
 764 444  95 884 745 154  71  35 823 143 868 915  42 852 254 432 359 609
 692 865 113 273 529 686 805 463 905 524 100 561 211 241 469 122 664 111
 616 301 174 124 548   7 380 157 186 123 828 228  57 620 574 322 817 861
 784 519 252 247 531 250  75 826 648 162 163 477  96 893 761 545 191 687
 859 554 833 496 535 729 921   8 445 201 137 894 358  46 747 749 829 606
 572 425 886 183 837 215 734 357 634 889  55 110 843 690 266 333  37 600
 902 489 260 831 256  87 238 311 307 237 239 741 265 851 624 219 468 528
 309 697 226 771 760 564 161 405 816 314 515 728 406 874 460 877 683 202
 858 442 876 436  56 335  36  40 701 895 576 614 931 274 190 341 780 603
 394 677 441 473 102   0 929 730 153 651 589 229 566 849  61  39  88 605
 601 622 106 337 604 556 632 386 708 795 762 193 520 351 423 621 717 724
  13  34 733  60  48 379 547 397 800 283  79 391 794 610 751 682 114 189
 138 340 827 281 288 887 567 497 501 431 930 349 742 130 578 679 626 493
 577 362 575 775 517 297 691 560 684 842 569 537 774 235 269 316 377 723
 417 240 155 661 750 243 282 776   6 857  62 647 257 353 796 813 424 824
 791 587 789 279 778 793 797 275   4  54 860 593 302 555 735 246 608 354
 714 707 613 746 203 478 704 363 586 722 348 580  31 792 756 450 715 446
  66 422 245 321 479 286 848 381 752 635 612  30 846 276  47 108 426 500
 387 313 392 107 278 700 676 815 581 885 696 369 484 703 773 611 345 490
 104 559 150 270  77  21 284 754 573 513  68  90 627 248 617  29 550 917
 476 368  15  69 305 814  20 271 571 407   9 786 757 847 592 344 551 584
 419 565 516 474 253 347 371 924  22 821 801 231 447 532 289 798 725  14
 582 588 287 753 654 285   3 471 232 258 557 255 558 670 420 570 781 705
 866 492 568 744 782 726 361 449  64 367 615 785 736 625 502 923 808 737
 818 491 739 267 503 809 470 533 159 504 595  51 698  10 142 414 268 549
 413 475 539 521 699 727 412 472 343 802 495 583 370 152  49 429 482 918
  12 498 486   1 374 131 598 594 291 718  11 671 373 522 428 591 783 675
 619 799 721 779 512 772 151 856 538 663 867 597 355 481 657 480 534 292
 644 372 630 585 427 695 366 336 430 364 596 421 410 365 156 770 628 645
 416  28 132 415  50 160 147 769 339 925 633 140 862 649 418 563 346 409
 920 562 658 118 223 631 660 926  27 618 599 863 673 139 145  23 766 694
 765 146 767 411  26 864 768 225 352 672 149 127 693 665 224 272 129 120
  25 141 653 652 126 148 674 650 666  24 919 669 667 659]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2996
INFO voc_eval.py: 171: [417 250 590 174 430 295 136 438  24 464  37   1 473 493 254 318 348 470
 475 320  39 392 434 346 252 126 415 248 347 465 124 571 307 296 349 545
 606  57  68 418 253 467 531 321 107  71 129 229  32 435 180 591 441 516
   3  17 325 472 427 182 337 314 301 345  43 317 594 614 579 154 624 225
 355 102 609 365 373 602 260 368 437 150 305 218 440 532 109 110 469 204
 319  50 137 256 363  19 207  73  70 481 432 221  29 299 596 143 461 303
   8 160 203 503 551 379 468 529 322 361 375 231 271 480 103 612 459 358
 356 329 364   9 391 390  40 120 244  58 629 162 186  16 161  25 618 212
  56  46 479 477 597  44 178 128  26 588 424 298 404  84 510 206 311 422
 297 258   6 471 572 285 501 304 342 538 302  41 312  53  72  30 222 498
 476 210 335 370 238 119 101 228 179 564 515 466  48 145 554 326 211 309
 555 214 208 429 474 593 619 423 376 315 395 213  38 263 172  90 175 616
 125 628   2 188 131 158 117 191 608 398 173 209 381 227 547 121   5 574
 575 115  75 306  89 431 354  93 352 511 362 106 184 543 578 403 332 478
  45  11 269 595 544 622 491  91 300 393  18 123 607 451 558 548 176 279
 412 290 617 396 339  23 556 449 610 205 112 436 541 100  12 308 130  87
 526 512 171  55  33 625 495 282 316 198 153 397 519 534 536 159 496 549
 613 224 557 533 234 605 410 274 359 580 276  98 357 522 283 530 278 484
 620 360 592 626 492 621 331 215 604 111 114 450  94 497 190 144 623 567
  95 502 611 383 494 237 563 148 217 406 377 202 500 277 330 514  85 313
 147 257 517  54 615 382 310 216  82  78 367  92  28 280 537 155 366 219
 539 181  63 138 447 272  42 561 116 535 583 141 453 513 351 369 581 416
 499  61 540  77 528 127 273 104  76 444 118 632 255 374  79 284 266 448
 507 566 425 372  49  36 378 223 166 350 353 433 419 380  34  81 371 562
 442  60 327 241 133 200 323 275  52   4  31 452 627 259 631 585 108 267
  51 157 446 586 336 387 281 520 334 149 142 426 328 460  15 587  35 286
 146 242  80 428 105  10 270  59 289 338 570  69 462 165 288 394   7 546
 122 151 482  64 226 630 113 139 156 542 603 582 408 483 527  22 553 560
 132 420 525 197 140 189 235 457 185 293 421 443 183 524 389 152 402  86
 239 196  27 135 240 552 523 405 134 569 506 324 292  83 407 439  88 401
 233  96 220 261 294 168 167 559 199 445  67 400  74 177 249 518 291  97
 340 236 164 385 230 333 409 262 341 251 550 170  66 521 386 577 388 163
 508 343 565 584 399 187 568 384 411 504  62 268 232  21 243  99  65 456
   0 505 509 287 169 344 576 601 486 600  20 455 247 598 454 246 599 265
 489 458  47 413 193 485 264 195 589 201 414 245 490 573 487  13 194 192
 463 488  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3267
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3029
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.221
INFO cross_voc_dataset_evaluator.py: 134: 0.396
INFO cross_voc_dataset_evaluator.py: 134: 0.193
INFO cross_voc_dataset_evaluator.py: 134: 0.239
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.424
INFO cross_voc_dataset_evaluator.py: 134: 0.275
INFO cross_voc_dataset_evaluator.py: 134: 0.087
INFO cross_voc_dataset_evaluator.py: 134: 0.306
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.209
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.502
INFO cross_voc_dataset_evaluator.py: 134: 0.538
INFO cross_voc_dataset_evaluator.py: 134: 0.412
INFO cross_voc_dataset_evaluator.py: 134: 0.316
INFO cross_voc_dataset_evaluator.py: 134: 0.131
INFO cross_voc_dataset_evaluator.py: 134: 0.300
INFO cross_voc_dataset_evaluator.py: 134: 0.327
INFO cross_voc_dataset_evaluator.py: 135: 0.303
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 9499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.459s + 0.039s (eta: 0:01:01)
person 0.9705721
person 0.9638955
person 0.99243116
person 0.9609949
person 0.9757496
person 0.93975526
person 0.9826788
person 0.9458348
person 0.91178805
person 0.9816558
person 0.93882114
bottle 0.9217535
bottle 0.91932845
bird 0.95101494
person 0.982554
person 0.91828984
person 0.9449561
person 0.90285313
person 0.90481365
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.330s + 0.043s (eta: 0:00:42)
person 0.94492155
person 0.9210288
person 0.96102935
person 0.9486358
person 0.9806004
person 0.9481881
person 0.96745604
person 0.9819219
person 0.93975824
person 0.9216996
person 0.92574054
person 0.93180066
person 0.9063062
person 0.9229873
person 0.9210228
person 0.92714614
person 0.97580314
person 0.91366184
person 0.98000747
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.330s + 0.040s (eta: 0:00:38)
person 0.92889446
person 0.9120953
person 0.95689154
chair 0.93819565
person 0.9724965
person 0.9162008
person 0.95770895
person 0.966888
chair 0.9656034
chair 0.93702745
chair 0.91495496
person 0.90504825
person 0.9791229
person 0.9879457
person 0.9276599
person 0.97719145
person 0.9915882
person 0.9813359
person 0.95205003
person 0.91073006
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.328s + 0.040s (eta: 0:00:34)
bird 0.94006807
person 0.9278245
person 0.92126876
person 0.9938368
person 0.9572405
person 0.9743033
person 0.97969717
person 0.9073141
person 0.95962554
person 0.9660759
person 0.9038653
person 0.9190299
person 0.96955264
person 0.9410204
person 0.9584072
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.332s + 0.040s (eta: 0:00:31)
diningtable 0.9029433
diningtable 0.93858266
chair 0.96391845
chair 0.9659492
chair 0.9381472
chair 0.95763457
pottedplant 0.9180829
pottedplant 0.9143238
pottedplant 0.9111104
pottedplant 0.90442276
pottedplant 0.94370013
pottedplant 0.93843746
pottedplant 0.928406
person 0.9658027
person 0.9829044
person 0.920313
person 0.9798543
person 0.9490096
person 0.98911756
person 0.9003505
person 0.9158
person 0.90403044
person 0.91197693
person 0.9223665
person 0.92268324
person 0.9740632
person 0.9551666
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.332s + 0.040s (eta: 0:00:27)
person 0.95815545
person 0.9239949
person 0.90733343
person 0.9196225
person 0.94808877
person 0.91514343
person 0.95361036
person 0.9369384
person 0.92989296
bird 0.922773
person 0.93771875
person 0.9876731
person 0.9169916
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.327s + 0.038s (eta: 0:00:23)
pottedplant 0.9027575
person 0.96246666
person 0.9340436
person 0.9131293
person 0.9538678
person 0.91912395
person 0.9586917
diningtable 0.9111675
chair 0.9826105
diningtable 0.9620834
chair 0.91611016
chair 0.94863987
chair 0.9807942
chair 0.92638594
bird 0.9930328
person 0.90283895
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.325s + 0.038s (eta: 0:00:19)
car 0.92877555
car 0.94247276
car 0.9154108
person 0.9884139
person 0.9005391
person 0.97415394
person 0.92991644
horse 0.9737453
horse 0.9135032
horse 0.9209363
person 0.938041
person 0.9533901
person 0.96815467
person 0.9186652
person 0.9432084
car 0.9459227
car 0.96402466
bicycle 0.96012187
bicycle 0.9310642
person 0.9369437
person 0.91807306
person 0.97359663
chair 0.9187364
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.323s + 0.037s (eta: 0:00:15)
person 0.9742104
person 0.95915484
person 0.973841
person 0.9178584
person 0.9133639
person 0.9018803
person 0.9886049
person 0.9863746
person 0.94360036
person 0.97115827
person 0.9154925
person 0.9639661
person 0.99118537
person 0.9089956
person 0.9665926
person 0.9119523
person 0.9879078
person 0.9798998
pottedplant 0.9115721
person 0.9898656
bus 0.9127482
person 0.9612762
bus 0.9065642
person 0.9415753
person 0.9548242
chair 0.94702196
person 0.9055131
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.327s + 0.037s (eta: 0:00:12)
motorbike 0.96321017
person 0.9749255
person 0.9105894
person 0.9159385
person 0.9571026
person 0.96772736
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.324s + 0.037s (eta: 0:00:08)
person 0.90693104
boat 0.98204434
boat 0.9483192
boat 0.9185529
person 0.9903887
person 0.9375469
person 0.9854968
person 0.91831607
person 0.9177862
person 0.9890976
person 0.9749253
person 0.91079456
person 0.975317
person 0.9767754
person 0.9140222
person 0.94845784
car 0.9506096
person 0.940741
person 0.9472824
person 0.9598501
person 0.98682576
person 0.90829927
person 0.92133725
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.326s + 0.037s (eta: 0:00:05)
diningtable 0.9599156
chair 0.92499995
chair 0.9738477
chair 0.9807036
chair 0.9155932
chair 0.98121303
chair 0.90194345
chair 0.9368304
chair 0.95996696
person 0.9428707
bird 0.92408544
bird 0.9157448
boat 0.9198103
person 0.96214885
person 0.92947245
person 0.9888433
person 0.914985
person 0.90381587
person 0.9137535
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.323s + 0.036s (eta: 0:00:01)
person 0.9835081
person 0.96406364
person 0.9564531
person 0.95549697
person 0.9490913
person 0.9662375
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.484s + 0.026s (eta: 0:01:03)
person 0.9821322
person 0.919414
person 0.97899216
person 0.9771457
person 0.914293
bicycle 0.9724096
person 0.93216294
person 0.91530216
bicycle 0.9747858
person 0.93617165
bird 0.9436731
bird 0.90089625
pottedplant 0.94303685
person 0.9684668
person 0.92901564
person 0.9023059
person 0.96110773
person 0.9661732
person 0.98692584
person 0.92684346
person 0.96607834
person 0.95276153
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.333s + 0.040s (eta: 0:00:42)
person 0.9126949
person 0.90646887
person 0.96404094
person 0.917923
person 0.97405064
person 0.9430423
person 0.93596876
person 0.98089874
person 0.9644243
chair 0.92524713
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.330s + 0.036s (eta: 0:00:38)
car 0.91584116
car 0.95092094
person 0.94709694
person 0.9615979
person 0.98175347
person 0.92499393
person 0.94922245
person 0.9650353
car 0.9823714
cow 0.92354804
person 0.91533595
person 0.95784736
person 0.9234237
person 0.9134281
person 0.98120946
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.324s + 0.037s (eta: 0:00:33)
person 0.9783409
chair 0.93479604
person 0.93037945
person 0.9252596
person 0.9515074
person 0.9536382
person 0.9331986
person 0.91813165
person 0.94043684
person 0.98070174
person 0.9869975
person 0.97719073
person 0.96915483
person 0.97753716
person 0.9055915
bird 0.9045314
bird 0.9908285
person 0.92212176
bird 0.9517138
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.321s + 0.037s (eta: 0:00:30)
person 0.9463721
person 0.9712496
person 0.98272806
person 0.92195666
person 0.9484358
car 0.9506096
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.326s + 0.038s (eta: 0:00:26)
bird 0.91004544
bird 0.95958024
bird 0.91362226
person 0.9150308
person 0.94445235
person 0.9188939
person 0.91512334
person 0.9884371
diningtable 0.94870776
person 0.9723971
diningtable 0.9790773
person 0.93979055
person 0.93615603
chair 0.9430995
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.329s + 0.037s (eta: 0:00:23)
person 0.9161624
person 0.90764767
person 0.9314766
person 0.94945955
person 0.9859277
bus 0.9463911
dog 0.95254505
diningtable 0.97388715
diningtable 0.92422396
chair 0.9126993
chair 0.9850857
chair 0.91121006
chair 0.9712539
chair 0.9359665
pottedplant 0.90182465
person 0.9043043
person 0.90126485
person 0.9765619
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.326s + 0.037s (eta: 0:00:19)
diningtable 0.921116
chair 0.9117101
person 0.9483322
person 0.966724
person 0.9561053
person 0.9371851
person 0.9451916
bird 0.95428836
person 0.92628926
person 0.9623567
person 0.96418476
person 0.9324655
person 0.98035014
person 0.91015655
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.323s + 0.037s (eta: 0:00:15)
person 0.96289176
person 0.9005354
person 0.9639981
person 0.9176471
person 0.9033847
train 0.9402816
person 0.95714736
person 0.96217465
person 0.94631475
person 0.9403531
person 0.9152777
person 0.9295147
person 0.9318082
person 0.9053266
person 0.9564406
person 0.9266225
person 0.9925916
person 0.9740327
person 0.9597963
person 0.9015965
person 0.9176814
person 0.9275
person 0.90338993
person 0.93121105
tvmonitor 0.9177046
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.323s + 0.037s (eta: 0:00:12)
person 0.96427643
person 0.9067498
person 0.94482785
horse 0.96011114
person 0.9668319
person 0.9310321
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.322s + 0.037s (eta: 0:00:08)
person 0.9752635
person 0.9652281
person 0.92214
person 0.91973233
person 0.95257
person 0.92842185
person 0.9363913
person 0.90338945
person 0.9160396
person 0.9867842
person 0.9467474
person 0.9001504
chair 0.97255874
person 0.9562142
person 0.9072789
person 0.9441525
person 0.9586858
person 0.90107083
person 0.9056886
person 0.9188967
car 0.94497776
car 0.9183185
car 0.9522486
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.319s + 0.036s (eta: 0:00:04)
person 0.9674471
person 0.9440801
person 0.9627208
person 0.9489725
person 0.9743759
person 0.9610309
person 0.9223725
person 0.90046453
person 0.9076688
person 0.92881596
person 0.90020305
person 0.98819876
person 0.98143077
person 0.9163521
person 0.9783959
person 0.92825633
person 0.93841624
diningtable 0.9337109
person 0.90135384
person 0.91798466
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.313s + 0.036s (eta: 0:00:01)
person 0.9383597
person 0.9083419
diningtable 0.91014063
person 0.97097313
person 0.94085526
person 0.98728335
person 0.96541524
person 0.9582094
person 0.9597336
person 0.9087029
person 0.91120756
person 0.9181801
person 0.9409308
person 0.9630609
person 0.9807951
person 0.9112264
person 0.94946426
person 0.9564716
person 0.9561829
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.458s + 0.028s (eta: 0:01:00)
diningtable 0.9021292
diningtable 0.92808276
person 0.960067
person 0.97618175
diningtable 0.97366184
person 0.93715423
person 0.9184856
person 0.90847164
bottle 0.9067166
person 0.97815037
pottedplant 0.91750443
diningtable 0.9287859
person 0.96598244
diningtable 0.9069689
chair 0.9493693
chair 0.96600515
chair 0.9229073
person 0.93158853
chair 0.9014699
person 0.90746146
person 0.93306124
person 0.99565387
person 0.98083156
person 0.934667
person 0.9662743
person 0.9654345
person 0.90485376
person 0.9707742
person 0.9601661
person 0.9390828
person 0.9409248
person 0.9655754
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.322s + 0.036s (eta: 0:00:40)
person 0.9474479
person 0.9859489
person 0.93361944
person 0.9709072
bottle 0.9368397
person 0.9371608
person 0.94418746
person 0.9538568
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.332s + 0.034s (eta: 0:00:38)
chair 0.93164384
bottle 0.909288
boat 0.9010799
person 0.97295624
person 0.93997043
person 0.9337907
person 0.9327324
person 0.9326751
person 0.9685932
person 0.9443834
person 0.90157527
bottle 0.9081152
bottle 0.95353675
person 0.98219246
person 0.95266783
person 0.9493408
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.335s + 0.035s (eta: 0:00:34)
person 0.96689814
person 0.9217626
bird 0.91687983
person 0.95378417
person 0.9068699
person 0.9681285
person 0.9706434
person 0.95996535
person 0.9037557
person 0.94994736
person 0.9083508
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.331s + 0.035s (eta: 0:00:30)
person 0.96960294
person 0.9692021
person 0.9532858
person 0.9070363
person 0.92773414
person 0.97073406
person 0.9207182
person 0.94102263
person 0.9144363
person 0.919998
person 0.9370218
person 0.96993935
person 0.9013422
person 0.95359993
person 0.9247786
person 0.9056103
bottle 0.95365816
chair 0.9311326
boat 0.90407413
person 0.97333276
person 0.9553297
person 0.90528274
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.331s + 0.035s (eta: 0:00:27)
car 0.9808234
car 0.9349802
person 0.92629296
person 0.977417
person 0.90992326
person 0.9114434
person 0.92073375
person 0.9353478
person 0.9650382
person 0.92320955
person 0.91810995
person 0.95756507
person 0.9184891
pottedplant 0.9140348
pottedplant 0.9313806
car 0.9199244
car 0.98508483
car 0.93773365
person 0.9326283
person 0.9877985
person 0.9559605
person 0.9281453
person 0.9368185
person 0.985999
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.328s + 0.035s (eta: 0:00:23)
person 0.9798021
car 0.9601274
person 0.9550367
person 0.9262233
tvmonitor 0.92636997
person 0.9059764
person 0.9340708
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.327s + 0.036s (eta: 0:00:19)
chair 0.9043237
person 0.9708985
person 0.9465986
train 0.95709985
train 0.95813465
bird 0.91129184
bird 0.9830619
person 0.9224872
person 0.9225437
bicycle 0.9204932
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.323s + 0.035s (eta: 0:00:15)
person 0.96998537
person 0.9522446
person 0.9289573
person 0.9295201
person 0.9781774
person 0.9868972
person 0.95891064
person 0.97378427
person 0.9891643
person 0.95545965
person 0.9651694
car 0.9525456
person 0.9690708
person 0.9078067
person 0.95031434
person 0.97718835
person 0.9345542
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.321s + 0.035s (eta: 0:00:12)
person 0.9157939
person 0.94567114
person 0.9242255
chair 0.97510684
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.321s + 0.035s (eta: 0:00:08)
person 0.9814386
person 0.9417295
person 0.929952
person 0.9641663
person 0.96221215
person 0.9026414
person 0.92645735
person 0.9074486
person 0.9408698
person 0.92137295
person 0.97897243
person 0.9071163
person 0.90890235
boat 0.90313315
person 0.9674508
person 0.9255031
person 0.9479479
person 0.91283035
person 0.95818
person 0.90420574
person 0.9268148
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.321s + 0.035s (eta: 0:00:04)
person 0.9674781
person 0.9737258
person 0.9781105
person 0.94953287
person 0.91821575
person 0.9306785
person 0.9250943
person 0.95092213
person 0.961044
person 0.919606
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.321s + 0.035s (eta: 0:00:01)
person 0.940782
person 0.9714174
person 0.92795724
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.478s + 0.030s (eta: 0:01:02)
person 0.9420522
chair 0.91956466
chair 0.9584481
person 0.9584423
person 0.9254712
person 0.95371944
person 0.9413779
diningtable 0.9318784
person 0.9745734
person 0.9719529
diningtable 0.9378388
person 0.9540468
person 0.96856904
person 0.9624694
person 0.9078242
diningtable 0.9115774
person 0.93204176
person 0.979198
diningtable 0.96620005
diningtable 0.92964995
diningtable 0.9407663
chair 0.9154414
chair 0.98157173
person 0.90706503
person 0.9316843
person 0.98906124
person 0.9196143
person 0.9613735
person 0.98524874
person 0.9162247
person 0.92384535
person 0.98363113
person 0.9684128
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.298s + 0.033s (eta: 0:00:37)
person 0.93981737
person 0.9548158
pottedplant 0.93349344
person 0.9047882
bird 0.9250123
bird 0.97177595
bird 0.94850934
bird 0.90785486
person 0.96742177
person 0.9209002
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.317s + 0.034s (eta: 0:00:36)
person 0.95044136
person 0.9106038
person 0.96238416
person 0.97019535
person 0.9767293
person 0.9464745
person 0.9023787
bottle 0.9057545
bottle 0.95242065
bottle 0.9036955
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.322s + 0.036s (eta: 0:00:33)
person 0.9103284
person 0.9182248
diningtable 0.90753734
pottedplant 0.9311656
chair 0.9399961
chair 0.9160518
person 0.9757101
person 0.9415927
bottle 0.90977573
bottle 0.9092746
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.330s + 0.037s (eta: 0:00:30)
person 0.9624843
person 0.9607306
person 0.9478802
person 0.99478674
person 0.9427887
person 0.9405482
person 0.920538
person 0.9795114
person 0.9130176
person 0.9581905
person 0.97352487
person 0.9159384
person 0.91617876
person 0.9347402
person 0.93906313
person 0.99201536
person 0.9397867
person 0.97450054
person 0.9253447
person 0.9611663
person 0.98383707
person 0.9666062
person 0.9275393
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.325s + 0.037s (eta: 0:00:26)
person 0.94814986
person 0.9650285
person 0.9026199
person 0.9707345
person 0.90720344
diningtable 0.98418844
chair 0.91676855
chair 0.94798654
chair 0.97941613
chair 0.98744607
aeroplane 0.91226107
person 0.9668109
person 0.98160523
person 0.93313164
person 0.9177402
person 0.9124329
person 0.94772655
person 0.9735961
car 0.9228543
car 0.9804176
car 0.9719091
car 0.95894235
bird 0.9584883
chair 0.9318552
chair 0.95092016
person 0.94570655
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.323s + 0.037s (eta: 0:00:23)
person 0.9841761
person 0.9335809
person 0.90943927
person 0.98130625
person 0.958485
person 0.9896124
person 0.9371836
person 0.9270272
person 0.941024
person 0.9902924
person 0.9387064
person 0.90857387
horse 0.9625053
person 0.91290843
person 0.90539014
person 0.9197223
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.318s + 0.036s (eta: 0:00:19)
person 0.9555132
person 0.946366
chair 0.97502524
chair 0.92189306
chair 0.9568289
pottedplant 0.9248221
pottedplant 0.9225093
pottedplant 0.9456457
person 0.9486471
person 0.90404963
person 0.9220269
person 0.93806535
person 0.9593426
person 0.9002156
person 0.9382067
person 0.9324913
person 0.9018802
person 0.90432954
person 0.95588034
bicycle 0.91378087
person 0.9553537
person 0.97986376
person 0.9029928
person 0.94298404
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.320s + 0.036s (eta: 0:00:15)
person 0.98808974
person 0.91813725
person 0.93216634
person 0.91665685
person 0.9593598
person 0.98546743
person 0.9401657
person 0.9769011
person 0.9780146
person 0.9718616
person 0.9371987
person 0.923023
person 0.91712785
bus 0.95546097
person 0.90629154
person 0.93961895
person 0.9483244
person 0.9652273
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.320s + 0.036s (eta: 0:00:12)
person 0.967574
person 0.91505915
person 0.93573785
chair 0.9126711
chair 0.948621
tvmonitor 0.90171665
chair 0.9712464
chair 0.9300319
chair 0.94002837
person 0.94971937
person 0.9701661
person 0.95705783
person 0.9775164
person 0.9188661
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.318s + 0.036s (eta: 0:00:08)
motorbike 0.9154959
motorbike 0.95150167
person 0.90194046
person 0.9780912
horse 0.9555979
horse 0.90104973
person 0.90392303
person 0.90502
bicycle 0.9589724
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.317s + 0.035s (eta: 0:00:04)
person 0.92480606
person 0.9590231
person 0.98107237
person 0.9493742
person 0.96462446
person 0.98437226
person 0.95099545
person 0.9569113
person 0.94347584
bird 0.9005555
chair 0.9595663
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.317s + 0.036s (eta: 0:00:01)
person 0.90689313
person 0.98656243
person 0.9857824
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 86.643s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1156  282  279 ...  985  983  986]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2210
INFO voc_eval.py: 171: [122 123  69 504  68 235 444 280 126 261 134 124 475 558 374 384  71 563
 535 449 561 556 502  72 565  70 372 546 469  84 130 132 562 266 573  87
 450 457 523 131 567 557  73 578 249  95 454 128  82 377 559 510 453 369
 370 289  76  89 284 135 125 514 537 548 448  90 290 236 241 461  74 555
 460 541 506 430 263 293 467 536 138 101  18 538 136 147 283 476 140 368
 511  42 459 381 281 239 240 534 233 513 237 312 375 273 400 333  78 414
 243 287 451 528 569  22 572 543 296 465 373 455 496 404  21  37 103 145
  24 539 142 509 420 274 505 428 105 508 575 127 551 259 158 452 379  40
 295 570 540 264 159 418 265 340 248 393 191 327 463 193 533  86 301 411
 146 564 200 516   2 579 431 550 464 272 234 576 133 211 401 250 143 242
 316 319 334 298 429 141 482 568 398 408  77 410 560 286 294 566 383  41
 544  19 512 456 254 471  81  43 547 292 288  32 308 244 423  56 530 527
 144 526 571 187 276 419 325 195 267 247 436 382 137  16 367 238 574 549
 521  96 421  91   5  85  88 517 129 300 104 552 102 531 397 139 499 270
 258 190 409  46 307 173 442  75 485 422  98  30 466 497 392 399 577  94
   1 182  20 522  25 260 246 474 503 285 433 257 371 481 174 201 518 395
 388  92  50 413  66  79 443 192 330 446 390 391 194 256  38 328 507 310
   0 356 275 157 188  99 171 271 184 529  64 320 416  34 553 458 162 181
 185 415 479 216 417 424 440 434  60  29  80 114 412  55   3  26 262 189
 183 480 119  61 322 245 297 178  36  47 196 376 520 432 445  17 386   8
  33  28 323 175 336 470 472 180 164 304 303 405 468  12 227 282 100 385
 151 545 402 113 217   6 305 309 299  15 494 407 500 161 435 163 345 278
 226  65 186 387 403 339 532  62 291 441 344 396 366 524  93  63 483 346
 350  23  35  31 110 542  27  51 495 306 152 120 219  11 326 165 166 205
 437 438 199 203 439  45 341 169 473 347 447 525 167 204 365 108  53 198
 406 225   7 515 352 277 389 160   4  59 253 477 202 252 209 111 427 221
  14 251 332  57 208 484 378 155 148 478 197 329 207 230 335 342 343 355
 337 107 394 519 364 179 380 302 462  83 311 149  67 121 176   9 170 354
  52  44 172 156  54  13 106 353 338 331  58 177 213 426 210  10 318 154
 153 112 168 351 501 109 218 255 279 348 313  49 212 321 315  48 425 314
 214 115 215 498 324 359 206 358 116 487  97 361 224 232 493 231 362 488
 363 223 220 490 360 580 317 118 489 357 486 228 268  39 491 492 150 117
 222 229 269 349 554]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3964
INFO voc_eval.py: 171: [ 644 1715 3242 ... 3483 3235 3240]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1926
INFO voc_eval.py: 171: [ 636  666  747 ...  126 2272 2276]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2396
INFO voc_eval.py: 171: [402 365 575 309  12  21 652 649 357 364 282 574 568 627 403 655 312 591
 331 545 373 603 288 669 311 141 479 548 826 658 589 689 322 757 825 320
  18 569 546 314 287 657 617 404 283 327 665 572 739 654  25 839 651 628
 724 358 670 329  22  16 660 667 406 289 703 376 367  20 515 366 552 827
 614 570 675 297 470 325 310  70 828 372 612 492 650 727 632 547 265 405
 285 678 315 499 292 217 836 587 324 590 659 371 514 255 738 831 656  13
 330 313 368 630 601 747 303 172 202  68 301 693 618 375 674 482 302 830
 681  43 611 732 676 582 551 318 493 360 472 576 175 114 489 573 691 713
 696 196 810 132 252 687 761 205 588  15 326 419 695 702 501 556 701 664
 211 176 526 604 671 571 586 712 247 260 672 615 328 471  19 594  31 213
 398 686 835 109 296 147 146 332 425 584 216 204 700 291 740 616 698 797
 257  50 765 795 143 170  17 155 684 162  80 773 369 613 166 319 495 685
 843 631 553 580 794 722 266  44 300 359 694 431 317 593  24 261 127 259
 549 152 662 400 663 467 387 237 743  69 577 469 679 198 634 855 600 598
 323 796 555 798 550 517 212 802 417 716  99 746 116 423 522 415 692 799
 316 809 361 668 443 497 370 607 239  34 583  79 653 264 321 374 101 248
 793 201 845 847 754 585 243 531 295 677 673 293 234 525 284 554 219  58
 167 182 442  45 333 113 177 294 498 840  81 680 690 480 581 636 521 592
  23 762 714 133 717 305 335 619 688 286 530 699 131 125 230  97 745 110
  96  14 491 666 647  57 473 390 112 609 262 597 842 334 458 578 242 683
 218 153 490 608 841  78 579  64 267 399 697 723 596 770  27 195 599  54
 206 355 130 595 115 760 445 397  95 832 606 223  36  87 822 197 744 298
 150 179 256 199 857 707 111 768 775 263 251 620 165   6 229 708 484  53
 610 774 528 494 481 602 783 500 605 823 418 250 485 156 171 790 527 771
 104 637 496 214 276 725 306 728 336 412 438 715 803 486 269 184 858 236
  32  42 388 346 215 268  59 742 682 661   1 137 516 729 299 290 769 241
 753 396 362 108 181   4  52 129 117 416 107 444 118 200 119 731  49 148
 523 750  73 850  46  91 185 751 144  98 193 806 756 100 704 789  86 461
 518 763  35 151 837 808 249 560 174 524 183 629 235 168 414 389 244 834
 420 363 529 829 859 224 145 149 504 811 758 413 238 488 180 520 253 270
 468  47 772 424 779 759 543 733 749 752 258  28  26  92 338 272 233 537
 203   2 465 755 128 173 154 421 466 192 804 824  65 730 800 460 852 851
 246 274 254 778 225 161 734 102  90 624 853 158 428 764  77 459  93 245
  30 741 429 126 505  29 462 439 273 345 422 401 240 220 539 849 559 791
  48 519 848 157 103 441 748  38 160 474 227 718 638 178  72 163 433 833
 446 776  85 633 142 164  75 648 483 159 540 538 719 383 354 121 304 807
 476 777   0 792 487 542  51   5  89 271 169 541 275 856 457 454 506   8
  94 478 106 726 140 558 567   3 720 621 191 278 350 378 805 380 532 139
 766 379 721 706 544  40  56  61 564 440 477 210 427 453 437  76 635 646
 622 464 475 563 846 566 507 337 801 349 534 535 138 736  82  39 781 348
 561 432 280 105 347 705 565 208 136 508 536 463 562 222 221 228 854 456
 393 426 435 411 844 436 838 785  41 784 533  62  33 277 626 281 353 641
 502  71 513  74 643  83 231 394 382 452 381 352 625 351 640 709  37 503
 623   9 434 391 384 307 279  63 737 782 308 226 344 189  11 819 818  67
  88 392 385 767 455  60 735 207 509 356 339 710 786 711 190  10 343 557
 232 395  66 377 342  55 120 209 512 449 430 787 409 450  84   7 134 645
 644 816 815 340 821 820 122 194 817 780 511 341 124 447 812 788 386 510
 188 186 813 123 408 639 407 135 410 814 187 642 448 451]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3380
INFO voc_eval.py: 171: [437 171  96  95 172 449  91  81 103 461  99  79  97 440 110 366  80 121
 180 401  26 106  78  93  83 210  45 409 393 414  89 109 260 209 388 163
 248  86  84 441 365 380  46 104  32 397 270 143 263 359 442 367 162  54
 290  92  53 394 197 404 188 361 164 232  87 259  90 482 156 333 118 426
 217  82  44 111 433 201 392 120 221 142 319  88 329  11 287 236 251 352
  48 257 196 199  13  85 117 308 334 264 279 195 335 119 469  30 339 176
 372 271 330 151 125 391 208 379 336 396 244 261  57 146 399 362 464   8
 354 139 250 416 350 211 246 187 206 376 332 205 318  24 378 315 177 243
 145 418 413 155 280 192 410 389 407 283 149 123 122 436 356 226 224  49
 227 281 326 377 432 256 200 462 360 423 254 311 241 375 358 294 168  31
 390 321 385 351 150 124 249 131 415 282  27 286 202 431  16 220 272 100
 370 129 421 140 444 355 253 327 222 225 292 420  35 198  52 353 265 328
 147   4  41 113 115 127 313 374 173 312 135 307 128 105 463 425 141 408
 481 471  59 114 126 214 419 291  98 247 161 357 138 384 468 402 204 473
 166 223 178 116 132  29 427 480  33  58 428 276  94 310 101 466   0 157
 169 398 160 148 382  47  36 269 231 472 203 108 387 242 274 341 255 331
 395 238 102   5 229 233 207 320   3 130 406 234 411  43 435 474 245 154
 400 342 417 455  68  37 258 153 237 368  55 316 369 434 212 453 181 322
 275  74  28 403 424 284  69 144 373 273 439 228 386 470 266 422 262  60
 167 184 158 323   7 475 170  40  10 443 371 216 438 476 252 240 112 277
 133 325 363 340 324   6 152 268 230 348 314 451   9 285  38 107 309 337
 194 267 317 405 289 412 456 465 452 445 174 165 288  17 383 467 302   1
  42 134  39  34 278 306 159 338 430   2 303 213 218 381 183  61  50 305
 296  63 191 429 364  56  75  22 189 235 446 215 300 239 457 297 454  62
 219  25 479  20 458 304 346 344 293 185 295  14 182 343 179  67 301 447
  72  64  19 136  23 450 478 298 299  18 477  76 175 190 186 460 459 137
  15 193  12 448  66  51  73  21  71  65  77  70 347 349 345]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4251
INFO voc_eval.py: 171: [1593  713 1527 ...  119  104  100]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2752
INFO voc_eval.py: 171: [ 29 266  64 108 157 316  65 109  19 118 289  24 317 315 267  66 156 258
 204 125 155 288 268  26 298 233 262 110 214 203 292  74  83 122  95  52
 290 234 251  94  48 133 243 117  50  16 265 137 102 145 115  69 172 174
 193 235 192 178 308 154 297  42  21  20  75 207 105  92  27  63  70 124
  17 220 228 175  96 294  80 113 116  18 167 121 202 257 296 218 244  22
 252  55 160  28 112 166  73 287  56 238 260 195  81  76 206 144 173 168
  54 242 216 191 130  91 159 293 259 217  23  51 236 153  85  86  82 295
 198 158 312 291 127 119 264 147 247 309  77  53 123 284 310 212 314  25
 196 239 227 151 152 221   5 194 146 185 237  47 225  88 270  68 240 241
 162 103 138  84  87 131 126 165 128 136  49 135 111 171 261 224 132 246
 219 311 114 273 286 107  44 313 226 281 140 223  45 319  14  93 148  10
 139 101  67 303 245 248 197 134  89 255 254 277  41  46 299 205 106  97
 120  90   8 253 104 184 215   4 100 208  43  35   0  39 231  15 282 143
  99  30 271 272 210 222   6 283 278   3   1  34 263 274 129 170 182  98
 279 318 285   9 250  60 249 169   2 176 232 200   7  36  38  31  12  59
 161 199 163 164 213 301 181 211  58  71 275 276 201 280 209 304 187 300
 179  32  33 307 306 229  78  62  57  13  79 302  61 149 141 186  72 269
 305 230 177 150 180  11 256 190 183  40  37 142 188 189]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0867
INFO voc_eval.py: 171: [5834 2511  698 ...  287 4352 4353]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3068
INFO voc_eval.py: 171: [330 257 194 442  58 200 283 387 271 254  50 762 270 695 256 259 284 450
 253 412 448 754 641 267  71 331 409 218 726  77 260 608 193 334 195 445
 196 630 154 382 288 180 558  64 753  73 177  55  53 228 136 446 198 255
 279 241 443 226 201 447 126 203 376 606 756 441 274 265 559 369 564  69
 439 457 127 169 758 757 321 440 112  68 642 452 765 419 277  21 534   7
  60 219 268  33  65 759 609  70 738 403 192 560  67 385 651 626 723 456
 454 202 388 444 535 264 701   6 725 343 358 199 637 622 307 263 262 732
 610 148 455 538 761 453 275  49 700 553 397 269 515 716 197 144 340 290
 393 386 471 531 362 216 291 211 644 342 524 418 426 627 175  57 631 727
 411 650 569 276 720 632 731 728 261 628 115  10 323 770 227 514 566 345
 730 141 242 237 282 221 266 665 537 755 729 449 333  40 420 381 383 612
 181 410 350 125 660 620 724 332 421 561 176 438 184 146  52 214 611 310
 240 183 658 413 763 567 565 614  15  63 206 667 555 293  84 462 703 337
 733 562 208 643 645 541 722 556 289 428 696 179 173 150 217 124 719 285
   9 739 629 129 322 607  48 734 735 416 772  79 389 516 415 185 234 315
 105 423  29 648 258 348 230 494 233  30 346 178 408 417 384 128 740 151
 132 717 764 368 744 619 424  46 768 172 182 475 130 451 220 715 736 749
 427 517 662 766 505 530 414 155 422 243 245 162 572 557 142 314 750 361
 391 287  66 647 706  75 633 705 711 521 690 205  22  47 621 335 336 317
 272 224 158 313 714 174 365 324 312 338 280 425 399 359 539 678 616 570
 527 743 286 707 554 718 235 380 682 784 495 143 656 782 394 134 210  99
 278 168 281 104 106 156 223 273   8 680 507 109 370 536 396 584 339 110
 133 204  93 238 540 760 769 737 503  17 698 661 328 568 341 316 783  31
 767 401  27 480 497 294 157 519 433 548 118 364 292 250 663 239 311 103
 563 741 482 327 708  23 694 400 721  87  14 576 114 623 137 139 618 625
 498 504 649 113 752  13 404 319 159 458  42 677 379 390 780  24  45 578
 571 712 747 481 215 533 664 354 636  56 135 542 510 236 745 170 248 225
 525 639 742  34 774  54 785 207 751 100 773 532 615 152 746 490 748 496
 654 392  91 398 372  89 187 229   1 640 635 488 529 592 402 395 652 320
 108 693 461 589 212 318 153 518  26 131 679 479 463 624 687 101 579 604
 684 222  32  25 771  20 509 601  90 329 363 308 191 209  94  36 373  59
 138 522 252 107 119 102 688 213 325 111 704 588 140 232  11 171  19 249
  35 547 659 692 603  78 508 600 775 326 360 247 585 487 166  28  82 160
 520  86 485 670 674  62 666 123 586 231 638 681 526 165 689   3 366 676
   0 591 436 478  76 602 596 431 476 367 528 251 377 655 246 375 161 545
  72 523  51 434 145 668 686 675 147  12 244 371 577  38  92  41 590 646
 432 697  37 189 699  18 634 781 552 709 435 190 491 492 691 116 483 167
  80 186 493 351 685 459  16 500 672 149 121 657  81  83 594 593  85  61
 484 164 120  74 580 163 597 673 489 595  95  88 506 356 671 353 501 477
 502 653 551 378 304 486 605 587 309 613 430 669 469  43 598  39 188  44
 429 467  98 549 544 543 546 710 117 499 405 511 122 306 464 407 550 599
 299 406 298 347 296 465 702   4 474 355   2 305   5 617 437 460 468 470
 301 512 779 473 713 683 374 472 349 295 303 357 352 297 583 466 778 513
 776 300 302 573 344  96 581  97 777 582 575 574]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2079
INFO voc_eval.py: 171: [2888 1280 1752 ... 1887 2307 2792]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3278
INFO voc_eval.py: 171: [341 408 344 839 443 499 351 373  50 479 462 446 441 547 146 347 618 500
 406  64 440 161 768 539 144  46 277 504 149 159  51 510 203 520  42 374
 841 153 273 353 836 730 236  49 791 411 128 782 459 407  66 525  45 204
 151 764 147 792 763 473 267 142 829 164 514 342  41 464 844 780  63 809
  52 433 372 833 467 219  83 369 818 352 797 619  54 752 268 245 726 518
 541 765 638 163 837 223 545 162  69 548 576 224 461 481 766 235 471 532
 503  65 483 657 414 241 246 850 280 419 848 435 354 415 152 814 594 343
 711 238 259 460  55 145 840 821 165 636 522 666 198 222 447 468 838 413
 206  44 126 412 437 237 472 432  53 544 665 213 609  86 294 199 150 769
 601 276  62 771 517 612 333 250 651 322 655 813 654 812 727 816 348 699
 695  32 193 220 610  68 584 798 767 227 801 158 697 132 778 722 831  70
 427 787 448 156 279 599 751 546 438 643 257 753 262 511 216 760 439 377
 196 429 593 229 141 591 380 228 738 410 854 345 130 543 157 349 830 214
 378 409 431 205 207 629 225 417 700 148 805 143  47 794 370 422 776 166
 131 667 195 799 434 316 754 136 637 795 597 806 733 551 339 842  60 773
 436 269 625 249 846 737 127 271 423 160 615 230  57 582 828 218 420 292
 332 635 346 306 155 310 116 617 442 639 264 509 677 804 110 739 690 817
 112 660 793 260 477  67 197  43 286 569  16 857 445 265 774 154 843 486
 542 540 729 194 129 287  87 278 133 291 478 858 590 570 243 330  61 611
 668 242 688 465  92 516 282 281 714 301 675 761 815 123 135 641 802  90
 350  31 381 779 623  85 777 295 232  94 825 770 781 800 240 233 728 270
 418  58 275 184 715  59 244 139 122 595 717 272 824  25 663 762 425 324
 568 382 616 290 221 772 602 117  22 852 823 734 613 274  19 247 853 474
 674  89 379 673 577 670 231 716 605 849 300  15 659 565 662 357 111 182
 604 588 553 211 285 820 113 114 120 215 444 645 564 426 299  56 102 217
  13 603 614 338 775 248 457 550 527 251 371 362 723  33 694 513 696 703
 526 740  29 476 808 303 536 387 606 567 664 305 832 744  88 691 549 312
 596 807 855 252 103   2 644 676   5 340 416 311 266 552 261 720 719 554
 811 679 289 185 810 421 834 212 323 819 258 531 118 856 685 428 293 803
 671  91 724 835 851 234 718 822 475 640  84 845 515 678  93 681 405 399
 847  35 187 124 319 628 424 331 642 653 284 566  20 721 263 686 631 186
 713 684 626 105 134 507 359 368 325 571 627 119 356 586  71 731 335   4
 581 398 656 317 455 796 672 650 735 298 589 523  26  82 587   6 579  10
 315 600 859 181 297 256  95 175 725 314 334  48 669 358 505  28 632 308
 239   3 302 537 512 501 530 189 559 255  79 360 598 390 608 756 320 784
   1 482 788 680 115 328 592 253 288 296 304 624  80 210 484 106 283 607
 490 558 741 321 732 742 313 449 307 361 376 533  40 366 452   0 178  96
 630  14 785  36  77 620 121  23  27  39 580 487 140 137 463 661 786 393
 168 174 634 560 363 706 648 702 538 534  99 100 556 125 535 748 492 202
 430 337 451 746 555 649 790 450  18 470 633 309 403  34 480 138  24 167
 169 208  78 183 254 365 180 783 682 519 495 397 494 454 401 502 736 466
 400 528  74 364  97 745 508 402 469 104  98 176 355 789 489 191 705 646
  21  75 488 707 652 485 574 647  30  73 453  81 101 572 585 578 177 188
 575 318 201 583 458 529 557 743 179 385 693  76 192 698 491  72 108 209
  17 375 749 701 689 658 683 573  37 747 750  38 190 404 200 109 392 456
 561 563 521 692 326   7 506 386 493 621 524 496 712 367  12 383 107 757
 759 562 708  11 710 226 622 336 327 704   9 173 384 329 709   8 172 171
 170 396 758 497 498 755 394 391 395 389 826 388 827 687]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2092
INFO voc_eval.py: 171: [133 943 434 ... 567 766 767]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3074
INFO voc_eval.py: 171: [ 46 269 256 141 145  25  45 150  47  50 142 277 118  72 159  33  68  30
 263 143  59  78  55 187  56 168 116 264 258  73  15  52 161 244 196 202
 130 266 265 172 192  35 237  32 162 251 279  31 240 257  16  29  77 260
 146 274 246 147 117 149 280 278 155   6 153 189  51  84  61 166 212 115
  49  21 195  42 272  83  62 144 182 114 222  60  76 100  85 158 140 194
 201 261 239  88 109 262 281  34  19  10  98 228  66 238 209 215 185  22
 259 151 250 255  65  14 126  53 245 167  28  57  80  67  18 241 134 199
  70 113  79 165 148 125 163  96  37 242  24 236 183 227   4  23 217  27
   1  94 154  58 231 267   8  20 152 111  38  99 131 157  64 249  41 198
 156  75   5   9  91  71 268  17  12 138 218  95 106 137 214 275 205   0
  81 233  36 104 186  93 247 200 225 135 216 204  26 177 103 252 248  89
  69 184 133 164  97 110 139  86   3 102 179  48 190  11 211  13 173 101
 107 120  87 276  39 108 273 122 224 170   7  40  82 123  44 174  92 180
 221 176 208  54 232 230 112 193 171  74  90 169 124 105 207 181 197 175
 213 178 210 253 234 203 128 270 206 188   2  43 219 220 119 160 127 191
 226 271 223 229  63 243 129 121 136 235 254 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5020
INFO voc_eval.py: 171: [ 8616 14081  1070 ... 10954   784 10952]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5393
INFO voc_eval.py: 171: [2514  255  708 ... 2714 1946 1829]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4124
INFO voc_eval.py: 171: [ 66 307 534  34 329 543  67  57 316 128  40 537  50 538 326 336 308  44
 311 613 535 150  46  35 344 478 214 131 324 340 548 539 248 305 153 337
 332 549 542 122 403  37 134 644 277 409 635 299 540 341  71 141 615 361
 419 140  68 317 142 536 158 133 151 415 220 310 219 624 485 623 360  60
 218 490 561 619 262 315 405 312 621 154 330  52 120 625 557 609 479 213
 628 148 461 303 638 136 347 246 408 249 159 617 495 562 342  39 618 524
 212 551 620 225 412 331 160 227 646 127 322 489 126 411  45  90 135 498
 162 512 130 314 190 640  43 444 612 363 309 488 304 630 252  65 282 325
 362 261 642 228 129 161  62 247 306  95 216 493 223 647 366  91 572 343
 418 645 673 278 345 302 406 637 631 226 670 121 147 321 323 504 639 138
 338 139 333 477 566 404 558 633 250 132 156 300 416 563 224 238 505 137
 430 301 313 320 124 502 145 259 616 567 407 272 168 560 610 507 334 606
 521 257 636 318 632 125 276 402 627 335 671 194 215 634 258 359 319 146
 669 626 681 547 328 327 123 339 410 527  77 552  42 482 607 641 217  49
 629 296 491 350 577 195 480 595 544 260  15 152 222 500 565 263 447 102
 483 460 101 286 496 643  64 622 463 155 349 197   7 191 503 614 608  70
 373 559 439  55  28 196  36 188 492 462  25 109  38 266 674 149 374 346
 518 525 264 541 574 550 611 119 201 239 165 182 375 254 251 649 118 221
 487 189 352  94 348 192 351 292 236 476 365   0 176 255  93 163 193  33
 515  89 508 516  24 184 683  17  92  72 501 678 253 413 389  47 180 294
  96 648 355  23 117 157 672 380  53 464 256 372 169 514 438 517 553 265
 240 425 435 650  14 653 279 554  87 112 173 475 467 367 679   4 520 506
 441 293  48 429 570 108 208 432 106 555 424 243 178 446   1 556  78  21
  31  61  76 569 442 177 104 164  80 417 206  83 594 601 241 509 675  26
 458 291 680 484 459 431 546  16   2 599 181 285 472 203 434  18 368 510
  58 456 271 575 100 682 651 519 511 204 684 420 275 268 578 396 604 486
 596 587 207 473 545 400 573   8   9 105 244 676 205  56 111 234 564 522
  19 600 445 393 172  69 297 677  41 110  22 597 497 528 284  32 576 513
 474 270 107  20 440 481  74 571 466  75 499 274 281 179 652 273  51 468
  79 114  73 364 113 392 523 298 414 267 115   3 369 242 202 494 526 436
 421 280 598 437 593  88 426 388 283  54 655  59 470 427 443 582  63  27
 245  29 354 175 381 428 187  97  30   5 269 210 295  85 423 356 186 357
 116 370 399 592 198 394 589 422 353 211  13 659 585 433  86 580  81 605
 532 378 658 584  98 171  82 401 395 579 581  84 199 471 583   6  99 568
 469 230 588  10 358 656 530 174 209 590  11 668  12 661 384 586 376 231
 185 229 531 602 382 200 448 232 591 379 166 235 391 287 386 167 170 660
 237 183 233 603 452 290 390 465 657 144 667 529 377 383 387 385 665 397
 103 457 664 143 451 288 398 654 289 533 449 453 663 666 455 371 454 450
 662]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3156
INFO voc_eval.py: 171: [ 43   3  47 121 190 280 184  48 126 338  50   6 193 188 236  52 191 158
  13 161 334 194 159 156  60  42 282 152 187 182 340 233 127 279 143 223
 186 157 277 237 231 276 101  80 128 246 100 151  78 337  49 252 281   5
  58 124 104  59 153 163 257  10 162 328 175  23 232 185 147 120 308  57
 129 213 273 247 135  44 311 176 107 189 183  75 245 326  11  46 235 239
 234 166  72 216   4  99 160 327 272 103 132 192 323 119  96  51 321   8
  12 254 106 251 336  89 203 264 269 255  25 250 145 130  14 253 102  97
  45 122 332 261 209  73 256 289 212 154 238 105 125 339 170  15  91  83
 309 331  63 320 225 244  98  90  74 329  61 265 200  84  26 263 248 343
 114 196 325  27 270 341 117 314 149  64 221 228 294 230 295 249  79 134
 123   7 330 113  85   9 167 195 118 285 211  82 260 322 313 241 150 271
 220 307 116 259  76 131  95 305  77 315 324 155  55 133 299 303 335 224
  39 317 333 226 274  33  62 146 310 242  65 278 266 218 267 298  30 262
  24 342 201 217 171  88 112 214  28 297 290 219 177 240 205 174 316 110
 222  81 144 148 296 227  87  56  53  31 198  54 141  32   1  22 284 136
 258   2 306 291   0 268 229 197 210 300 202 169 208  86 283  18 109  29
 139 207 115 304 199 301 108 168 293  41 204 243 206 137 111 179 302 286
  35  37 164 138 178 172  19  21  20 173  17 319 275 142  71  16 180  36
 140  40 292 288 165  69 181 287 312  34 318  70  67  38  66  68  92 215
  94  93]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1308
INFO voc_eval.py: 171: [642 637 383 406 115 382 325  80 207 293 121  76 168  78 897 644 640 294
 656 327 324  83 181 117 400 320 807 125  89 212  92 210 804 295 506  81
 208 638 537 391 188 643 454 657 384 453 300  97 206 119 456 404 116 326
 187 197 905 911  84 647 899 306 342 296 176 813 836 511 711 465  86 542
 524 846 390 882 855 908 405 165 135 216 134 841 218 398 909 641 392 527
 334 545 299  43 870  63 489 457 329 387 435 851  98 112 171 732 386 217
  18 749 136 195 330 315 312 681 328 192 205 883 712 912 663 902 458 811
 837 805 105 910 196 509 166 173 669 531 261 169 280 308 839 872 440 220
 182 733 444 439 913 760  45  93 812 304 510  19 823 401  70 230 624 199
 167 884 541 900 554 332 526 756 880 721 831 840 178 209 893 744 185 310
 172 164 845 714 917  33 898 739 553 194  67  44 889  82 128 463  65 214
 244 901 242 459  74 842 101 512 213 791 488 788 449 249 821 455 227 833
 462 710 874 544 547 319 184  58  85 808 222 263 221 873   2 452 397 717
  53 639 854 375 707 720 385 338  91  94 904 179 180 713  16 264 703 259
 500 317 298 690 528 686 402  59 764 580 508 603 892  17 438 881 929 177
 158 198 331 856 871 484 923 204 318 591 467 409 323 133 303 915 109 789
 434  99 759 460 378 876 441 741 515 356 144 778 103  52 486 170 820  72
 679 630 519 395 466 234 914 891 233 376 608  32 826  38 495 468 262 236
 907 175  41 350 689 879 928 200 835 436 277  73 251   5 682 507 360 765
 543 290 445 885 403  95 154 746  35 824  71 869 916 143 853  42 254 433
 359 113 866 693 610 273 530 806 464 687 525 906 100 562 122 470 665 211
 241 111 617 301 174 380 124   7 549 157 123 186  57 228 322 621 862 785
 575 818 520 649 247 250 532 827  75 252 162 478 163 894  96 762 546 191
 688 860 555 922 536 497 730 834 201   8 446 895 748 750 137 358  46 887
 573 426 607 830 215 635 183 838 357 890 735 691  55 844 110 266 333  37
 903 601 490 832 260 238 311 307  87 256 265 309 239 742 852 237 625 469
 219 529 772 161 226 761 565 698 314 817 684 407 859 729 516 461 878 875
 443  56 437 877 202  36  40 335 702 615 896 577 932 274 190 678 341 474
 604 781 442 396   0 930 731 153 102 567  61 850 590 229 652  88  39 606
 337 602 623 106 709 763 633 796 388 193 557 605 521 725 622 718 424 351
  34  13 734  48  60 379 548 283 399 393 801  79 795 611 829 683 752 281
 288 828 432 114 189 340 138 498 888 568 502 931 349 743 579 130 362 680
 776 578 627 494 576 692 518 297 561 843 775 538 685 570 240 377 316 269
 235 724 418 243 155 777   6 282 662 751  62 648 353 797 858 257 814 425
 588 825 792 594 794  54 779 790 275 279 798 861 354 246 736 708   4 302
 715 609 614 556 747 203 479 363  31 581 793 723 587 348 705 716  66 757
 447 245 423 451 480 381 321 849 286 753  30 613 636 847 276 108  47 313
 394 427 278 107 501 389 701 582 816 677 886 697 369 345 704 774 612 485
 150 270 104 560 491 284  21  77 514  90 574 755 628  68 551 248 618  29
 918  15 368 477  20 815  69 305 271 572 787   9 408 420 585 848 758 593
 344 552 566 253 475 347 517 925 822 289 231 802  22 448 371 533 726 799
  14 589 285 754 583 287 655 671   3 232 258 472 559 558 255 421 571 493
 569 782 706 867 783 450 361  64 727 745 616 367 786 503 626 737 924 809
 738 819 504 267 740 492 810 471 159 505 534 596  51 699 142  10 415 550
 414 476 268 728 540 700 473 413 522 803 496 343 584 430  49 919  12 152
 370 483   1 487 499 291 131 719 374 595 373 599 672  11 429 523 592 620
 722 676 800 784 513 773 780 151 857 539 664 868 481 482 355 535 658 598
 372 292 645 696 631 586 428 336 364 431 366 597 422 646 771 629 411 156
 365 417  28 416 132 147  50 160 770 339 863 634 926 140 650 419 564 410
 346 921 563 659 118 661 632 927 223  27 619 600 864 139 145 674 766 767
 695  23 412 768  26 146 865 769 352 225 149 673 127 694 666 224 272 653
 120 129 126 654  25 141 148 920 675  24 651 667 668 660 670]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2739
INFO voc_eval.py: 171: [418 250 591 174 431 295 136 439  24 465  37   1 474 494 254 318 348 471
 476 320  39 393 435 252 346 126 416 248 347 466 124 572 307 296 349 546
 607  57  68 419 253 468 532 321 107  71 129 229  32 436 180 442 592 517
   3  17 325 473 182 428 337 314 301 345  43 317 595 615 580 154 625 225
 355 102 610 365 373 368 260 603 438 305 150 441 218 533 109 110 470 204
 319  50 256 137 363  19 207  73 482  70 433 221  29 299 597 143 462 303
   8 203 160 552 504 379 469 530 361 322 375 231 271 481 103 613 460 356
 358 329 364 392   9  40 391 120 244 630  58 162  16 186 161  25 619 212
  56  46 480 478 598 178  44 128  26 589  84 298 511 425 405 206 311 423
 258 297   6 472 573 285 304 342 502 539 302  41 312  53  72  30 222 499
 477 335 210 370 238 119 101 179 228 565 516 467  48 145 555 326 309 211
 556 214 208 430 475 620 594 424 376 396 315 213 263  38 172  90 175 617
 125 629   2 131 188 158 117 399 191 609 173 381 209 227 548   5 121 115
 575 576  75 306  93 354 432  89 352 512 362 106 184 544 579 404 332 479
  45  11 269 596 545 623  91 492 394 300  18 123 608 452 559 176 549 279
 413 290 618 397 339 557  23 450 205 611 112 437 542 100  12 130 308 171
  55  87 527 513 626  33 282 496 316 198 153 398 520 159 537 535 550 497
 614 558 224 534 234 606 411 274 359 581  98 276 357 531 283 485 523 278
 621 593 493 360 627 622 331 111 215 451 114 605 190 498 144  94 624 568
  95 503 237 384 495 612 217 148 564 407 377 202 501 277 330 515  85 313
 147 257 616 518  54 216 382 310  78  82 367 219  92 538  28 280 155 366
 181 540  63 272  42 138 448 141 536 562 116 514 383 369 454 351 584 251
 417 500 582  61 541  77 104 273 127 529  76 118 445 633 374  79 255 284
 266 449 508 567 426 372  49  36 378 223 166 350 353 420  34  81 380 434
 443 371 563  60 327 241 133 200 323   4  52 275  31 453 628 259 108 586
 632 267  51 336 587 157 447 281 388 334 521 149 142 588 328 427  15 461
  35 242 286 146 429  80  10 105 289  59 270  69 571 463 338 288 395 165
 151   7  64 122 483 547 226 631 113 156 139 543 409 583 604 484 528 554
  22 561 421 132 526 197 189 140 185 235 458 293 183 422 444 525 390 152
  27 403 196  86 239 240 135 553 524 134 570 507 406 292 324 408  83 440
 402  88 233 261 220  96 168 167 294 560 199 446 177  74 249 401  67  97
 291 340 519 230 386 164 236 410 333 262 341 551  66 170 522 578 387 163
 389 509 343 585 566 400 187 569 385 412 232 505  62 268  21  99 243  65
 457   0 506 510 287 169 344 577 602 487  20 601 456 247 599 246 455 600
 265 490 459  47 414 486 193 264 195 201 590 415 245 491 574 488  13 192
 194 464 489  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3270
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3017
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.221
INFO cross_voc_dataset_evaluator.py: 134: 0.396
INFO cross_voc_dataset_evaluator.py: 134: 0.193
INFO cross_voc_dataset_evaluator.py: 134: 0.240
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.425
INFO cross_voc_dataset_evaluator.py: 134: 0.275
INFO cross_voc_dataset_evaluator.py: 134: 0.087
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.209
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.502
INFO cross_voc_dataset_evaluator.py: 134: 0.539
INFO cross_voc_dataset_evaluator.py: 134: 0.412
INFO cross_voc_dataset_evaluator.py: 134: 0.316
INFO cross_voc_dataset_evaluator.py: 134: 0.131
INFO cross_voc_dataset_evaluator.py: 134: 0.274
INFO cross_voc_dataset_evaluator.py: 134: 0.327
INFO cross_voc_dataset_evaluator.py: 135: 0.302
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 9999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=True)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.418s + 0.042s (eta: 0:00:57)
person 0.9705903
person 0.9639124
person 0.99243647
person 0.9610109
person 0.97576004
person 0.9398033
person 0.98269075
person 0.9458652
person 0.91182065
person 0.98166245
person 0.9388433
bottle 0.9217496
bottle 0.9193262
bird 0.9510077
person 0.98255956
person 0.91830766
person 0.9449783
person 0.90286034
person 0.90483475
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.328s + 0.043s (eta: 0:00:42)
person 0.944942
person 0.9210785
person 0.9610551
person 0.9486545
person 0.98060566
person 0.9482192
person 0.9674577
person 0.9819296
person 0.93977135
person 0.9217182
person 0.92571735
person 0.9318164
person 0.9063294
person 0.92300844
person 0.92103934
person 0.9271683
person 0.9758217
person 0.913709
person 0.9800094
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.329s + 0.039s (eta: 0:00:38)
person 0.9289088
person 0.9121076
person 0.9569197
chair 0.93821716
person 0.9725056
person 0.91626334
person 0.95773005
person 0.9669116
chair 0.9656054
chair 0.9370214
chair 0.9149679
person 0.9051047
person 0.9791287
person 0.9879483
person 0.92771095
person 0.9772021
person 0.99159044
person 0.9813401
person 0.9520629
person 0.9107617
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.326s + 0.038s (eta: 0:00:34)
bird 0.9400883
person 0.9278433
person 0.92127836
person 0.99383897
person 0.95726043
person 0.9743037
person 0.9796988
person 0.9072981
person 0.9596361
person 0.9660872
person 0.9038449
person 0.9190625
person 0.9695574
person 0.9410391
person 0.9584081
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.336s + 0.038s (eta: 0:00:31)
diningtable 0.90297353
diningtable 0.93856966
chair 0.9639279
chair 0.96595556
chair 0.93815756
chair 0.9576467
pottedplant 0.91808593
pottedplant 0.91432357
pottedplant 0.9111254
pottedplant 0.9044261
pottedplant 0.94370514
pottedplant 0.938447
pottedplant 0.92838925
person 0.9658113
person 0.98291034
person 0.9203218
person 0.9798645
person 0.9490202
person 0.9891243
person 0.90043515
person 0.9158306
person 0.90406746
person 0.9119915
person 0.9224083
person 0.9227216
person 0.9740854
person 0.9551898
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.329s + 0.037s (eta: 0:00:27)
person 0.9581871
person 0.9240284
person 0.9073893
person 0.91963106
person 0.9481354
person 0.91518813
person 0.95362943
person 0.93696696
person 0.9299312
bird 0.92277795
person 0.9377624
person 0.9876773
person 0.9170032
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.327s + 0.036s (eta: 0:00:23)
pottedplant 0.9027464
person 0.9624808
person 0.934082
person 0.91318285
person 0.9538812
person 0.9191594
person 0.9587027
diningtable 0.9112047
chair 0.9826096
diningtable 0.96210027
chair 0.91610795
chair 0.9486509
chair 0.9807989
chair 0.92636967
bird 0.9930328
person 0.90287256
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.324s + 0.036s (eta: 0:00:19)
car 0.92877597
car 0.94248307
car 0.9154427
person 0.98841584
person 0.9005553
person 0.9741615
person 0.9299355
horse 0.97373736
horse 0.9134929
horse 0.9209396
person 0.93805933
person 0.95340097
person 0.968158
person 0.91868365
person 0.9432486
car 0.9459263
car 0.96401745
bicycle 0.9601062
bicycle 0.93106896
person 0.9369449
person 0.9181158
person 0.9736118
chair 0.9187598
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.322s + 0.036s (eta: 0:00:15)
person 0.9742132
person 0.95916384
person 0.97385365
person 0.91791123
person 0.91339564
person 0.90193164
person 0.98860776
person 0.98637795
person 0.9436187
person 0.9711567
person 0.91549563
person 0.96396637
person 0.99118876
person 0.90904737
person 0.96659744
person 0.9119825
person 0.9879147
person 0.9799039
pottedplant 0.9115668
person 0.9898678
bus 0.9127439
person 0.96127623
bus 0.90652657
person 0.94158787
person 0.9548457
chair 0.947044
person 0.9055269
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.321s + 0.036s (eta: 0:00:12)
motorbike 0.96319634
person 0.9749348
person 0.9105984
person 0.9159754
person 0.9571277
person 0.9677521
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.319s + 0.036s (eta: 0:00:08)
person 0.9069909
boat 0.9820458
boat 0.94832605
boat 0.9185641
person 0.9903923
person 0.93755966
person 0.9855003
person 0.918332
person 0.91774833
person 0.9891027
person 0.9749332
person 0.9108198
person 0.9753353
person 0.97678417
person 0.9140657
person 0.9484675
car 0.95060855
person 0.9407801
person 0.9473015
person 0.95986193
person 0.98683065
person 0.90835613
person 0.92137176
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.321s + 0.036s (eta: 0:00:04)
diningtable 0.9599249
chair 0.9250219
chair 0.97384757
chair 0.98070395
chair 0.91562134
chair 0.9812183
chair 0.9019645
chair 0.9368477
chair 0.9599621
person 0.94290507
bird 0.92408365
bird 0.9157806
boat 0.9198132
person 0.962163
person 0.9295078
person 0.98884916
person 0.91502124
person 0.9038735
person 0.91380817
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.319s + 0.035s (eta: 0:00:01)
person 0.98351395
person 0.9640672
person 0.9564681
person 0.95550644
person 0.9491106
person 0.96625626
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.507s + 0.027s (eta: 0:01:06)
person 0.9821401
person 0.91944665
person 0.9789995
person 0.9771507
person 0.9143218
bicycle 0.9724076
person 0.93218136
person 0.9153487
bicycle 0.9747892
person 0.9362058
bird 0.9436689
bird 0.90090203
pottedplant 0.9430291
person 0.9684778
person 0.92903805
person 0.9023183
person 0.9611285
person 0.96618986
person 0.9869316
person 0.9268486
person 0.9660872
person 0.95277625
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.316s + 0.032s (eta: 0:00:39)
person 0.9127096
person 0.90648365
person 0.9640507
person 0.9179493
person 0.9740703
person 0.94308436
person 0.9359715
person 0.9809059
person 0.9644336
chair 0.9252491
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.339s + 0.034s (eta: 0:00:38)
car 0.9158265
car 0.9509312
person 0.94709617
person 0.96161544
person 0.9817634
person 0.925046
person 0.94924605
person 0.96505195
car 0.9823727
cow 0.92361
person 0.9153539
person 0.9578687
person 0.9234576
person 0.9134714
person 0.9812227
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.331s + 0.034s (eta: 0:00:34)
person 0.97835165
chair 0.9348232
person 0.9304044
person 0.9252557
person 0.9515202
person 0.953651
person 0.93323594
person 0.91815126
person 0.9404581
person 0.98070633
person 0.9870027
person 0.9771969
person 0.969163
person 0.9775524
person 0.9056563
bird 0.90452456
bird 0.99082994
person 0.92214245
bird 0.9517173
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.328s + 0.037s (eta: 0:00:30)
person 0.94640154
person 0.9712653
person 0.9827352
person 0.92196
person 0.94846946
car 0.95060855
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.328s + 0.037s (eta: 0:00:27)
bird 0.9100355
bird 0.9595808
bird 0.9136298
person 0.9150904
person 0.9444784
person 0.91895044
person 0.9151443
person 0.98844355
diningtable 0.9487109
person 0.97240555
diningtable 0.97908396
person 0.93980384
person 0.9361822
chair 0.94311285
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.330s + 0.036s (eta: 0:00:23)
person 0.9162233
person 0.90770936
person 0.93156016
person 0.9495022
person 0.98593473
bus 0.9463756
dog 0.9525549
diningtable 0.9738898
diningtable 0.92426133
chair 0.91272134
chair 0.9850884
chair 0.9112041
chair 0.97126526
chair 0.9359766
pottedplant 0.9018139
person 0.90431666
person 0.901319
person 0.9765772
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.327s + 0.036s (eta: 0:00:19)
diningtable 0.9211276
chair 0.91174346
person 0.9483585
person 0.96673846
person 0.956132
person 0.9372219
person 0.94521254
bird 0.95428836
person 0.9263118
person 0.9623732
person 0.9642004
person 0.9324968
person 0.980355
person 0.9102093
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.326s + 0.036s (eta: 0:00:15)
person 0.9629047
person 0.9005798
person 0.9640272
person 0.9177128
person 0.9034142
train 0.9402809
person 0.9571602
person 0.9621782
person 0.94631547
person 0.9403879
person 0.9153381
person 0.9294949
person 0.9318354
person 0.9053553
person 0.956457
person 0.92658323
person 0.992596
person 0.9740381
person 0.959788
person 0.9016576
person 0.91769975
person 0.92751694
person 0.90344316
person 0.9312645
tvmonitor 0.9177147
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.326s + 0.036s (eta: 0:00:12)
person 0.96429515
person 0.9068091
person 0.94484544
horse 0.96011025
person 0.9668451
person 0.9310643
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.324s + 0.036s (eta: 0:00:08)
person 0.97527224
person 0.9652308
person 0.92215574
person 0.9197522
person 0.9526061
person 0.92847115
person 0.93641186
person 0.9034168
person 0.91607845
person 0.9867899
person 0.9467814
person 0.9002418
chair 0.9725662
person 0.9562213
person 0.90733975
person 0.9441562
person 0.9587023
person 0.9011385
person 0.90570986
person 0.9189452
car 0.9449686
car 0.9183405
car 0.95226574
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.321s + 0.035s (eta: 0:00:04)
person 0.9674489
person 0.9441047
person 0.9627399
person 0.9489891
person 0.97439337
person 0.96105117
person 0.92241216
person 0.90051997
person 0.9077378
person 0.92886895
person 0.9002792
person 0.9882024
person 0.98143274
person 0.9163386
person 0.97840506
person 0.9282786
person 0.93844545
diningtable 0.9337275
person 0.9013756
person 0.9180147
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.315s + 0.035s (eta: 0:00:01)
person 0.9383915
person 0.90837383
diningtable 0.9100974
person 0.9709907
person 0.94088334
person 0.9872852
person 0.96541435
person 0.95821685
person 0.9597626
person 0.9087936
person 0.9112758
person 0.9182432
person 0.9409457
person 0.9630664
person 0.98079634
person 0.9112721
person 0.9494823
person 0.9564828
person 0.9561998
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.347s + 0.041s (eta: 0:00:48)
diningtable 0.90214634
diningtable 0.92810035
person 0.9600717
person 0.97619027
diningtable 0.97367245
person 0.93718
person 0.9184878
person 0.9085261
bottle 0.9067238
person 0.978161
pottedplant 0.9175062
diningtable 0.92882276
person 0.9659941
diningtable 0.9069713
chair 0.9493681
chair 0.96601015
chair 0.92293024
person 0.9316299
chair 0.90151006
person 0.90749806
person 0.9330965
person 0.9956542
person 0.9808373
person 0.9346927
person 0.9662771
person 0.9654502
person 0.90487933
person 0.9707831
person 0.96018994
person 0.93911785
person 0.94093704
person 0.96559364
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.298s + 0.037s (eta: 0:00:38)
person 0.947479
person 0.9859532
person 0.93364066
person 0.9709198
bottle 0.93683636
person 0.93719256
person 0.94422346
person 0.9538793
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.316s + 0.035s (eta: 0:00:36)
chair 0.93162864
bottle 0.90929204
boat 0.90108883
person 0.9729734
person 0.9400033
person 0.9338219
person 0.93277276
person 0.93271816
person 0.96861446
person 0.9444203
person 0.9016715
bottle 0.90812093
bottle 0.95354575
person 0.98219657
person 0.95267075
person 0.94935876
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.323s + 0.037s (eta: 0:00:33)
person 0.96691906
person 0.92178786
bird 0.91684705
person 0.9537933
person 0.9069094
person 0.9681501
person 0.9706562
person 0.9599807
person 0.9037925
person 0.949973
person 0.90840465
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.327s + 0.037s (eta: 0:00:30)
person 0.96961135
person 0.96920794
person 0.95329034
person 0.90708417
person 0.9277301
person 0.97074336
person 0.9207413
person 0.94103014
person 0.9144652
person 0.9200319
person 0.9370223
person 0.9699472
person 0.9013728
person 0.9536158
person 0.9247871
person 0.9056512
bottle 0.9536639
chair 0.93112844
boat 0.9040553
person 0.97334146
person 0.95535105
person 0.90533656
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.327s + 0.035s (eta: 0:00:26)
car 0.980824
car 0.93497354
person 0.92631394
person 0.977428
person 0.9099425
person 0.9114943
person 0.9207711
person 0.9353761
person 0.9650512
person 0.9232308
person 0.91815686
person 0.9575887
person 0.91853315
pottedplant 0.9140075
pottedplant 0.9313721
car 0.9199063
car 0.98508865
car 0.9377276
person 0.93265676
person 0.9878016
person 0.9559958
person 0.9282278
person 0.9368278
person 0.9860042
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.323s + 0.035s (eta: 0:00:22)
person 0.9798063
car 0.96013385
person 0.9550505
person 0.9262587
tvmonitor 0.9263807
person 0.9060177
person 0.9340957
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.322s + 0.035s (eta: 0:00:19)
chair 0.9043283
person 0.9709084
person 0.94664556
train 0.9571072
train 0.9581359
bird 0.91130114
bird 0.98306686
person 0.92251647
person 0.9225622
bicycle 0.9204622
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.318s + 0.035s (eta: 0:00:15)
person 0.9700057
person 0.9522652
person 0.9289618
person 0.9295365
person 0.9781793
person 0.9869053
person 0.9589289
person 0.9737921
person 0.9891656
person 0.9554749
person 0.9651739
car 0.95255923
person 0.9690814
person 0.9076995
person 0.9503294
person 0.97719425
person 0.93459105
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.319s + 0.035s (eta: 0:00:12)
person 0.9158204
person 0.9456897
person 0.9242576
chair 0.97510725
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.320s + 0.035s (eta: 0:00:08)
person 0.9814466
person 0.94177043
person 0.9299974
person 0.9641873
person 0.96221954
person 0.9026606
person 0.92647165
person 0.90747
person 0.9409096
person 0.9214021
person 0.9789782
person 0.9071586
person 0.90894336
boat 0.9031242
person 0.9674658
person 0.9255439
person 0.9479781
person 0.91287714
person 0.9581918
person 0.9042461
person 0.92685205
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.321s + 0.035s (eta: 0:00:04)
person 0.96748817
person 0.9737386
person 0.97811663
person 0.9495555
person 0.9182243
person 0.93069094
person 0.92511797
person 0.9509456
person 0.96106404
person 0.9196494
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.319s + 0.035s (eta: 0:00:01)
person 0.9408085
person 0.9714256
person 0.92804384
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.0002,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 10000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 5000, 8000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.0,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': True,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': True,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/ckpt/model_step9999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.511s + 0.044s (eta: 0:01:08)
person 0.942078
chair 0.9195774
chair 0.958459
person 0.9584535
person 0.92547756
person 0.95373213
person 0.9414078
diningtable 0.93189114
person 0.9745857
person 0.9719701
diningtable 0.9378657
person 0.95407665
person 0.9685846
person 0.9624908
person 0.9078765
diningtable 0.91156363
person 0.93208855
person 0.9792068
diningtable 0.9662094
diningtable 0.9296693
diningtable 0.9407768
chair 0.91546816
chair 0.9815787
person 0.90708286
person 0.9317223
person 0.9890639
person 0.9195937
person 0.96137804
person 0.9852527
person 0.9162328
person 0.9238615
person 0.98363566
person 0.9684239
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.344s + 0.034s (eta: 0:00:43)
person 0.9398386
person 0.95484734
pottedplant 0.9334953
person 0.9048322
bird 0.92501646
bird 0.9717776
bird 0.9485068
bird 0.90783364
person 0.9674298
person 0.9209139
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.329s + 0.034s (eta: 0:00:37)
person 0.950457
person 0.9106456
person 0.96239716
person 0.9702102
person 0.9767447
person 0.946471
person 0.9024423
bottle 0.90577424
bottle 0.9524339
bottle 0.90372545
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.322s + 0.035s (eta: 0:00:33)
person 0.9103791
person 0.9182833
diningtable 0.9075175
pottedplant 0.9311587
chair 0.94000614
chair 0.9160592
person 0.9757119
person 0.9416311
bottle 0.90977526
bottle 0.90928984
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.322s + 0.035s (eta: 0:00:29)
person 0.9624958
person 0.9607454
person 0.94789433
person 0.99478793
person 0.9428071
person 0.9405548
person 0.92055386
person 0.97951835
person 0.9130595
person 0.9582091
person 0.9735415
person 0.9159922
person 0.916213
person 0.93474853
person 0.9390978
person 0.9920174
person 0.93979055
person 0.97451925
person 0.9253474
person 0.96117043
person 0.9838434
person 0.9666316
person 0.927564
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.317s + 0.034s (eta: 0:00:25)
person 0.9481622
person 0.96505016
person 0.9026427
person 0.9707363
person 0.90726894
diningtable 0.9841934
chair 0.91678715
chair 0.94799405
chair 0.9794257
chair 0.98744744
aeroplane 0.91224545
person 0.9668229
person 0.9816143
person 0.933169
person 0.9177865
person 0.9124732
person 0.94774795
person 0.9736023
car 0.9227859
car 0.9804201
car 0.9719101
car 0.9589509
bird 0.9584981
chair 0.9318767
chair 0.95092815
person 0.9457333
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.316s + 0.034s (eta: 0:00:22)
person 0.984185
person 0.93357515
person 0.90950173
person 0.981307
person 0.95849794
person 0.98961556
person 0.93720865
person 0.927066
person 0.9410407
person 0.9902964
person 0.93873215
person 0.9086253
horse 0.96250343
person 0.91293734
person 0.90541387
person 0.9197465
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.313s + 0.034s (eta: 0:00:18)
person 0.95554227
person 0.94639546
chair 0.97502387
chair 0.9219131
chair 0.95681965
pottedplant 0.9248102
pottedplant 0.9225214
pottedplant 0.94565827
person 0.9486641
person 0.9041046
person 0.9220532
person 0.93810123
person 0.9593614
person 0.9002741
person 0.93820894
person 0.932523
person 0.9019145
person 0.9043793
person 0.95590335
bicycle 0.9137543
person 0.95536137
person 0.9798728
person 0.90303916
person 0.94302607
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.316s + 0.034s (eta: 0:00:15)
person 0.9880924
person 0.9181449
person 0.93218035
person 0.91667986
person 0.9593752
person 0.98547596
person 0.9401706
person 0.9769057
person 0.97802246
person 0.9718711
person 0.9372053
person 0.92307204
person 0.91717607
bus 0.955466
person 0.9063242
person 0.9396378
person 0.9483529
person 0.9652454
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.318s + 0.034s (eta: 0:00:11)
person 0.96757454
person 0.9151003
person 0.9357687
chair 0.9126674
chair 0.9486238
tvmonitor 0.901718
chair 0.9712535
chair 0.9300015
chair 0.94002426
person 0.9497579
person 0.9701855
person 0.95707726
person 0.977527
person 0.918868
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.316s + 0.034s (eta: 0:00:08)
motorbike 0.91547906
motorbike 0.95149434
person 0.9019892
person 0.97809225
horse 0.9555814
horse 0.9010428
person 0.90399545
person 0.90506893
bicycle 0.9589805
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.314s + 0.033s (eta: 0:00:04)
person 0.9248333
person 0.959036
person 0.98108083
person 0.94939333
person 0.96464145
person 0.98438144
person 0.95103544
person 0.9569413
person 0.9435074
bird 0.90059215
chair 0.9595636
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.315s + 0.033s (eta: 0:00:01)
person 0.90694225
person 0.9865702
person 0.98578715
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_copy-cls-to-det/Nov06-06-44-32_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 86.483s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [1153  282  279 ...  982  983  980]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.2210
INFO voc_eval.py: 171: [121 122  68 503  67 234 443 279 125 260 133 123 474 557 373 383  70 562
 534 448 560 555 501  71 564 371  69 545 468  83 129 131 561 265 572  86
 449 456 522 130 566 556  72 577 248  94 453 127  81 376 558 509 452 368
 369 288  75  88 283 134 124 513 536 547 447  89 289 235 240 460  73 554
 459 540 505 429 292 262 466 137 535 100  18 537 146 135 282 475 139 367
 510  42 458 380 280 238 239 533 232 236 512 311 374 272 399 332  77 242
 413 286 527 450 568 571  22 542 295 464 372 454 495 403  37  21 102 144
  24 538 141 508 419 273 504 427 104 574 507 126 550 258 157 451  40 378
 294 569 539 263 158 417 264 339 247 392 190 326 462 192 532  85 300 145
 410 563 515 199   2 578 430 549 463 271 233 575 132 210 400 249 142 241
 315 318 333 428 297 140 481 567 397  76 407 409 293 565 285 559 382 543
  41  19 511 253 455 470  43  80 291 546 287  32 307 243 422  55 143 529
 526 525 570 186 275 418 324 194 266 246 435 381 136  16 237 573 366 548
 520  95  90 420   5  84 516  87 128 299 103 101 551 530 396 138 498 189
 269 257 408  46 172 306 441 484  74 421  97  30 465 496 398 576 391  93
 181   1  20 521  25 259 245 473 502 284 256 432 370 173 480 200 517 387
 394  91  78  65 412  49 442 191 445 329 389 390 193 255  38 327 506 309
   0 274 355 156  98 187 170 270 183 528  63 319  34 552 415 180 184 414
 161 457 478 416 423 215 439  29  59 433  79 113 411  26   3  54 261 321
 118 479 188  60 182 244 296  47  36 177 195 519  17 431 375 444 385   8
 322 335  33  28 469 179 174 471 163 467 303 302  12 404 281 226 384 150
  99 112 401 216 544   6 304 308  15 406 298 493 499 434 160  64 277 344
 162 185 225 386 290  61 531 402 338 440 343 395  92 365 523  62  35 345
 349  23 482  31 109 494 541  50  27 305 151 119 218 325 204 165 164  11
 437 436 438 202 198 340 472 168  45 446 346 166 524 107 203 364 197  52
 405   7 224 351 276 388 514 159 251  58 252 476 201 208 110   4  14 331
 250 426 220 483 207 377  56 154 328 477 196 147 229 206 341 334 342 354
 106 336 393 363 379 178 518 301 461 310  82  66 120 148 175  51 353   9
 169  44 171 155 105  53  13 337 330 352  57 176 212 425  10 209 317 153
 111 350 152 167 108 500 217 254 347 312 278 211  48 320 314 424 313 114
 213 497 357 323 205 358 214 486 115  96 360 223 231 492 230 361 222 362
 487 219 489 359 579 117 316 488 356 485 227 267  39 490 491 149 116 221
 228 268 348 553]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.3964
INFO voc_eval.py: 171: [ 644 1715 3244 ... 3485 3237 3242]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1947
INFO voc_eval.py: 171: [ 637  667  748 ...  126 2273 2277]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.2388
INFO voc_eval.py: 171: [402 365 575 309  12  21 652 649 357 364 282 574 568 627 403 655 312 591
 331 545 373 603 288 669 311 141 479 548 826 658 589 689 322 757 825 320
  18 569 546 314 287 657 617 404 283 327 665 572 739 654  25 839 651 628
 724 358 670 329  22  16 660 667 406 289 703 376 367  20 515 366 552 827
 614 570 675 297 470 325 310  70 828 372 612 492 650 727 632 547 265 405
 285 678 315 499 292 217 836 587 324 590 659 371 514 255 738 831 656  13
 330 313 368 630 601 303 747 172 202  68 301 693 618 375 674 482 302 830
 681  43 611 732 676 582 551 318 493 360 472 576 175 114 489 573 691 713
 696 196 810 132 252 687 761 205 588  15 326 419 695 702 501 556 701 664
 211 176 526 604 671 586 571 712 247 260 672 615 328 471  19 594  31 213
 398 686 835 109 296 147 146 332 425 584 204 700 216 291 740 616 698 797
 257  50 765 795 143 170  17 155 684 162  80 773 369 613 319 495 685 166
 843 631 553 580 794 722 266  44 300 359 694 431 317 593  24 261 127 259
 549 152 662 400 663 467 387 237 743  69 577 469 679 198 634 855 600 598
 323 796 555 798 550 517 212 802 417 716  99 746 116 423 522 415 692 799
 316 809 361 668 443 497 370 607 239  34 583  79 653 264 321 374 101 248
 793 201 845 847 754 585 243 531 295 677 673 293 234 525 284 554 219  58
 167 182 442 305  45 333 113 177 294 498 840  81 680 690 581 480 592 521
 636  23 762 714 133 717 335 619 688 286 530 699 131 125 230  97 745 491
 110  96  14 666 647  57 473 390 112 609 262 597 842 334 458 578 242 683
 218 153 490 608 841  78 579  64 267 399 697 723 596 770  27 195 599  54
 206 355 130 595 115 445 397  95 760 606 223 832  87 822  36 197 298 150
 744 179 256 199 707 857 768 251 263 620 111 775 165   6 229 708 484 610
  53 774 528 602 481 494 500 783 823 605 418 250 156 485 171 527 790 104
 771 214 637 276 496 725 728 412 336 486 803 715 269 438 858 184 236 388
  32  42 346 268 215  59 742 661 682 137 516   1 299 729 241 290 769 753
 362 396 108 181  52   4 129 306 117 416 107 118 444 200 731 119 523  49
 148 750  73 850 185  46  91 144 751  98 193 806 100 756 789 704  86 461
 763 518  35 151 837 560 808 249 174 524 183 629 414 168 235 389 244 834
 363 529 420 829 859 224 145 149 504 811 758 413 238 180 488 520 253 270
  47 468 424 543 772 759 779 752 749 733  26  28 258 338  92 272 233 537
   2 465 203 755 173 128 154 421 192 466 804 824  65 730 800 460 852 851
 246 254 274 778 225 161  90 734 102 624 764 428 853 158  77  93 245 459
  30 741 429 126 505  29 462 439 273 345 422 401 240 220 539 849 559 791
  48 519 848 157 103 441 748  38 160 474 227 718 638 178  72 163 433 833
 446 776  85 633 142 164  75 648 483 159 540 538 719 383 354 121 304 807
 476 777   0 792 487 542  51   5  89 271 169 541 275 856 457 454 506   8
  94 478 106 726 140 558 567   3 720 621 191 278 350 378 805 380 532 139
 766 379 721 706 544  40  56  61 564 440 477 210 427 453 437  76 635 646
 622 464 475 563 846 566 507 337 801 349 534 535 138 736  82  39 781 348
 561 432 280 105 347 705 565 208 136 508 536 463 562 222 221 228 854 456
 393 426 435 411 844 436 838 785  41 784 533  62  33 277 626  83 281 353
 641 502  71  74 643 231 513 381 394 382 452 352 625 351 434 640 709  37
 503   9 623 391 737  63 307 384 279 308 782 226 189 344 819  11  67  88
 818 392 385 767 455  60 735 207 509 356 339 710 786 711 190  10 343 557
 232 395  66 377 342  55 120 209 512 449 430 787 409 450  84   7 134 645
 644 816 815 340 821 820 122 194 817 780 511 341 124 447 812 788 386 510
 188 186 813 123 408 639 407 135 410 814 187 642 448 451]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.3380
INFO voc_eval.py: 171: [437 171  96  95 172 449  91  81 103 461  99  79  97 440 110 366  80 121
 180 401  26 106  78  93  83 210  45 409 393 414  89 109 260 209 388 163
 248  86  84 441 365 380  46 104  32 397 270 143 263 359 442 367 162  54
 290  92  53 394 197 404 188 361 164 232  87 259  90 482 156 333 118 426
 217  82  44 111 433 201 392 120 221 142 319  88 329  11 287 236 251 352
  48 257 196 199  13  85 117 308 334 264 279 195 335  30 119 469 339 176
 372 271 330 151 125 391 208 379 336 396 244 261  57 146 399 362   8 464
 354 139 250 350 416 246 211 206 187 376 332 205 318  24 378 315 177 243
 145 418 413 280 155 192 410 389 407 283 149 123 122 436 356 226 224 281
  49 227 326 377 432 256 200 462 360 423 254 311 241 375 358 294 168  31
 390 385 321 351 150 124 249 131 415 282  27 202 286 431  16 220 272 100
 370 129 421 140 444 355 253 327 222 225 292 420  35 198 265  52 353 328
 147   4 113 115 127  41 313 374 173 312 135 307 128 105 463 425 141 408
  59 114 126 471 481  98 247 291 214 419 161 468 384 357 138 402 166 204
 473 223 178 116 132  29 427  58  33 480 310  94 276 428 101 382 466   0
 169 157 160 398 148  47 269  36 472 231 203 108 387 341 242 274 331 395
 255 102 238 233   5 229 207 320   3 130  43 406 411 234 474 435 400 154
 245 417 455 342  68  37 368 153 258 237  55 316 369 434 212 453 181 322
  74 275 424 403  28 284  69 373 144 273 439 228 470 386 262 266 422  60
 167 184  40 475 158 170 323   7 443 371  10 438 216 476 252 112 133 240
 277 324 325 340 363 152   6 451 314 230 268 348  38 107 309 285   9 317
 445 337 405 194 267 174 289 465 456 412 452 165 288 467  42 383 302   1
  17 278 134  39  34 306 338 159 430 381 303   2 183 218 213  61  50 305
 296  63 191  22  56 364 429  75 446 235 189 215 239 457 300 297 454  25
  62 219 479  20 304 458 346 344 293 295 185  14 343  67 182 179 301 447
  19  72  64 450  23 136 478 298 299  18 477  76 175 186 459 190 460 137
  15 193  12 448  66  51  73  21  65  71  70  77 347 345 349]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.4251
INFO voc_eval.py: 171: [1595  714 1529 ...  119  104  100]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.2760
INFO voc_eval.py: 171: [ 29 266  64 108 157 316  65 109  19 118 289  24 317 315 267  66 156 258
 268 204 125 155 288  26 298 233 262 110 214 203 292  74  83 122  95  52
 290 234 251  94  48 133 243 117  50  16 265 137 102 145 115  69 174 172
 193 235 192 178 308 154 297  42  21  20  75 105 207  92  27  63  70 124
  17 220 228  96 294 175  80 116 113 121 167  18 202 257 296 218 244  22
 252  55 160 112  28 166 287  73  56 238 260 195  81 206  76 144 173 168
  54 242 216 130 191  91 293 159 259 217  23 153  51 236  85  86  82 295
 198 158 291 312 127 119 264 147 247 309  53  77 123 314 310 212 284  25
 196 239 227 151 152 221 194   5 146 185  47 237 225  88 270  68 240 162
 241 103  87 138  84 131 165 126 128 136  49 135 111 261 224 171 132 246
 219 311 114 273 286 107  44 226 140 281 313 223  45 319  14  93  10 148
 139 101  67 303 245 248 197 134  89 255 254 277  41  46 299 205 106  97
 120  90 253   8 104 184 215   4 100 208  43  35   0  39 231  15 282 143
  99  30 271 272 210 222   6 283 278   3   1  34 263 274 129 170 182  98
 279 318 285   9 250  60 249 169   2 176 232 200   7  36  38  31  12  59
 161 199 163 164 213 301 181 211  58  71 275 276 201 280 209 304 187 300
 179  32  33 307 306 229  78  62  57  13  79 302  61 149 141 186  72 269
 305 230 177 150 180  11 256 190 183  40  37 142 188 189]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0868
INFO voc_eval.py: 171: [5838 2510  698 ...  288 4355 4356]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.3064
INFO voc_eval.py: 171: [330 257 194 442  58 200 283 387 271 254  50 762 270 695 256 259 284 450
 253 412 448 754 641 267  71 331 409 218 726  77 260 608 193 334 195 445
 196 630 154 382 288 180 558  64 753  73 177  55  53 228 446 136 255 198
 279 241 443 226 201 447 126 203 376 606 756 441 274 265 559 369 564  69
 439 457 127 169 758 757 440 321 112  68 642 452 765 419 277  21 534   7
  60 219 268  33  65 759 609  70 738 403 192 560  67 385 651 626 723 456
 454 202 388 444 535 264 701   6 343 725 358 199 637 622 307 263 262 732
 610 148 455 538 761 453 275  49 700 553 397 269 515 716 197 144 340 290
 393 471 386 531 362 216 291 211 644 342 524 418 426 627 175  57 631 727
 411 650 569 276 720 632 728 731 261 115 628  10 770 227 323 514 566 345
 730 242 237 141 266 282 665 221 537 755 729 449 333  40 420 381 383 612
 350 410 181 125 620 660 332 724 421 438 561 176 146 184  52 214 611 310
 240 183 658 614  15 567 565 413 763 206 555  63 667 293 703  84 462 337
 733 208 643 562 645 541 556 722 289 428 179 696 173 150 217 285 124 719
   9 129 739 629 322  48 607 735 734 416 772 516 389  79 234 415 185  29
 423 315 105 258 648 348 230 233 494  30 417 178 408 384 346 128 717 740
 132 151 764 368 744 619  46 424 475 172 130 182 768 451 220 736 715 749
 427 517 662 766 155 505 530 422 245 243 162 414 142 557 572 314 750 361
 391 287 647 706  66  75 711 633 705 521  22 205 690 335 621  47 317 336
 224 174 158 272 313 312 714 365 324 338 280 539 425 616 359 678 399 743
 570 527 380 718 235 707 286 554 143 784 682 495 134 394 782  99 210 656
 168 278 106 104 281 223 156 273   8 109 507 680 536 370 396 133 110 339
 584 204 238  93 540 760 737 769 503 328  17 698 661 568 783 341 316 401
 767 480 157  31  27 497 294 519 548 118 364 292 250 663 239 311 433 103
 563 400 741 482 327  23 708 694 721  14 576 114  87 623 137 139 618 625
 498 504 649 113 752  13 404 319 159 458  42 677 379 390 780  24  45 578
 571 712 747 481 215 533 664 354 636  56 135 542 510 236 745 170 248 225
 525 639 742  34 774  54 785 207 751 100 773 532 615 152 746 490 748 496
 654 392  91 398 372  89 187 229   1 640 635 488 529 592 402 395 652 320
 108 693 461 589 212 318 153 518  26 131 679 479 463 624 687 101 579 604
 684 222  32  25 771  20 509 601  90 329 363 308 191 209  94  36 373  59
 138 522 252 107 119 102 688 213 325 111  11 704 588 140 232 171  19 249
  35 547 659 692 603  78 508 600 326 775 360 247 585 487  62 166  28  82
 666 520 670 674 160  86 485 123 165   3 689 366 681 586 231 638 526 676
   0 591 436 478  76 602 528 596 431 476 367 251 377 655 246 375 161 545
  72 523  51 434 145 668 686 675 147  12 244 371 577  38  92  41 646 590
 432 697  37 189 552 699  18 634 781 709 435 190 491 492 691 116 483 167
  80 186 493 351 685 459  16 500 672 149 121 657  81  83 594 593  85  61
 484 164 120  74 580 163 597 673 489 595  95  88 506 356 671 353 501 477
 502 653 551 378 304 486 605 309 587 613 430 669 469 598  39  44  43 188
 429 467  98 549 544 546 710 543 117 499 405 511 122 306 464 407 550 599
 299 465 406 298 347 296   4 702 474 355 305   2   5 437 617 460 468 470
 301 512 779 473 713 683 374 472 349 295 303 357 352 297 583 466 778 513
 776 300 302 573 344  96 581  97 777 582 575 574]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2076
INFO voc_eval.py: 171: [2886 1279 1334 ... 1886 2305 2790]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.3278
INFO voc_eval.py: 171: [341 408 344 839 443 499 351 373  50 479 462 446 441 547 146 347 500 618
 406  64 440 161 768 539 144  46 277 504 149 159  51 510 203 520  42 374
 841 153 273 353 836 730 236 791  49 411 128 782 459 407  66 525  45 204
 151 764 147 792 763 473 267 142 829 164 514  41 342 464 844 780  63 809
  52 433 372 833 467 219  83 369 818 352 619 797  54 752 268 245 726 518
 541 765 638 163 223 837 545 548 162  69 576 461 224 481 766 235 471 503
  65 532 483 657 414 241 246 851 280 419 849 435 354 415 152 814 594 343
 711 259 460 238  55 145 840 821 165 636 666 522 198 222 447 468 413 838
 206  44 126 412 437 237 432 472  53 544 665 213 609  86 294 150 199 769
 601 276  62 612 517 771 333 250 651 655 322 813 654 812 727 816 348 695
 699  32 193 584 610 220  68 798 767 227 801 158 132 697 778 722 831  70
 427 787 448 156 599 279 751 438 546 643 257 753 262 511 216 760 377 439
 196 429 229 593 141 591 380 738 228 410 345 855 130 543 349 157 830 378
 214 409 431 207 205 700 417 629 143 148 225  47 805 422 776 794 166 370
 195 667 131 799 434 316 754 136 637 597 795 733 806 551 842 339 436  60
 773 269 625 249 127 847 737 271 423 230 615 160  57 828 420 582 218 292
 332 635 306 155 310 346 116 617 442 639 264 509 804 677 739 110 690 112
 817 660 793 260 477  67  43 197 286 569  16 858 445 265 774 154 843 486
 542 540 194 729 129  87 287 278 291 133 859 478 330  61 243 611 590 570
 465 668 688 242  92 516 282 281 714 301 675 761 815 350 123 802 135  31
 641  90 381 779  85 623 777 295 232 770 825  94 781 275 800  58 233 418
 715 184 240 270 728 122  59 244 139 595 717 272 663  25 824 425 762 568
 324 290 382 616 772 221 117 602  22 853 613 823 734 274 854 474  19 247
 379 673 674  89 577 605 850 670 716 231 565 662 357  15 300 659 111 182
 553 588 604 211 820 113 114 285 120 215 645 444 299 564 426  56 217  13
 102 603 614 248 550 457 338 775 527 251  33 362 723 371 694 513 703 696
 526 740 536 808 476 303 606  29 386 664 567 305 832 691  88 744 596 312
 549   2 856 103 676 644 252 807 266   5 311 340 416 552 720 719 261 811
 554 810 289 679 185 834 421 212 118 323 819 857 258 531 685 293 428 803
 671  91 724 852 835 718 822 640 234 475 515 845  84 678  93 681 399 405
 642 628 653 331  35 124 424 848 319 187  20 566 284 721 263 686 684 631
 186 713 507 626 134 105 368 359 627 325 846 119 571 356 731  71 586 398
 335 317 656 581   4 455 796 672 735 650 298 589  82  26 523 587 579  10
   6 315 256 297 181 600 860 175 725  95 334 314  48 669 358 632  28 505
 537 302 239   3 308 512 501 530 559 189  79 390 360 598 608 255 784   1
 320 756 115 788 328 253 288 482 296 680 592 304 624  80 484 210 106 490
 283 558 607 741 313 732 742 449 307 321 376 361 533 366  40   0  96 178
 452  77 785 630  14 620 121  39  36  27  23 140 487 580 661 168 137 392
 786 463 174 634 560 556  99 648 363 100 702 534 706 538 492 748 202 125
 430 535 337 451 470 555 450 649 790  18 746 309 633 403  34 480 138 167
 169  24 183 208 365 254  78 682 397 519 783 180 495 502 736 401 454 494
 466 400  74 528 745 402 508 364  97 104 176  98 469 191 789 489 355 705
 646 652  21 488 707 485  75 574 101  73 585  30 572 453 647  81 188 177
 578 318 575 201 529 458 583 557 179 743 693 385  76 192 698  72 491 375
  17 108 209 658 749 689 701 683 573 750 190 747  37  38 404 200 456 561
 109 521 563   7 692 326 389 506 493 524 621 496 712 367  12 757 383 107
 562 759 708 336 622  11 327 710 226 173   9 704 329 384 709   8 172 170
 171 395 396 758 498 497 755 391 393 394 826 387 388 827 687]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.2074
INFO voc_eval.py: 171: [133 942 434 ... 566 765 766]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.3074
INFO voc_eval.py: 171: [ 46 268 255 140 144  25  45 149  47  50 141 276 118  72  33 158  68  30
 262 142  59  78  55 186  56 167 116 263 257  73  15  52 160 243 195 201
 129 265 171 264 191  35 236  32 161 250 278  31 239 256  16  77  29 259
 273 145 245 146 117 148 279 277 154   6 152 188  51  84  61 165 211  49
 115  21 194  42 271  83  62 143 181 114 221  60  76 100  85 139 157 200
 193 260 238  88 109 261 280  19  34  10  98 227  66 237 208 214  22 184
 258 150 254 249  65  14 125  53 244 166  28  67  80  57  18 240 133 198
  70  79 113 147 164 162  96 124  37 241 182  24 235 226   4  23 216  27
   1  94  58 153 230 266   8  20 151 111  38  99 130 156  64 248  41 197
 155  75   5   9  71 267  91  17  12 137 217 106  95 136 213 204 274  81
   0 185  36 232 104  93 215 199 246 134 224  26 203 176 247 251 103  69
  89 183 132 163  97 110 138  86   3 178  48 189  11 102 210  13 172 101
 107 120  87 275 108  39 272 121 169 223   7  40  82 122  92  44 173 179
 220 207 175  54 229 231 112  74 170 192  90 168 105 123 196 180 206 174
 177 212 209 233 252 202 127 269 205 187   2  43 218 219 119 159 126 190
 225 270 222  63 228 242 128 135 234 253 131]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.5020
INFO voc_eval.py: 171: [ 8612 14080  1069 ... 10954 10952   783]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.5391
INFO voc_eval.py: 171: [2514  257  710 ... 2714 2703 1829]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.4122
INFO voc_eval.py: 171: [ 66 306 533  34 328 542  67  57 315 128  40 536  50 537 325 335  44 307
 310 612 534 150  46  35 343 477 213 131 323 339 547 538 247 304 153 336
 331 541 548 122 402  37 134 643 276 408 634 298 539 340  71 141 614 360
 418 140  68 316 142 535 158 133 151 414 219 309 218 623 484 359 622  60
 217 489 560 618 261 314 404 311 620 154 329  52 120 624 556 608 478 212
 148 627 460 302 637 136 407 346 245 248 159 616 494 561 341  39 617 523
 211 550 619 224 411 330 160 226 127 645 321 126 488 410  45  90 497 135
 162 511 313 130 189 639  43 443 611 362 308 487 303 629 251  65 281 324
 361 260 129 641 227  62 161 246 305  95 215 492 222 646  91 365 571 342
 417 277 672 636 344 644 301 405 121 630 669 225 320 147 322 503 138 337
 638 332 139 476 403 565 557 632 249 299 415 156 132 223 562 237 504 137
 429 501 124 312 300 319 145 258 615 566 406 168 271 559 609 506 333 520
 605 317 256 635 631 125 334 275 401 626 193 670 633 257 214 358 318 668
 625 680 146 326 546 327 409 338 123 526 551  77  42 606 481 640 216 628
  49 490 295 349 576 194 479 594 543  15 152 259 101 102 564 446 499 495
 262 459 221 482 285 642 462 621  64 155 196   7 348 190 613 502  70 607
 372 438 558 195  28  55  36 187 491  25 461 265 673  38 109 373 345 149
 517 540 524 263 549 573 610 119 200 238 182 374 165 253 220 118 648 250
  94 347 188 351 486 291 350 191  93 235 475 364   0 254 163 176  33 192
  89 514 507 515  24  17  92 682 500 677  72 252 388 412  47 180 293  96
 647 117 354  23 671 379 157 371  53 463 255 437 169 513 264 516 552 239
 434 649 424  87 112 173 553  14 278 652 474   4 519 678 466 366 505 569
  48 428 440 292 106 554 207 431 108   1 178 423 445 242  31 555  61  78
  21  76 568 441 177 205 593 416 164 104  80  83 679 240 290 508  26 457
 674 600 483 458 430 545  16   2 598 181 284 471 202 433  18 367 509  58
 455 270 203 574 100 681 650 518 510 683 274 419 395 267 577 472 603 485
 595 206 586 572 544 399 105   9 243 204 675 563 111 233  19 521  56 599
   8 444 392  69 172 296 676 283 512 527  22 110 596  32 575  41 473 496
 439  20 269 107  74 480  75 570 465 273 498 280 272 179 651  79  51 467
 114  73 113 297 522 391 413 363 266   3 115 201 525 241 435 368 493 420
 436 597 592 279 425 654 387  88 282  54 469  63  59 581 244 426 442  27
  29 175 353 427 380   5 186  97  30 294 268 209 355 422 185  85 369 356
 116 591 398 197 393  13 210 421 352 588 658  86 584 432 579  81 604 657
 377 531 583  98 171  82 400 394 580 578  84 470 582 198   6  99 567  10
 468 229 587 357 529 655 174  11 589 208 660  12 383 667 230 375 585 184
 601 530 381 228 378 199 234 166 231 590 447 390 286 385 167 170 659 236
 232 183 602 451 289 464 389 656 528 666 376 144 382 386 384 664 396 103
 456 663 143 450 287 397 653 288 532 448 452 662 665 454 370 453 449 661]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.3156
INFO voc_eval.py: 171: [ 43   3  47 121 190 280 184  48 126 339  50   6 193 188 236  52 191 158
  13 161 335 194 159 156  60  42 282 152 182 187 341 233 127 279 143 223
 186 157 277 237 231 276 101  80 246 128 100 151 338  49  78 252 281  58
   5 124 104  59 257  10 163 153 329 162 175  23 185 232 147 120  57 308
 129 273 213 247 135  44 311 176 107 183 189 245  75 327  11 239  46 235
 234 166  72 216   4 328  99 160 272 103 132 192 324  96  51  12   8 119
 322 106 254 251 337  89 264 203 269 255  25 130 250 145  14 253 102  45
  97 122 333 261 209  73 256 289 212 154 238 340 125 105 170  83  91  15
 309 332  63 321 225 244  98  90  74 330  61  84 265 200  26 263 248 344
 114 196 326 270  27 342 312 149 117 315  64 221 230 228 295 294 249  79
 134 123   7 331 113  85   9 167 195 118 285 211  82 260 323 314 150 241
 220 271 116 307 259  76 131 305  77  95 325 316  55 155 299 133 336 303
 224 334  39 226 318  62  33 274 146 310 242  65 278 218 266 267  24 262
  30 298 343 201 217  88 171 214 112 290 297  28 177 219 205 240 110 222
  81 317 174 148 144 296 227  87  53  56  31 198  54 141  32   1  22 136
 284   2 306 258 291   0 268 229 197 300 210 202 169 208  86  18 283 109
  29 139 207 115 304 199 108 301 168 293  41 204 243 206 137 111 179 302
 286  35  37 138 164 172  19 178  20  21  17 173 320 275  16 180  71 142
 292 140  36  40 288  69 181 165 287  34 313 319  70  67  38  66  68  92
 215  94  93]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1308
INFO voc_eval.py: 171: [642 637 383 406 115 382 325  80 207 293 121  76 168  78 898 644 640 294
 656 327 324  83 181 117 400 320 808 125  89 212  92 210 295 805 506  81
 208 638 537 391 188 643 454 657 384 453 300  97 206 119 456 404 116 326
 187 197 906 912  84 647 900 306 342 296 176 814 511 837 712 465  86 524
 542 847 390 883 856 909 405 165 135 216 134 842 218 398 910 641 392 527
 545 334 299 871  43  63 489 457 329 387 435 852  98 112 171 733 386 217
  18 750 136 195 330 315 312 682 328 192 205 913 713 884 903 664 838 458
 812 806 105 196 911 509 166 173 261 670 531 169 280 308 840 873 440 182
 220 734 439 444 914 761  45  93 813 510  19 304 824  70 230 401 624 199
 167 885 554 901 541 332 526 757 881 722 832 841 178 894 745 209 185 310
 172 846 164 715 918  33 740 899 553 194  67  44 890 128  82 214  65 463
 244 902 242 459  74 843 101 512 213 792 488 822 449 789 249 455 227 834
 462 711 319 544 547 875  85  58 184 222 809 263 221 874   2 452 396 639
  53 718 375 708 855 721 385 338 905  91  94 179 180 714 264  16 704 259
 500 317 691 298 687 528 402 765  59 580 603 508 893  17 177 438 930 882
 158 198 331 857 872 484 204 318 591 924 467 323 409 133 303 916 109 434
 790  99 760 378 460 356 515 441 742 877 144 779  52 103 486 170 821 680
  72 519 630 395 466 234 915 892 233 376 262 608 236 495  38 827  32  41
 468 908 175 690 350 880 200 929 836 436 277  73   5 251 683 507 360 766
 543 290 445 886 403  95 154 747  35 825  71 870 917 143 854  42 254 433
 359 867 694 610 113 530 273 464 807 688 525 907 122 100 562 241 666 470
 211 301 617 174 111 380 124   7 549 123 186 228 157  57 520 621 322 863
 786 575 819 649 247 250 532 828  75 252 162 478 163 895  96 763 546 191
 689 861 555 923 536 497 731 835 201   8 749 358 446 751 137  46 896 573
 888 831 426 607 215 635 183 839 110 891 736 357 692  55 845  37 266 333
 601 904 833 490 260 256 307  87 238 311 265 743 309 239 853 237 625 469
 219 529 773 161 226 762 565 699 314 818 685 407 860 516 461 879 876 730
 443  56 437 878 202  36  40 335 703 615 897 577 933 274 190 679 341 474
 604 782 442 397   0 931 732 153 102 652  61 851 590 229  88  39 567 606
 337 602 623 106 710 764 633 797 388 193 557 605 521 726 622 719 424 351
  34  13 735  48  60 548 283 399 379 393 802  79 796 753 611 684 830 281
 288 829 432 114 189 340 138 498 889 568 502 349 932 744 681 579 130 576
 362 777 578 627 494 693 518 297 686 561 538 570 844 776 377 269 316 725
 240 235 418 243 778 155   6 282 662 752 648  62 353 798 859 257 815 425
 588 826 793 594 795  54 354 791 862 275 279 799 780 246 737 709   4 302
 716 614 556 609 203 748 363 348  31 706 479 724 581 794 587  66 717 245
 447 758 423 451 286 850 321 480 381 754  30 613 848 636  47 276 108 427
 278 817 107 313 394 389 501 702 582 678 485 369 698 705 887 345 775 612
 270 104 150 560 491  21 284  77  68 756 574 514  90 628 248 551 618 305
  29 919  69 816  20  15 477 368 271 572   9 408 788 585 420 344 849 759
 593 552 253 347 566 475 517 289 823 448 803 371 231  22 926 727 533  14
 800 755 285 287 589 655 583 258 472   3 232 672 559 558 421 571 255 707
 569 783 868 493 450  64 746 728 361 784 367 616 738 787 626 503 820 267
 810 492 741 925 504 739 811 505 534 471 159  51 700 596 415  10 142 414
 550 476 522 268 473 729 701 540 413 343 496 804 584 370 483 430  49 152
 920  12 487   1 499 291 720 131 595 374 599 373  11 673 523 592 429 677
 723 620 801 785 513 781 151 774 539 858 665 481 869 355 658 482 598 535
 292 372 645 631 428 697 586 336 366 364 597 431 422 646 365 156 772 411
 629 417  28 416 132  50 147 160 339 771 140 927 634 864 419 650 564 346
 410 922 563 659 118 223 632 661 928 619  27 600 865 675 139 145  23 767
 768 696  26 769 146 412 866 352 770 674 225 149 127 695 224 667 272 120
 129 653 141  25 126 654 148 921 668 676  24 651 660 669 671 663]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.2736
INFO voc_eval.py: 171: [419 251 592 174 432 296 136 440  24 466  37   1 475 495 255 319 349 472
 477 321  39 394 436 347 253 126 249 417 348 467 124 573 308 297 350 547
 608  57  68 420 254 469 322 533 107  71 129 229  32 437 180 443 593   3
 518  17 326 474 182 429 338 302 315 346  43 318 596 616 581 154 626 225
 356 102 611 366 374 369 604 261 439 306 150 442 218 534 109 110 471 204
 320  50 257 137  19 364 207  73  70 483 434 221  29 300 598 143 463   8
 203 304 160 505 553 380 470 531 376 362 323 231 272 482 103 614 461 357
 359 330 365 393   9 392  40 244 120 631  58 162 186  16 161  25 620 212
  56  46 481 479 599 178  44 128  26 590  84 406 512 299 426 206 312 424
 298 259   6 473 574 286 503 305 343 540 303  41 313  53  72  30 222 500
 478 336 210 371 238 119 101 179 228 566 517 468  48 145 556 327 310 211
 557 208 214 431 476 595 621 425 377 397 213 316 175 172  38 264  90 618
 125 630   2 188 131 117 158 191 610 400 173 382 209 227 549 121   5 115
 576 577  75 307 355 433  93  89 353 363 513 106 184 545 580 405 333 480
  45  11 270 597 546 493 624  91 395 301  18 123 609 453 560 280 550 176
 291 414 619 398 340  23 558 451 612 205 112 543 438 100 528 130 309  12
  87 171 514  55  33 627 283 497 317 198 153 521 399 159 536 538 551 498
 559 224 615 535 234 412 607 275 360 582 358  98 277 532 524 284 486 279
 622 361 594 494 628 623 215 332 111 114 452 606  94 190 499 144 625 569
  95 504 385 237 613 496 148 217 565 408 378 202 502 331 278 258 516  85
 147 314 519  54 617 383 216 311  82  78 368  28  92 281 155 367 219 539
 541 181  63 138 449 273  42 563 116 537 141 455 515 352 384 370 585 583
 418 501 252  61 542 274  77 127 530 104 118  76 446 634 256 375  79 285
 267 450 509 568 427 373  49  36 379 223 166 351 354 435 421  81 381  34
 372 564 444  60 328 241 133 200 324 276  52   4  31 454 629 260 633 587
 108 268  51 157 448 588 337 389 282 522 335 142 149 329 462 428  15 589
  35 287 146 242  80 430  10 105 290 271  59 339  69 572 464 151 289 165
 484 396 548   7 122  64 226 632 113 139 156 544 605 584 410 485 529 555
  22 562 132 422 527 197 140 185 189 235 459 294 526 445 183 423 391 152
 196  27  86 239 404 135 240 554 407 508 134 525 571 293 325  83 409 441
 403 233  88 220 262  96 295 167 168 561 199 447  74 177 250 402  67 520
 292  97 341 230 236 164 387 334 411 342 263 552 170  66 523 579 388 390
 163 510 344 586 567 401 187 570 386 413 506 269 232  62  21 243  99  65
 458   0 507 511 288 169 345 578 603 488 602  20 457 247 600 456 246 601
 266 491  47 460 415 193 487 265 201 591 195 416 245 492 248 489 575  13
 194 192 465 490  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.3267
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.3017
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.221
INFO cross_voc_dataset_evaluator.py: 134: 0.396
INFO cross_voc_dataset_evaluator.py: 134: 0.195
INFO cross_voc_dataset_evaluator.py: 134: 0.239
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.425
INFO cross_voc_dataset_evaluator.py: 134: 0.276
INFO cross_voc_dataset_evaluator.py: 134: 0.087
INFO cross_voc_dataset_evaluator.py: 134: 0.306
INFO cross_voc_dataset_evaluator.py: 134: 0.208
INFO cross_voc_dataset_evaluator.py: 134: 0.328
INFO cross_voc_dataset_evaluator.py: 134: 0.207
INFO cross_voc_dataset_evaluator.py: 134: 0.307
INFO cross_voc_dataset_evaluator.py: 134: 0.502
INFO cross_voc_dataset_evaluator.py: 134: 0.539
INFO cross_voc_dataset_evaluator.py: 134: 0.412
INFO cross_voc_dataset_evaluator.py: 134: 0.316
INFO cross_voc_dataset_evaluator.py: 134: 0.131
INFO cross_voc_dataset_evaluator.py: 134: 0.274
INFO cross_voc_dataset_evaluator.py: 134: 0.327
INFO cross_voc_dataset_evaluator.py: 135: 0.302
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
